"""
ç®€åŒ–çš„å¬å›ç³»ç»ŸDemo - å•ä¸ªæµ‹è¯•ç”¨ä¾‹

ä½¿ç”¨æ–¹æ³•:
  python scripts/simple_recall_demo.py "ä½ çš„Ideaæè¿°"

ç¤ºä¾‹:
  python scripts/simple_recall_demo.py "ä½¿ç”¨Transformerè¿›è¡Œæ–‡æœ¬åˆ†ç±»"
"""

import json
import pickle
import sys
from collections import defaultdict
from pathlib import Path

# ===================== è·¯å¾„é…ç½® =====================
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
OUTPUT_DIR = PROJECT_ROOT / "output"

NODES_IDEA = OUTPUT_DIR / "nodes_idea.json"
NODES_PATTERN = OUTPUT_DIR / "nodes_pattern.json"
NODES_DOMAIN = OUTPUT_DIR / "nodes_domain.json"
NODES_PAPER = OUTPUT_DIR / "nodes_paper.json"
GRAPH_FILE = OUTPUT_DIR / "knowledge_graph_v2.gpickle"

# ===================== é…ç½®å‚æ•° =====================
TOP_K_IDEAS = 10
TOP_K_DOMAINS = 5
TOP_K_PAPERS = 20
FINAL_TOP_K = 10

PATH1_WEIGHT = 0.4
PATH2_WEIGHT = 0.2
PATH3_WEIGHT = 0.4


# ===================== å·¥å…·å‡½æ•° =====================
def compute_similarity(text1, text2):
    """åŸºäºå­—ç¬¦çš„ Jaccard ç›¸ä¼¼åº¦ (é€‚é…ä¸­æ–‡)"""
    if not text1 or not text2:
        return 0.0

    # è½¬æ¢ä¸ºå­—ç¬¦é›†åˆ
    tokens1 = set(text1.lower().replace(" ", ""))
    tokens2 = set(text2.lower().replace(" ", ""))

    if not tokens1 or not tokens2:
        return 0.0

    intersection = len(tokens1 & tokens2)
    union = len(tokens1 | tokens2)
    return intersection / union


# ===================== ä¸»å‡½æ•° =====================
def main():
    # è·å–ç”¨æˆ·è¾“å…¥
    if len(sys.argv) > 1:
        user_idea = " ".join(sys.argv[1:])
    else:
        user_idea = "ä½¿ç”¨è’¸é¦æŠ€æœ¯å®ŒæˆTransformerè·¨é¢†åŸŸæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œå¹¶åœ¨å¤šä¸ªæ•°æ®é›†ä¸ŠéªŒè¯æ•ˆæœ"

    print("=" * 80)
    print("ğŸ¯ ä¸‰è·¯å¬å›ç³»ç»Ÿ Demo")
    print("=" * 80)
    print(f"\nã€ç”¨æˆ·Ideaã€‘\n{user_idea}\n")

    # åŠ è½½æ•°æ®
    print("ğŸ“‚ åŠ è½½æ•°æ®...")
    with open(NODES_IDEA, 'r', encoding='utf-8') as f:
        ideas = json.load(f)
    with open(NODES_PATTERN, 'r', encoding='utf-8') as f:
        patterns = json.load(f)
    with open(NODES_DOMAIN, 'r', encoding='utf-8') as f:
        domains = json.load(f)
    with open(NODES_PAPER, 'r', encoding='utf-8') as f:
        papers = json.load(f)
    with open(GRAPH_FILE, 'rb') as f:
        G = pickle.load(f)

    # æ„å»ºç´¢å¼•
    idea_map = {i['idea_id']: i for i in ideas}
    pattern_map = {p['pattern_id']: p for p in patterns}
    domain_map = {d['domain_id']: d for d in domains}
    paper_map = {p['paper_id']: p for p in papers}

    print(f"  âœ“ Idea: {len(ideas)}, Pattern: {len(patterns)}, Domain: {len(domains)}, Paper: {len(papers)}")
    print(f"  âœ“ å›¾è°±: {G.number_of_nodes()} èŠ‚ç‚¹, {G.number_of_edges()} è¾¹\n")

    # ===================== è·¯å¾„1: ç›¸ä¼¼Ideaå¬å› =====================
    print("ğŸ” [è·¯å¾„1] ç›¸ä¼¼Ideaå¬å›...")

    similarities = []
    for idea in ideas:
        sim = compute_similarity(user_idea, idea['description'])
        if sim > 0:
            similarities.append((idea['idea_id'], sim))

    similarities.sort(key=lambda x: x[1], reverse=True)
    top_ideas = similarities[:TOP_K_IDEAS]

    print(f"  æ‰¾åˆ° {len(similarities)} ä¸ªç›¸ä¼¼Ideaï¼Œé€‰æ‹© Top-{TOP_K_IDEAS}")

    path1_scores = defaultdict(float)
    for idea_id, similarity in top_ideas:
        idea = idea_map[idea_id]
        # æ‰“å°åŒ¹é…åˆ°çš„ç›¸ä¼¼ Idea è¾…åŠ©è°ƒè¯•
        if similarity > 0.2:
            print(f"    - åŒ¹é… Idea: {idea['description'][:40]}... (sim={similarity:.3f})")

        # è·¯å¾„ 1 ç›´æ¥ä» Idea èŠ‚ç‚¹çš„ pattern_ids å¬å›
        pattern_ids = idea.get('pattern_ids', [])
        for pid in pattern_ids:
            path1_scores[pid] += similarity

    print(f"  âœ“ å¬å› {len(path1_scores)} ä¸ªPattern\n")

    # ===================== è·¯å¾„2: é¢†åŸŸç›¸å…³å¬å› =====================
    print("ğŸŒ [è·¯å¾„2] é¢†åŸŸç›¸å…³æ€§å¬å›...")

    # é€šè¿‡æœ€ç›¸ä¼¼Ideaçš„Domain
    top_idea = idea_map[top_ideas[0][0]] if top_ideas else None
    domain_scores = []

    if top_idea and G.has_node(top_idea['idea_id']):
        for successor in G.successors(top_idea['idea_id']):
            edge_data = G[top_idea['idea_id']][successor]
            if edge_data.get('relation') == 'belongs_to':
                domain_id = successor
                weight = edge_data.get('weight', 0.5)
                domain_scores.append((domain_id, weight))

    domain_scores.sort(key=lambda x: x[1], reverse=True)
    top_domains = domain_scores[:TOP_K_DOMAINS]

    print(f"  æ‰¾åˆ° {len(domain_scores)} ä¸ªç›¸å…³Domainï¼Œé€‰æ‹© Top-{TOP_K_DOMAINS}")

    path2_scores = defaultdict(float)
    for domain_id, domain_weight in top_domains:
        for predecessor in G.predecessors(domain_id):
            edge_data = G[predecessor][domain_id]
            if edge_data.get('relation') == 'works_well_in':
                pattern_id = predecessor
                effectiveness = edge_data.get('effectiveness', 0.0)
                confidence = edge_data.get('confidence', 0.0)
                path2_scores[pattern_id] += domain_weight * max(effectiveness, 0.1) * confidence

    print(f"  âœ“ å¬å› {len(path2_scores)} ä¸ªPattern\n")

    # ===================== è·¯å¾„3: ç›¸ä¼¼Paperå¬å› =====================
    print("ğŸ“„ [è·¯å¾„3] ç›¸ä¼¼Paperå¬å›...")

    similarities = []
    for paper in papers:
        # å°è¯•å¤šä¸ªå¯èƒ½çš„ Idea æè¿°å­—æ®µ
        paper_idea = paper.get('idea', {}).get('core_idea', '') or paper.get('abstract', '')[:100]
        if not paper_idea:
            continue

        sim = compute_similarity(user_idea, paper_idea)
        if sim > 0.1 and G.has_node(paper['paper_id']):
            reviews = paper.get('reviews', [])
            if reviews:
                import numpy as np
                scores = [r.get('rating', 5) for r in reviews]
                avg_score = np.mean(scores)
                quality = (avg_score - 1) / 9
            else:
                quality = 0.5

            combined = sim * quality
            similarities.append((paper['paper_id'], sim, quality, combined))

    similarities.sort(key=lambda x: x[3], reverse=True)
    top_papers = similarities[:TOP_K_PAPERS]

    print(f"  æ‰¾åˆ° {len(similarities)} ä¸ªç›¸ä¼¼Paperï¼Œé€‰æ‹© Top-{TOP_K_PAPERS}")

    path3_scores = defaultdict(float)
    for paper_id, similarity, quality, combined_weight in top_papers:
        if not G.has_node(paper_id):
            continue
        for successor in G.successors(paper_id):
            edge_data = G[paper_id][successor]
            if edge_data.get('relation') == 'uses_pattern':
                pattern_id = successor
                pattern_quality = edge_data.get('quality', 0.5)
                path3_scores[pattern_id] += combined_weight * pattern_quality

    print(f"  âœ“ å¬å› {len(path3_scores)} ä¸ªPattern\n")

    # ===================== èåˆç»“æœ =====================
    print("ğŸ”— èåˆä¸‰è·¯å¬å›ç»“æœ...\n")

    all_patterns = set(path1_scores.keys()) | set(path2_scores.keys()) | set(path3_scores.keys())

    final_scores = {}
    for pattern_id in all_patterns:
        score1 = path1_scores.get(pattern_id, 0.0) * PATH1_WEIGHT
        score2 = path2_scores.get(pattern_id, 0.0) * PATH2_WEIGHT
        score3 = path3_scores.get(pattern_id, 0.0) * PATH3_WEIGHT
        final_scores[pattern_id] = score1 + score2 + score3

    ranked = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
    top_k = ranked[:FINAL_TOP_K]

    # ===================== è¾“å‡ºç»“æœ =====================
    print("=" * 80)
    print(f"ğŸ“Š å¬å›ç»“æœ Top-{FINAL_TOP_K}")
    print("=" * 80)

    for rank, (pattern_id, final_score) in enumerate(top_k, 1):
        pattern_info = pattern_map.get(pattern_id, {})

        score1 = path1_scores.get(pattern_id, 0.0) * PATH1_WEIGHT
        score2 = path2_scores.get(pattern_id, 0.0) * PATH2_WEIGHT
        score3 = path3_scores.get(pattern_id, 0.0) * PATH3_WEIGHT

        print(f"\nã€Rank {rank}ã€‘ {pattern_id}")
        print(f"  åç§°: {pattern_info.get('name', 'N/A')}")
        print(f"  æœ€ç»ˆå¾—åˆ†: {final_score:.4f}")

        if final_score > 0:
            print(f"  - è·¯å¾„1 (ç›¸ä¼¼Idea):   {score1:.4f} (å æ¯” {score1/final_score*100:.1f}%)")
            print(f"  - è·¯å¾„2 (é¢†åŸŸç›¸å…³):   {score2:.4f} (å æ¯” {score2/final_score*100:.1f}%)")
            print(f"  - è·¯å¾„3 (ç›¸ä¼¼Paper):  {score3:.4f} (å æ¯” {score3/final_score*100:.1f}%)")

        print(f"  èšç±»å¤§å°: {pattern_info.get('cluster_size', 0)} ç¯‡è®ºæ–‡")

        summary = pattern_info.get('summary', 'N/A')
        print(f"  æ‘˜è¦: {summary[:120]}...")

    print("\n" + "=" * 80)
    print("âœ… å¬å›å®Œæˆ!")
    print("=" * 80)


if __name__ == '__main__':
    try:
        main()
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

