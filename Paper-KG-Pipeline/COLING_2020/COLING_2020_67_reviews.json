[
  {
    "review_id": "5a3d20262ffbd2b2",
    "paper_id": "COLING_2020_67",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "in this work. What would be human annotation guidelines for such data set creation? How would one assess agreement if one were to create such data with human experts?\nTherefore, it appears that the actual task taken on by this paper is that of learning the latent decisions behind the pull-quote identification of *these particular* publications.\nHaving said that, the approach and analysis undertaken by the paper is very insightful. While the task can be construed as learning to extract pull-quotes in a manner similar to that of these selected publications, the methodical approach taken in the paper is commendable. It was enjoyable to see the paper build from hand-crafted features used in a traditional ML classifier to more recent deep learning models with character and word-based features, to cross-task of approaches used in similar tasks (headline, clickbait, summarization).\nThe observations and conclusions from the experiments are perceptive, and readers of the paper would certainly learn about interesting linguistic characteristics that are useful in identifying noteworthy sentences in any given text.\nIt was great to see the human evaluation in Section 5.5 of the paper. This really helped to see the impact of the pull-quotes on human readers. It would have been neat to see such an analysis of the data as part of the task definition early on... to perhaps help more clearly define what a human reader (or a human writer) is expecting to highlight as quotable. ( a.k.a., crowd-sourcing pull-quote extraction?)",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  }
]