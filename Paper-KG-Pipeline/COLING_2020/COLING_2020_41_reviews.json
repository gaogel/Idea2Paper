[
  {
    "review_id": "6df2c662d2f9c88b",
    "paper_id": "COLING_2020_41",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper contributes a new task, dataset, and experimental results on Chinese lexical fusion resolution.\nSection 2 presents the phenomenon: lexical fusion is a phenomenon in Chinese where a combination of words (called the \"separation words\", typically a VP, it seems) is referred to anaphorically by a \"fusion word\". Fusion words are formed ad-hoc and are therefore often out-of-vocabulary; they consist of multiple (typically 2) characters, each of which corresponds to one of the separation words. Often, the characters of the fusion word are characters from the corresponding separation words, but they can also be characters that are in some purely semantic relationship with the corresponding separation word.\nSection 3 presents a machine learning model to resolve this type of anaphora. It consists of a character-wise BERT encoder, a CRF decocer for mention detection, i.e., labeling spans of characters as either separation or fusion words, and a biaffine model for anaphora resolution, i.e., to recognize which fusion word refers to which separation words. To capture semantic relationships between fusion word characters and separation words, each character is assigned an additional semantic representation that is fed into the model. This semantic representation is obtained by looking up every word that the character could be part of in this context in the HowNet lexical database, further mapping each of these words to its HowNet representation as a \"sememe graph\", and then aggregating a vector representation via sememe embeddings and graph attention networks.\nSection 4 presents a test set created by manually annotating 17,000 paragraphs from the SogouCA news corpus. It also presents an artificial training set constructed by generating possible triples of fusion word and separation words using an electronic lexicon, and using those triples as seeds for finding presumed occurrences in a large corpus. To assess the impact of sememe knowledge, the full model is compared with various degraded variants and combinations thereof: one where the expansion to characters into words is skipped and only characters (1-character words) are looked up in HowNet, one where the structure of of HowNet's sememe graphs is ignored and complete graphs between the included sememes are used instead, and one where sememe knowledge is left out entirely. It is found that the model has the highest accuracy when using the full model, confirming the usefulness of fine-grained sememe knowledge for this task. An analysis shows that in-vocabulary fusion words are resolved with greater accuracy than out-of-vocabulary ones, and that fusion word characters that are borrowed from their respective separation word are correctly mapped with much greater accuracy than ones where the relationship is purely semantic, despite using sememe information. An example of how senses with shared sememes across triples can mutually enhance their attention weights is shown. It is also found that anaphors are more difficult to resolve than cataphors, that resolution becomes more difficult as the distance between the two separation words increases, and that the model performs better when mention detection and anaphora resolution is done jointly rather than in a pipeline.\nI think it's a good and important paper, as it tackles an apparently understudied phenomenon in the Chinese language, resolution of which is important for natural language understanding. The technical choices seem well motivated and are presented fairly clearly. The promised release of the datasets will be a valuable contribution. And the analysis is fairly extensive. However, there are also several ways in which the paper could be improved: - Surely Chinese lexical fusion has been described in the linguistic literature? A reference should definitely be given. Also, the claim \"Lexical fusion is used frequently in the Chinese language\" should be backed up either by a reference or by data, e.g., from the annotation project presented here.\n- Lexical fusion should be more clearly defined. From the examples given in Section 1-2, I get the impression that the antecedent is always a VP consisting of a single-word verb and an adjacent single-word nominal object. But apparently the separation words need not be exactly two and need not be adjacent? Examples of this should be given as well, and it should be explained what possible syntactic types the antecedent can have.\n- Relatedly, the annotation guidelines should be explained. Specifically, what counts as lexical fusion for the purposes of this dataset? Are there edge cases that were excluded?\n- Relatedly, the annotation process should be explained in more detail. How many annotators checked each paragraph? What was done with paragraphs with disagreement?\n- The test set does not seem to contain any paragraphs without lexical fusion, so it is presumably rather skewed. In a more realistic setting, paragraphs without lexical fusion would not be removed from the test set.\n- I would like to see more detail on the construction of the artifical training dataset. How do you \"check if [a two-character word] can be split into two separation words\"?\n- Terminology: in my mind, the reference is from the anaphor to the antecedent, so shouldn't cataphors be called forward references and anaphors backward references, rather than the other way around?\n- Terminology: \"separation words\" sounds a bit odd, it seems to imply that these words are the result of a separation process. How about \"antecedent words\"?\nSmall stuff: - The bibliography entry for Mitkov (1999) is broken.\n- p. 4 overall -> over all - Typography: please ensure that opening parentheses (e.g., in citations) are preceded by spaces.",
    "overall_score": "4",
    "confidence": "4"
  }
]