{
  "paper_id": "COLING_2020_49",
  "title": "Don’t Patronize Me! An Annotated Dataset with Patronizing and Condescending Language towards Vulnerable Communities",
  "conference": "COLING",
  "domain": {
    "research_object": "针对弱势群体的居高临下和施恩式语言的自动识别与数据标注。",
    "core_technique": "构建并标注包含施恩和居高临下语言的数据集，结合自然语言处理方法进行分析。",
    "application": "用于社交媒体、新闻评论等文本中识别和过滤不当语言，保护弱势群体权益。",
    "domains": [
      "自然语言处理",
      "社会计算"
    ]
  },
  "ideal": {
    "core_idea": "构建并标注媒体中针对弱势群体的居高临下语言数据集",
    "tech_stack": [
      "数据集构建",
      "文本标注",
      "语言分析"
    ],
    "input_type": "媒体文本数据",
    "output_type": "带有居高临下语言标注的数据集"
  },
  "skeleton": {
    "problem_framing": "论文通过定义PCL（居高临下和施恩式语言）及其对弱势群体的潜在危害，引出研究主题。作者强调媒体中PCL的普遍性及其对社会排斥和不平等的影响，从而凸显该问题的现实意义和紧迫性。",
    "gap_pattern": "作者指出，尽管PCL常常出于善意，但其潜在的歧视性和隐蔽性却被忽视。通过引用前人研究，强调媒体中对弱势群体的不公正待遇尚未被充分建模和研究，从而明确现有研究的不足。",
    "method_story": "方法部分采用基线法，系统介绍了用于PCL建模的不同机器学习方法。作者详细说明了输入特征（如段落嵌入、词袋模型）和具体实现细节，为后续实验提供了清晰的技术路径。",
    "experiments_story": "实验部分将任务分为二分类和多标签分类两种设置，分别对应PCL的有无及其具体类别。通过对比多种方法，旨在为后续研究提供基准，实验设计结构清晰、目的明确。"
  },
  "tricks": [
    {
      "name": "定义核心概念",
      "type": "writing-level",
      "purpose": "明确研究对象和范围",
      "location": "论文开头对PCL进行定义",
      "description": "在论文开头对Patronizing and Condescending Language (PCL)进行详细定义，阐明其在媒体中的表现及潜在影响，为后续研究奠定概念基础。"
    },
    {
      "name": "文献对比与差异突出",
      "type": "writing-level",
      "purpose": "突出研究创新点",
      "location": "介绍相关领域已有工作与本研究的区别",
      "description": "通过回顾offensive language、hate speech等相关NLP领域的研究，突出PCL的独特性和先前未被关注的空白，强调本研究的创新性。"
    },
    {
      "name": "任务分解与设置",
      "type": "method-level",
      "purpose": "清晰划分研究任务",
      "location": "方法部分对任务进行分类",
      "description": "将PCL的建模任务分为二分类（是否存在PCL）和多标签分类（PCL类别），便于针对不同目标设计和评估模型。"
    },
    {
      "name": "多方法对比实验",
      "type": "experiment-level",
      "purpose": "提供多种基线，增强实验说服力",
      "location": "方法与实验部分",
      "description": "采用SVM-WV、SVM-BoW、BiLSTM和BERT等多种模型方法进行实验，为后续研究提供丰富的基线和对比。"
    },
    {
      "name": "详细超参数说明",
      "type": "method-level",
      "purpose": "保证实验可复现性",
      "location": "每种模型方法后",
      "description": "对每种模型的关键超参数（如SVM的C、gamma、kernel，BiLSTM的units、dropout、epochs等）进行详细说明，方便他人复现和比较。"
    },
    {
      "name": "利用预训练词向量",
      "type": "method-level",
      "purpose": "提升模型表现和泛化能力",
      "location": "SVM-WV、BiLSTM方法说明中",
      "description": "采用Google News语料训练的Word2Vec词向量作为特征输入，增强模型对文本语义的理解。"
    },
    {
      "name": "早停机制",
      "type": "experiment-level",
      "purpose": "防止模型过拟合",
      "location": "BiLSTM方法描述中",
      "description": "在训练BiLSTM模型时采用early stopping机制，并设置patience参数，确保模型在最佳时刻停止训练，避免过拟合。"
    },
    {
      "name": "多任务设置",
      "type": "method-level",
      "purpose": "增强模型应用广度",
      "location": "任务分解部分",
      "description": "同时设置二分类和多标签分类任务，满足不同实际应用需求，展示方法的灵活性。"
    },
    {
      "name": "引用权威文献支持论述",
      "type": "writing-level",
      "purpose": "增强论述可信度",
      "location": "引言和相关工作部分",
      "description": "通过引用Ng (2007)、Zampieri et al. (2019)、Basile et al. (2019)、Mikolov et al. (2013)、Devlin et al. (2018)等权威文献，为观点和方法提供理论和技术支持。"
    }
  ]
}