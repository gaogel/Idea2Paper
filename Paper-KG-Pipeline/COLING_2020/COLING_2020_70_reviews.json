[
  {
    "review_id": "bdfe32c46c23a450",
    "paper_id": "COLING_2020_70",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The authors provide a model (Lin) that uses syntax (dependency based) and semantic (VerbNet based) features to determine tasks. They focus not just on whether a sentence contains a task but also which verb encodes the task. Their approach is unsupervised and they compare across two datasets which they prepared that are built on existing NLP datasets (email and chat).\nWill the authors make the datasets available for research purposes? These would be particularly valuable since they are based on top of existing NLP datasets.\nSyntactic features in section 2.1: Are there not enough imperatives (\"Send the minutes out next week\") to make them a useful class? I expect that most of these will be captured by the tense rule that identifies VB POS, but if the authors have a way of directly identifying imperatives, that could be very helpful. As a minor point, for 5 verb association, it would be better to focus on an example where the COMP was a valid task to contrast with dependents which are not valid tasks (or ideally include an example of both; there should be room to include this by judicious trimming).\nSection 3 is really about the dataset, not the experimental setup. It would be helpful to know in this section whether all of the annotated data was part of the test/eval set or whether some of it was used for training. ( Later it is mentioned that the authors use 5-fold cross validation, which is fine, but if this section is about experimental setup, it should describe that.) To save some space, section 3 could be merged into section 4.\nIn the results discussion, the authors state that Lin only performs slightly better than BERT. If the difference is statistically significant, then a couple of percentage points is a reasonable improvement, although not as significant as one might expect given the features that Lin uses. If there is room, it would be great to have more discussion of (1) why full Lin is so much better than the syntax and semantics separately (even if these are just hypotheses for future work) and (2) why BERT comes so close to Lin (building on the one sentence the authors provide about this). ( 1) appears some in the qualitative analysis below, but the density of the discussion makes it hard to understand without rereading several times.\nIn the qualitative analysis, it was took a couple of readings to understand what the authors meant by \"low recall\" since at first glance the description is about a low precision issue (marking too many things as tasks when they are not). It would help if the authors did *not* include the bad precision example of \"You should thank me for this\" and instead focused on the later example \"I think we can send the details tomorrow\". Given how frequent the phrase \"I think\" is in emails and chats (it is a form of politeness to soften an order or strong opinion), making mistakes on this could have a significant impact on performance for many datasets.",
    "overall_score": "4",
    "confidence": "4"
  }
]