[
  {
    "review_id": "40d85283ba19fcb7",
    "paper_id": "COLING_2020_58",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "of the study are more significant and I'd recommend accepting this paper at the conference. However, I can do that only with limited confidence, since while I found the technical part very clear and easy to follow, I'm not familiar with the application area here, which seems to contain quite a few previous articles judging by the provided bibliography.",
    "weaknesses": "",
    "comments": "",
    "overall_score": "4",
    "confidence": "2"
  },
  {
    "review_id": "4d5cd3e9027f2516",
    "paper_id": "COLING_2020_58",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper focusses on the analysis and identification of press release documents that deviate from the original observational study that they refer to, in the sense of exaggerating results. More specifically they focus on health topics and propose a pipeline for identifying press releases that propose a causal relation while the corresponding publication proposes a correlational relation.\nThe task is interesting, and the corpus would be a useful addition to the community, allowing for the development of further systems addressing misinformation. I have some concerns on the strategy and annotation process: More specifically: -- I am wondering if there was some manual estimation while annotating sentences of whether, in the case of matching/contrasting classes between the press release and the scientific paper, were actually referring to the same thing. I understand that this was performed on the article level, but was there some sentence-level evaluation as well? \n-- It would be useful to present the process of deciding whether a scientific paper is correlational or causal, in the case where the conclusion contains multiple sentences, classified under different taxonomical classes, e.g., direct causal + correlational. Are only papers containing solely correlational and non-claim sentences considered correlational? You seem to be hinting at the latter later on in the paper, but it should be better clarified at the part where the classification strategies are described (section 5.1).\nThe paper is well written and the authors explain in detail the procedure for selecting the final documents in the corpus. Some minor re-organisation would benefit the paper since some information is mentioned in some section and revisited-expanded in some later section leaving open questions in between and obstructing the reading (see comments below).\nI am missing the details of fine-tuning the BERT models on the task: What parameters were used? Likewise, for the use of augmented data, the pre-processing details should be explained either in the main paper or the appendix. These are important for reproducibility. Furthermore, I am wondering if the authors considered other models apart from BERT that have shown to outperform it such as XLNet, ELECTRA, etc. These additions would further aid in the interpretation of results and appreciation of the task.\nFinally, for the interpretation of results, it would be good to know how fig.3a was calculated. I assume that the rate was calculated as #of exaggerated press releases over the total number of press releases referring to papers with correlational conclusions? Or was it over the total number of press releases? In either case, it would be useful to visualise the evolution of the rate of press releases referring to correlational versus direct causal conclusions, in order to be able to better interpret the results. As it is currently presented I find it hard to properly interpret the presented results and claims in the discussion.\nOther comments: Section 1- Introduction: -- \"Furthermore, 14,426 observational studies contained structured abstracts, in which the sentences in the conclusion subsection were used as main statements in research papers.\" -- > While it is made clear later on that these constitute the final set of papers used, it is not properly explained here. It is better to clarify early on. \n-- Provide either a link or a citation for EurekAlert Section 2 - Related Work: -- \"missing of information\" --> missing information -- it is better to use consistently either the numerical or the word form of numbers (e.g., you mention '16 questions', and then '10 criteria' in the next sentence. \n-- 'Causal relation can be broadly defined ... ' --> 'Causal relations ...? or 'A causal relation... -- 'Causal relation can be broadly defined as any type of cause-effect relations, such as in the NLP task' --> it would be better to provide an example here.\nSection 3 Corpus Construction: -- Perhaps precision would be a more suitable metric compared to accuracy for the LightGBM classifier, as I believe the important task is to avoid a high number of false negatives (regardless,  accuracy does seem sufficiently high). \n-- 'conclusion subsections' refers to both conclusions from the abstract and conclusions from the main body? It is a bit unclear here. I understand based on the conclusions section of your own paper that you probably take only the abstract conclusions, but it would be better to state this clearly and earlier on. \n-- In our taxonomy, “can + causal cue” belongs to the category of “direct causal”. -- > I am a bit uncertain of what is implied in this sentence. Is this rule in the taxonomy wrong based on the authors' error analysis, or is it the case that there are more examples that satisfy the rule rather than those that contradict it? In either case, is there a way to revise the markers in the taxonomy in ways that better capture the task at hand? \n-- The error analysis examples are very interesting! I am wondering if it would be possible to group them and show what percentage of errors do they represent in the results? \n-- Figure 2: Claim aggregation algorithm for press releases --> for clarity I would change to: \"Claim aggregation algorithm for press releases that correspond to papers identified as correlational\"",
    "overall_score": "3",
    "confidence": "4"
  }
]