[
  {
    "review_id": "bf5b2ba60cb66749",
    "paper_id": "COLING_2020_28",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper is about characters in narrative texts, and it claims to contribute a) an operational definition of characters that is „narratologically grounded“, b) an annotated corpus (which will be released) and c) classification experiments on the automatic distinction between characters and non-characters.\nThis paper is well written and good to read. The topic is interesting and clearly relevant. I have some concerns, however: 1. The definition of a ‚character‘ is based on the concept ‚plot‘. While this is naturally following from the narratological literature, it begs the question what a plot is. And of course, this also presumes that there is ‚the plot‘ — what if there are more than one, or if it is highly subjective? Another term that is used for defining a ‚character’ is animacy. In factual texts, there is a pretty clear distinction between animate and inanimate beings, but in fictional texts, this boundary might become blurry quickly, because it is entirely conceivable that objects have properties that are usually reserved for animate beings. Thus, this term would need to be defined more concretely. The definition thus rests on other, not defined terms. \n2. The annotation experiments yields high agreement, so maybe this is not so relevant in practice. But the agreement has been measured on only one of the three sub corpora, and presumably on the easiest one: Fairy tales, which have a pretty clear plot. It would be much more convincing if the annotation comparison would have been done on a portion from each corpus, and I do not see a reason why this was not done. \n3. The annotation procedure description contains the sentence „First, we read the story and ﬁnd the events important to the plot.“ I am not sure what this means exactly — was there an agreement across the annotators what the events important to the plot are, before the annotation? This of course would make the annotation task much easier. \n4. One of the corpora the authors use consists of broadcast news transcripts from OntoNotes. I would need a lot more arguing about this in the paper, in order to believe the authors that news broadcast is a narrative. While it clearly has narrative elements, they have very different goals and textual properties. Firstly, the ‚plot‘ (understood as a sequence of events in the real world) is only partially represented in a news text, while you have a full plot in many narrative texts. \n5. From the third corpus, the authors annotated only one chapter from each novel. This also seems questionable to me, in particular because length of a coreference chain later is such an important feature. In a full novel, the picture might be very different than in a single chapter. Concretely: The evaluation of an event being relevant to a plot could be very different if the full plot is known. \n6. What I feel is missing from the paper is a quantitative data analysis independent of the classification experiments. What is the distribution of character- and non-character-chains? How long are they in comparison? This would make it much easier to interpret and evaluate the results properly. \n7. The length of a coreference chain has been used as „an integer feature“ (4.2.1). Should this not be normalized in some way, given the very different text lengths? \n8. Why is there no baseline for the OntoNotes and CEN corpora?\nTo sum up: While I think this is an interesting task, and the paper is very well written, it makes several assumptions that do not hold general and has a somewhat weak theoretical basis. The classification experiments are pretty straightforward (as the title suggests), and — given the assumptions and restrictions introduced earlier — deliver not very surprising results.",
    "overall_score": "3",
    "confidence": "5"
  }
]