[
  {
    "review_id": "36d61631251d50e0",
    "paper_id": "COLING_2020_64",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- interesting and comprehensive comparison of the new dataset with existing ones - dataset is much larger than previous ones, covering many topics",
    "weaknesses": "Clarity - Introduction is unclear. What \"unsupervised datasets\" are is not explained? the four requirements are presented but not clearly justified - When desiderata are described in detail they refer to 'unsupervised data-to-text models,' that have not been discussed in any ways. They are only briefly described too late in the paper,  at the end in the evaluation section. They should be presented upfront so that the desiderata would be justified more clearly - the first requirement then is very confusing \"Text and graphs should have similar contents, such as a Wikipedia article and a knowledge graph of the same article.\" Isn't this what you need for supervised graph2text?\n- the second one \"To fit for recent unsupervised models...\"  this is an example of why recent unsupervised models should have been explained upfront to justify this requirement.\n- Specific questions on  Sec4  \"and use the titles of these articles to query their corresponding knowledge graphs from DBPedia\" so it is only a matter of retrieving an already existing graph?\n- \"relations that are unlikely to be described in text such as “latitude” and “longitude.”\" how many relations are of this type? what is the accuracy of this heuristic?\n- Step 4 sounds very heuristics. why 10? Was it tested in any ways? ( see more",
    "comments": "",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "a27390e7c7583af1",
    "paper_id": "COLING_2020_64",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "In this paper, the authors present a new dataset for unsupervised data-to-text generation compiled from existing manually and automatically annotated datasets. The paper is clearly written, and the description of the dataset is extensive and clearly detailed. The dataset is likely to be of great value for the community.\nThe authors also evaluate their dataset by testing some off-the-shelf generators and show that some systems achieve high scores in terms of automatic evaluations. It would be a great addition to provide a human assessment of the outputs in order to fully demonstrate the usefulness (and limitations) of the presented data.\n- Intro: Even though it looks obvious, explain clearly why the need to have non-parallel data.\n- 4.1 Step1: Do you have any insights with respect to the quality of the tokenization, i.e. the type of errors observed in the processed data?\n- 4.1 Step2: Is it possible to provide a full list of the relations that are kept as supplementary material?\n- 4.1 Step3: See the paper of Castro Ferreira et al, who performed a similar task manually; https://www.aclweb.org/anthology/W18-6521.pdf - 4.1 Step4: Is there a mechanism to ensure that the text does not express more than the input triples? What are the specific challenges here?\n- 4.3: According to the WebNLG 2020 website, there were 15 categories  in 2017:  “The 10 seen categories used in 2017: Airport, Astronaut, Building, City, ComicsCharacter, Food, Monument, SportsTeam, University, and WrittenWork. The 5 unseen categories of 2017, which will now be part of the seen data: Athlete, Artist, CelestialBody, MeanOfTransportation, Politician.” \nAlso, the number of reported data-text pairs is over 25,000 (https://www.aclweb.org/anthology/W17-3518.pdf) Typos/style: - DBPedia -> DBpedia - Conclusions: “which implies that even WebNLG and GenWiki” -> even though?",
    "overall_score": "4",
    "confidence": "4"
  }
]