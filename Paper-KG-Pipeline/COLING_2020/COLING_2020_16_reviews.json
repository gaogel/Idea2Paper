[
  {
    "review_id": "704d293920a1b101",
    "paper_id": "COLING_2020_16",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper investigates different pre-trained language models for 'extreme' multilabel classification. The authors also compare against a baseline logistic regression model. The experiments are carried out on the American National Election Study (ANES) dataset, with the task of automatically coding (i.e. classification) answers to open-ended survey responses. The classification task is interesting and challenging, and I think a good example of important computational social science work. Overall the paper was well written and I appreciate the thoughtfulness that has gone in the data processing.\nMy main reasons for recommending a rejection are the following: - Framing: I think the paper sets the wrong expectations, and it tries to do too much by trying to sell the paper as being about multi-label classification in general.  - Not enough analyses to really understand when/why pre-trained language models (don’t) work for this task.\nMore specific comments are below: GOAL OF THE PAPER I would advise not to try to sell the paper as being a general paper about multi-label classification. I think the current title \"Multi-label classification with pre-trained language models\" is too general and sets the wrong expectations. In fact, I wish that the authors had focused more specifically on this particular task (which could have made a very strong paper if done well), rather than trying to sell their paper as a general paper about multi-label classification, as I think it doesn't meet the expectations that the paper is setting.\nIf the paper is really about investigating pre-trained language models for multi-label classification, I would expect multiple datasets and in-depth analyses of when/why pre-trained language models work for multi-label classification.\nThe authors aim to sell the ANES dataset as a useful dataset for NLP researchers to look at. But this motivation could be stronger by explaining more clearly how this dataset differs from current multi-label datasets (including the ones that are out there but not included in public leaderboards). What makes this particular dataset, or the goal of using it for social science, challenging? “ Because of the resulting structure (a text + multiple labels), this type of data presents an interesting type of task (multi-label classification) for NLP models” isn’t really convincing.\nANALYSES The technical contribution is limited. The overall setup is very similar to Card and Smith (2015), which they cite heavily, but the authors investigate various pre-trained language models (using default parameters). The results aren’t great (often outperformed by the baseline). This doesn't have to be a reasons to reject the paper, as it can be a very interesting data point to understand when to choose which model. However, the paper was lacking in-depth analyses to understand -why- the results weren’t good. The main take away I got was that the limited dataset sizes were a challenge, but this could have been investigated more systematically (e.g. by varying the amount of training data for the same dataset). An in-depth error analysis could have also shed more light on this.\nADDITIONAL COMMENTS - Pay attention to (opening) quotation marks - Why the choice for logistic regression without regularization? Even as a baseline, regularization (e.g. L1, L2 or elastic net) is very common and usually much more effective.\n- It was really good to see that the authors have been careful with all the data processing steps. The steps were very well documented. However, I think at the moment that the balance is a bit off in terms of space (3 pages dedicated to data processing of the ANES dataset). I would suggest moving some of this information (e.g. about the file structure of the dataset, and how to go from numeric codes to a binary vector) to an appendix/supplementary material, to allow for more space to discuss the experiments.\n- \"code sets\" could have been defined more explicitly (I think these are just the set of possible labels/codes? and so all questions with the same possible labels/codes are grouped into one dataset?)\n- I didn't understand \"The latter has been assigned multiple times, as there were many more code columns available then needed\" Why not leave the remaining columns empty?\n- As the idea is to make the dataset available to other researchers, I'd recommend also having a development split (the current dataset only contains fixed train/test splits), to ensure that model development and parameter tuning in future research isn't done on the test set.  - \"Verbatim-level averaged \" what do you mean with this?\n- Section 5: are the differences significant?",
    "overall_score": "2",
    "confidence": "5"
  }
]