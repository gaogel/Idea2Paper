[
  {
    "review_id": "7bfc15e50269b6e0",
    "paper_id": "COLING_2020_17",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- The paper addresses a fundamental issue that arises from the combination of vision and language. It is fit for Coling because it starts from a classic linguistic problem (namely referential uncertainty), and applies its implications to resources that are of practical use.\n- The practical implications are convincingly demonstrated by the evaluation of Bottom-Up on this dataset. I believe the dataset will be very useful to visually grounded language research, particularly in evaluation.\n- The collection procedure is carefully designed and reported.\n- Statistics accross domains are interesting!",
    "weaknesses": "Some imprecisions or unclarities in the reporting of statistics, e.g.: - In several places it is unclear whether 'names' refers to tokens or types, making it difficult to interpret reported results: are percentages calculated per token or type? I believe it is mostly tokens, not types, but this should be made clearer.  - In 3.2 it is said that there are on average 4 names per object, yet in 3.4 it is said that v1 has 2.9 objects on average (which then is reduced by the additional annotation in v2). Where does this difference come from?\n- \"Only 3% of the annotators disagreed on whether the token and ntop are co-referring\" --> \"Annotators disagreed on ... in 3% of cases\" The writing&figures could do with some polishing - see feedback below.\nOther feedback/questions - Upon release, will you only release the 'cleaned' MNv2, or also the annotated mistakes? The latter could be very useful too, particularly in evaluation (as you show in comparing bottom-up to human naming variation).\n- Some terms and abbreviations need introduction before using them; e.g. 'MN', 'inadequ-type', 'same-obj', 'bbox', 'turkers'. In most cases readibility would be improved by sticking with the more verbose descriptor rather than the ad-hoc abbreviation/term.\n- fig 1 is very pixelated, making it difficult to read the numbers.\n- for fig 1b, it is mostly impossible to gauge the inadequacy type distribution for cases where the same-object annotation is lower than 1. Perhaps it would be better to report percentages? Also, why were these large bins chosen? If I understand correctly, there are only 4 possible exact values depending on how many of the 3 annotators think it is the same object (0, 1/3, 2/3, or 1).\n- The authors describe that in many cases, there simply is referential uncertainty - for example 'food' or 'table' might both be adequately encompassed by the bounding box. Yet the term used by the majority is considered the 'target' and use of a different term is in some cases referred to as a 'mistake'/'error' (even if it is a plausible alternative). This is not really in line with (sensible!) statements such as 'this suggests that the standard single-label evaluation schemes for recognition methods in computer vision might not be very reliable, punishing models for 'errors' that actually constitute plausible alternatives' (section 4.2). Perhaps the use of terms throughout the paper could be more consistent with these conclusions.\n- the tables and figures often appear quite far from where they are discussed",
    "comments": "",
    "overall_score": "4",
    "confidence": "3"
  }
]