[
  {
    "review_id": "87c4c3d2c6f34dd3",
    "paper_id": "COLING_2020_2",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper is about data selection from general domain parallel corpora for the task of bilingual lexicon induction. Three methods for data selection are presented and are evaluated on two specialized and two general-domain datasets. Each data selection method would create a ranking of the sentences from the general domain corpora which would allow different subparts to be selected - 10%, 20%, ... 100% of the general corpus - and combined with the specialized data. It is obvious from the experiments that focused data selection boosts the performance scored in terms of MAP; adding all the data, however, is not beneficial due to the problem of polysemy. The paper also analyses the performance in terms of consumed computational time.\nThe paper is clear, well-written and presents valuable insights on the addressed problem and the described solutions.\nFollowing are my remarks: GENERAL: - Section 1, last sentence. Needs to be revised. Stating \"more efficient\" would imply comparison which is not clear here.\n- Section 2, \"proposed unsupervised mapping methods getting interesting results.\" What do you mean interesting? Can you elaborate?\n- Section 2 discusses in details word representations, which is only part of the related work. This part is rather detailed and is not well reflected in the rest of the paper. Furthermore, this section does not present related work on data selection. Consider for example the work of Alberto Poncelas, e.g. https://paperswithcode.com/paper/data-selection-with-feature-decay-algorithms; the work of Bicici, e.g. E. BiÃ§ici and D. Yuret, \"Instance selection for machine translation using feature decay algorithms\". Also there is no literature review on previous work on data selection specifically for BLI. In fact the BLI -relate work is discussed in the introduction (Section 1). If you are the first to do data selection for BLI - state it; otherwise, present a comprehensive overview of related work.  - Section 2: \"For this reason, we focused this study on uncontextualized word embeddings (fastText).\" The reasons are not clear. Needs to be elaborated more.\n- Section 4: \"..., BC is of better quality than WE\" - how do you judge quality?\n- Expand the caption of table 2 and clarify which data selection you used.\n- It is interesting that the random performs well. In some cases better or at a par with the other + methods. Could you discuss why that is the case?\n- The sentence that starts with \"We can clearly see the improvement ...\" is unclear. In fact, it makes it sound as if there is an improvement going from 10% to 100% while it should be the other way round.\n- Figure 2: Having number of words on the x axis makes it difficult to compare to Figure 1. You can add additional labels for the percentages for the WE and the JRC corpora.\n- \"(75%) but does not beat the previous best results.\" - state which are these previous best results.\n- \"words present in the general corpus are also enriched\" - not clear what is enriched. Elaborate.\n- \" The colors in table 4 are not very good for color blind or if the paper is printed black and white.\n- Replace CoRR and arXiv references with references to published proceedings or journals  OTHER: *Abstract: - \"machine translation' data selection\" -> \"machine translation data selection\" - Section 1: - \"...with the deployment of open access, is nevertheless\" -> \"...with the deployment of open access data, is nevertheless\"  - \"...corpora for bilingual lexicon induction (BLI) task is ...\" -> \"...corpora for bilingual lexicon induction (BLI) is ...\" - \"Therefore, this strategy improves the quality of bilingual lexicons extracted ...\" -> \"We show that this strategy improves the quality of bilingual lexicons extracted ...\" - Section 2:  - \"...; Mikolov et al., 2013), have come to renew traditional approaches.\" - > \"...; Mikolov et al., 2013), have come to renew traditional ones.\"\n- CBOW - abbreviation needs to be explained. What does C stand for?\n- \"It is thus possible from a general language corpus to have, for a given word different embeddings, each reflecting one of its meanings.\" - > \"It is thus possible from a general language corpus to have, different embeddings for a given word, with each of these embeddings reflecting one of the word's meanings.\" \n*Section 3: - \"...and is performed at the sentence level\" -> \"... and is performed at the document level\" - Section 4: - \"Table 1 shows the size of each part of the comparable corpora...\" -> \"Table 1 shows the size of each part of the corpora...\" - \"...the quality of bilingual terminology\" -> \"... the quality of the bilingual terminology\" - \"... from comparable corpora are the same \" -> \"... from comparable corpora is the same\" - \"We first use our data selection methods to find for a given specialized corpus...\" -> \"For a given specialized corpus we first use our data selection methods...\" - \"Then, we create our training data on both languages by concatenating our full specialized corpus with sub-parts of our general corpus, adding it per sample of 10%...\" -> \"Then, we create our training data for both languages by concatenating our full specialized corpus with sub-parts of our general corpus. We build several training data sets by incrementally adding samples of 10%...\" - Section 5: - \"... for BLI on various corpora\" -> \"... for BLI on our corpora\" - \"... for our four data selections...\" -> \" ... for our four data selection methods...\" - \"... and their different combinations with two general corpora.\" - > \"... and the different combinations with data from the two general corpora.\"\n- \"corresponds to a given percentage of general corpus addition.\" - > \"...corresponds to a combination of the specialized corpus with a different percentage of data selected from the general corpus.\"\n- \"...to the results with the full general corpus.\" - > \"... to the results with adding the full general corpus.\"\n- caption of Figure 1: \"WE corpora and two general corpora\" -> \"and the two general corpora\" - \"Table 2 resumes\" -> \"Table 2 presents\" - \"...for what we assume to be the optimal...\" -> \"...for what the best results indicate to be the optimal...\" *Section 6: - \"Our previous results establish the relationship between data selection and the quality of bilingual lexicons, more precisely it shows that ...\" -> - \"The results showed in Section 5 establish a relationship between data selection and quality of bilingual lexicons. More precisely they show that ...\" - \"... while reducing the computation time.\" - > \"... while reducing the computation time if we would use data augmentation.\"\n- \"(i.e. from general domain)\" -> \"(i.e. from a general domain)\" - \"(rank 3) is a real improvement comparing\" -> \"(rank 3) leads to a significant improvement compared \" - \"These examples reflect the interest of...\" -> \"These examples reflect the benefits of...\" - \"in the most interesting one selected\" -> \"among the highest ranked\" - \"And the data augmentation column (100%) really show \" -> \"And the data augmentation column (100%) shows \"",
    "overall_score": "4",
    "confidence": "3"
  }
]