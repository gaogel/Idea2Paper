[
  {
    "review_id": "497e9cf35fb51afb",
    "paper_id": "COLING_2020_44",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This article presents several parsing models that are able to do domain adaptation from large scale source-domain labeled data to small scale target-domain labeled data via adversarial learning. They also use large scale unlabeled data to improve word representations. Experiments show that these approaches lead to consistent improvements, reaching SOTA results comparable to those for more complex ensemble models.\nThe article is well written and in my view the contribution is important and well supported. All the proposed ideas are compared and validated with experiments.\nIn terms of results, the biggest contribution seems to be given by the usage of BERT and fine-tuned BERT, maybe the least original aspect of the work, but the adversarial models consistently show improvements in every comparison w.r.t. non-adversarial ones.\nThe final results and comparison with previous work is also meaningful. In particular, it is interesting to observe that the best results are obtained for the PC domain, the one with the largest difference w.r.t. the source domain, and that the adversarial training plays a key role here.\nMinor comments: 1: “kim et al.” -> “Kim” 1: feature argumentation -> feature augmentation 1: last paragraph, 1st sentence, too long. Or not? \n1 “a large-scale” -> “large-scale” 1: “our codes” -> “our code” 1: Put link in footnote. \nUse “Fig.” to reference figures in text. \n2: “Additionally. we …” -> “Additionally, we …” 2: BiLSTM “forward and backward two directions” 2: (3) use two formulas 2: loss is standard, no need to include it. \n3.1: “the adversarial network on BiAffine parser” -> “an adversarial network on BiAffine parser” 3.3: “fine-tuning domain embedding” -> “fine-tuned domain embedding” 3.3: “domain-special features” -> “domain-specific features” 3.3: “another BiLSTM” -> “the other BiLSTM” 3.4: “start point” -> “starting point” 3.4: “computation resource” -> “computation[al] resources” 3.4: “... once, Thus …” -> “... once. Thus …” 3.4: explicitly say that no “next sentence” loss is used. \n4: “product blog” -> “product blog (PB)” 4: “dependence parsing” -> “dependency parsing” 4.2: “directly applies” -> “directly applying” 4.4: “compared the results” ->  “comparing the results”",
    "overall_score": "4",
    "confidence": "3"
  }
]