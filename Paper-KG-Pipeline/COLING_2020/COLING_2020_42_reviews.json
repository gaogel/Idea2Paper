[
  {
    "review_id": "a13bd46160cb868b",
    "paper_id": "COLING_2020_42",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper proposes a heterogeneous-event graph network to help solve the missing event prediction task. Based on the reported experimental results, the proposed method achieved slightly better performance than previous works (e.g., EventTransE) on this task. However, I still have some concerns about both the experimental details and the paper writing, details are as follows.\n1. The term \"heterogeneous\" is a little bit misleading. Typically, we use that term to refer to the graphs whose edges have multiple complex types. However, in the proposed model, there are only three relation types (word-word, word-event, event-event). If I understand correctly, both the word-word and event-event are just homogeneous networks and what the author did is adding an additional connection between these two homogeneous graphs. So a better term might be something like \"cross-graph\"?\n2. It seems like the author didn't mention how they construct the graphs in the paper. As the main contribution of this paper is using the word/event graphs, without knowing how did the author create them (e.g., what is the meaning of edges and how to get those connections), we couldn't evaluate the technical soundness of the proposed model.\n3. The author did experiments on the NYT dataset. My concern is that the quality of that dataset is not very satisfying because the test set is also automatically extracted from the documents. I suggest the author try other human-curated event sequence datasets (e.g., RocStory, Wikihow) as the evaluation dataset.\n4. I am not sure whether the evaluation is fair for baseline models. For example, to the best of my knowledge, none of the baseline methods mentioned in this paper use strong pre-trained language models (BERT), and the proposed model does use BERT as part of its model. And as suggested by the ablation study, BER contributes a lot to the final success and I found it is hard to be convinced that the main improvement is coming from the graph model rather than BERT. A better way to do the evaluation is adding one more baseline, which uses BERT to encode the sequence and directly make predictions. A comparison of that can better prove the value of the proposed graph model.\n5. By the way, another suggestion is that maybe you may want to use stronger pre-trained models (RoBERTa) if you want to get better performance.\nTo conclude, I think this paper proposes an interesting solution to the missing event prediction task, but it still needs some further improvement to be good enough to be published in COLING.",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "1770dd00a1e9fae1",
    "paper_id": "COLING_2020_42",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper explores the use of graph neural networks to identify missing elements in an event chain in a multiple-choice scenario. Overall, it is an interesting approach to the task, which surpasses the performance of previous systems. Furthermore, the paper presents an interesting discussion of the results, including ablation studies that reveal the usefulness of every system component. However, the paper also has some issues, which I discuss below: First of all, the task is defined as a multiple-choice task in which, for each missing event, there is a golden choice and four wrong choices. However, there are no examples of the wrong choices nor information regarding how they vary in relation to the correct one. This makes it hard to assess the actual difficulty of the task.\nFurthermore, although the evaluation is performed on the MCNC dataset, the model is trained on a portion of the Gigaword corpus. In this case, the event chains are obtained by mapping the coreference chains identified by the Stanford CoreNLP library. This means that the system is probably also learning to identify those coreference chains. Thus, it would be interesting to compare the performance of the system with a simple selection based on the coreference chains identified using Stanford CoreNLP.\nThe system relies on BERT to generate the embedding representation of the text. Furthermore, the pre-trained model is fine-tuned on the used corpus. However, this tuning is still based on masked language modeling task. It would be more interesting to fine-tune the model to a task closer to event chain prediction, such as next sentence prediction, which was also used to pre-train the original model.  Finally, regarding typos/grammar/style, when references are used as part of the sentences, the format should be different (e.g. \"(Lv et al. 2019) found\" -> Lv et al. (2019) found). Furthermore, the placing of references should be consistent across the paper. For instance, in the second paragraph of the introduction, the placement of the references varies between before and after \"based\", which makes it confusing. More importantly, the paper has several issues regarding number and verb tense mismatches or inconsistencies, as well as \"a\" that should be \"an\". Thus, it requires a thorough proof-reading and revision.",
    "overall_score": "4",
    "confidence": "4"
  }
]