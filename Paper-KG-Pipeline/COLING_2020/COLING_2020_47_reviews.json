[
  {
    "review_id": "e29390d4d9b4d866",
    "paper_id": "COLING_2020_47",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper present a curriculum learning method for NMT which is shown to work relatively well for low-resource settings. The method is applied to the standard Transformer and it is evaluated on several language pairs and different data sizes. The method determines what data samples should be presented to the model based on their difficulty and the model competence. The experiments show that the approach provides for consistent improvements.  The paper is interesting and relatively clear, but there are some points in the paper that need clarification.  The approach is interesting. It is based on Platanios et al. (2019) where sample difficulty and model competence were used. This paper proposes new ways to compute these values, and more importantly, sample difficulty is reestimated during training. \nThe improvements are consistent across different language pairs, although often relatively small. The highest improvements are when training on 50K WMT data.  My main concern is that the improvements are rather small, on many test sets it is under 1 BLEU. Also, I am concerned if the model used for 50K WMT is overparametrized. This is the setting where the highest improvements are obtained, but it seems like this is a large model for such a small dataset. Dropout should probably be larger as well. Sennrich and Zhang (2019) (Revisiting Low-Resource Neural Machine Translation: A Case Study) provide details on how to better train low-resource models. \nI am concerned if the method would prove as effective in low-resource settings if the models are more optimized for the low-resource setting.\nI am not sure why are there some differences in model sizes across the different language pairs. These seem rather arbitrary. How were these hyperparameters determined? I do not see a connection between training dataset size and model size or number of layers.\nThe analysis in Figure 2, 3 and 4 is interesting. However, I am curious if these effects would be noticeable in the other experimental settings, where in almost all cases, improvements are under 1 BLEU.\nThe results in Koehn and Knowles (2017) with regards to low BLEU scores in low-resource settings have been addressed in Sennrich and Zhang (2019). \nI agree with the sentiment that performance in low-resource settings is often lacking, but using this reference may not be appropriate.  Typos and writing style:  including language model -> including language modeling mini-batch contains sentences -> mini-batch containing sentences Algorithm 1, line 1: Randomly initial -> Randomly initialize I would suggest normalizing all scores in Table 3 to 2 decimal points.\nIn this paper, we propose a dynamic model competence (DMC) estimation method... - this paragraph is confusing. I did not understand what is the \"prior hypothesis of the training process\" nor how is BLEU superior if the loss is already the optimal method to estimate competence.",
    "overall_score": "4",
    "confidence": "4"
  }
]