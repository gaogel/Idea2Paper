{
  "paper_id": "COLING_2020_63",
  "title": "Investigating Catastrophic Forgetting During Continual Training for Neural Machine Translation",
  "conference": "COLING",
  "domain": {
    "research_object": "神经机器翻译模型在持续训练过程中灾难性遗忘现象的研究",
    "core_technique": "分析和缓解神经机器翻译中灾难性遗忘的算法与方法",
    "application": "提升神经机器翻译系统在多任务或增量学习下的性能稳定性",
    "domains": [
      "自然语言处理",
      "机器学习"
    ]
  },
  "ideal": {
    "core_idea": "分析NMT模型在持续训练中灾难性遗忘现象及其影响。",
    "tech_stack": [
      "神经机器翻译",
      "持续学习",
      "灾难性遗忘分析"
    ],
    "input_type": "NMT模型及不同领域训练数据",
    "output_type": "模型性能变化及遗忘现象评估结果"
  },
  "skeleton": {
    "problem_framing": "论文通过回顾NMT模型的成功与局限，强调其在大规模数据下表现优异，但在特定领域小数据场景下存在挑战。通过引用相关文献，逐步引出实际应用中模型迁移与适应的需求，为后续研究动机埋下伏笔。",
    "gap_pattern": "作者指出现有NMT模型在特定领域小数据情况下表现不足，虽然常用的持续训练（微调）方法能提升领域内性能，但未能系统性解决灾难性遗忘等问题，暗示当前方法存在改进空间。",
    "method_story": "方法部分采用理论推导与文献借鉴相结合的方式，详细介绍基于泰勒展开的参数重要性评估准则。通过公式推导，明确阐释如何量化参数对损失函数的影响，为后续实验设计提供理论基础。",
    "experiments_story": "实验部分通过对比不同模块冻结与更新的策略，结合可视化结果（如柱状图），系统分析各模块对泛化与领域适应的影响。通过定量指标（BLEU）和现象解释，突出关键发现并与前述理论假设呼应。"
  },
  "tricks": [
    {
      "name": "引用前沿研究作为背景",
      "type": "writing-level",
      "purpose": "建立研究背景和权威性",
      "location": "论文开头",
      "description": "通过引用多个领域内权威和前沿的论文（如Kalchbrenner and Blunsom, 2013; Vaswani et al., 2017），展示NMT模型的研究进展和广泛应用，为后续问题引出做铺垫。"
    },
    {
      "name": "问题引入与实际应用场景结合",
      "type": "writing-level",
      "purpose": "突出研究的实际意义",
      "location": "背景介绍段",
      "description": "结合实际应用场景指出NMT模型在特定领域数据稀缺时的局限性，引出后续研究的必要性和实际价值。"
    },
    {
      "name": "方法命名与定义",
      "type": "writing-level",
      "purpose": "明确术语和方法，便于后续讨论",
      "location": "方法介绍段",
      "description": "对‘continual training’和‘fine-tuning’进行定义和说明，使读者明确后续讨论的对象。"
    },
    {
      "name": "问题现象描述与图示辅助",
      "type": "writing-level",
      "purpose": "直观展示问题和效果",
      "location": "catastrophic forgetting现象描述处",
      "description": "通过文字描述和引用图表（如Figure 1）直观展示in-domain与general-domain性能变化趋势，使问题更加具体和易于理解。"
    },
    {
      "name": "文献对比与现有方法总结",
      "type": "writing-level",
      "purpose": "展示领域已有解决方案，为创新做铺垫",
      "location": "catastrophic forgetting方法介绍处",
      "description": "简要列举已有方法（如Freitag and Al-Onaizan, 2016的模型集成），为后续提出新方法做铺垫。"
    },
    {
      "name": "基于泰勒展开的参数重要性评估",
      "type": "method-level",
      "purpose": "评估模型参数对损失的影响，指导参数选择或剪枝",
      "location": "方法部分",
      "description": "采用泰勒展开近似评估移除某参数对损失的影响，量化每个参数的重要性，为模型优化提供依据。"
    },
    {
      "name": "假设参数独立性以简化计算",
      "type": "method-level",
      "purpose": "降低计算复杂度，便于理论推导",
      "location": "参数重要性评估方法介绍处",
      "description": "假设模型各参数之间相互独立，简化损失变化的计算公式，使理论分析更可行。"
    },
    {
      "name": "利用ReLU激活性质简化推导",
      "type": "method-level",
      "purpose": "合理忽略高阶项，简化公式",
      "location": "泰勒展开推导末尾",
      "description": "利用ReLU激活函数的特性（损失一阶导趋于常数，二阶导趋于零），合理忽略泰勒展开中的高阶余项，最终得到简洁的参数重要性评价公式。"
    },
    {
      "name": "直观解释技术指标",
      "type": "writing-level",
      "purpose": "增强可理解性，便于非专业读者理解",
      "location": "参数重要性评价公式后",
      "description": "对最终的参数重要性评价公式进行直观解释（如“该准则不重视梯度平坦的参数”），帮助读者理解公式的实际意义。"
    },
    {
      "name": "分步推导公式并编号",
      "type": "writing-level",
      "purpose": "提升论述条理性和可追溯性",
      "location": "方法推导全过程",
      "description": "将复杂公式分步推导，逐步编号（如公式(2)-(6)），方便读者跟踪推导逻辑，增强论文结构清晰度。"
    }
  ]
}