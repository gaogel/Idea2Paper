[
  {
    "review_id": "7dfacbd9d54a6266",
    "paper_id": "COLING_2020_61",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper describes (1) new corpus resources for the under-resourced Kinyarwanda and Kirundi languages, (2) preliminary experiments on genre classification using these corpora. The resources are described thoroughly, and a useful survey of related work on these languages is presented. A variety of models are used in the experiments, and strong baseline results on this task are achieved, including experiments on transfer learning from the better-resourced Kinyarwanda to Kirundi; an approach likely to play an important role in scaling NLP to the Bantu language family, which has a small number of reasonably-resourced languages, e.g. Swahili, Lingala, Chichewa. Overall the paper should be of interest to COLING attendees.\nGeneral comments: Abstract: \"datasets... for multi-class classification\".  It would be good to note here and in the introductions that this is specifically a genre or subject classification task.\nIntroduction: \"has made access to information more easily\" => \"has made access to information easier\" Introduction, p.2 \"In this family, they are...\" => \"In this family, there are...\" Introduction: \"fourteen classes... twelve classes\". Again, as in the abstract, should make clear what these classes are!\nLast line of p. 2 \"who have not been\" => \"which have not been\" Related work.  You might also note Jackson Muhirwe's PhD work at Makerere; some of which was published here: Muhirwe J. (2010) Morphological Analysis of Tone Marked Kinyarwanda Text. In: Yli-Jyr√§ A., Kornai A., Sakarovitch J., Watson B. (eds) Finite-State Methods and Natural Language Processing. FSMNLP 2009. Lecture Notes in Computer Science, vol 6062. Springer, Berlin, Heidelberg. https://doi.org/10.1007/978-3-642-14684-8_6 3.3 Dataset cleaning.  I know it's just a change in perspective, but I'd prefer viewing the cleaning and stopword removal as standard pre-processing steps; suggesting distributing these as tools vs. distributing the corpora with these steps applied.  Classifiers should work on un-preprocessed text in any case.   3.4 I don't understand how the cleaning steps you described could reduce the vocabulary from 370K to 300K.  Please clarify.\n4.1 In training the word embeddings, you say \"removing stopwords\".  Does that mean removed from the corpus before training?  I'm not sure I see the value in doing so, and wonder if it negatively impacts the quality of the embeddings.\n4.1 Given the morphological complexity of these languages, I wonder whether results might be improved by working at the subword level (syllables, or morphemes... cf. Muhirwe's work above). This could conceivably help is the cross-lingual training as well. You do have Char-CNN experiments but there may not be enough data to get competitive results at the character level.\n4.3.2 \"different epochs and number of features... different train sets\"; this is fine, but you should refer to the table where these choices are actually laid out 4.4.1 Had the Char-CNN converged at 20 epochs?",
    "overall_score": "4",
    "confidence": "4"
  }
]