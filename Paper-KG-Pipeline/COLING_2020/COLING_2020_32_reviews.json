[
  {
    "review_id": "c99723dedc4f5811",
    "paper_id": "COLING_2020_32",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper proposes an extension of length-ratio and length-difference positional encoding with the injection of noise into the length constraint. They also propose to predict the output length with a BERT model instead of relying on the source length. \nThe paper is well motivated and clearly written. The experimental setup is well described and the results are rigorously analyzed.\nA few comments: 1. Although the authors seem to focus on under-translation (hypothesis shorter than the reference), their method works best (with statistical significance) when the baseline Transformer is over-translating (likely repetitions) as shown in the first column of table 2. So I don't understand the emphasis on under-translation if we can discuss both phenomena with the same model.\n2. For a complete comparison with the Transformer baseline, the authors could consider injecting noise in the regular positional encodings, this is inexpensive to train and would be a fairer comparison to the noisy LDPE/LRPE.\n2. Although the authors acknowledge that the difference between their proposed model and the baseline Transformer is not statistically significant, if we factor in the inference time then the baseline is a clear winner. Depending on the BERT model used to predict the source length, you could be more than doubling the cost of encoding your source sequence. Did you consider conditioning the length predictor on the encoder states?\n3. For a short paper and by my standards, experimenting on a single corpus is sufficient, however it would be a stronger paper if the authors could at least consider the opposite translation direction to cover both cases of the reference_length / source_length smaller and larger than 1.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "32f1f22df9b21fd2",
    "paper_id": "COLING_2020_32",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "Summary     - The paper studies the problem of under-translation common in auto-regressive neural machine translation. \n    - Two main pieces are introduced in this research work, random noise to the length constraint, output length prediction using BERT. \n    - The English-Japanese ASPEC dataset is used to evaluate the contribution of the two proposed improvements. \n    - A stronger or similar performance is shown for all the 4 length language groups using the proposed approach. Especially in the shorted range, the authors show more than 3 points improvement over the vanilla transformer. \n     - An interesting insight I got for the long sentences, vanilla transformer tend to produce shorter length sentences. The proposed approach generated translation close to the gold reference length, atleast for the dataset in use.\nStrenghts     - Ablation is performed for both the new component, random noise and BERT based output length prediction. \n    - Strong BLEU score for short sentence range and relatively close to gold reference length compared to the vanilla transformer.  Concerns     - The work of Lakew et al, uses English-Italian and English-German datasets for evaluation. These datasets should be used to have a consistent evaluation with the past work. \n    - Following from the last one, any specific reason why the English-Japanese dataset is a better choice for your proposed methods? Perhaps you can **motivate on linguistic grounds** why the language Japanese is a better testing ground for your method. \n    - Including an extra BERT-based-output-length prediction can incur additional computational overhead. The overhead of this computation should be stated in the work. \n    - In the introduction, you mention __However, the input sentence length is not a good estimator of the output length.__. I'm not sure why is this the case.",
    "overall_score": "3",
    "confidence": "4"
  }
]