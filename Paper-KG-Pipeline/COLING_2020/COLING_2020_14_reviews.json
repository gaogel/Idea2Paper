[
  {
    "review_id": "787813da7147b3dc",
    "paper_id": "COLING_2020_14",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper presents an exploratory work made on the evaluation of justified (or not) disagreement in the task of annotations. \nAs far as my skills on the subject allowed me to evaluate and understand, here are the positive, neutral and negative aspects I could identify.\n--- Positive aspects --- (1) The paper tackles a very interesting subject for NLP and especially for CL. It thus is a good match for COLING.\n(2) As far as I could tell, the paper respects all formal submission criteria (e.g. abstract size, anonymity).\n(3) The paper is written in a very pleasant English that allowing a smooth reading (even though I have some concern about the content structure).\n(3) I found the methodology and results sound and convincing. There was a number of choices made by the authors that felt very adequate (e.g. boolean crowdsourcing, Prolific etc.).\n(4) Even though the results are probably only preliminary, my impression is that the amount of work to obtain them seems substantial.\n--- Neutral aspects --- (1) the citations made inside parenthesis do not display inline as they should and have their own pair of parentheses.  (2) in Section 2 the authors said \"Despite the central nature of phenomena triggering disagreement in annotation tasks, we are not aware of evaluation methods that do not mainly rely on agreement\". I don't know any too. Nonetheless, if the authors haven't already checked, I would suggest to give a look at the publications within the hcomp community => https://www.humancomputation.com/ (3) If I got that right, in Section 6.2 I suppose that \" The unit-quality-score (uas)\" should be changed as \" The unit-quality-score (uqs)\".\n(4) In section 7.2, something went wrong with this sentence \". In contrast, a simple majority vote achieves an f1-score of 0.78 and the unit-annotation-score. \"\n(5) \"Most importantly, it would be highly valuable if the existing metrics could be combined in such a way that we could use them for the identification of different types of disagreements. \" => sounds like something a Machine Learning algorithm could be used for by using the metrics as features.\n--- Negative aspects --- (1) The paper has one major issue: whereas the overall subject is coherent and well defined, its focus is quite blurry and it has been hard to understand what is exactly the contribution of the authors. Indeed the authors tend to present things in a not-so-consistent fashion from one section to the other. Also, the paper seems to be focusing on studying disagreement between annotations, yet a consequent part of the contribution described is about a filtering method to discard incoherent annotations or annotators and  thus improve aggregation. Likewise, diagnostic datasets are often mentioned but nothing is done about them in the results... Another example is Table 2 => why go into such details if they don't contribute directly to the results or argumentation presented in this paper? All this details tend to confuse the reader in the end.\n(2) While the paper tackles a very interesting subject for CL, I was under the impression that it is written in a very NLP-oriented fashion. I believe COLING try to bridge (as much as possible) NLP and Linguistics and the authors should consider that for their final version.    (3) The authors should have provided examples in Section 3 and 6.3 to help the reader understanding.\n--- Conclusion --- I think that the work presented is a good piece of research and this paper is already nice (and can become even nicer). \nI would thus gladly recommend to accept it and hope that, if it gets accepted, the final version will have amended the shortcomings mentioned above.",
    "overall_score": "5",
    "confidence": "3"
  }
]