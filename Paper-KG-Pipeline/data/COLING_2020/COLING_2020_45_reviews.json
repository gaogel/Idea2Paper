[
  {
    "review_id": "81ddee5ab5d6445d",
    "paper_id": "COLING_2020_45",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper presents what the authors call Definition Frames (DF), representations that combine word embeddings with explicit relations between concepts. Differently from previous approaches that integrate external knowledge into word embeddings (e.g. retrofitting word embeddings to a semantic network), DF retains the explicit relations that characterize concept. The authors follow the ideas brought forward by Pustejovsky and his work on qualia structures. Given a concept, Pustejovsky theorizes that such concept can be fully described by a set of attributes/properties. While in traditional word embedding techniques each word is represented by a single vector, DF represents each word with a matrix where each row is a dense representation of one of its attributes/properties (e.g. a row for \"IsA\", another one for \"PartOf\", etc.). The authors show the effectiveness of their proposed approach in word similarity and word relatedness.\nThe paper is well-written and very easy to follow. I really enjoyed the work: it is a rather simple approach backed up by very interesting linguistic theories, a combination that is not easy to find in most of the NLP papers today. In general, I appreciated this work and I think it deserves a place in a conference such as COLING.\nSaid that, the fact that I appreciated this work is one of the reasons I expected more and was let down by some parts of the paper. First, I think that a paper on word embeddings in 2020 must include some kind of comparison with current contextualized word embedding techniques, say BERT or ELMo. Of course, word similarity is not the best task to compare DF against BERT-like embeddings. However, I still believe that BERT could have been easily integrated into DF itself to obtain a BERT-based DF. For example, the Basis row/vector of a word w could be the BERT state corresponding to the CLS token of the WordNet definition of w, while the qualia dense representation could be the average BERT state of the qualia in the ConceptNet sentences that define the relations of w. Since the authors already make use of WordNet and ConceptNet, I think this would have cost little extra work for a much stronger and up-to-date paper.\nAnother issue of the paper is the evaluation. Word similarity and relatedness, while traditional NLP tasks, are still intrinsic evaluations. I understand this is a short paper, but I feel like one of the two tasks (similarity or relatedness) could have been sacrificed for a downstream task. I would also suggest not to show just the average Spearman correlation coefficient of several similarity datasets, as this makes it more difficult for the reader to compare DF to other word embedding techniques.\nReasons to accept: - DF is a perfect fit for COLING as it combines strong linguistic theories with existing distributional approaches.\n- The authors present a clever way to take advantage of knowledge resources (WordNet and ConceptNet) and word embeddings.\n- Since the resulting representations retain the qualia structure of the represented words, they are more interpretable than traditional word embeddings.\nReasons to reject: - The evaluation is not strong as it only comprises word similarity and relatedness, two intrinsic tasks.\n- The authors do not include BERT or BERT-like contextualized embeddings. As time passes, this may become a stronger reason to reject this work.\nOther (minor) comments: - In case of accept, please include in the appendix the individual results on each word similarity/relatedness dataset (helps reproducibility).\n- The font in Figures 2 and 3 in Appendix D is too small.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "938799444969a089",
    "paper_id": "COLING_2020_45",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper presents an interesting attempt to build a word embedding for a word by considering relations in which this word is involved in wordnet definitions. These relations are those from Pustejovsky's qualia structure (partOf, madeOf, etc.). Then, the word embedding is the average of the vectors corresponding to each of the relations (which I'm not sure how they are constructed, another average over the words involved in the relation, the LSTM hidden state?).  This is a linguistically motivated approach to learn better word embeddings, but I think the main problem with it is that the experimental results are not convincing. Specifically: 1) You map words in similarity/relatedness datasets into wordnet to extract their definitions, but these datasets are not disambiguated. So how do we know the definitions extracted correspond to the intended sense? There needs to be a discussion about this.\n2) I think it would have been better to find standard datasets for relation extraction instead of annotating a few definitions from Wordnet. First, these definitions are short and unlikely to contain all the target relations. Second, they are quite fixed in structure (as opposed to e.g., Wikipedia definitions), which means probably a rule-based method would work as well. In [1] there is a dataset with annotated definitions with hypernyms, which at least would make a challenging dataset for the isA relation. I think this could be replicated with other relations, but again I doubt these will be very prevalent. And this makes sense because a definition should in nature contain the minimal set of features for a term to be out of its kind. Why not mine a large corpus and apply the 'relation retriever'? Probably using ERNIE would be a good choice here it's been precisely trained to do well on relation extraction. Bottomline is we need statistics of the prevalence of extracted qualia structures from wordnet definitions, probably plotted against definition lengths.  [1] Navigli, R., Velardi, P., & Ruiz-Mart√≠nez, J. M. (2010, May). An Annotated Dataset for Extracting Definitions and Hypernyms from the Web. In LREC.\n3) The results are confusing to me because these similarity/relatedness datasets, in theory, should be used to evaluate the word embedding model in itself. Training a linear regression with the gold standard similarity scores to me seems quite unorthodox and against the point of these datasets. I would like to see some references to back this choice. Why not use word-level classification datasets where a supervised approach would be less frowned upon, like McRae feature norms, diffvec or the Glasgow norms dataset?\n4) There is no discussion of why dict2vec outperforms glove and retrofitting. I think this would be interesting, especially since retrofitting is also using wordnet (I believe?).\n5) I think most of the results in the appendix should be part of a longer, more broken down paper. In its current state it feels too much like work in progress.",
    "overall_score": "2",
    "confidence": "4"
  }
]