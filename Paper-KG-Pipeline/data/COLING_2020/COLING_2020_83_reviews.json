[
  {
    "review_id": "1092ba4012c277d6",
    "paper_id": "COLING_2020_83",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper describes a new collection for multi-hop Question Answering (QA). The new collection represents an important contribution to the community given that: 1) there are few datasets of this type, 2) other datasets are smaller and 3) this dataset contains explanations about how to obtain the answer. Besides, the process for obtaining the dataset is mostly automatic (it only requires to create some logical rules). So, I think this paper would help to advance the current state of the art in multi-hop QA systems.\nI only miss a deeper analysis of errors in Section 5.3. I think it is important to manually analyze the errors in order to detect why human performance is below 90. The authors claim that it could be because some questions could be unanswerable given the supporting information. I think it is important to know this feature of the dataset before releasing it to the research community.\nI also think that some of the inferred relations can be explicit in some text. For example, a text could contain that A is the grandmother/grandchildren of B (e.g. grandchildren of Queen Victoria). How do you check that a multi-hop inference is required? With the experiments using single-hop?\nI give below some details comment per section: Section 2.2: - Please, include an example for each type of question (or at least for type 4) Section 3.1: - \"entity. We used only a summary from each Wikipedia article as a paragraph that describes the entity\": How do you obtain such a summary?\nSection 4: - \"the system understand several logical rules.\" -- > \"the system understandS several logical rules.\"\nSection 5.3: - \"which might be because the mismatch information between Wikipedia and Wikidata makes questions unanswerable\": Before publishing the paper, the authors should check this information. It is important to know if come questions are unanswerable.\n- \"We conjecture that there are two main reasons why the score of the evidence generation task was low.\" : Again, I think you should perform a deeper analysis beyond these conjectures for the camera-ready version.\nSection 6:  - Include the main disadvantages of datasets for bullet \"Multi-hop questions in MRC domain\"  (they are already given in the Intro, so maybe you can refer to that section) - Appendix A6 seems to be empty. Maybe you should include a reference to algorithm 1 in such section.",
    "overall_score": "4",
    "confidence": "5"
  }
]