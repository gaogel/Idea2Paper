[
  {
    "review_id": "07cd0df768d6b1cd",
    "paper_id": "COLING_2020_10",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper focuses on referring expression generation. The main contribution is an extension to an influential, end-to-end encoder-decoder REG model, NeuralREG by Castro-Fereira et al, to handle previously unseen entities at test time. In its original formulation, NeuralREG was restricted to generating references for entities it had seen during training time. To deal with this, the present paper proposes a number of extensions, namely: 1. Input data representation: rather than an identifier, input entities have a richer representation, consisting of a sequence of tokens and wiki-derived information including type and gender, which therefore constitute additional inputs (in the form of embedding representations), together with the = representation of pre- and post-context computed by a biLSTM encoder. \n2. Encoder-decoder with copy: like the original NeuralREG architecture, an encoder-decoder architecture with attention and shared word embeddings is used, but augmented with a copy mechanism, which would allow the decoder to 'copy' the input entity to the output, or alternatively, to generate a referring expression for it.\nThe neural architecture is very clearly described. Insofar as it is an extension to an existing model, and uses a well-established copy mechanism from See et al (2017), one could characterise this work as incremental in nature. However, the work is rigorously evaluated against strong baselines, including the original Castro Fereira approach (without copy) and the more recent ProfileREG of Cao and Cheung (2019). Sensibly, the evaluation distinguishes between seen and unseen cases, showing that the copy mechanism contributes to better scores on the automatic evaluation metrics used. This is followed up by a human evaluation which, despite including only two partiicpants, seems to have been carried out quite rigorously. Unfortunately, the analysis of these results is somewhat sketchy. I don't buy the statement that \"our prposed model outperformed the previous version and the current state-of-the-art\". As the authors themselves state in the same sub-section, this is only clearly the case for the Grammaticality criterion. On Fluency and Semantic Adequacy, results of the new model are very close to the best model, namely the one using OnlyNames, but this doesn't warrant the blanket statement that the model outperforms all baselines. In fact, it is somewhat surprising that OnlyNames should come out top on the Semantic Adequacy criterion, when one would expect that the repetition of names is bound to exact a cost in human judgment. This, however, could be due to the fact that this baseline was augmented to handle pronouns - the section discussing the evaluation results could really do with more discussion of these issues.\nI very much appreciated the inclusion of an ablation study. The results here are interesting, and altogether interpretable. E.g., it's to be expected that the biggest drop in accuracy on test data for unseen entities would come with the ablation of the copy mechanism. This does, however, seem like a vanishingly small drop compared to other parts of the ablation. Is copy really doing all that much after all?  There were a few open issues which the paper could benefit from addressing: - is the wikipedia lookup required to establish entity type and gender always successful, that is, are there any cases where this information is unavailable? What is the impact of this on the evaluation results?\n- Section 5.2 detalis a number of hyperparameters. Were these established based on a tuning procedure, or were they set according to trial-and-error (or perhaps simply based on the previous settings used by Castro-Fereira et al)?\n- The authors state that the evaluation with human judges followed the best practice recommendations of van der Lee et al (2019). However, nothing is said about which such recommendations were followed. If anything, there's a strikingly small sample of human judges (N=2), which might compromise the robustness of the results. The authors might wish to comment further on this issue.",
    "overall_score": "4",
    "confidence": "5"
  }
]