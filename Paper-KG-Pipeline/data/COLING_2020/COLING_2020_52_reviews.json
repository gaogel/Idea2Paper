[
  {
    "review_id": "f2ffd4153ab09c80",
    "paper_id": "COLING_2020_52",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper focuses on lexical simplification of text, which is a well known task. \nThe paper specifically focuses on multi-word simplification, which is a relatively new focus. \nThis covers cases where the original phrasing and/or the replacement can be one or more words (up to 3 in this paper). \nThe research is well motivated. \nNotably, most of the recent (and cited) research focuses on complexity of words/MWEs for non-native English speakers, and the current paper does not mention whether it is in the same 'game' - this should be explicitly stated. \n(complexity for native speakers is a related but quite a different story).\nThe authors developed a new corpus of manually annotated lexical simplifications, which is a good contribution to the field.\nThe paper describes corpus creation, and development of a lexical simplification system, which is evaluated on the corpus data.\nCorpus Data for the corpus came  from the same sources as the annotated corpus of Shardlow et al. (2020). \n10K sentences were selected from each source.   Identification of complex words (potential targets for replacement) is a crucial step. \nFor corpus preparation, the authors used a CWI system from Gooding&Kochmar (2019). \nThen 1K sentences (per each source) were selected from that data for manual annotation. \nDid you try to verify the automatic CWI annotation?  The crowdsourcing section needs some more detail. \nDid you use only native English speakers for annotation (as Shardlow did) ? \nbecause workers could be from English-speaking countries but still non-native English speakers. \nSo this aspect needs clarification (and justification if non-native speakers were used).\nAlthough some measures of work-quality were taken, I am absolutely not convinced about the annotation is pure and gold-quality. \nI tend to accept that turkers working in good faith tried to provide rephrasings that were close to original meaning  (as they understood it). However, the problematic point is to what extent their rephrasing are simplifications, and not just equivalent complexity or even introducing more complexity. \nI would expect some validation measure, to show that most of the rephrasings are indeed simplifications. \nYou could give a sample to teachers for evaluation. Educators (teachers) are skilled at this. \n As it stands, I see no guarantee that the MWLS1 data set contains only simplifications, it might be slightly 'contaminated'. The low annotator agreement may be related to this. \nSince the corpus  is used for evaluation, validation is a crucial step.  It  is lacking.\nThe simplification system, Plainifier, includes several steps. \nSection 4 describes it and already makes an important omission. \nThe description directly goes into candidate generation. \nBut how does the system identify potential targets, how is CWI performed? \nIs the CWI system from Gooding and Kochmar used / included in Plainifier? \nOr some other module?  Does Plainifier include CWI at all? \nQiang et al. (2020)  also omit this aspect. But for a full system, this must be described.\nCandidates for target-replacement are generated by using BERT, following Qiang et al., but with a twist for MWE targets and for generating MWE candidates as well. \nCandidates are then ranked using three aspects: 1) probability in context (how well they fit with the surrounding context) - obtained from the BERT LM 2) preservation of original meaning (via semantic similarity using embeddings) 3) familiarity - using frequency in Google Books ngrams as a proxy. \nThis again closely follows the methods from Qiang et al. Evaluation is carried out on the MWLS1 corpus. \nThe results look promising. \nSeveral measures are used for evaluation, and this is really good. \nI am happy you didn't use BLEU, it is a bad measure  (see paper \"BLEU is Not Suitable for the Evaluation of Text Simplification\" in EMNLP 2018).  One specific aspect is not well covered. \nAccording to table 2, about 500 instances in MWLS1 corpus are one-word targets (about 1/3 of the corpus). \nTheir  replacements  are on average 1.2 words long (so mostly one-word). \nSince the paper focuses on MWEs, it would nice to see how the system performs on just MWEs separately, and the 500 separately. \nThis would provide a better understanding on how well it works for MWEs.  Another aspect. \nAs stated above, CWI in Plainifier is not explained. \nWhat did you use for evaluations? \nIt seems no CWI was used in evaluation (annotated segments were taken directly as complexity sites?) \nPlease state it explicitly or explain.   Interestingly, tuning results in section 5.2 show that similarity and familiarity are more 'important' than fit with context, and that preservation of meaning (similarity) is most important by far. Since candidates are coming from BERT, this is not surprising. If candidates were coming from a synonym-resource, contextual-fit and familiarity would be more important.\nI scored \"Originality\" as 4. Much of the work is based on ideas in Qiang et al., but the provision of the corpus covinced me to go for 4.\nOverall, nice work!",
    "overall_score": "4",
    "confidence": "4"
  }
]