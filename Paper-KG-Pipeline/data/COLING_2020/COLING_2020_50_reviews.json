[
  {
    "review_id": "3da1f78e7f93de78",
    "paper_id": "COLING_2020_50",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- Proposed method is intuitive and well motivated, and explained clearly for the most part. There are several moving parts, but they are all tied together well.\n- Experimental results are very promising, and the provided examples illuminate the advantages of the proposed solution well.\n- In addition to automatic evaluation, the authors also took the effort to perform human evaluation and other attention-based analyses of the model.\n- Paper is well-structured and easy to follow.",
    "weaknesses": "1) There's very little dataset analysis in this paper, which makes it hard to know exactly how challenging the problems posed by these datasets really are. The original SAMSum dataset paper doesn't have any analysis either, which makes it all the more important to have some kind of analysis here in this paper. The other dataset used here (the Automobile Master Corpus) has neither an analysis nor a citation, and there are also no examples whatsoever of this dataset. Some questions that would be important to answer, at the very least, are:    a) What are the length distributions of the summaries and the dialogues respectively, and what's the relationship between the length of a dialogue and the length of its summary? \n  b) How many topic words does each dialogue have? How many of those actually occur in the final summary? \n  c) What are some interesting discourse phenomena that need to be correctly modeled in order to generate an accurate summary? \n2) There's no analysis of examples that this model performs poorly on, or other gaps that need to be addressed. \n3) While it's great that human evaluation was performed, it's lacking in a couple of different ways:   a) The human evaluation metrics should be described in further detail. What exactly do \"relevance\" and \"readability\" encompass? What were the guidelines given to the evaluators to rate these? \n  b) Were the examples double/triple reviewed in any way? \n  c) Related to (a) - another important aspect of a good summary is its completeness. Did either of the human evaluation metrics encompass completeness? \n  d) Human evaluation wasn't conducted on the gold summaries, which is unfortunate because it would provide a sense of an appropriate ceiling for the human evaluation numbers. \n  4) Some of the claims in the paper don't really seem to follow from the results of the experiments. For example, the paper makes the following claim from the human evaluation results:      \"As we can see, Pointer Generator suffers from repetition and generates many trivial facts. For Fast Abs RL Enhanced model, it successfully concentrates on the salient information, however, the dialogue structure is not well constructed. By introducing the topic word information and coverage mechanism, our TGDGA model avoids repetitive problems and better extracts the core information in the dialogue.\" \n   However, I don't know if any of these claims can be directly inferred from the results of the human evaluation (unless there are aspects of the human evaluation that were not described).\n5) For a model as complex as the one proposed here, I would have liked to have seen some kind of ablation analysis that shows the importance of each of the moving parts. While the proposed approach makes sense intuitively, there's not enough convincing experimental evidence to show that each of its parts is crucial (even though the results section seems to claim this).\nOther",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  }
]