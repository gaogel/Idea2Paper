[
  {
    "review_id": "87a3e229e569e6a8",
    "paper_id": "COLING_2020_75",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper presents experiments on modeling the language and writing style employed by twitter users in posts and descriptions, as representations used to predict whether news pieces shared by them are real or fake. The idea is well motivated and novel, and the paper reads very well. Experiments and modeling choices are described with adequate details and well justified and I particularly appreciated the further linguistic analysis and interpretation of the model (section 7).  My main concern refers to the lack of comparison with other work. Indeed the comparison with the baseline and the ablation tests adequately confirm the positive impact of user representations based on the language they use of fake news prediction. However, it would be further convincing and helpful to other researchers if there was a comparison with other work that does not use user linguistic representation and could put this approach into perspective.  Also, I am a bit concerned/curious regarding the data preprocessing and the impact of the authors' choices on the user selection. I am not sure how large the set of users excluded due to oversharing (>1 piece of news) is, but I am wondering if \"over-sharers\" of fake news would impact the identified linguistic features and the learned models. While the reasoning for excluding them is well explained and motivated, I would like to see a break down of the percentage of users excluded in the different preprocessing steps (perhaps as an extension to Table 1?) and even some comparative tests. I am not entirely sure that having users sharing more than one piece of news would be an issue, so long as these news pieces wouldn't be split across training and test data.\nFinally, as a minor comment, it is not clear how the topics for n-grams are chosen. are they attributed manually? It should be clarified in the main text.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "d1388d463fa663c3",
    "paper_id": "COLING_2020_75",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This work built a fake news prediction model using both news and user representation from user-generated texts. Experimental results showed that the user text information contributed to predicting the fake. Moreover, the paper showed linguistic analysis to show typical expressions by users in real and fake news. Cosine similarities between users are calculated using proposed user vectors to confirm the echo chamber effect. \n  Introducing vectors of news spreading users sounds an interesting idea. The paper's investigation, the user vector made from linguistic features contribute, is interesting and important. The results of active topics by users for both real and fake is also impressive. \n  There are some ways to build user vectors not only from their timeline and profiles but also from their tweets itself (e.g., Persona chat model). Does the proposed method have a clear advantage to such models?",
    "overall_score": "4",
    "confidence": "4"
  }
]