[
  {
    "review_id": "e91b87bc53fe1409",
    "paper_id": "COLING_2020_74",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- interesting idea; - going against the mainstream approaches to contextualized meaning representations; - the choices are well-motivated; - convincing error analysis; - well-motivated, solid background in lexical semantics; Limitations: The method is currently limited to nouns, and the constructed representations are 6-dimensional (for coarse supersenses). That clearly does not have enough expressive power for lexical semantics of all nouns, and is confirmed by the fact that MLP was able to make better use of more training data with FlauBERT. There is definitely room for improvement if the SLICE embeddings were more fine-grained, but the authors should discuss where the data for such improvements would come from. The choice of coarse senses was already motivated by data availability.\n- focus on monosemous words is necessary for pseudo-annotation, but arguably the classifiers were never taught to deal with polysemous words, and then tasked with WSD; - The description for Table 2 could be made more clearer; it is not obvious what the letters in the columns R, L, and C stand for; - Table 2: while the overall pattern for the linear model and MLP are the same, MLP reaches relatively high accuracy in all configurations. What does that say about the configurations themselves?\n- It would be great to include a section outlining how this could possibly be extended to other parts of speach. A big problem for interpretable representations is scoring words on inapplicable categories (e.g. \"animacy\" for prepositions); what's the plan for that?",
    "weaknesses": "",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "a24f22640c3b57be",
    "paper_id": "COLING_2020_74",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper proposes a hybrid model for the representation of contextual embeddings (BERT) that aims to disambiguate French nouns at supersense level. The model's performance does not improve the BERT state of the art performance on the same corpus (FlauBERT). However, it has the advantage of making embeddings interpretable, helping in the error analysis. In the error analysis section, the main finding is that errors are due to the polysemous nature of nouns: it is not surprising since polysemy is the primary source of errors for all the WSD systems. The paper does not propose how to improve on that aspect - just using a different corpus is mentioned as a possibility, but not changes in the methodology are tried or suggested -. In other words, even if the methodology is interesting and it is true that makes the embeddings less opaque, at the moment it is not able to guarantee improvements in the results because, from error analysis, it is not possible to get insights on how to improve the hybrid embedding representations of polysemous words.",
    "overall_score": "3",
    "confidence": "3"
  }
]