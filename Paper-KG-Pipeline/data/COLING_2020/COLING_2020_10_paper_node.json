{
  "paper_id": "COLING_2020_10",
  "title": "Referring to what you know and do not know: Making Referring Expression Generation Models Generalize To Unseen Entities",
  "conference": "COLING",
  "domain": {
    "research_object": "提升指代表达生成模型在未见实体上的泛化能力，增强模型理解与表达能力。",
    "core_technique": "采用知识推理与不确定性建模方法改进指代表达生成模型的泛化性能。",
    "application": "用于自然语言生成任务中的自动指代表达生成，提升对新实体的描述能力。",
    "domains": [
      "自然语言处理",
      "人工智能"
    ]
  },
  "ideal": {
    "core_idea": "提升指称表达生成模型对未见实体的泛化能力",
    "tech_stack": [
      "数据到文本生成",
      "指称表达生成",
      "泛化建模"
    ],
    "input_type": "结构化非语言数据及实体信息",
    "output_type": "针对实体的自然语言指称短语"
  },
  "skeleton": {
    "problem_framing": "论文通过定义数据到文本自然语言生成（NLG）及其子任务指称表达生成（REG），将研究置于自动化文本生成的背景下。引言部分以具体例子说明REG的重要性和应用场景，帮助读者理解任务的实际意义和研究价值。",
    "gap_pattern": "作者通过回顾传统REG系统的两步流程，指出现有方法在指称表达生成时的局限性，尤其是在实体表示和上下文利用方面。通过引用前人工作，明确提出当前方法在处理复杂实体描述和上下文建模上的不足，为后续方法创新埋下伏笔。",
    "method_story": "方法部分采用对比叙述策略，先简述前人NeuralREG的做法，再突出自身改进点：将目标实体由单一token扩展为多token描述，并引入实体类型和性别特征。通过分步说明输入、输出及建模方式，突出方法的创新性和针对性。",
    "experiments_story": "实验部分采用定量对比和案例分析相结合的策略。先通过表格展示与多个基线模型在不同指标下的全面对比，突出自身方法的优越性；再通过具体文本案例，直观展示生成效果，增强实验说服力和可读性。"
  },
  "tricks": [
    {
      "name": "分步阐述传统方法",
      "type": "writing-level",
      "purpose": "清晰介绍领域背景和传统流程",
      "location": "论文开头对REG系统的描述",
      "description": "将传统REG系统的流程分为两个明确步骤：首先选择指称形式（如代词、专有名、描述），然后根据语境实现具体文本表达。通过分步说明，有助于读者理解后续创新点。"
    },
    {
      "name": "对比传统与现代方法",
      "type": "writing-level",
      "purpose": "突出方法创新性和发展趋势",
      "location": "介绍数据驱动系统前后",
      "description": "先描述传统规则驱动的模块化架构，再说明数据驱动的端到端系统如何改变了REG系统架构，突出技术演进。"
    },
    {
      "name": "详细定义输入输出结构",
      "type": "method-level",
      "purpose": "明确任务建模及实现细节",
      "location": "方法部分对输入输出的描述",
      "description": "将输入（pre-context、post-context、实体描述、类型、性别）和输出（指称表达的token序列）用数学符号和集合明确定义，为模型设计和复现提供清晰基础。"
    },
    {
      "name": "引入实体多维特征",
      "type": "method-level",
      "purpose": "丰富实体表示，提升生成质量",
      "location": "方法部分描述实体表示",
      "description": "不仅用单一token表示实体，还引入实体的标识符token序列、类型和性别等多维特征，增强模型对实体的理解与区分能力。"
    },
    {
      "name": "采用Encoder-Attention-Decoder架构",
      "type": "method-level",
      "purpose": "利用神经网络捕捉上下文信息，实现端到端生成",
      "location": "模型架构说明",
      "description": "使用编码器-注意力-解码器结构，结合copy机制，提升模型对上下文和实体信息的融合能力，适应数据驱动的任务需求。"
    },
    {
      "name": "共享输入词嵌入矩阵",
      "type": "method-level",
      "purpose": "参数共享，提升训练效率与泛化能力",
      "location": "模型架构说明",
      "description": "模型各部分共享同一输入词嵌入矩阵，减少参数量，提高各输入源之间的语义一致性。"
    },
    {
      "name": "详细列出模型训练参数",
      "type": "experiment-level",
      "purpose": "方便复现与对比实验",
      "location": "模型训练参数说明",
      "description": "详细给出epoch数、dropout率、early stopping、beam size、最大输出长度、batch/state/attention size以及词嵌入维度等关键训练参数，增强实验可复现性。"
    },
    {
      "name": "分实体类别展示实验结果",
      "type": "experiment-level",
      "purpose": "更细致地分析模型表现",
      "location": "实验结果展示部分",
      "description": "不仅展示全部实体的实验结果，还分别展示已见和未见实体的结果，便于分析模型在不同泛化场景下的性能。"
    },
    {
      "name": "引用并对比前人方法",
      "type": "writing-level",
      "purpose": "突出自身改进点及贡献",
      "location": "方法介绍与相关工作引用",
      "description": "明确引用前人工作（如NeuralREG），并指出自身在实体表示等方面的改进，突出创新点。"
    },
    {
      "name": "适当使用数学符号和公式",
      "type": "writing-level",
      "purpose": "增强技术表达的严谨性与清晰度",
      "location": "方法建模与输入输出定义",
      "description": "用集合和符号（如X(pre), X(post), X(wiki), y）表示输入输出结构，使方法描述更规范易懂。"
    }
  ]
}