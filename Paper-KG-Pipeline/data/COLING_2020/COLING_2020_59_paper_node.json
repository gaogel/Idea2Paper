{
  "paper_id": "COLING_2020_59",
  "title": "Named Entity Recognition for Chinese biomedical patents",
  "conference": "COLING",
  "domain": {
    "research_object": "针对中文生物医学专利文本中的命名实体进行识别与抽取。",
    "core_technique": "利用命名实体识别（NER）技术，结合自然语言处理方法处理专利文本。",
    "application": "提升生物医学专利信息的自动化处理与知识抽取能力。",
    "domains": [
      "自然语言处理",
      "生物医学信息学"
    ]
  },
  "ideal": {
    "core_idea": "针对中文生物医学专利进行命名实体识别以促进知识发现",
    "tech_stack": [
      "命名实体识别",
      "生物医学自然语言处理",
      "专利文本处理"
    ],
    "input_type": "中文生物医学专利文本",
    "output_type": "生物医学实体及其类别标注"
  },
  "skeleton": {
    "problem_framing": "论文通过介绍生物医学命名实体识别（NER）在知识发现和药物研发中的重要性，强调该领域的研究热度和实际应用价值。引言部分以任务目标和实际应用场景为切入点，快速建立研究意义和背景。",
    "gap_pattern": "作者在回顾已有英文Bio-NER数据集和任务的基础上，指出中文生物医学NER研究尝试较少，数据资源匮乏，明确提出当前研究领域存在的空白和挑战，从而为后续工作奠定合理性。",
    "method_story": "方法部分采用分步骤叙述，先总述整体流程，再细分为数据收集、标注、模型设计与训练等环节。通过对比三种不同的BERT微调方法，突出创新点和实验设计的多样性，逻辑清晰，便于理解。",
    "experiments_story": "实验部分首先关注数据质量，解释为何采用互F1而非Kappa指标，体现对领域特性的理解。随后说明训练测试集划分原则和数据集规模，为实验结果的可信度和局限性提供背景说明，结构严谨。"
  },
  "tricks": [
    {
      "name": "领域背景与问题陈述",
      "type": "writing-level",
      "purpose": "明确研究背景，突出研究问题和意义",
      "location": "开头段落",
      "description": "通过介绍生物医学命名实体识别（Bio-NER）的研究现状及其在知识发现和药物研发中的重要性，阐明了研究动机，并指出中文领域在数据集和实体类别定义等方面的不足，从而引出本文的研究重点。"
    },
    {
      "name": "相关工作与数据集梳理",
      "type": "writing-level",
      "purpose": "展示已有成果，突出自身工作的创新点和差异性",
      "location": "背景介绍部分",
      "description": "列举了英文Bio-NER的主流数据集和任务（如GENIA, JNLPBA, BC2GM），并分析了中文Bio-NER研究的局限性（例如数据类型、规模、实体类别预定义等），为后续研究工作提供铺垫。"
    },
    {
      "name": "多样本源数据集构建",
      "type": "method-level",
      "purpose": "提升模型泛化能力，覆盖更多实际应用场景",
      "location": "方法部分",
      "description": "不仅关注临床文本和科学出版物，还强调专利文本的重要性，扩展了数据来源，使模型能够适应更复杂的生物医学文本。"
    },
    {
      "name": "多策略模型训练方法设计",
      "type": "method-level",
      "purpose": "比较不同训练方法的效果，提升模型性能",
      "location": "方法部分",
      "description": "设计并实现了三种不同的训练策略，包括全监督微调、语言模型混合微调和部分BERT+CRF微调，分别针对不同的数据利用方式和模型结构进行优化。"
    },
    {
      "name": "预训练模型迁移",
      "type": "method-level",
      "purpose": "利用大规模通用语料提升下游任务表现",
      "location": "方法部分",
      "description": "采用在中文维基百科训练的BERT-base-Chinese作为基础模型，在领域数据上继续微调，使其更适合生物医学领域的命名实体识别任务。"
    },
    {
      "name": "数据集划分与子集构建",
      "type": "experiment-level",
      "purpose": "提升训练效率，便于实验对比和调参",
      "location": "方法部分",
      "description": "从大规模无标注数据中构建两个较小的子集（partBC和partHG），并进行训练集和测试集划分，以便于快速实验和评估。"
    },
    {
      "name": "复用与集成开源工具包",
      "type": "experiment-level",
      "purpose": "提高实验效率与可复现性",
      "location": "实验实现部分",
      "description": "集成使用FlairNLP、PyTorch版BERT和Huggingface实现，减少重复造轮子，保证实验的效率和结果的可复现性。"
    },
    {
      "name": "微调学习率选择",
      "type": "experiment-level",
      "purpose": "提升模型训练稳定性和效果",
      "location": "模型训练描述",
      "description": "在微调BERT及NER层时，选择较小的学习率（5*10^-5），以适应小规模标注数据，防止过拟合和模型退化。"
    },
    {
      "name": "任务流程结构化描述",
      "type": "writing-level",
      "purpose": "清晰展示研究流程，便于读者理解",
      "location": "方法部分开头",
      "description": "将数据收集与清洗、数据标注、模型与学习方法、评估、后分析等步骤结构化描述，提升论文的逻辑性和可读性。"
    }
  ]
}