[
  {
    "review_id": "eec4304e9d51f5ec",
    "paper_id": "COLING_2020_59",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper is about named entity (genes, diseases) recognition of Chinese biomedical texts. The authors present a large collection of unannotated patents, and a small subset (5k sentences, 21 documents) manually annotated. The paper also includes experiments with named entity recognition, and relation extraction (between the named entities of interest).\nThe data set presented (released publicly) is likely to be useful for others; the NER method follows a rather standard, but reasonable, approach; and the relation extraction experiments, although preliminary, may be of interest to NLP community as well. The paper is generally well structured, and well written (see below for a few detailed comments).\nMy only (minor) reservation of the paper is that it may be slightly too specific and technical for COLING. The paper mainly is about a practical NLP problem, involving rather little (computational) linguistics.\nI have some detailed comments/suggestions below.\n- I could not fully understand the averaging. In particular, does the   averaged scores on Table 4, include 'O' label? It seems not (good),    but it would be best to explicitly state it. Related to this, is   there a reason for reporting micro-averaged scores? Clearly there is   no clear preference towards recognizing majority class(es) better   than the minority class. \n   - The NER results presented show really large variation over the cross    validation folds. With the variation at hand, I think it is very   difficult to decide the \"best\" model with certainty. Hence, I   suggest: (1) softer conclusions, and (2) maybe trying model   averaging or a similar method to reduce the variation. I definitely   like that authors reported the variation in the scores, the   suggestions should not be understood as artificially reducing it   (e.g., testing on a single validation split), or definitely not    reporting the variation. \n   - The authors report that they reserve one of the documents as 'dev   set'. However, the results presented are 5-fold CV results, and   there is no report of additional hyperparameter tuning. Hence, it   sis not clear what the purpose of the dev set is.\n- The 'mutual F1 score' used for IAA is not defined, nor there is a   reference in the paper.\n- More a question than a criticism: the authors present code-mixing as    a difficulty. Isn't code-mixing helpful for NER? I'd expect more   code mixing in named entity boundaries, hence, overall I expect it   to be helpful (although, it may be specific to the domain/task).\n- I suggest also specifying the sources (BC/HG) of the annotated data    set in Table 2 (like in Table 3).\n- Typos:     - I suggest use of more descriptive words or abbreviations in       table headings. \n    - Section 3.2, paragraph 3: \"entity type type\" -> \"entity type\"     - Suggest using (or not using) punctuation after captions       consistently. \n    - The references contain some lowercase \"named entities\", like       'bert', 'chinese' (likely BibTeX normalization issues), I       recommend a through check.",
    "overall_score": "4",
    "confidence": "4"
  }
]