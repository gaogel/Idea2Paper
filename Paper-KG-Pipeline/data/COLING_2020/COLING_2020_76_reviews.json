[
  {
    "review_id": "d082a6bc04471482",
    "paper_id": "COLING_2020_76",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "In the present paper, the authors describe the results of a quantitative analysis of various genres in terms of coreference. They analyse a number of coreference-related features and compare the genres from the point of view of their distribution. The aim is to find the differences between spoken and written genres. The work is interesting and useful, as the number of studies of coreference in spoken texts is limited. The paper address a number of important issues in both coreference analysis (e.g. how should the distance be measured) and the analysis of spoken language. As the authors use a number of existing resources, they also assure the comparability of the categories used. Here, it would be interesting to know, if there were any problems, e.g. if there were still some incompatible categories that the authors came across.\nSpecific comments I like the discussion about the variation in distance measured by different means at the beginning of section 2. Specifically, in a cross-lingual task, token-based measure is a problem. However, there could be differences across studies using various metrics. If measured in sentences or clauses, the distance may vary depending on a genre, if there is a variation in sentence length in terms of words (in spoken texts, there could be shorter sentences, etc.). The question is, if the distance should be measured in characters, but I believe that the decision depends on the conceptual background on what one wants to find out.\nAnother point in Section 2 in the discussion of the diverging results could be variation within spoken and written texts various authors use in their analysis. There could be further dimensions that have an impact on the choice of referring expressions in a language, e.g. narrativeness, if there are dialogues or monologues, etc.\nConcerning related work, Kunz et al. (2017) point out that coreference devices (especially personal pronouns) and some features of coreference chains (chain length and number of chains) contribute to the distinction between written and spoken registers.\nThere are several works concerned with lexicogrammar suggesting that distinction between written vs. spoken, and also between formal vs. colloquial are weak in English  (Mair 2006: 183).\nTable 1: the statistics on different parts of OntoNotes and the total number in OntoNotes are given in one table in the same column formatting, which is slightly misleading. \n  4.1: large NP spans vs. shot NP spans – sometimes only heads of  nouns or full NPs are considered.\nReferences to examples: 1→ (1), etc.\nPersonal pronouns: 1st and 2nd person pronouns are not considered in the analysis of coreference in some frameworks. The authors should verify which cases they include into their analysis.\nThe finding about NPs being more dominant is not surprising (and was also expected by the authors) and has also something to do with the fact that spoken texts reveal a reduced information density if compared to the written ones.\nThe discussion about the results on spoken vs. written is good and important. Even within written text, there could be a continuum, e.g. political speeches, which are written to be spoken or fictional texts that contain dialogues (as the authors point out themselves), could be closer to further spoken texts. At the same time, academic speeches or TED talks that contain less interaction with the audience (depending on a speaker’s style) could be closer to written texts, also in terms of referring expressions – we would expect them to contain more NPs, and probably complex Nps describing some notions.\nOverall, it is interesting to know if there are more dimensions than just the difference between spoken and written in the data, e.g.  narrativeness (narrative vs. non-narrative) or dialogicity (dialogic vs. monologic), etc. In fact, genre classification can and should be sometimes more fine-grained than just drawing a rigid line between texts that are considered to be spoken and those that are considered to be written.\nTextual problems: Page 2, Section 2: interfering mentions. - These → There are some typographical problems in the text.\nPage 3, Section 3: I am not sure if the abbreviation Sct. is allowed by the Coling style. \nIn the reference list, the authors should check spelling of the some entries, e.g. english→ English in Berfin et al. (2019), Zeldes (2018). There is an empty space in Godfrey et al. (1992).\nCited references: Kunz, Kerstin and Degaetano-Ortlieb, Stefania and Lapshinova-Koltunski, Ekaterina and Menzel, Katrin and Steiner, Erich (2017).  GECCo -- an empirically-based comparison of English-German cohesion. In De Sutter, Gert and Lefer, Marie-Aude and Delaere, Isabelle (eds.), Empirical Translation Studies: New Methodological and Theoretical Traditions. Mouton de Gruyter, pages 265–312.\n@BOOK{Mair2006,   title = {Twentieth-Century English: History, Variation and Standardization},   publisher = {Cambridge University Press},   year = {2006},   author = {Mair, Christian},   address = {Cambridge} }",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "d17520c5626fdc7a",
    "paper_id": "COLING_2020_76",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper presents a corpus study of coreferences comparing different genres (news, blogs, conversations) and media (written, transcribed speech, microblogging) based on the Ontonotes and Switchboard corpora and a dataset from Twitter sub-threads. The analysed factors include the use of pronouns and noun phrases, the characteristics of such NP mentions (syntactic complexity) and various distances measured between mentions of the same entity.  This is an interesting study, and could be potentially useful for models trying to do domain adaptation, as coreferecen systems for written text perform poorly on conversations and microblogging. \nOverall it seems the contributions are only moderately significant however, for the following reasons: (1) the paper builds on two papers:  (Aktas et al., 2018), where the twitter data was collected and described, and (Aktas et al., 2019) which described coreferences in  Ontonotes sub-corpora/genres in what I assume is a similar manner (the paper is not freely available, only the abstract). It is not clear how the present paper adds to these papers, and should be made more explicit. \n(2) the interest for coreference model is rather vaguely described, and it would have been interesting to have a more detailed descriptions of how the knowledge derived from the study could be used in such models. The paper mentions experiments using models trained on written texts applied to other genres/media, how hard would it have been to experiment training on other data, or to combine them ? \nThis seems too preliminary to assess the real interest for automated models.  More minor points: - the introduction is rather strangely constructed, and almost reads as a post-introduction section/a related work section already. The context should be made clearer and a few examples wouldn't hurt.\n- i'm not sure I understand the term \"coreference strategies\", which seem to imply an intentionality in the way coreferences are produced in different contexts. A lot of what is shown in the paper could be attributed to more general aspects of the genres/media (longer sentences for purely written text, more context available, etc) and some of the properties of coreferences could just be by-product of that. The use of specific personal pronouns (1st/2nd/3rd) is another example.  - there is zero descriptions of the statistical tests used, and of the assumptions made, if a parametric model was used. This should be addressed. Also some conclusions are based on multiple-testing, which should include some kind of correction (it might have been done, but again, there is zero details about this).  - some technical details are presented a little vaguely, which could be understood given size constraints, but sometimes it is a bit too much: for instance, instead of explaining what hierarchical clustering method was applied, the paper only mentions using some R implementation with default settings, which is rather uninformative.  - about the clustering, why not cluster on all the dimensions at the same time ? ( with some normalization of features of course) Details: - Tables/figures have rather cursory captions. For instance table 1 coud recall the meanings of abbreviations for all sub-corpora, especially from Ontonotes. It is also not a good idea to have ontonotes as a whole *and* all the subcorpora without making it clear.  - section 3.1, the paper mentions the use of a sentence splitter from (Proisl and Uhrig, 2016) which is a German sentence splitter ?\n- table 2: why not give relative (to corpus size) frequency instead of absolute frequency ? this would make it easier to interpret.",
    "overall_score": "2",
    "confidence": "4"
  }
]