[
  {
    "review_id": "e90565a81657b82b",
    "paper_id": "COLING_2020_85",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "- First line of page 2, this bit does not seem very fluent, maybe something is missing, please check and amend if/as needed: \"... enabling us to easily create large number of coherent test examples...\" - I am dubious about the orthodoxy of using the image with the pussycat's face in the title as well as to replace the name of the project/system in the running text of the paper: while it may look cute (to some), I suspect that other readers (like this innovation-averse reviewer) may find it annoying, if not outright inappropriate. If the paper is accepted (as incidentally I think it should be), I think that the author(s) would be well advised to consult with the conference chairs and proceedings editors to check that this rather unusual practice is deemed acceptable and technically feasible for the proceedings.\n- Similar to the previous point, I wonder if the deliberately eccentric heading of the very short and merely descriptive section 3 (i.e. \"Do Androids Dream of Coreference Translation Pipelines?\", which consists of merely 14 lines) is entirely justified and appropriate for a paper to be published in conference proceedings, although I accept that to some extent these are matters of personal taste.\n- Possible typo on page 2, please check and edit as required: \"follows this work, but _create_ the challenge set in an automatic way.\" >>> should it be 'creates'?\n- The last paragraph of Section 3 on page 3 seems to include some typos such as the following, so please check and amend the passage as appropriate: \"The coreference steps resembles\" (one of the last two words is incorrect), \"the rules-based approach\" ('rule-based'?), \" each of these phenomenon\" ('phenomena'?).\n- The caption of Figure 1 on page 4 seems too (and unnecessarily) long: a caption is a caption; data analysis and comments should be given in the running text of the paper, referring to the figure whose data is being illustrated and discussed.\n- Line 3 on page 5: \"This _filters_ our subset to 4,580 modified examples.\" >>> For clarity, should 'filters' be 'reduced' (or something similar) instead?",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "fc4493ef6f6a54e0",
    "paper_id": "COLING_2020_85",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper provides a thorough analysis of coreference resolution capabilities of a simple context-aware NMT system on the basis of two datasets: (1) an adaptation of the existing ContraPro test suite, and (2) a new test suite called ContraCAT consisting of examples automatically generated from templates. The results from the adapted ContraPro shows that NMT systems are easily distracted by the addition of irrelevant phrases. The results from ContraCAT show that NMT performs much better at certain steps of a hypothesized coreference resolution pipeline than on others. Finally, the authors perform some targeted training data augmentation, but I found it hard to fully understand that part without resorting to the appendix.\nThe paper is clearly written and in large parts easy to follow. The results are insightful, but are hampered by the general problem of using artificially created sentences (rather than naturally occurring ones -- how often do you see apple-eating cats in OpenSubtitles?) and by the small number of evaluated models (see below).\nQuestions and suggestions: - There are a few details missing in the adversarial attack generation: how many distinct phrases are used for phrase addition? Do you randomly choose one phrase for a given ContraPro example, or do you combine every example with every phrase? Also, your claim that \"it is true\" necessarily introduces an event reference may not hold (which you acknowledge later on).\n- Your work assumes that context-aware NMT models should follow the coreference resolution pipeline depicted in Table 1. How \"universally accepted\" is this pipeline in the CR field? Couldn't some end-to-end model provide equivalent CR performance than a pipeline?\n- I understand that the emphasis of this paper is on the resource construction and its analysis, but I would find it instructive to have two additional systems in your analysis: (1) a pure CR system that only operates on the source side, and (2) a somewhat more sophisticated context-aware NMT model than the concatenation one. Your resources are well adapted to compare different approaches to document-level NMT, and it would be a pity not to take advantage of that.\n- How well are the vocabulary items of your templates represented in the training data? Did you check this, and did you find any influence on the results?\n- In the category \"MT test suites with animal names\", there is also MuCoW (Raganato et al., LREC 2020), but it might be thematically too far away to warrant citation... Typos, presentation and style: - I find the first half of the Abstract somewhat incoherent, please consider revising it.\n- ยง1: Your consideration applies to NMT models in general, not only those based on Transformers, I would say.\n- Table 1: Is it on purpose that cat and actor are swapped between Step 2 and 3?\n- It would be practical if the categories were named the same way in Table 2 and Figure 2.\n- As mentioned above, I was not able to follow your data augmentation strategy properly. Where do you take your data from, and what exactly do you generate as additional instances?",
    "overall_score": "4",
    "confidence": "4"
  }
]