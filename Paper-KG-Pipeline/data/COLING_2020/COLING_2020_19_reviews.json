[
  {
    "review_id": "0919e028d112ab4c",
    "paper_id": "COLING_2020_19",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper presents a neural unsupervised dependency parser that incorporates sibling/grandparent information into the parser's decisions. This second-order information is incorporated into an unlexicalized model that is trained jointly with a lexicalized single-order model.  The work describes two training approaches: one based on EM and one based on direct optimisation of the log-likelihood of the input. There are experiments conducted on the WSJ corpus, where the authors observe state-of-the-art performance, and on the UD corpus where the model does not perform to the sota. There are also some analysis showing the effect of skip connections, the training dynamics for the different methods and the effect of joint parsing.\nI am not at all familiar with the parsing literature, so this is going to be an educated guess. The paper seems to be doing a relatively good job at describing their approach, although it is so complex, and -as the authors show- so sensitive to small variations, that I would doubt the results are easily reproducible. However, that might be the case for a large corpus of current work as well, so I don't see a reason for penalising this paper on that ground. The writing is also relatively clear, although the structure is sometimes hard to follow. For instance, when describing the different training methods it is a priori unclear that the authors are discussing several alternatives, and this could have been specified in advance.  One important aspect that I find missing is a bit of discussion and analysis of the kinds of decisions that are favoured by having second-order information. Also, and more importantly, while the grand-parent node is unique and clearly defined, I have absolutely no idea of how the authors choose the sibling element that goes into the model. Incidentally, this is the one variant that performs the best. It would be very important for the authors to clarify that.\nA few other minor points: - I gather that you have one single embedding layer for all three subnetworks, right? Otherwise it would not make sense to perform a linear transformation on the embeddings.\n- In Section 3.2, I do not understand what kind of product you are using on the equation defining S(p, s, c) - I guess this is standard, but in eq. 3 you seem to be making a simplifying assumption that rules are independent (whilst this is not strictly true because they are sampled following a path).\n- The discussion on online-EM confused me a bit. Are you saying that online-EM is part of the DMO procedure?\n- Figure 5 is interesting, as it shows some immediate room for improvement. Have you looked at how do the model's predictions change from one iteration to another when the drop occurs? This may show what kind of modelling capacity is in the model that does not align well with good dependency parses, and which it could be trimmed in the future.\n- Reporting also the sota results on UD would be more intellectually honest, as they are reported when the authors surpass them (in WSJ), but not when they don't.\nTypos Section 5.4: quiet -> quite",
    "overall_score": "4",
    "confidence": "2"
  }
]