[
  {
    "review_id": "826f19cf6a6779d8",
    "paper_id": "COLING_2020_1",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- The novel architecture that combines two types of memories for efficient processing and aggregation of information from the user reviews. The Figure 3 is a particularly good example of the way the new architecture works.",
    "weaknesses": "- The method focuses on a single data set/language. No perspective on the applicability of the given method beyond the very specific AmazonQA task is given.\n- The experimental set-up description is missing several crucial details: Why was evaluation limited to the validation set of AmazonQA? Was the model applied on the test set as well?  - Baselines include a BERT-based retrieval, but not a simple technique such as TF/iDF.\nSuggestions/Questions for the authors: - It would be great to include more examples like Figure 3. I have found it very insightful.\n- Section 3.1 \"whether answerable\" -> \"if it is answerable\" - Section 3.2 \"As has been shown in (Petroni et al. 2019), pre-trained LMs cam be used as implicit knowledge bases\" - I believe Petroni et al. actually show that it is not necessarily the case, so not sure if this argument holds.\n- Section 4.1 BLEURT seem to have negative values? What is the range of the metric? Is higher better?\n- Section 4.2 You mention that CHIME consumes less memory. I think it is an important advantage and you should expand on this comparison.  - Section 4.2",
    "comments": "",
    "overall_score": "4",
    "confidence": "3"
  }
]