{
  "paper_id": "COLING_2020_25",
  "title": "Exploring Question-Specific Rewards for Generating Deep Questions",
  "conference": "COLING",
  "domain": {
    "research_object": "针对自动生成深层次问题的奖励机制进行探索和优化。",
    "core_technique": "采用问题特定的奖励函数来提升深度问题生成模型的性能。",
    "application": "用于教育、智能问答系统和辅助学习等场景中的深度问题自动生成。",
    "domains": [
      "自然语言处理",
      "教育技术"
    ]
  },
  "ideal": {
    "core_idea": "提出问题特定奖励机制提升自动深度问题生成质量",
    "tech_stack": [
      "深度学习",
      "强化学习",
      "自然语言处理"
    ],
    "input_type": "文本或文档内容",
    "output_type": "针对输入内容生成的高质量深度问题"
  },
  "skeleton": {
    "problem_framing": "论文通过定义Question Generation（QG）的目标和实际应用场景（如教育、对话系统、FAQ等）来引入研究问题，强调其现实意义和广泛用途。引用大量前人工作，展示QG领域的研究基础和发展脉络，帮助读者快速理解问题的重要性。",
    "gap_pattern": "作者通过回顾现有方法，指出当前主流方法多采用Seq2Seq模型，并简要提及其局限性，如生成高质量问题的挑战。通过对比前人工作，暗示现有方法在生成相关、流畅且可回答问题方面仍有提升空间，从而引出自身研究的必要性。",
    "method_story": "方法部分采用自上而下的叙述策略，先从任务定义和数学建模切入，明确目标函数。随后介绍整体框架及其组成部分，结合图示结构，逐步细化到具体机制（如attention、copying、coverage），并通过引用相关文献表明方法的合理性和创新点。",
    "experiments_story": "实验部分以数据集选择和任务难度为切入点，突出实验的挑战性和代表性。详细说明数据预处理和分割方式，确保实验可复现。接着介绍模型配置和实现细节，引用相关工作，保证实验的严谨性和与前人工作的可比性。"
  },
  "tricks": [
    {
      "name": "文献综述和应用场景引入",
      "type": "writing-level",
      "purpose": "展示研究背景和实际意义，增强论文说服力",
      "location": "开头段落",
      "description": "通过引用多篇相关文献，介绍Question Generation（QG）的实际应用场景，如教育、对话系统、FAQ和数据集构建，明确研究的重要性和实用性。"
    },
    {
      "name": "现有方法梳理与问题指出",
      "type": "writing-level",
      "purpose": "明确现有主流方法和存在的核心问题，引出创新点",
      "location": "背景介绍与方法动机部分",
      "description": "对比现有Seq2Seq+Attention方法，并指出其训练目标仅基于似然函数，存在exposure bias，无法覆盖所有等价问题表达。"
    },
    {
      "name": "问题公式化",
      "type": "method-level",
      "purpose": "将任务数学化，便于后续模型设计和优化目标明确",
      "location": "方法介绍部分",
      "description": "将QG任务形式化为条件概率最大化问题，给出公式Ŷ = argmax_Y P(Y|D)，并详细说明序列生成的概率分解方式。"
    },
    {
      "name": "模块化模型结构设计",
      "type": "method-level",
      "purpose": "结构清晰，便于突出创新点和模型扩展",
      "location": "模型框架介绍部分",
      "description": "将模型分为Question Generator和QG-specific Rewards两大模块，分别负责问题生成和奖励优化，结构层次分明。"
    },
    {
      "name": "多机制集成提升生成质量",
      "type": "method-level",
      "purpose": "提升生成文本的多样性和准确性",
      "location": "Question Generator细节描述部分",
      "description": "在Seq2Seq基础上，集成Attention、Copying和Coverage机制，结合前沿NQG工作，增强模型对输入的理解和信息覆盖能力。"
    },
    {
      "name": "奖励函数设计",
      "type": "method-level",
      "purpose": "针对目标任务设计专属优化目标，弥补传统损失函数的不足",
      "location": "QG-specific Rewards部分",
      "description": "设计三个QG特定奖励函数，分别评价生成问题的流畅性、相关性和可回答性，更贴合任务需求，引导模型生成高质量问题。"
    },
    {
      "name": "强化学习优化与自批判训练",
      "type": "method-level",
      "purpose": "解决暴露偏差问题，提升生成序列的多样性和实用性",
      "location": "模型训练策略部分",
      "description": "采用自批判序列训练（SCST）算法，在预训练基础上用强化学习优化奖励函数，提升模型泛化能力和生成质量。"
    },
    {
      "name": "预训练-微调范式",
      "type": "experiment-level",
      "purpose": "提升模型性能，利用已有知识加速收敛",
      "location": "训练流程描述部分",
      "description": "先用最大似然训练进行预训练，再用强化学习和奖励函数微调，借鉴语言生成领域主流实践，确保模型训练稳定和效果优异。"
    }
  ]
}