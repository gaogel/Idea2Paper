[
  {
    "review_id": "1b6b577ca1497d32",
    "paper_id": "COLING_2020_38",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper presents a study on how summarization evaluation metrics correlate. The authors conduct experiments showing that the metrics may disagree in any “scoring range” and that the metrics are worse as the summaries are more abstractive, extending a previous initiative of Peyrard (2019).\nThe ideas are clearly presented and the paper makes its point. The results are somewhat expected, but, given it is a short paper, the overall contribution is good. I comment two issues that might be improved in the paper: - although the authors clearly explain the EoS and abstractiveness metrics in the paper, they do not do the same for coverage. Please, let it clearly explained in the paper.\n- I was convinced about the paper conclusions, but I missed some considerations on the potentialities of the metrics and reasons for some of them to agree more than others. For instance, we may see that ROUGE-1 and ROUGE-L are more likely to agree. Why do you think that this happens? What other conclusions we may take for each metric based on its correlation to other metrics? A metric that correlates more with other metrics is a better metric?",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "0f8f231128d560a4",
    "paper_id": "COLING_2020_38",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The authors present a refined study on summarization evaluation metrics. The work is straightforward and well presented. The main result I would say is that we need to collect human judgements, which this, and most other attempts to assess the quality of automatic summarization techniques unfortunately does not.",
    "overall_score": "4",
    "confidence": "4"
  }
]