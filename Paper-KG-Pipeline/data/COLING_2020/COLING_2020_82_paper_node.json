{
  "paper_id": "COLING_2020_82",
  "title": "Modality Enriched Neural Network for Metaphor Detection",
  "conference": "COLING",
  "domain": {
    "research_object": "该论文研究多模态神经网络在隐喻检测任务中的表现与方法。",
    "core_technique": "采用融合多种模态信息的神经网络模型，以提升隐喻识别的准确性。",
    "application": "可应用于自然语言处理中的文本理解、情感分析及自动内容生成等场景。",
    "domains": [
      "自然语言处理",
      "人工智能"
    ]
  },
  "ideal": {
    "core_idea": "融合多模态信息的神经网络提升隐喻检测准确率",
    "tech_stack": [
      "神经网络",
      "多模态融合",
      "隐喻识别"
    ],
    "input_type": "文本及其他模态数据（如图像、语音）",
    "output_type": "文本中隐喻的检测结果"
  },
  "skeleton": {
    "problem_framing": "论文通过强调隐喻在日常语言中的普遍性和无意识性，引出其在认知和交流中的核心作用。引用Lakoff等权威理论，将隐喻定位为理解世界和结构化经验的基本机制，从而凸显研究隐喻的重要性和现实意义。",
    "gap_pattern": "作者通过回顾隐喻相关理论和现有方法，暗示传统方法在处理复杂隐喻识别时的局限性，尤其是在语义迁移和多模态信息整合方面未能充分捕捉隐喻本质，进而为提出新模型奠定理论和实际需求基础。",
    "method_story": "方法部分采用逐步递进的叙述策略，先介绍模型整体架构与关键技术（如LSTM、注意力机制），再详细说明各模块功能及其在隐喻识别中的作用。通过公式和流程图，增强方法的透明度和可复现性，突出创新点。",
    "experiments_story": "实验部分通过明确的数据集划分和基线设定，突出模型评估的科学性。先对比传统方法和各子模型，逐步展示新模型的性能提升，最终用量化指标（如F1提升）突出方法有效性，形成递进式论证结构。"
  },
  "tricks": [
    {
      "name": "引言中定义与背景引入",
      "type": "writing-level",
      "purpose": "为研究设定理论基础，帮助读者理解研究对象的重要性和背景",
      "location": "论文开头关于隐喻的定义和Lakoff的理论引用",
      "description": "通过引用权威文献（如Lakoff等人）介绍隐喻的认知机制和语言属性，为后续研究方法和实验设计提供理论依据。"
    },
    {
      "name": "明确提出研究问题和创新点",
      "type": "writing-level",
      "purpose": "突出本研究与已有工作的区别和创新，吸引读者注意",
      "location": "提出‘我们提出一种语言学增强的深度学习模型’及对已有工作的延续和改进",
      "description": "在介绍相关研究基础上，明确指出本研究在已有工作的基础上进行了哪些拓展（如引入modality norms），强调研究的创新性。"
    },
    {
      "name": "采用标准公开数据集",
      "type": "experiment-level",
      "purpose": "保证实验结果的可比性和可复现性",
      "location": "说明使用VUA corpus及其共享任务数据集",
      "description": "详细说明数据集来源及其在领域中的权威性，使得实验结果具有说服力和可与其他方法直接比较。"
    },
    {
      "name": "特征融合输入表示",
      "type": "method-level",
      "purpose": "增强模型输入的表达能力，提升模型性能",
      "location": "描述用glove嵌入和modality向量作为输入，进行拼接",
      "description": "将词嵌入和语言学特征（modality向量）进行拼接，作为模型输入，使得模型能同时利用分布式语义信息和语言学知识。"
    },
    {
      "name": "引入注意力机制",
      "type": "method-level",
      "purpose": "自动聚焦于输入序列中与任务最相关的信息，提高模型表现",
      "location": "详细描述attention机制如何计算权重并与LSTM输出结合",
      "description": "在BiLSTM的基础上加入attention机制，对每个词的隐藏状态分配权重，生成加权句子表示以提升分类性能。"
    },
    {
      "name": "损失函数正则化",
      "type": "method-level",
      "purpose": "防止模型过拟合，提高泛化能力",
      "location": "损失函数部分包含L2正则项",
      "description": "在交叉熵损失函数中加入L2正则化项，对参数进行约束，减少模型在训练集上的过拟合。"
    },
    {
      "name": "模型结构图示说明",
      "type": "writing-level",
      "purpose": "帮助读者直观理解模型架构",
      "location": "提及Figure 1展示模型结构",
      "description": "通过图示展示模型各部分（如输入、拼接、LSTM、注意力、线性层等）及其关系，增强论文的可读性和表达力。"
    },
    {
      "name": "详细步骤化方法描述",
      "type": "writing-level",
      "purpose": "让方法流程清晰易懂，便于复现",
      "location": "逐步描述从输入到输出的处理流程",
      "description": "系统性地描述每一步数据处理和模型计算（如输入拼接、LSTM处理、注意力加权、线性层、softmax输出），确保方法可操作性和复现性。"
    },
    {
      "name": "与前人工作的对比与延续",
      "type": "writing-level",
      "purpose": "展示研究的连续性和改进点",
      "location": "说明基于WAN et al., 2020工作的延续和改进",
      "description": "在方法介绍中明确指出本研究对前人工作的继承与拓展，体现学术研究的传承与创新。"
    }
  ]
}