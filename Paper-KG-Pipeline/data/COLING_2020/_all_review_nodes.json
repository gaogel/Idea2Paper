[
  {
    "review_id": "6cbc68c283c23340",
    "paper_id": "COLING_2020_0",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- Interesting new dataset - both for new phenomena, and a language pair with limited resources for this type of evaluation - Solid evaluation using the new dataset, to come to interesting conclusions about a range of NMT techniques I think \"brand-new solution\" is a bit of over-selling your point. The work that you mentioned are related to yours, and share some aspects. So I think you can phrase this a bit differently.\nIt is not clear to me if there is always one phenomena per sentence, or if there can be more. Can you please specify!\nIn 5.1 I think you mean an improvement of 1.2 Bleu points, not 1.7 (DeepL vs Google), or otherwise I misunderstood the comparison.\nSection 6 has a weird title: \"Discussion\", which implies that it contains a discussion of the previous parts. I would call it something actually related to the contents of the section, like \"correlation with human evaluation\".\nPlease provide a reference to the robustness shared task when it is first mentioned in the introduction.\nThere are some grammatical issues, here are issues from the abstract: \"plentiful of noises\" -> \"considerable noise\" \"these input\" -> \"these types of input/expressions\" \"Though its importance being recognized\" -> \"Though its importance has been recognized\"",
    "weaknesses": "",
    "comments": "",
    "overall_score": "5",
    "confidence": "4"
  },
  {
    "review_id": "07cd0df768d6b1cd",
    "paper_id": "COLING_2020_10",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper focuses on referring expression generation. The main contribution is an extension to an influential, end-to-end encoder-decoder REG model, NeuralREG by Castro-Fereira et al, to handle previously unseen entities at test time. In its original formulation, NeuralREG was restricted to generating references for entities it had seen during training time. To deal with this, the present paper proposes a number of extensions, namely: 1. Input data representation: rather than an identifier, input entities have a richer representation, consisting of a sequence of tokens and wiki-derived information including type and gender, which therefore constitute additional inputs (in the form of embedding representations), together with the = representation of pre- and post-context computed by a biLSTM encoder. \n2. Encoder-decoder with copy: like the original NeuralREG architecture, an encoder-decoder architecture with attention and shared word embeddings is used, but augmented with a copy mechanism, which would allow the decoder to 'copy' the input entity to the output, or alternatively, to generate a referring expression for it.\nThe neural architecture is very clearly described. Insofar as it is an extension to an existing model, and uses a well-established copy mechanism from See et al (2017), one could characterise this work as incremental in nature. However, the work is rigorously evaluated against strong baselines, including the original Castro Fereira approach (without copy) and the more recent ProfileREG of Cao and Cheung (2019). Sensibly, the evaluation distinguishes between seen and unseen cases, showing that the copy mechanism contributes to better scores on the automatic evaluation metrics used. This is followed up by a human evaluation which, despite including only two partiicpants, seems to have been carried out quite rigorously. Unfortunately, the analysis of these results is somewhat sketchy. I don't buy the statement that \"our prposed model outperformed the previous version and the current state-of-the-art\". As the authors themselves state in the same sub-section, this is only clearly the case for the Grammaticality criterion. On Fluency and Semantic Adequacy, results of the new model are very close to the best model, namely the one using OnlyNames, but this doesn't warrant the blanket statement that the model outperforms all baselines. In fact, it is somewhat surprising that OnlyNames should come out top on the Semantic Adequacy criterion, when one would expect that the repetition of names is bound to exact a cost in human judgment. This, however, could be due to the fact that this baseline was augmented to handle pronouns - the section discussing the evaluation results could really do with more discussion of these issues.\nI very much appreciated the inclusion of an ablation study. The results here are interesting, and altogether interpretable. E.g., it's to be expected that the biggest drop in accuracy on test data for unseen entities would come with the ablation of the copy mechanism. This does, however, seem like a vanishingly small drop compared to other parts of the ablation. Is copy really doing all that much after all?  There were a few open issues which the paper could benefit from addressing: - is the wikipedia lookup required to establish entity type and gender always successful, that is, are there any cases where this information is unavailable? What is the impact of this on the evaluation results?\n- Section 5.2 detalis a number of hyperparameters. Were these established based on a tuning procedure, or were they set according to trial-and-error (or perhaps simply based on the previous settings used by Castro-Fereira et al)?\n- The authors state that the evaluation with human judges followed the best practice recommendations of van der Lee et al (2019). However, nothing is said about which such recommendations were followed. If anything, there's a strikingly small sample of human judges (N=2), which might compromise the robustness of the results. The authors might wish to comment further on this issue.",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "c2307f969f51154c",
    "paper_id": "COLING_2020_11",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "“An Anchor-Based Automatic Evaluation Metric for Document Summarization” Authors propose a variant of the ROUGE family of metrics for summarization evaluation that uses the source document rather than reference summaries. They evaluate the proposed metric for correlation with human judgement of summaries on TAC 2008 and 09 datasets. Relevant comparison with all recently proposed alternatives for ROUGE including embedding based metrics.\nIdea for anchors is clean and novel and accounts for lexical and semantic variations between reference and system outputs and the source documents.  Nice, focussed contribution for a short paper.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "9e7049b8cce12d81",
    "paper_id": "COLING_2020_12",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper proposes to learn word embeddings for each user, thus creating personalized word embeddings. The approach uses a simple combination of personalized matrix embeddings jointly with the standard word embeddings, similar to what is done for the geographically situated language embeddings. The paper shows a reduction of perplexity when using their embeddings on language models for predicting a subset of words belonging to specific psycholinguistic categories. They also show that personalized word embeddings can improve the accuracy of automatic authorship attribution.\nThe paper is well-written and presented. \nThere is no much novelty in the used technique but it is interesting to see that words embeddings can have a better impact than users' emebddings.  The results show an improvement for both on language model perplexity and authors' attribution task. \nHowever, the amount of parameters for competing the personalized embedding is larger. \nIt would be interesting to see a comparison with exactly the same parameters used for both methods.\nAlso, as a proof of concept, the authors may select other cut of the data instead of divide it with respect the users. If the embeddinds generated with other data subdivision do not generate improvement, the authors may more comfortably claim that they are really learning user embeddings instead of just better embeddings.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "51722aadc01f5787",
    "paper_id": "COLING_2020_13",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "It is a meaningful work to construct an event-oriented dataset with user relations, TWEETSUM, for social summarization task. The authors filter and annotate the data in a disciplined manner to get the dataset, and make several typical baselines of extractive summarization methods to verify the effectiveness of the TWEETSUM dataset. But I think this paper still needs to be improved in the following aspects. \n1. The TWEETSUM dataset only contains 12 real world hot events, and the scale of this dataset maybe limited. Could the authors expand the dataset easily? or Could the authors supply the plan for the expansion of the dataset in the future? \n2. I suggest the authors describe the content of user profile in the dataset in more detail, and the authors should further discuss Table 1 and Figure 2, so that readers can understand the dataset details easily. \n3. The authors should select one more model as comparison to verify the influence of social signals, not only SNSR. \n4. The authors should make revisions to the references according to the format of COLING template.",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "787813da7147b3dc",
    "paper_id": "COLING_2020_14",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper presents an exploratory work made on the evaluation of justified (or not) disagreement in the task of annotations. \nAs far as my skills on the subject allowed me to evaluate and understand, here are the positive, neutral and negative aspects I could identify.\n--- Positive aspects --- (1) The paper tackles a very interesting subject for NLP and especially for CL. It thus is a good match for COLING.\n(2) As far as I could tell, the paper respects all formal submission criteria (e.g. abstract size, anonymity).\n(3) The paper is written in a very pleasant English that allowing a smooth reading (even though I have some concern about the content structure).\n(3) I found the methodology and results sound and convincing. There was a number of choices made by the authors that felt very adequate (e.g. boolean crowdsourcing, Prolific etc.).\n(4) Even though the results are probably only preliminary, my impression is that the amount of work to obtain them seems substantial.\n--- Neutral aspects --- (1) the citations made inside parenthesis do not display inline as they should and have their own pair of parentheses.  (2) in Section 2 the authors said \"Despite the central nature of phenomena triggering disagreement in annotation tasks, we are not aware of evaluation methods that do not mainly rely on agreement\". I don't know any too. Nonetheless, if the authors haven't already checked, I would suggest to give a look at the publications within the hcomp community => https://www.humancomputation.com/ (3) If I got that right, in Section 6.2 I suppose that \" The unit-quality-score (uas)\" should be changed as \" The unit-quality-score (uqs)\".\n(4) In section 7.2, something went wrong with this sentence \". In contrast, a simple majority vote achieves an f1-score of 0.78 and the unit-annotation-score. \"\n(5) \"Most importantly, it would be highly valuable if the existing metrics could be combined in such a way that we could use them for the identification of different types of disagreements. \" => sounds like something a Machine Learning algorithm could be used for by using the metrics as features.\n--- Negative aspects --- (1) The paper has one major issue: whereas the overall subject is coherent and well defined, its focus is quite blurry and it has been hard to understand what is exactly the contribution of the authors. Indeed the authors tend to present things in a not-so-consistent fashion from one section to the other. Also, the paper seems to be focusing on studying disagreement between annotations, yet a consequent part of the contribution described is about a filtering method to discard incoherent annotations or annotators and  thus improve aggregation. Likewise, diagnostic datasets are often mentioned but nothing is done about them in the results... Another example is Table 2 => why go into such details if they don't contribute directly to the results or argumentation presented in this paper? All this details tend to confuse the reader in the end.\n(2) While the paper tackles a very interesting subject for CL, I was under the impression that it is written in a very NLP-oriented fashion. I believe COLING try to bridge (as much as possible) NLP and Linguistics and the authors should consider that for their final version.    (3) The authors should have provided examples in Section 3 and 6.3 to help the reader understanding.\n--- Conclusion --- I think that the work presented is a good piece of research and this paper is already nice (and can become even nicer). \nI would thus gladly recommend to accept it and hope that, if it gets accepted, the final version will have amended the shortcomings mentioned above.",
    "overall_score": "5",
    "confidence": "3"
  },
  {
    "review_id": "d8faa73118eb65d6",
    "paper_id": "COLING_2020_15",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper presents a method for semantic role labelling based on the integration of syntactic information coming from heterogenous syntactic representations. More precisely, the proposal consists in extracting dependency information from treebanks using what authors call different annotation guidelines. It seems to me that there is at this stage a misunderstanding: the difference between the different treebanks mentioned in the paper relies on different formalisms, some are phrase-structure, others are dependency based. Moreover, there is no precise motivation of the reasons for doing that. There exists for example algorithms for generating homogeneous dependency-based representations from other formalisms. Nothing is said about the type of representation that can be extracted with this approach. The authors propose to join explicit and implicit methods for heterogenous parsing. The same type of problem appears there: it is not clear how heterogeneity is taken into account, the nature of the nodes being totally different depending on the representation. An experiment is presented, but the discussion does not propose an analysis of the observed improvements. The paper is not well organized and remains often obscure. It is not well motivated and does not really explains the impact of the method.",
    "overall_score": "2",
    "confidence": "3"
  },
  {
    "review_id": "704d293920a1b101",
    "paper_id": "COLING_2020_16",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper investigates different pre-trained language models for 'extreme' multilabel classification. The authors also compare against a baseline logistic regression model. The experiments are carried out on the American National Election Study (ANES) dataset, with the task of automatically coding (i.e. classification) answers to open-ended survey responses. The classification task is interesting and challenging, and I think a good example of important computational social science work. Overall the paper was well written and I appreciate the thoughtfulness that has gone in the data processing.\nMy main reasons for recommending a rejection are the following: - Framing: I think the paper sets the wrong expectations, and it tries to do too much by trying to sell the paper as being about multi-label classification in general.  - Not enough analyses to really understand when/why pre-trained language models (don’t) work for this task.\nMore specific comments are below: GOAL OF THE PAPER I would advise not to try to sell the paper as being a general paper about multi-label classification. I think the current title \"Multi-label classification with pre-trained language models\" is too general and sets the wrong expectations. In fact, I wish that the authors had focused more specifically on this particular task (which could have made a very strong paper if done well), rather than trying to sell their paper as a general paper about multi-label classification, as I think it doesn't meet the expectations that the paper is setting.\nIf the paper is really about investigating pre-trained language models for multi-label classification, I would expect multiple datasets and in-depth analyses of when/why pre-trained language models work for multi-label classification.\nThe authors aim to sell the ANES dataset as a useful dataset for NLP researchers to look at. But this motivation could be stronger by explaining more clearly how this dataset differs from current multi-label datasets (including the ones that are out there but not included in public leaderboards). What makes this particular dataset, or the goal of using it for social science, challenging? “ Because of the resulting structure (a text + multiple labels), this type of data presents an interesting type of task (multi-label classification) for NLP models” isn’t really convincing.\nANALYSES The technical contribution is limited. The overall setup is very similar to Card and Smith (2015), which they cite heavily, but the authors investigate various pre-trained language models (using default parameters). The results aren’t great (often outperformed by the baseline). This doesn't have to be a reasons to reject the paper, as it can be a very interesting data point to understand when to choose which model. However, the paper was lacking in-depth analyses to understand -why- the results weren’t good. The main take away I got was that the limited dataset sizes were a challenge, but this could have been investigated more systematically (e.g. by varying the amount of training data for the same dataset). An in-depth error analysis could have also shed more light on this.\nADDITIONAL COMMENTS - Pay attention to (opening) quotation marks - Why the choice for logistic regression without regularization? Even as a baseline, regularization (e.g. L1, L2 or elastic net) is very common and usually much more effective.\n- It was really good to see that the authors have been careful with all the data processing steps. The steps were very well documented. However, I think at the moment that the balance is a bit off in terms of space (3 pages dedicated to data processing of the ANES dataset). I would suggest moving some of this information (e.g. about the file structure of the dataset, and how to go from numeric codes to a binary vector) to an appendix/supplementary material, to allow for more space to discuss the experiments.\n- \"code sets\" could have been defined more explicitly (I think these are just the set of possible labels/codes? and so all questions with the same possible labels/codes are grouped into one dataset?)\n- I didn't understand \"The latter has been assigned multiple times, as there were many more code columns available then needed\" Why not leave the remaining columns empty?\n- As the idea is to make the dataset available to other researchers, I'd recommend also having a development split (the current dataset only contains fixed train/test splits), to ensure that model development and parameter tuning in future research isn't done on the test set.  - \"Verbatim-level averaged \" what do you mean with this?\n- Section 5: are the differences significant?",
    "overall_score": "2",
    "confidence": "5"
  },
  {
    "review_id": "7bfc15e50269b6e0",
    "paper_id": "COLING_2020_17",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- The paper addresses a fundamental issue that arises from the combination of vision and language. It is fit for Coling because it starts from a classic linguistic problem (namely referential uncertainty), and applies its implications to resources that are of practical use.\n- The practical implications are convincingly demonstrated by the evaluation of Bottom-Up on this dataset. I believe the dataset will be very useful to visually grounded language research, particularly in evaluation.\n- The collection procedure is carefully designed and reported.\n- Statistics accross domains are interesting!",
    "weaknesses": "Some imprecisions or unclarities in the reporting of statistics, e.g.: - In several places it is unclear whether 'names' refers to tokens or types, making it difficult to interpret reported results: are percentages calculated per token or type? I believe it is mostly tokens, not types, but this should be made clearer.  - In 3.2 it is said that there are on average 4 names per object, yet in 3.4 it is said that v1 has 2.9 objects on average (which then is reduced by the additional annotation in v2). Where does this difference come from?\n- \"Only 3% of the annotators disagreed on whether the token and ntop are co-referring\" --> \"Annotators disagreed on ... in 3% of cases\" The writing&figures could do with some polishing - see feedback below.\nOther feedback/questions - Upon release, will you only release the 'cleaned' MNv2, or also the annotated mistakes? The latter could be very useful too, particularly in evaluation (as you show in comparing bottom-up to human naming variation).\n- Some terms and abbreviations need introduction before using them; e.g. 'MN', 'inadequ-type', 'same-obj', 'bbox', 'turkers'. In most cases readibility would be improved by sticking with the more verbose descriptor rather than the ad-hoc abbreviation/term.\n- fig 1 is very pixelated, making it difficult to read the numbers.\n- for fig 1b, it is mostly impossible to gauge the inadequacy type distribution for cases where the same-object annotation is lower than 1. Perhaps it would be better to report percentages? Also, why were these large bins chosen? If I understand correctly, there are only 4 possible exact values depending on how many of the 3 annotators think it is the same object (0, 1/3, 2/3, or 1).\n- The authors describe that in many cases, there simply is referential uncertainty - for example 'food' or 'table' might both be adequately encompassed by the bounding box. Yet the term used by the majority is considered the 'target' and use of a different term is in some cases referred to as a 'mistake'/'error' (even if it is a plausible alternative). This is not really in line with (sensible!) statements such as 'this suggests that the standard single-label evaluation schemes for recognition methods in computer vision might not be very reliable, punishing models for 'errors' that actually constitute plausible alternatives' (section 4.2). Perhaps the use of terms throughout the paper could be more consistent with these conclusions.\n- the tables and figures often appear quite far from where they are discussed",
    "comments": "",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "aae807e326b118ed",
    "paper_id": "COLING_2020_18",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- The paper design a sophisticated process to normalize three graph-based MR, achieving a significant improvement in several cross-formalism similarity measurements.\n- The paper is well written.",
    "weaknesses": "- Although a large improvement is shown in cross-formalism similarity, it isn't clear whether the normalization can benefit downstream applications. The paper reports only a small improvement in parsing F1 score under a low-resource multi-task setting. In the full resource setting, there is a \"slight to moderate decrease\" in terms of parsing accuracy. Admittedly it's related to the specific parsers being used, but there is still a possibility that the normalization process leads to some unwanted changes.",
    "comments": "",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "0919e028d112ab4c",
    "paper_id": "COLING_2020_19",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper presents a neural unsupervised dependency parser that incorporates sibling/grandparent information into the parser's decisions. This second-order information is incorporated into an unlexicalized model that is trained jointly with a lexicalized single-order model.  The work describes two training approaches: one based on EM and one based on direct optimisation of the log-likelihood of the input. There are experiments conducted on the WSJ corpus, where the authors observe state-of-the-art performance, and on the UD corpus where the model does not perform to the sota. There are also some analysis showing the effect of skip connections, the training dynamics for the different methods and the effect of joint parsing.\nI am not at all familiar with the parsing literature, so this is going to be an educated guess. The paper seems to be doing a relatively good job at describing their approach, although it is so complex, and -as the authors show- so sensitive to small variations, that I would doubt the results are easily reproducible. However, that might be the case for a large corpus of current work as well, so I don't see a reason for penalising this paper on that ground. The writing is also relatively clear, although the structure is sometimes hard to follow. For instance, when describing the different training methods it is a priori unclear that the authors are discussing several alternatives, and this could have been specified in advance.  One important aspect that I find missing is a bit of discussion and analysis of the kinds of decisions that are favoured by having second-order information. Also, and more importantly, while the grand-parent node is unique and clearly defined, I have absolutely no idea of how the authors choose the sibling element that goes into the model. Incidentally, this is the one variant that performs the best. It would be very important for the authors to clarify that.\nA few other minor points: - I gather that you have one single embedding layer for all three subnetworks, right? Otherwise it would not make sense to perform a linear transformation on the embeddings.\n- In Section 3.2, I do not understand what kind of product you are using on the equation defining S(p, s, c) - I guess this is standard, but in eq. 3 you seem to be making a simplifying assumption that rules are independent (whilst this is not strictly true because they are sampled following a path).\n- The discussion on online-EM confused me a bit. Are you saying that online-EM is part of the DMO procedure?\n- Figure 5 is interesting, as it shows some immediate room for improvement. Have you looked at how do the model's predictions change from one iteration to another when the drop occurs? This may show what kind of modelling capacity is in the model that does not align well with good dependency parses, and which it could be trimmed in the future.\n- Reporting also the sota results on UD would be more intellectually honest, as they are reported when the authors surpass them (in WSJ), but not when they don't.\nTypos Section 5.4: quiet -> quite",
    "overall_score": "4",
    "confidence": "2"
  },
  {
    "review_id": "826f19cf6a6779d8",
    "paper_id": "COLING_2020_1",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- The novel architecture that combines two types of memories for efficient processing and aggregation of information from the user reviews. The Figure 3 is a particularly good example of the way the new architecture works.",
    "weaknesses": "- The method focuses on a single data set/language. No perspective on the applicability of the given method beyond the very specific AmazonQA task is given.\n- The experimental set-up description is missing several crucial details: Why was evaluation limited to the validation set of AmazonQA? Was the model applied on the test set as well?  - Baselines include a BERT-based retrieval, but not a simple technique such as TF/iDF.\nSuggestions/Questions for the authors: - It would be great to include more examples like Figure 3. I have found it very insightful.\n- Section 3.1 \"whether answerable\" -> \"if it is answerable\" - Section 3.2 \"As has been shown in (Petroni et al. 2019), pre-trained LMs cam be used as implicit knowledge bases\" - I believe Petroni et al. actually show that it is not necessarily the case, so not sure if this argument holds.\n- Section 4.1 BLEURT seem to have negative values? What is the range of the metric? Is higher better?\n- Section 4.2 You mention that CHIME consumes less memory. I think it is an important advantage and you should expand on this comparison.  - Section 4.2",
    "comments": "",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "6901815cd91279ba",
    "paper_id": "COLING_2020_20",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper aims to uncover the linguistic features that are most relevant to the task of predicting referential form (e.g., proper noun vs. pronoun use). Sets of features selected from the literature Random forest classification are used to empirically rank features. Broadly, across two analysis approaches (ranking all features and forward feature selection), animacy, grammatical role, distance in number of sentences, and whether the referent has been mentioned before in the same sentence appear to be important in several feature sets. In a 3rd experiment, several (9) new combinations of features (most important features from each set, most important features across all sets, etc) were used to classify referential form. In particular, classification with six features (selected based on their performance in the two previous experiments) led to classification accuracy of 75%, similar to the 79% accuracy achieved in a previous study using 18 features.\nThe paper is interesting and has implications both for linguistics and natural language processing. The methods are described clearly and appear to be appropriate. However, several inferences seem somewhat ad hoc and the conclusion is missing a deeper discussion surrounding the top six linguistic features. I elaborate on these issues below.\nThe results are described in a very qualitative way. The most important features are considered important because they have the highest “importance” values from the random forest analysis, but no effort is made to quantify where the differences in importance values are meaningful. In fact, it’s not exactly clear how the top six features are pulled out for the final analysis. These are clearly the same features that were highlighted in the text of the results, but it’s not clear how systematic this evaluation was. Statements such as “In all models (except kibrik), grammatical role is among the most important features.” Suggest that the authors simply selected those features which “jumped out” to them. Given that the kibrik set is the one with the highest performance and it ranks grammatical role low, an equivalent argument could be made for leaving grammatical role out of the set of most important features. I’m not saying this would be correct either, I’m just saying that whatever method they are using needs to be systematic and stated explicitly.  Similarly, despite the fact that small differences in \"importance\" are interpreted as meaningful, the difference between 75% accuracy and 79% accuracy is considered not meaningful. Perhaps it isn't but it's unclear what the basis for this assessment is.\nSince the authors suggest that linguists can use similar approaches to make theoretical contributions, it is important to show that the features being discussed are reasonable and interpretable. Currently the discussion very briefly lists the six chosen features and cites prior work which has also discussed those features. However, no interpretation is provided so it is unclear if there is any coherence between the current results and the prior literature. For example, how does plurality affect referential form in the current investigation? Similarly, what grammatical roles seem to elicit more frequent pronoun use rather than proper noun use and is this pattern consistent with what has been reported in the literature? Expanding on these and the other features would help situate the results presented here.",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "b3d9e742568eda9a",
    "paper_id": "COLING_2020_21",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "Very interesting paper on a novel domain with a clear presentation and interesting findings.  The paper should state more clearly at the outset that the research was performed on English data. I would even argue for \"100,000 Podcasts: A Large-Scale Spoken English Document Corpus\" See https://thegradient.pub/the-benderrule-on-naming-the-languages-we-study-and-why-it-matters/ Table 3 is opaque to me and would profit from column headers or a more elaborate caption.",
    "overall_score": "4",
    "confidence": "2"
  },
  {
    "review_id": "3992488df59373ea",
    "paper_id": "COLING_2020_22",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "1. This paper is clearly written and it contributes a novel annotation scheme to the interpretation of modality in human-robot dialogues. \n2. Detailed related works are included. \n3. The annotation results demonstrate the effectiveness and expressiveness of the proposed scheme.",
    "weaknesses": "1. It would be better if the authors promise to release the annotated results for public research in the field of dialogue. \n2. This paper only analyzes the annotation scheme and the annotation results theoretically. How about conducting a baseline model to validate the effectiveness of such an annotation scheme compared with other schemes in terms of the representation ability of modality?",
    "comments": "",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "4f0667b5b59dcf21",
    "paper_id": "COLING_2020_22",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The article presents two-level  two-level annotation for human-robot interaction. One of the levels is content, namely a logic-based semantic representation, and the other level is intent, namely tasks-based pragmatics.\n The article provides an overview of existing annotation schemes on these two topics, and their limitations, and offers the SCOUT annotation scheme. The article offers a description of the scheme and its details, an analysis of the results, and a revised theoretical framework for human-robot interaction.\nOverall, I think the article makes a good job at disentangling semantics from pragmatics, which surprisingly often taken for granted in dialogue annotation, or biased towards semantics hoping that the module responsible execution of a formal structure will know what to do (i.e. the pragmatics). In particular, i appreciated the emphasis on the very important role of imperative utterance in dialogue systems and their model-and-not-modal interpretation. Some of the softer conclusions on \"sociolinguistics\" of humans are robot are unexpected, given that dialogue agents (esp. task oriented) are created as service tools, and thus won't often use the imperative.\nQuestions:    How do you think the temporal index can be useful to describe tense differences in semantics and pragmatics, and what kind of refinements would it require? \nDo you think the relatively low kappa score for temporal index is caused by overly aggressive chance correction?   I would recommend the authors to publish the annotation scheme as a flowchart so that other people can benefit from the work in a straightforward manne",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "7420b9259a5bb05d",
    "paper_id": "COLING_2020_23",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper presents BioMedBERT a transformer-based model (derived from BERT large) trained on the domain specific large-scale corpus called BREATHE and tuned on the SQUAD2 corpus.  According to the authors, BioMedBERT achieves state-of-the-art results in QA, especially in the medical domain, as suggested from the results obtained in the BioASQ 5b, 6b and 7b datasets. \nMoreover, interesting results are obtained in the NER and RE tasks, always with respect to evaluations in the medical domain.  Although the core contribution of this work seems “only” the tuning of an existing BERT model on a brand-new corpus, the proposed models seems quite effective.  Unfortunately, it is very difficult to confirm the improvements with respect to the state-of-the-art. In fact, while only the standard BERT is evaluated with respect to the SQUAD dataset, all the more effective methods presented in the SQuAD v2.0 leaderboard are completely ignored. \nMoreover, I tried to figure out the state-of-the-art reported in this paper with respect to the results provided at http://participants-area.bioasq.org or  https://www.aclweb.org/anthology/W18-5301.pdf but it was really difficult.  Overall, the novelty of the contribution (especially from a linguistic perspective) is quite limited (the adopted method is quite used as is), even though a large and interesting (but not really clear) experimental evaluation is provided. More reference to previous work (especially the state-of-the-art) would be beneficial.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "b7daba2533c998c0",
    "paper_id": "COLING_2020_23",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper introduces BioMedBert, a BERT-like model trained on BREATHE, a very large bio-medical dataset (10Bwords). The model is trained started from BERT_Large model and then fine-tuned on some of the original task datasets (Squad 1 and Squad 2) but also more interestingly on bio-medical task datasets (NER, Relation Extration, Question-Answering). Evaluations are provided for these tasks that show that BioMedBert outperforms BERT and also that BioMedBert is state-of-the-art on QA tasks over several BioASQ datasets. \nThe paper also shows the interest of BioMedBert (versus other generic embeddings) to improve information retrieval of medical papers by reranking the list of documents returned by Elastic Search. \nThe paper does not introduce new techniques or notions but is clear and well written. And BioMedBert should be an interesting resource for health applications which shows there are a room for domain-specific BERT-like models when there are enough data for training. However, the paper mentions that it is still better to start from the general domain BERT, and it could also be interesting to check how much specific domain data is really necessary to add in order to achieve good results. Also, in order to test the stability of BioMedBert (on general domain), as done for Squad, it would be nice to get the performance of BioMedBert on the other original BERT-tasks In table 5 about Relation Extraction, I was a bit surprised to observe that recall decreases strongly when using BioMedBert. Do you have some explanation ?",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "36354bbf52c492f6",
    "paper_id": "COLING_2020_24",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper describes an extension to standard BERT, LIBERT, pretraining by adding synonym and hypo-hypernym prediction task as specialization.  LIBERT adds lexical relation classification (LRC) as the third pretraining task to BERT’s multi-task learning framework. The authors then reported the comparison between BERT and LIBERT in results summarized in Table 2 and Figure 2 (major results). As the authors pointed out that according Table 2, LIBERT outperformed BERT in 9 out of 10 tasks (i.e. except MNLI-mm).\nThe motivation to add lexical-semantic constraints to BERT training is to help distributional similarity-based pretraining models to distinguish between pure semantic similarity and conceptual relatedness. LIBERT achieved this by adding a set of linguistic constraints of synonym pairs or hyponym-hypernym pairs as a training batch of k positive training instances and 2k negative training instances (see Section 3.2). The authors collected 1,023,082 synonymy pairs from WordNet and Roget’s Thesaurus and 326,187 direct hyponym-hypernym pairs from WordNet. In total, about1.35M more positive training instances than BERT. Besides the added Lexical Relation Classification task to BERT, these additional training instances and their negative training pairs intuitively should help LIBERT outperform BERT in downstream tasks where less training examples are available. This assumption can be confirmed in Table 2.\nIn Table 2, for the tasks where their training instances are over 100K, i.e. QQP, MNLI-m, MNLI-mm, and QNLI, the performance between BERT and LIBERT does not differ much. For tasks which have less training data such as COLA and RTE, the gaps in performance are more pronounced. Given this observation, it would be interesting to see what if BERTs training on larger dataset were used. Although, the authors noted in the footnote #4, obtaining state-of-the-art downstream performance is not the main goal of this paper, it does matter to learn how the effect of adding linguistic constraints sustain or diminish when larger size BERTs are trained and used in the downstream tasks.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "c119b80a3639e9c5",
    "paper_id": "COLING_2020_24",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The purpose of this work is to fine-tune BERT word embeddings with lexical resources (LIBERT). The Authors perform this task and also evaluate the models obtained on several applications from the GLUE benchmark and on three simplification datasets. The results obtaine with LIBERT are mostly superior to those obtained with BERT. \nThis is a serious work.\nI have some comments: - it is not clear which resources are exploited for fine-tuning: WordNet and Roget's OR WordNet and BabelNet - as far as I understand, resources with only simple words are used. What about complex terms - what about applications in specialized domains (medicine, biology, law...)?",
    "overall_score": "5",
    "confidence": "4"
  },
  {
    "review_id": "4c7120ec0dd2a7af",
    "paper_id": "COLING_2020_25",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "(Summary) This paper proposes a question generation (QG) method in a text-based QA system. The proposed method that adopts a reinforcement learning (RL) framework tries to improve question quality by considering three kinds of rewards: fluency, relevance, and answerability. In the experiments using the HotpotQA dataset, the proposed method partly achieved a comparative performance, in the BLEU4 automatic metrics, with the SOTA model. The paper further presented a detailed analysis that compares the efficacy of the proposed rewards and concluded that the relevance reward was most effective in improving the question quality.\n(Major comments) The incorporation of the three rewards, respectively associated with fluency, relevance, and answerability, can be appreciated as a highly reasonable solution to improve the quality of generating questions. Accordingly, the proposed RL-based framework can be thought of as a plausible one, even it may not be very novel from the perspective of deep learning. Although the overall experimental results with respect to the automatic evaluation metrics are steady but not very impressive, the more important with the present paper is the insights gained from the carefully designed experiments.  Among them, the observation that optimizing the relevance reward achieved a substantial improvement in the human evaluation ratings would be highly beneficial. As exemplified by this insight, the paper successfully delivers several informative technical discussions. In addition, the paper itself is well-organized and highly readable. In sum, the reviewer would like to recommend the paper to be accepted.\n(Questions/Minor comments) - In the proposed training regime, the self0critical sequence training (SCST) plays a substantial role, but it is not well described. A brief description of this algorithm should be provided.\n- In this regard, the impact of the parameters $\\alpha_{flu}$ in equation (3), and $\\alpha_{rel} in (6), and $\\alpha_{ans}$ in (8) should be detailed, and preferably experimentally discussed.\n- The paper concluded that the relevance reward played the most crucial role in improving the question quality, and it could be a fairly important insight. However, it is not clear that this applies to other experimental situations. More specifically, it would be more interesting and beneficial if the impact of the balancing parameters  ($\\gamma$s) in the integrated loss function is discussed.\n- The interaction between the relevancy and the answerability could be further discussed. As far as the reviewer conceives, these two aspects would correlate somehow and suspects that the present results with the answerability might be associated with this interaction.\n- The paper argued that the results with the answerability would be associated with the ability of the underlying QA model. Would it be possible to discuss how the overall results might change or not by introducing an \"ideal\" stub QA module?",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "1d09ab39ffa57f68",
    "paper_id": "COLING_2020_26",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper presents a method for predicting hypernym relationships, similar to those found in WordNet, and can be considered to be in line with the tasks at SemEval and RUSSE on this task. This paper mostly seems to provide some minor extensions to a method already carried out by Cai et al. and Aly et al., and the results presented suggest that it is not significantly better than other systems submitted to RUSSE. As the papers for this workshop don't seem to be available it is hard to figure out if the comparison is fair, although the authors claim that the baselines make substantial use of external resources, explaining why they perform better. The paper is generally well-written, although a more formal description of the methodology would be helpful as there were several details (such as the similarity calculation) that were not clear to me. The evaluation is sound and goes into some detail that is nice for this task. Another issue is that the title of the paper promises evaluation over 'diachronic WordNet versions', however I could not understand what this refers to... my best guess is that they mean that they are predicting WordNet 3.0 relations based on WordNet 2.0 (??), however this may be problematic due to restructurings made between these versions. FYI to the authors: there are more recent releases of WordNet created by the Open English WordNet project, which you may want to investigate as well.",
    "overall_score": "3",
    "confidence": "5"
  },
  {
    "review_id": "38b01bf237916094",
    "paper_id": "COLING_2020_27",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper describes a multi-faceted project to provide tools to support a range of indigenous languages spoken across Canada. The paper is not analytical but rather descriptive of the various tools that have been implemented so far.  The paper is well-written and clear, and provides a nice overview of the work that has been done. There are no particularly innovative or novel solutions developed or applied. Would make a good poster.\nIn my view, the paper would have benefited from a few examples of the linguistic phenomena that are described (e.g. subsets of verb conjugations).",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "49245c740a56bc76",
    "paper_id": "COLING_2020_27",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This is a highly satisfying paper. It is a report of various NLP efforts for several Indigenous languages of Canada It goes deeply enough into the technical details of the projects to show that the efforts are viable and successful, without getting bogged down in numbers or linguistic details that are unimportant to people external to the projects. Where the paper does get technical is in a discussion of the differing difficulties of speech recognition for different languages, providing a useful case study to demonstrate that one-size technology approaches are not necessarily universal stand-alone solutions.\nThe paper understates two points that could be further investigated.  1 \"Rule based approaches may seem outdated in contrast to statistical or neural methods. However, with most Indigenous languages, existing corpora are not large enough to produce accurate statistical models.\" Why apologize for using a better approach? Rules may be \"outdated\" because they are inefficient for certain languages with reams of available data and scads of phenomena that don't fit. For polysynthetic languages, though, one could posit that a fairly small set of rules might be highly predictive - humans invoke algorithms to construct patterned speech that would otherwise be incomprehensible for the listener to deconstruct, and those same algorithms can be encoded for use by machines. At the least, it would be worth proposing that the languages in this study can offer a test of rule-based vs. inference-based processes, and propose performing such comparisons when the data for the study languages is sufficiently mature.\n2. This paper shows remarkable achievement for minority languages as a result of a $6 million grant. This is a crucial scientific finding: money works! Important research can make great strides regarding languages that are usually neglected, if and only if funding is available for people to take the time to do the work. The billions that have been pumped into languages like English have in fact resulted in technologies that can be applied at much lower cost to languages like Kanyen’kéha, but there are still costs. The paper could make more of an advocacy point for what relatively modest funding could do for languages in places where leaders have not yet had the same impetuses as witnessed in Canada, including India and Africa where \"minority\" language is often a misnomer.\nThe paper nicely shows what can be done for languages well outside of the research mainstream, particularly in collaboration between the researchers and the communities. Without a doubt, this paper should be part of the program.",
    "overall_score": "5",
    "confidence": "5"
  },
  {
    "review_id": "14f9ba21b06454da",
    "paper_id": "COLING_2020_27",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper describes the first phase of the Indigenous language project in Canada. The paper is very well-written and easy to follow. And the project is fascinating! I am quite impressed by the scale at which the authors have worked across so many technologies and so many languages. I wish there were more projects like this. Really well done. \nI think while the paper is very informative, for Coling audience, it might be probably more interesting as well as important to know about high level takeaways from this project. For instance, I would loved to know: a) what were some of the top challenges - scientific, linguistic, administrative, political and logistic - that you faced during this project?\nb) what were some of the strategies that you tried to overcome these challenges, and which ones worked the best?\nc) Which of these technologies have been deployed? Who are using them? What is the impact - socially and/or economically?\nd) What challenges have you faced/do you foresee in deployment or adoption of some of these technologies?\ne) What would be some of your recommendations on which technologies one should focus on while helping the indigenous communities? How should these decisions be made? What role does the speaker communities have to play in this decision making?\nand so on.\nI believe with your profound experience and expertise, you could really inform the readers about these questions. On the other hand the process of building the technologies, while useful to learn from, are mostly known to this community and probably can be described in a table where you list the language, the technologies you built, accuracy indicators and any other remark on approach used etc.\nSome specific comments: - You mention that many indigenous languages do not have a script. For those, TTS (and ASR) could be built using the following approach:  'Text to Speech in New Languages without a Standardized Orthography', Sunayana Sitaram , Gopala Anumanchipalli, Justin Chiu, Alok Parlikar and Alan W Black, SSW 8 (2013), Barcelona, Spain - You could consider PrathamBooks and their Storyweaver platform for crowdsourced creation of indigenous stories and books, whenever there is a script. https://storyweaver.org.in/prathambooks - It is unclear to me why a polysynthetic language will be difficult to teach. It is possible that the way syncretic languages are taught - that methodology may not be effective for polysynthetic languages. However, there might be other teaching methodologies that works.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "bf5b2ba60cb66749",
    "paper_id": "COLING_2020_28",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper is about characters in narrative texts, and it claims to contribute a) an operational definition of characters that is „narratologically grounded“, b) an annotated corpus (which will be released) and c) classification experiments on the automatic distinction between characters and non-characters.\nThis paper is well written and good to read. The topic is interesting and clearly relevant. I have some concerns, however: 1. The definition of a ‚character‘ is based on the concept ‚plot‘. While this is naturally following from the narratological literature, it begs the question what a plot is. And of course, this also presumes that there is ‚the plot‘ — what if there are more than one, or if it is highly subjective? Another term that is used for defining a ‚character’ is animacy. In factual texts, there is a pretty clear distinction between animate and inanimate beings, but in fictional texts, this boundary might become blurry quickly, because it is entirely conceivable that objects have properties that are usually reserved for animate beings. Thus, this term would need to be defined more concretely. The definition thus rests on other, not defined terms. \n2. The annotation experiments yields high agreement, so maybe this is not so relevant in practice. But the agreement has been measured on only one of the three sub corpora, and presumably on the easiest one: Fairy tales, which have a pretty clear plot. It would be much more convincing if the annotation comparison would have been done on a portion from each corpus, and I do not see a reason why this was not done. \n3. The annotation procedure description contains the sentence „First, we read the story and ﬁnd the events important to the plot.“ I am not sure what this means exactly — was there an agreement across the annotators what the events important to the plot are, before the annotation? This of course would make the annotation task much easier. \n4. One of the corpora the authors use consists of broadcast news transcripts from OntoNotes. I would need a lot more arguing about this in the paper, in order to believe the authors that news broadcast is a narrative. While it clearly has narrative elements, they have very different goals and textual properties. Firstly, the ‚plot‘ (understood as a sequence of events in the real world) is only partially represented in a news text, while you have a full plot in many narrative texts. \n5. From the third corpus, the authors annotated only one chapter from each novel. This also seems questionable to me, in particular because length of a coreference chain later is such an important feature. In a full novel, the picture might be very different than in a single chapter. Concretely: The evaluation of an event being relevant to a plot could be very different if the full plot is known. \n6. What I feel is missing from the paper is a quantitative data analysis independent of the classification experiments. What is the distribution of character- and non-character-chains? How long are they in comparison? This would make it much easier to interpret and evaluate the results properly. \n7. The length of a coreference chain has been used as „an integer feature“ (4.2.1). Should this not be normalized in some way, given the very different text lengths? \n8. Why is there no baseline for the OntoNotes and CEN corpora?\nTo sum up: While I think this is an interesting task, and the paper is very well written, it makes several assumptions that do not hold general and has a somewhat weak theoretical basis. The classification experiments are pretty straightforward (as the title suggests), and — given the assumptions and restrictions introduced earlier — deliver not very surprising results.",
    "overall_score": "3",
    "confidence": "5"
  },
  {
    "review_id": "a463596b6558bea9",
    "paper_id": "COLING_2020_29",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper proposes a rule-based system to convert from STREUSLE (Supersense-Tagged Repository of English with a Unified Semantics for Lexical Expressions\"), a shallow(er) meaning representation (based on syntax and lexical semantic units) into UCCA (\"Universal Conceptual Cognitive\"), a document-level meaning representation framework. In addition, supervised parsers are trained, which incorporate syntactic and lexical semantics features. The authors also perform an analysis of the similarities and differences of the two approaches. \nThis research builds further upon the UD-to-UCCA converter of Herschcovich et al. (2019a).\nComments: - The paper is addressing an interesting topic, as the authors contribute with their research to evaluate how different frameworks represent meaning in text. \nOf course, the analysis is somehow indirect, as the authors apply two different approaches, a rule-based and data-driven (supervised) approach to convert STREUSLE into UCCA representations.\n- The authors come across various difficult linguistic issues, which are well-known in NLP research, such as noun compound interpretation. \nGiven the complexity of the task, the obtained results are reasonable.\n- Although this paper presents an interesting contribution to the field, I feel the content is unbalanced between the introduction+related research and the novel research proposed by the authors. I fully understand this topics requires an extensive introduction about STREUSLE and UCCA before the own research can be explained, but now a lot of interesting information (e.g. details and examples for the rules implemented) is included in the supplementary material because of lack of space in the actual paper. It is clear from the supplementary material and programming code that a substantial amount of work has been performed for this research, but this is not sufficiently clear when reading the paper. \nMaybe this research is better suited for a journal paper, where the authors have more space to elaborate on the approach and evaluation?\n- In the current approach, the parser now has access to syntactic features derived from gold UD annotations. What would be the expected drop in performance when automatically predicted features (with a UD parser and STREUSLE tagger) would be used? Did you perform such experiments?\n- I have some doubts about the approach being language independent, as stated by the authors. Although this might be the case on a conceptual level, I see some practical issues when implementing the rules for different languages.",
    "overall_score": "3",
    "confidence": "2"
  },
  {
    "review_id": "87c4c3d2c6f34dd3",
    "paper_id": "COLING_2020_2",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper is about data selection from general domain parallel corpora for the task of bilingual lexicon induction. Three methods for data selection are presented and are evaluated on two specialized and two general-domain datasets. Each data selection method would create a ranking of the sentences from the general domain corpora which would allow different subparts to be selected - 10%, 20%, ... 100% of the general corpus - and combined with the specialized data. It is obvious from the experiments that focused data selection boosts the performance scored in terms of MAP; adding all the data, however, is not beneficial due to the problem of polysemy. The paper also analyses the performance in terms of consumed computational time.\nThe paper is clear, well-written and presents valuable insights on the addressed problem and the described solutions.\nFollowing are my remarks: GENERAL: - Section 1, last sentence. Needs to be revised. Stating \"more efficient\" would imply comparison which is not clear here.\n- Section 2, \"proposed unsupervised mapping methods getting interesting results.\" What do you mean interesting? Can you elaborate?\n- Section 2 discusses in details word representations, which is only part of the related work. This part is rather detailed and is not well reflected in the rest of the paper. Furthermore, this section does not present related work on data selection. Consider for example the work of Alberto Poncelas, e.g. https://paperswithcode.com/paper/data-selection-with-feature-decay-algorithms; the work of Bicici, e.g. E. Biçici and D. Yuret, \"Instance selection for machine translation using feature decay algorithms\". Also there is no literature review on previous work on data selection specifically for BLI. In fact the BLI -relate work is discussed in the introduction (Section 1). If you are the first to do data selection for BLI - state it; otherwise, present a comprehensive overview of related work.  - Section 2: \"For this reason, we focused this study on uncontextualized word embeddings (fastText).\" The reasons are not clear. Needs to be elaborated more.\n- Section 4: \"..., BC is of better quality than WE\" - how do you judge quality?\n- Expand the caption of table 2 and clarify which data selection you used.\n- It is interesting that the random performs well. In some cases better or at a par with the other + methods. Could you discuss why that is the case?\n- The sentence that starts with \"We can clearly see the improvement ...\" is unclear. In fact, it makes it sound as if there is an improvement going from 10% to 100% while it should be the other way round.\n- Figure 2: Having number of words on the x axis makes it difficult to compare to Figure 1. You can add additional labels for the percentages for the WE and the JRC corpora.\n- \"(75%) but does not beat the previous best results.\" - state which are these previous best results.\n- \"words present in the general corpus are also enriched\" - not clear what is enriched. Elaborate.\n- \" The colors in table 4 are not very good for color blind or if the paper is printed black and white.\n- Replace CoRR and arXiv references with references to published proceedings or journals  OTHER: *Abstract: - \"machine translation' data selection\" -> \"machine translation data selection\" - Section 1: - \"...with the deployment of open access, is nevertheless\" -> \"...with the deployment of open access data, is nevertheless\"  - \"...corpora for bilingual lexicon induction (BLI) task is ...\" -> \"...corpora for bilingual lexicon induction (BLI) is ...\" - \"Therefore, this strategy improves the quality of bilingual lexicons extracted ...\" -> \"We show that this strategy improves the quality of bilingual lexicons extracted ...\" - Section 2:  - \"...; Mikolov et al., 2013), have come to renew traditional approaches.\" - > \"...; Mikolov et al., 2013), have come to renew traditional ones.\"\n- CBOW - abbreviation needs to be explained. What does C stand for?\n- \"It is thus possible from a general language corpus to have, for a given word different embeddings, each reflecting one of its meanings.\" - > \"It is thus possible from a general language corpus to have, different embeddings for a given word, with each of these embeddings reflecting one of the word's meanings.\" \n*Section 3: - \"...and is performed at the sentence level\" -> \"... and is performed at the document level\" - Section 4: - \"Table 1 shows the size of each part of the comparable corpora...\" -> \"Table 1 shows the size of each part of the corpora...\" - \"...the quality of bilingual terminology\" -> \"... the quality of the bilingual terminology\" - \"... from comparable corpora are the same \" -> \"... from comparable corpora is the same\" - \"We first use our data selection methods to find for a given specialized corpus...\" -> \"For a given specialized corpus we first use our data selection methods...\" - \"Then, we create our training data on both languages by concatenating our full specialized corpus with sub-parts of our general corpus, adding it per sample of 10%...\" -> \"Then, we create our training data for both languages by concatenating our full specialized corpus with sub-parts of our general corpus. We build several training data sets by incrementally adding samples of 10%...\" - Section 5: - \"... for BLI on various corpora\" -> \"... for BLI on our corpora\" - \"... for our four data selections...\" -> \" ... for our four data selection methods...\" - \"... and their different combinations with two general corpora.\" - > \"... and the different combinations with data from the two general corpora.\"\n- \"corresponds to a given percentage of general corpus addition.\" - > \"...corresponds to a combination of the specialized corpus with a different percentage of data selected from the general corpus.\"\n- \"...to the results with the full general corpus.\" - > \"... to the results with adding the full general corpus.\"\n- caption of Figure 1: \"WE corpora and two general corpora\" -> \"and the two general corpora\" - \"Table 2 resumes\" -> \"Table 2 presents\" - \"...for what we assume to be the optimal...\" -> \"...for what the best results indicate to be the optimal...\" *Section 6: - \"Our previous results establish the relationship between data selection and the quality of bilingual lexicons, more precisely it shows that ...\" -> - \"The results showed in Section 5 establish a relationship between data selection and quality of bilingual lexicons. More precisely they show that ...\" - \"... while reducing the computation time.\" - > \"... while reducing the computation time if we would use data augmentation.\"\n- \"(i.e. from general domain)\" -> \"(i.e. from a general domain)\" - \"(rank 3) is a real improvement comparing\" -> \"(rank 3) leads to a significant improvement compared \" - \"These examples reflect the interest of...\" -> \"These examples reflect the benefits of...\" - \"in the most interesting one selected\" -> \"among the highest ranked\" - \"And the data augmentation column (100%) really show \" -> \"And the data augmentation column (100%) shows \"",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "5d50c22342110fb4",
    "paper_id": "COLING_2020_30",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "*The corpus, though small, is very useful for the NLP community, as empathy annotated corpora are not really available. I also agree with the limitations mentioned by the authors about existing empathy labeled corpora.\n- I appreciate the decision to mark spans of text while marking for empathy, and also the decision to independently label cognitive and emotional empathy.\n- The paper is very clearly written, and very easy to follow for most parts.",
    "weaknesses": "- The primary weakness of this paper, in my opinion, is the presentation - not the language, rather the content and substance. Being a 9 page full-paper, you had plenty of space to squeeze in a lot of information (I mention below some crucially missing points, and some good to have points). Yet, the paper has a lot of repetitive information which took too much of the space which could have been useful. I would have suggested to write a short paper, but I do think you have enough things to discuss. But let me first point out the repetitions: *Figure 1, the paragraph preceding it, the last paragraph of section 1, all talks of exactly the same thing.  - Table 3 and the paragraph around it essentially describe the same values.\n- What is the significance of Figure 2? What does the arrow mean? \n*Definition of empathy has been stated multiple times. \nand so on.\n*On the other hand, there are many important information which I could not find in the paper: - How exactly was the annotation for strength and weakness done?\n- how were the annotators trained?  - What is the background of the annotators?\n- the disagreement analysis is not very insightful. It just emphasizes that there is agreement, which we already know from Table 3. What would be interesting is to point out specific cases where annotators disagree and then reason it out: whether it was a mistake on part of one of the annotators, or was it due to underspecification or misinterpretation of the guidelines and so on.\n- Some parameters were not explained. Why did you choose to do the analysis on 92 instances? Why only 500 examples were annotated?  - it would have been tremendously helpful to show examples (not necessarily taken from data) for all the 5 categories of cognitive and emotional empathy. For instance, I could not understand the difference between label 1 and 2 for cognitive empathy.  - It is not clear if the authors have come up with the description of these labels themselves or is it based on previous work (you say inspired by, but that doesn't mean \"adopted from\", hence the confusion).\n*There are also a few technical issues that I am not quite convinced about: - The emotional empathy labels, as described in Table 2, seemed more like \"linguistic politeness\" levels to me (such as the use of modal verbs, hedging, etc.). How would you distinguish between politeness and empathy? Or are they inherently correlated?\n- Table 1, score 5 - \"she completely stepped out of her perspective\" - how would an annotator know the perspective of the writer, unless one mentions it in the review itself?  - What was the correlation between \"strength\"/\"weakness\" annotations and empathy scores, if any? I would imagine if a review points primarily to weaknesses, it will also be probably low on emotional empathy, because according to your definition of score-5, it should use words like \"excellent\" or similar positive adjectives. Yet, I can imagine a review pointing to a lot of weaknesses, yet being very empathetic. How does this interaction play out in your annotations, and how did you train annotators recognize empathetic negative reviews?  Some typos and minor",
    "comments": "",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "ca503ea7387185ba",
    "paper_id": "COLING_2020_30",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "and 3) suggestions for improvements (to answer...\" -> So you spot those areas in the review and label them? But what if it is not so easy? I mean 1-3 can be mixed up, would you then label 1-3 multiple times?\n3.2.2: - Do you provide the empathy level per review component? What if (as I mentioned above) the same review component, such as \"weakness\" would occur multiple times, do you then give one overall empathy level per \"weakness\" or for each single one? Also empathy level on sentence level might be interesting as well.\n4.2: - Interesting analysis!  References: -> some references have errors, please check. here some examples: - Emotion Representation Mapping for Automatic Lexicon Construction (Mostly) Performs on Human Level -> venue missing - \"University of New . . .\"\n- \"Introduction to Reasoning.\" - > where was this published?",
    "comments": "",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "a324f4502d532320",
    "paper_id": "COLING_2020_31",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "I find the absence of error analysis and its possible implications for improving the model. This is related to the absence of any linguistic insight of which cases are handled well and which not (the numbers, especially on the NEWS10 dataset, are really low, so some examples and/or analysis would be definitely very helpful).",
    "comments": "",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "c99723dedc4f5811",
    "paper_id": "COLING_2020_32",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper proposes an extension of length-ratio and length-difference positional encoding with the injection of noise into the length constraint. They also propose to predict the output length with a BERT model instead of relying on the source length. \nThe paper is well motivated and clearly written. The experimental setup is well described and the results are rigorously analyzed.\nA few comments: 1. Although the authors seem to focus on under-translation (hypothesis shorter than the reference), their method works best (with statistical significance) when the baseline Transformer is over-translating (likely repetitions) as shown in the first column of table 2. So I don't understand the emphasis on under-translation if we can discuss both phenomena with the same model.\n2. For a complete comparison with the Transformer baseline, the authors could consider injecting noise in the regular positional encodings, this is inexpensive to train and would be a fairer comparison to the noisy LDPE/LRPE.\n2. Although the authors acknowledge that the difference between their proposed model and the baseline Transformer is not statistically significant, if we factor in the inference time then the baseline is a clear winner. Depending on the BERT model used to predict the source length, you could be more than doubling the cost of encoding your source sequence. Did you consider conditioning the length predictor on the encoder states?\n3. For a short paper and by my standards, experimenting on a single corpus is sufficient, however it would be a stronger paper if the authors could at least consider the opposite translation direction to cover both cases of the reference_length / source_length smaller and larger than 1.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "32f1f22df9b21fd2",
    "paper_id": "COLING_2020_32",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "Summary     - The paper studies the problem of under-translation common in auto-regressive neural machine translation. \n    - Two main pieces are introduced in this research work, random noise to the length constraint, output length prediction using BERT. \n    - The English-Japanese ASPEC dataset is used to evaluate the contribution of the two proposed improvements. \n    - A stronger or similar performance is shown for all the 4 length language groups using the proposed approach. Especially in the shorted range, the authors show more than 3 points improvement over the vanilla transformer. \n     - An interesting insight I got for the long sentences, vanilla transformer tend to produce shorter length sentences. The proposed approach generated translation close to the gold reference length, atleast for the dataset in use.\nStrenghts     - Ablation is performed for both the new component, random noise and BERT based output length prediction. \n    - Strong BLEU score for short sentence range and relatively close to gold reference length compared to the vanilla transformer.  Concerns     - The work of Lakew et al, uses English-Italian and English-German datasets for evaluation. These datasets should be used to have a consistent evaluation with the past work. \n    - Following from the last one, any specific reason why the English-Japanese dataset is a better choice for your proposed methods? Perhaps you can **motivate on linguistic grounds** why the language Japanese is a better testing ground for your method. \n    - Including an extra BERT-based-output-length prediction can incur additional computational overhead. The overhead of this computation should be stated in the work. \n    - In the introduction, you mention __However, the input sentence length is not a good estimator of the output length.__. I'm not sure why is this the case.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "cb4aa4ed5b1849f7",
    "paper_id": "COLING_2020_33",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "of the work.",
    "comments": "",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "e298db5794a33283",
    "paper_id": "COLING_2020_34",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper describes a morphological disambiguation task for Kinyarwanda, a Bantu language. Particularly, it describes the problem of multiple morphological analyses and the problem of finding a single correct answer given several possible stem realizations. The challenge is an interesting one, and one barely sees languages from this family represented at CL conferences, so this investigation is very welcome. The paper focuses on verb segmentation, and utilizes several resources including a morphological analyser and a manually stemmed (crowdsourced) dataset. The possible segmentations are then classified (ranked) based on a set of features. \nWhile the paper is overall well-written, its structure could be improved much more. The 'Experimental setup' section for instance does not explain exactly how the annotated dataset is being used for the experiments. If the dataset was crowdsourced, then how did the authors know which annotator was the best trained, and had better understanding of morphology ? Going further, what was the motivation behind crowdsourcing the data if the task does seem to require specialized linguistic knowledge? \nThe experimental section also describes the creation of 'invalid' segmentations which are provided as input to the classifier. Does the morphological analyser (section 3.2) play a role here? Or are the ranked segmentations then directly used to choose the best output from the analyser? This part is not clear to me. Section 3.2 could have better clarification on what exactly is its relation to the experiments being carried out. The section on 'Features' needs much more clarity and some examples. Perhaps the proposed features could be split into two sections- this section was a bit frustrating to read.\nTable 4 and Table 5 require some more explanation, particularly the active learning part- as this is not how the dataset was described earlier. Some more explanation of how fine-tuning the model was carried out would be useful as well.  Overall, I think this paper describes a good effort but structure and clarity need much more improvement.",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "38eeec0b4f34d03d",
    "paper_id": "COLING_2020_34",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Exploring new methods to improve Kinyarwandais NLP is highly desirable and relevant work that can lead to a more general contribution. The current version of the paper presents some potentially good ideas and a new data set. The characteristics of Kinyarwanda’s morphology and the problems highlighted by this work, e.g., several sources of ambiguity, could be helpful for other languages that are facing similar conditions.",
    "weaknesses": "There is considerable lack of clarity and meaningful comparison with the related work. The problem addressed by the paper needs to be defined more precisely. For this, the authors might want to refer to some pre-2000 work on morphological analysis, including popular textbooks. A much clearer description of the classifier input, output, training and test instances (with examples) is necessary in order to understand the procedure and to properly assess the novelty of the presented work. Crucial details are also missing in the description of the procedure for selecting the features of interest (Section 4.2).  Smaller issues: Not all tables (results)  specify that they are showing accuracy as the performance metric. \nProofread is strongly recommended.",
    "comments": "",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "5e8f098719ea5809",
    "paper_id": "COLING_2020_35",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper presents a new approach that improves the past transition-based RST parser. The authors propose a new nuclear type and a new action to handle specific cases that the past systems have ignored.  - The authors did a good job conducting a series of experiments along the ablation studies that help us understand how each component contributes to the improvement.\n- The reviewer is quite confused with the description for the \"Separate\" system.\n- The paper will benefit from a diagram similar to the one presented in Dozat's paper. Bi-Affine model is not a well-known model. The authors should include a diagram also.\n- It takes some guesswork to understand how EDU representation is derived from the sentence representation. We simply average across all of the words in the EDU? Should there be a better way to derive that? Could we instead run BiLSTM over the EDU instead and then pass it through the Bi-Affine model?  - What's the differences between row1 and row2 in Table2? Please explain more.\n- The claim of this paper is that the new nuclear type and new action (and Bi-Affine features too?) contributes to the improvement. Table2 should label more clearly which feature is involved here.  - The idea of R^ and N^ is somewhat similar to the idea of markovization in phrase structure parsing, which is a classic technique in CYK parsing with PCFG. It would be nice to relate to the idea or argue why it is different.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "e14ec3343c062669",
    "paper_id": "COLING_2020_36",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This is an interesting paper that investigates an applied NLP task of extracting the maturity of a company’s digital strategy from its earnings calls.  The paper introduces a new dataset for the task, and evaluates some existing state-of-the-art models on it.  Assuming the authors are going to release their data set (I did not see this explicitly noted in the work), this seems like a worthwhile contribution overall.\nA couple of things I felt were missing from the discussion of the data set compilation process.  First, more justification for the 17 aspects isolated in the experiment would be helpful---how did you choose those, and how do we know that they are “good choices\"?  Secondly, although the data set includes 1300 hand-annotated labels which seems like a potentially valuable data source, we are not told anything about the qualification of the labeler(s), or what interannotator agreement might be.  These last things are really critical in order to evaluate the data set as a contribution.  The fact that this information was missing was the primary reason I did not assign a higher score.\nA handful of more minor notes:  The paper is entirely missing details of the models evaluated in Section 4.2.  When you say RoBERTa performed best without language model fine-tuning, what were the details of the LM finetuning that you tried?  How many epochs and what learning rate did you use (what did you mean by a “discriminative” learning rate?)?\n“Second, we found a few conventionally asset-heavy companies from the industrials sector clustered with predominantly digital native companies from the technology sector. This indicates that some companies have made significant progress in digitally transforming their business.” \n -- This is interesting, but it would be helpful to back this claim up with specific examples or a quantitative analysis of the actual companies, to ensure this wasn’t due to clustering or extraction error.\nThere’s a typo in Table 2, recall value of 653.8",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "70cef37f3e3c8e50",
    "paper_id": "COLING_2020_37",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper proposes a method of using treebanks that are comparable across languages (e.g. Universal Dependencies) to evaluate linguistic \"universals\". The idea is to express the universals as formulas of propositional logic, with logical atoms corresponding to things like \"has SVO word order\". Then, for any given language, the atoms are interpreted with reference to relative frequencies in treebanks, and fuzzy logic interpretations are used for the connectives, to arrive at a real-valued \"truth value\" representing that language's degree of conformity to the universal (i.e. each language acts as a model); these values are averaged across languages, in a way that prevents large groups of closely-related languages from having undue influence, to calculate a single value for the universal. The authors show that the Greenberg universals get generally high scores (i.e. high \"degrees of truth\"), that the scores are relatively stable across samples of languages, and that the method can be used to identify new candidate universals via brute-force \"search\" of arbitrary propositional formulas.\nI think this is an interesting and well-written paper. It provides a new confirmation of some well-known universals. The potential for uncovering new universals seems clear in principle, although care is needed in interpreting the scores, as the authors point out on p.8 with the confound raised by the majority of languages having subject-verb order. My main concerns are about exactly how clearly the calculations here actually correspond to the intended interpretations of the (fuzzy) logical formulas. It's possible that some small tweaks to the system aimed at shoring up its \"logical grounding\" could fix or reduce the interpretability problems like the one caused by the abundance of subject-verb order.\nFirst, the fact that expressing the universals in Table 1 as formulas required addition as well as disjunction (see footnote 5) is a clue that plain propositional logic is not quite what we want. There's no discussion of how the authors chose between addition and disjunction in \"translating\" Greenberg's statements of the universals, but the idea seems to be basically the addition is used between mutually-exclusive alternatives (hence same denominator) and disjunction is used otherwise. ( I think disjunction is only used once though, in #24?) I get the intuition here, that disjunction is for \"real disjunction\" whereas addition is just for tallying up things that we don't happen to have an atomic symbol for. That makes sense for #1, but I'm less sure about #6: why is the antecedent included in the sum? Why isn't this just \"verb-nsubj:noun-obj:noun => nsubj:noun-verb-obj:noun\"? I worry that these choices are a bit ad hoc. It might be useful to think about real-valued logics with distinct connectives interpreted as maximum and addition, and how they are usually used -- see e.g. the strong and weak disjunction in Łukasiewicz logic.\nI think that oddity in the translation of Greenberg #6 points to another issue in thinking of this as logic: what exactly does the logical atom \"nsubj:noun-verb-obj:noun\" mean? Does it mean that SVO is the \"dominant\" order, or that it's an \"alternative\" order, or just that it's a possible option in the language? What actually gets calculated from the treebank is the relative frequency of that order, among the six possible orders. But suppose that this frequency turns out to be 0.2 in a certain language. Does this mean that it's \"20% true\" that SVO is the dominant word order in that language? Not entirely straightforward. ( A proportion around 20% might be taken as indicating that the language has this as an *alternative* order, with a very very high degree of truth ...) These sorts of complications, I think, mean that the authors had to add together VSO+SVO to try to get a measure of \"dominant VSO with SVO as an alternative\" -- but, among many other things one might worry about, this is presumably the same as what we'd get if we tried to translate \"dominant SVO with VSO as an alternative\".\nI suspect that in practice the concerns of the sort in the last paragraph just get \"washed out\" because most of the time the universals are talking about dominant worder, and it works well enough if we assume that, e.g., 70% relative frequency for SOV order indicates 70%-true for the proposition \"has SOV as dominant word order\". But that's a substantive assumption the system is relying on -- or, perhaps in different terms, the way this system interprets \"X is 70% true for language L\" is as meaning that \"X happens 70% of the time in language L\". That's fine, but it brings out the general issue of whether we're really dealing with fuzzy logic here rather than just calculations on relative frequencies. And it makes the success of the system seem slightly less principled than its description suggests.\nThe presentation of some of the formal details is slightly sloppy:  - In (4), applying the valuation function to \"1 \\wedge 1\" makes no sense; it applies for formulas.\n - The formula in (5) implies that there's a different weight for each language-formula pair (i.e. w(\\sigma,\\ell)), but this wouldn't make much sense, and doesn't seem to be what actually happens (i.e. it should be just w(\\ell); note that (13) doesn't use \\phi.)\n - The notation for logical atoms, with hyphens and colons, should be introduced explicitly. I figured it out using (6) and (7) as clues but the reader shouldn't have to.\nAnd some other minor comments:  - A natural alternative to consider, from a logical perspective, would be to move to a first-order logic where things like SVO correspond to one-place predicates, and then the individual languages could be entities in the model with each Greenberg-style universal being evaluated just once in that single logical model. ( As opposed to the current setup where each language is its own model for propositional formulas, and then the averaging has to happen \"outside the logic\".) I'm not sure how the weighting of the average to account for genealogical structure would work though.\n - I didn't understand the cryptic two sentence justifying the choice of fuzzy logic at the end of section 3.2.\n - Why constrain the invented candidate universals to have the same logical structure as existing ones? Why not just some formally-natural class such as, say, all well-formed formulas with at most two connectives?\n - Rather than #13 being an example of \"two conflicting universal tendencies\" (p.8), couldn't this just be described as discovering that #13 as written is not a good universal after all.\n - I'm not sure whether it makes sense to talk about the experiments in section 4.2 in terms of \"predicting\" scores on one subset from those on the other. Isn't the issue just about how stable/reliable these scores are across the dataset as a whole? ( Even if the reason we care about this might be so that we can assess the validity of inferences from incomplete data.)\n - I couldn't understand the first few sentences after Figure 3 and Table 2. Is there a typo -- should 80% be 50%?\n - I'm not sure about the reasoning around complementary antecedents at the bottom of page 8. Are those two implications uninformative? Don't they basically tell us that all languages have nsubj-head order? ( Notice that their two antecedents are not only complementary numerically, they have the form P and not-P, given the fuzzy logic interpretation of negation.)\n - (#13) on page 9 does not seem \"completely new\": this seems like a straightforward instance of head-finality.",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "1b6b577ca1497d32",
    "paper_id": "COLING_2020_38",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper presents a study on how summarization evaluation metrics correlate. The authors conduct experiments showing that the metrics may disagree in any “scoring range” and that the metrics are worse as the summaries are more abstractive, extending a previous initiative of Peyrard (2019).\nThe ideas are clearly presented and the paper makes its point. The results are somewhat expected, but, given it is a short paper, the overall contribution is good. I comment two issues that might be improved in the paper: - although the authors clearly explain the EoS and abstractiveness metrics in the paper, they do not do the same for coverage. Please, let it clearly explained in the paper.\n- I was convinced about the paper conclusions, but I missed some considerations on the potentialities of the metrics and reasons for some of them to agree more than others. For instance, we may see that ROUGE-1 and ROUGE-L are more likely to agree. Why do you think that this happens? What other conclusions we may take for each metric based on its correlation to other metrics? A metric that correlates more with other metrics is a better metric?",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "0f8f231128d560a4",
    "paper_id": "COLING_2020_38",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The authors present a refined study on summarization evaluation metrics. The work is straightforward and well presented. The main result I would say is that we need to collect human judgements, which this, and most other attempts to assess the quality of automatic summarization techniques unfortunately does not.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "74371c30c16f8cbd",
    "paper_id": "COLING_2020_39",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The authors try to propose a holistic taxonomy for text mining features with a meta-analysis method. The proposed taxonomy is helpful for understanding different types of text features, but its novelty is limited as the paper just combines and summarizes text features used in some other studies. Moreover, the investigation process has some flaws. How did you choose the 211 papers and why? With what search terms? Do you specify any other search conditions? We can imagine that there are much more papers relating to text features. For a meta-analysis study, it is crucial to choose important and representative studies in the literature (if we cannot cover all of them). Features have also been widely studied in machine learning domain, so results from the ML community could be useful as well. For example, as to the representation dimension, nominal values (e.g. topic or domain) may exist in many text mining applications. How do we process this kind of features? It would be also expected to know how the proposed taxonomy could guide the design of a text mining application.\nSection 4, as step 3 of the investigation process, should be placed under section 3.\nTypos: 1. page 5, line 1, \"136 papers\" => \"116 papers\" 2. page 6, table 1, the last line, the last cell, \"real umber\" => \"integer\"? \n3. section 4, line 8, \"The analysis shows that researchers SOLELY use ...\" 4. section 4, line 13, \"In Figure 5\" => \"In figure 6\"",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "76b6d9e6de67d62d",
    "paper_id": "COLING_2020_39",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "of this paper is its attempt to systematically classify text mining features. Its main",
    "weaknesses": "are: 1. The paper does not provide enough motivation as to how such a taxonomy might be useful. What could it serve? For example, in the following ACL-2020 paper: https://arxiv.org/pdf/2005.04118.pdf The authors suggest a checklist with linguistic capabilities aka text mining features (e.g., negation, synonyms, semantically related terms, typos, sentence addition, etc) to test NLP models. Making a list of features regardless of application might also be too",
    "comments": "",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "f51683b92cfd22c0",
    "paper_id": "COLING_2020_39",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- Significant work of literature review.\n- The paper is mostly clear.",
    "weaknesses": "Despite the efforts of the authors to justify this work, I have difficulties in understanding the added value of such classification. Not that this is not useful per se, but rather because, on the one hand, it already exists (most of the classes belong to",
    "comments": "",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "79120777ee4ef25b",
    "paper_id": "COLING_2020_3",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper presents the application of relatively simple data augmentation techniques to a NER task, evaluated on two different datasets.\nThe paper is clearly written and the results are interesting, although not specially exciting. In particular, the experiments with RNNs show much lower performance than those with the transformer model (and hence, the improvements are also smaller, as the baseline is also higher).\nSome details should be given, for example: - \"(LwTR): For each token, we use a binomial distribution to randomly decide whether it should be replaced\"   Does this mean to replace a token with 50% probability? Or any other proportion? Why ths number? Have this been tested on a small subset of the data to adjust it conveniently?\nMinor corrections: these augmentation do not rely ---> these augmentations do not rely previous studies mainly investigative the effectiveness ---> previous studies mainly investigate the effectiveness RoBERTa (Liu et al., 2019), which have captured various knowledge. --- > I think that the sentence is not correct (but I am not totally sure). Please revise.",
    "overall_score": "2",
    "confidence": "3"
  },
  {
    "review_id": "307b26f30b2ffa03",
    "paper_id": "COLING_2020_40",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- A simple method to incorporate traditional features that is shown to improve neural scoring models quite significantly (p-value < 0.05).",
    "weaknesses": "- There's no comparison with existing work, or the state-of-the-art in the topic, especially the DNN-based ones which are listed in the related work section (Sec 2.3 and 2.4).\n- The study on handcrafted essay features is not eye-opening, Table 5 only suggests that \"each handcrafted feature contributes to some extent\". It would be better if a comprehensive ablation study was conducted, to understand which traditional features contribute the most (and the least), especially given the BERT pretrained models for encoding.",
    "comments": "",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "6df2c662d2f9c88b",
    "paper_id": "COLING_2020_41",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper contributes a new task, dataset, and experimental results on Chinese lexical fusion resolution.\nSection 2 presents the phenomenon: lexical fusion is a phenomenon in Chinese where a combination of words (called the \"separation words\", typically a VP, it seems) is referred to anaphorically by a \"fusion word\". Fusion words are formed ad-hoc and are therefore often out-of-vocabulary; they consist of multiple (typically 2) characters, each of which corresponds to one of the separation words. Often, the characters of the fusion word are characters from the corresponding separation words, but they can also be characters that are in some purely semantic relationship with the corresponding separation word.\nSection 3 presents a machine learning model to resolve this type of anaphora. It consists of a character-wise BERT encoder, a CRF decocer for mention detection, i.e., labeling spans of characters as either separation or fusion words, and a biaffine model for anaphora resolution, i.e., to recognize which fusion word refers to which separation words. To capture semantic relationships between fusion word characters and separation words, each character is assigned an additional semantic representation that is fed into the model. This semantic representation is obtained by looking up every word that the character could be part of in this context in the HowNet lexical database, further mapping each of these words to its HowNet representation as a \"sememe graph\", and then aggregating a vector representation via sememe embeddings and graph attention networks.\nSection 4 presents a test set created by manually annotating 17,000 paragraphs from the SogouCA news corpus. It also presents an artificial training set constructed by generating possible triples of fusion word and separation words using an electronic lexicon, and using those triples as seeds for finding presumed occurrences in a large corpus. To assess the impact of sememe knowledge, the full model is compared with various degraded variants and combinations thereof: one where the expansion to characters into words is skipped and only characters (1-character words) are looked up in HowNet, one where the structure of of HowNet's sememe graphs is ignored and complete graphs between the included sememes are used instead, and one where sememe knowledge is left out entirely. It is found that the model has the highest accuracy when using the full model, confirming the usefulness of fine-grained sememe knowledge for this task. An analysis shows that in-vocabulary fusion words are resolved with greater accuracy than out-of-vocabulary ones, and that fusion word characters that are borrowed from their respective separation word are correctly mapped with much greater accuracy than ones where the relationship is purely semantic, despite using sememe information. An example of how senses with shared sememes across triples can mutually enhance their attention weights is shown. It is also found that anaphors are more difficult to resolve than cataphors, that resolution becomes more difficult as the distance between the two separation words increases, and that the model performs better when mention detection and anaphora resolution is done jointly rather than in a pipeline.\nI think it's a good and important paper, as it tackles an apparently understudied phenomenon in the Chinese language, resolution of which is important for natural language understanding. The technical choices seem well motivated and are presented fairly clearly. The promised release of the datasets will be a valuable contribution. And the analysis is fairly extensive. However, there are also several ways in which the paper could be improved: - Surely Chinese lexical fusion has been described in the linguistic literature? A reference should definitely be given. Also, the claim \"Lexical fusion is used frequently in the Chinese language\" should be backed up either by a reference or by data, e.g., from the annotation project presented here.\n- Lexical fusion should be more clearly defined. From the examples given in Section 1-2, I get the impression that the antecedent is always a VP consisting of a single-word verb and an adjacent single-word nominal object. But apparently the separation words need not be exactly two and need not be adjacent? Examples of this should be given as well, and it should be explained what possible syntactic types the antecedent can have.\n- Relatedly, the annotation guidelines should be explained. Specifically, what counts as lexical fusion for the purposes of this dataset? Are there edge cases that were excluded?\n- Relatedly, the annotation process should be explained in more detail. How many annotators checked each paragraph? What was done with paragraphs with disagreement?\n- The test set does not seem to contain any paragraphs without lexical fusion, so it is presumably rather skewed. In a more realistic setting, paragraphs without lexical fusion would not be removed from the test set.\n- I would like to see more detail on the construction of the artifical training dataset. How do you \"check if [a two-character word] can be split into two separation words\"?\n- Terminology: in my mind, the reference is from the anaphor to the antecedent, so shouldn't cataphors be called forward references and anaphors backward references, rather than the other way around?\n- Terminology: \"separation words\" sounds a bit odd, it seems to imply that these words are the result of a separation process. How about \"antecedent words\"?\nSmall stuff: - The bibliography entry for Mitkov (1999) is broken.\n- p. 4 overall -> over all - Typography: please ensure that opening parentheses (e.g., in citations) are preceded by spaces.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "a13bd46160cb868b",
    "paper_id": "COLING_2020_42",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper proposes a heterogeneous-event graph network to help solve the missing event prediction task. Based on the reported experimental results, the proposed method achieved slightly better performance than previous works (e.g., EventTransE) on this task. However, I still have some concerns about both the experimental details and the paper writing, details are as follows.\n1. The term \"heterogeneous\" is a little bit misleading. Typically, we use that term to refer to the graphs whose edges have multiple complex types. However, in the proposed model, there are only three relation types (word-word, word-event, event-event). If I understand correctly, both the word-word and event-event are just homogeneous networks and what the author did is adding an additional connection between these two homogeneous graphs. So a better term might be something like \"cross-graph\"?\n2. It seems like the author didn't mention how they construct the graphs in the paper. As the main contribution of this paper is using the word/event graphs, without knowing how did the author create them (e.g., what is the meaning of edges and how to get those connections), we couldn't evaluate the technical soundness of the proposed model.\n3. The author did experiments on the NYT dataset. My concern is that the quality of that dataset is not very satisfying because the test set is also automatically extracted from the documents. I suggest the author try other human-curated event sequence datasets (e.g., RocStory, Wikihow) as the evaluation dataset.\n4. I am not sure whether the evaluation is fair for baseline models. For example, to the best of my knowledge, none of the baseline methods mentioned in this paper use strong pre-trained language models (BERT), and the proposed model does use BERT as part of its model. And as suggested by the ablation study, BER contributes a lot to the final success and I found it is hard to be convinced that the main improvement is coming from the graph model rather than BERT. A better way to do the evaluation is adding one more baseline, which uses BERT to encode the sequence and directly make predictions. A comparison of that can better prove the value of the proposed graph model.\n5. By the way, another suggestion is that maybe you may want to use stronger pre-trained models (RoBERTa) if you want to get better performance.\nTo conclude, I think this paper proposes an interesting solution to the missing event prediction task, but it still needs some further improvement to be good enough to be published in COLING.",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "1770dd00a1e9fae1",
    "paper_id": "COLING_2020_42",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper explores the use of graph neural networks to identify missing elements in an event chain in a multiple-choice scenario. Overall, it is an interesting approach to the task, which surpasses the performance of previous systems. Furthermore, the paper presents an interesting discussion of the results, including ablation studies that reveal the usefulness of every system component. However, the paper also has some issues, which I discuss below: First of all, the task is defined as a multiple-choice task in which, for each missing event, there is a golden choice and four wrong choices. However, there are no examples of the wrong choices nor information regarding how they vary in relation to the correct one. This makes it hard to assess the actual difficulty of the task.\nFurthermore, although the evaluation is performed on the MCNC dataset, the model is trained on a portion of the Gigaword corpus. In this case, the event chains are obtained by mapping the coreference chains identified by the Stanford CoreNLP library. This means that the system is probably also learning to identify those coreference chains. Thus, it would be interesting to compare the performance of the system with a simple selection based on the coreference chains identified using Stanford CoreNLP.\nThe system relies on BERT to generate the embedding representation of the text. Furthermore, the pre-trained model is fine-tuned on the used corpus. However, this tuning is still based on masked language modeling task. It would be more interesting to fine-tune the model to a task closer to event chain prediction, such as next sentence prediction, which was also used to pre-train the original model.  Finally, regarding typos/grammar/style, when references are used as part of the sentences, the format should be different (e.g. \"(Lv et al. 2019) found\" -> Lv et al. (2019) found). Furthermore, the placing of references should be consistent across the paper. For instance, in the second paragraph of the introduction, the placement of the references varies between before and after \"based\", which makes it confusing. More importantly, the paper has several issues regarding number and verb tense mismatches or inconsistencies, as well as \"a\" that should be \"an\". Thus, it requires a thorough proof-reading and revision.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "40df350b76559d86",
    "paper_id": "COLING_2020_43",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "In this paper, the authors argue that using the topmost encoder output alone is problematic or suboptimal to neural machine translation. They propose multi-view learning, where the topmost encoding layer is regarded as the primary view and one intermediate encoding layer is used as an auxiliary view. Both views, as encoder outputs, are transferred to corresponding decoder steams, with shared model parameters except for the encoder-decoder attention. Prediction consistency loss is used to constrains these two streams. The authors claim that this method can improve the robustness of the encoding representations. Experiments on five translation tasks show better performance compared to vanilla baselines, and generalization to other neural architectures.\nOn one hand, the experiments conducted in this paper are rich, including five translation tasks, two NMT architectures, shallow and deep models, and many ablations and analysis.  On the other hand, I have several concerns regarding motivation, claims and experiments: - The authors pointed out two problems for using the topmost encoder output alone: 1) overfitting; 2) “It cannot make full use of representations extracted from lower encoder layers,”. I’m not convinced by the second one especially. For example, in PreNorm-based Transformer, the final encoder output is actually a direct addition of all previous encoding layers. Although there is a layer normalization, I believe this output carries critical information from lower layers.\n- The authors claim that “circumventing the necessity to change the model structure.”, but the proposed method requires to change the decoder, and manipulate the parameter sharing pattern. In my opinion, the method still requires structure modification.\n- The major ablations and analysis are performed on IWSLT De-En task, which is actually a low-resource task, where regularization is the main bottleneck. From Table 1, it seems like the proposed approach yields much smaller gains on large-scale WMT En-De task compared to low-resource tasks. Thus, it’s still questionable whether the conclusion from experiments on low-resource task can generalize to high-resource tasks.\n- Which WMT En-De test set did you use? WMT14 or WMT16? It seems like the authors used WMT16 for test, but the baseline (33.06 tokenized BLEU) is below standard (~34 BLEU).\n- Besides, some experiment has mixed results and is hard to draw convincing conclusions. For example, in Table 5, MV-3-6 (shared) achieves the best performance on De->En while MV-3-6 is the best on Ro->En. It seems like different tasks have different preferences (share or separate). In the paper, the author only highlights the superiority of separate settings on Ro->En task.\nOverall, I'm not convinced by the motivation and the analysis on low-resource tasks (In particular, this paper doesn't target at low-resource translation. Note that the authors claim that \"our method has a good generalization for the scale of data size.\"). I think the score of this paper is around 3.5 with several unclear questions to be solved. Since we don't have this option, I prefer to give the score of 3.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "497e9cf35fb51afb",
    "paper_id": "COLING_2020_44",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This article presents several parsing models that are able to do domain adaptation from large scale source-domain labeled data to small scale target-domain labeled data via adversarial learning. They also use large scale unlabeled data to improve word representations. Experiments show that these approaches lead to consistent improvements, reaching SOTA results comparable to those for more complex ensemble models.\nThe article is well written and in my view the contribution is important and well supported. All the proposed ideas are compared and validated with experiments.\nIn terms of results, the biggest contribution seems to be given by the usage of BERT and fine-tuned BERT, maybe the least original aspect of the work, but the adversarial models consistently show improvements in every comparison w.r.t. non-adversarial ones.\nThe final results and comparison with previous work is also meaningful. In particular, it is interesting to observe that the best results are obtained for the PC domain, the one with the largest difference w.r.t. the source domain, and that the adversarial training plays a key role here.\nMinor comments: 1: “kim et al.” -> “Kim” 1: feature argumentation -> feature augmentation 1: last paragraph, 1st sentence, too long. Or not? \n1 “a large-scale” -> “large-scale” 1: “our codes” -> “our code” 1: Put link in footnote. \nUse “Fig.” to reference figures in text. \n2: “Additionally. we …” -> “Additionally, we …” 2: BiLSTM “forward and backward two directions” 2: (3) use two formulas 2: loss is standard, no need to include it. \n3.1: “the adversarial network on BiAffine parser” -> “an adversarial network on BiAffine parser” 3.3: “fine-tuning domain embedding” -> “fine-tuned domain embedding” 3.3: “domain-special features” -> “domain-specific features” 3.3: “another BiLSTM” -> “the other BiLSTM” 3.4: “start point” -> “starting point” 3.4: “computation resource” -> “computation[al] resources” 3.4: “... once, Thus …” -> “... once. Thus …” 3.4: explicitly say that no “next sentence” loss is used. \n4: “product blog” -> “product blog (PB)” 4: “dependence parsing” -> “dependency parsing” 4.2: “directly applies” -> “directly applying” 4.4: “compared the results” ->  “comparing the results”",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "81ddee5ab5d6445d",
    "paper_id": "COLING_2020_45",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper presents what the authors call Definition Frames (DF), representations that combine word embeddings with explicit relations between concepts. Differently from previous approaches that integrate external knowledge into word embeddings (e.g. retrofitting word embeddings to a semantic network), DF retains the explicit relations that characterize concept. The authors follow the ideas brought forward by Pustejovsky and his work on qualia structures. Given a concept, Pustejovsky theorizes that such concept can be fully described by a set of attributes/properties. While in traditional word embedding techniques each word is represented by a single vector, DF represents each word with a matrix where each row is a dense representation of one of its attributes/properties (e.g. a row for \"IsA\", another one for \"PartOf\", etc.). The authors show the effectiveness of their proposed approach in word similarity and word relatedness.\nThe paper is well-written and very easy to follow. I really enjoyed the work: it is a rather simple approach backed up by very interesting linguistic theories, a combination that is not easy to find in most of the NLP papers today. In general, I appreciated this work and I think it deserves a place in a conference such as COLING.\nSaid that, the fact that I appreciated this work is one of the reasons I expected more and was let down by some parts of the paper. First, I think that a paper on word embeddings in 2020 must include some kind of comparison with current contextualized word embedding techniques, say BERT or ELMo. Of course, word similarity is not the best task to compare DF against BERT-like embeddings. However, I still believe that BERT could have been easily integrated into DF itself to obtain a BERT-based DF. For example, the Basis row/vector of a word w could be the BERT state corresponding to the CLS token of the WordNet definition of w, while the qualia dense representation could be the average BERT state of the qualia in the ConceptNet sentences that define the relations of w. Since the authors already make use of WordNet and ConceptNet, I think this would have cost little extra work for a much stronger and up-to-date paper.\nAnother issue of the paper is the evaluation. Word similarity and relatedness, while traditional NLP tasks, are still intrinsic evaluations. I understand this is a short paper, but I feel like one of the two tasks (similarity or relatedness) could have been sacrificed for a downstream task. I would also suggest not to show just the average Spearman correlation coefficient of several similarity datasets, as this makes it more difficult for the reader to compare DF to other word embedding techniques.\nReasons to accept: - DF is a perfect fit for COLING as it combines strong linguistic theories with existing distributional approaches.\n- The authors present a clever way to take advantage of knowledge resources (WordNet and ConceptNet) and word embeddings.\n- Since the resulting representations retain the qualia structure of the represented words, they are more interpretable than traditional word embeddings.\nReasons to reject: - The evaluation is not strong as it only comprises word similarity and relatedness, two intrinsic tasks.\n- The authors do not include BERT or BERT-like contextualized embeddings. As time passes, this may become a stronger reason to reject this work.\nOther (minor) comments: - In case of accept, please include in the appendix the individual results on each word similarity/relatedness dataset (helps reproducibility).\n- The font in Figures 2 and 3 in Appendix D is too small.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "938799444969a089",
    "paper_id": "COLING_2020_45",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper presents an interesting attempt to build a word embedding for a word by considering relations in which this word is involved in wordnet definitions. These relations are those from Pustejovsky's qualia structure (partOf, madeOf, etc.). Then, the word embedding is the average of the vectors corresponding to each of the relations (which I'm not sure how they are constructed, another average over the words involved in the relation, the LSTM hidden state?).  This is a linguistically motivated approach to learn better word embeddings, but I think the main problem with it is that the experimental results are not convincing. Specifically: 1) You map words in similarity/relatedness datasets into wordnet to extract their definitions, but these datasets are not disambiguated. So how do we know the definitions extracted correspond to the intended sense? There needs to be a discussion about this.\n2) I think it would have been better to find standard datasets for relation extraction instead of annotating a few definitions from Wordnet. First, these definitions are short and unlikely to contain all the target relations. Second, they are quite fixed in structure (as opposed to e.g., Wikipedia definitions), which means probably a rule-based method would work as well. In [1] there is a dataset with annotated definitions with hypernyms, which at least would make a challenging dataset for the isA relation. I think this could be replicated with other relations, but again I doubt these will be very prevalent. And this makes sense because a definition should in nature contain the minimal set of features for a term to be out of its kind. Why not mine a large corpus and apply the 'relation retriever'? Probably using ERNIE would be a good choice here it's been precisely trained to do well on relation extraction. Bottomline is we need statistics of the prevalence of extracted qualia structures from wordnet definitions, probably plotted against definition lengths.  [1] Navigli, R., Velardi, P., & Ruiz-Martínez, J. M. (2010, May). An Annotated Dataset for Extracting Definitions and Hypernyms from the Web. In LREC.\n3) The results are confusing to me because these similarity/relatedness datasets, in theory, should be used to evaluate the word embedding model in itself. Training a linear regression with the gold standard similarity scores to me seems quite unorthodox and against the point of these datasets. I would like to see some references to back this choice. Why not use word-level classification datasets where a supervised approach would be less frowned upon, like McRae feature norms, diffvec or the Glasgow norms dataset?\n4) There is no discussion of why dict2vec outperforms glove and retrofitting. I think this would be interesting, especially since retrofitting is also using wordnet (I believe?).\n5) I think most of the results in the appendix should be part of a longer, more broken down paper. In its current state it feels too much like work in progress.",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "3127ebe0e97223f2",
    "paper_id": "COLING_2020_46",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- extensive experiment section which assesses separately the role of sharing each component of the network between the two tasks (word embeddings, bi-lstm layers, tree-lstm).\n- strong empirical results: the mtl system obtains a substantial improvement over the baseline - excellent writing: the description of the proposal is very clear and motivated.",
    "weaknesses": "- final test results only provide the baseline and the proposed system. For additional context, I would have appreciated having the current best published results for both datasets in a comparable setting (supervised system, no pretraining), and overall.",
    "comments": "",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "e29390d4d9b4d866",
    "paper_id": "COLING_2020_47",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper present a curriculum learning method for NMT which is shown to work relatively well for low-resource settings. The method is applied to the standard Transformer and it is evaluated on several language pairs and different data sizes. The method determines what data samples should be presented to the model based on their difficulty and the model competence. The experiments show that the approach provides for consistent improvements.  The paper is interesting and relatively clear, but there are some points in the paper that need clarification.  The approach is interesting. It is based on Platanios et al. (2019) where sample difficulty and model competence were used. This paper proposes new ways to compute these values, and more importantly, sample difficulty is reestimated during training. \nThe improvements are consistent across different language pairs, although often relatively small. The highest improvements are when training on 50K WMT data.  My main concern is that the improvements are rather small, on many test sets it is under 1 BLEU. Also, I am concerned if the model used for 50K WMT is overparametrized. This is the setting where the highest improvements are obtained, but it seems like this is a large model for such a small dataset. Dropout should probably be larger as well. Sennrich and Zhang (2019) (Revisiting Low-Resource Neural Machine Translation: A Case Study) provide details on how to better train low-resource models. \nI am concerned if the method would prove as effective in low-resource settings if the models are more optimized for the low-resource setting.\nI am not sure why are there some differences in model sizes across the different language pairs. These seem rather arbitrary. How were these hyperparameters determined? I do not see a connection between training dataset size and model size or number of layers.\nThe analysis in Figure 2, 3 and 4 is interesting. However, I am curious if these effects would be noticeable in the other experimental settings, where in almost all cases, improvements are under 1 BLEU.\nThe results in Koehn and Knowles (2017) with regards to low BLEU scores in low-resource settings have been addressed in Sennrich and Zhang (2019). \nI agree with the sentiment that performance in low-resource settings is often lacking, but using this reference may not be appropriate.  Typos and writing style:  including language model -> including language modeling mini-batch contains sentences -> mini-batch containing sentences Algorithm 1, line 1: Randomly initial -> Randomly initialize I would suggest normalizing all scores in Table 3 to 2 decimal points.\nIn this paper, we propose a dynamic model competence (DMC) estimation method... - this paragraph is confusing. I did not understand what is the \"prior hypothesis of the training process\" nor how is BLEU superior if the loss is already the optimal method to estimate competence.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "00961b7cb1722b16",
    "paper_id": "COLING_2020_48",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper describes work on automatic emotion assignment to Kannada-English code-mixed tweets, with the emotions tags being ‘Happy,’ ‘Angry,’ ‘Sad,’ ‘Fear,’ ‘Disgust,’ ‘Surprise’ and ‘Multiple Emotion’. The first part of the paper outlines the procedure of corpus creation and manual emotion annotation. The second part describes experiments in automatic emotion assignment using SVMs and LSTM as machine learning techniques, based on 9 features: character n-grams, word n-grams, repetitive characters, negation words, punctuation, emoticons, capitalization, emotion words and intensifiers. Accuracy rates of 30% and 32% are reported, respectively.  The paper is relevant for COLING, and the data it is based on are quite interesting, as they illustrate a particularly challenging context for automatic text handling. It is not clear though if the corpus is/will be publicly available. The description of the manual annotation process is fairly complete, although information about the guidelines provided to the annotators would be more useful than an almost full page of examples.  The experiments are described in a fair amount of detail. However, in the results, nothing is said about how the accuracies obtained in the experiments compare to related work. In fact, related work in section 2 only covers previous studies on the automatic processing of Kannada. In other words, the study is presented in a vacuum in terms of emotion prediction studies, and unless the reader already knows about the results obtained for other languages/contexts, there is no way of situating the present study in a bigger picture. This is what the present reviewer finds to be the most negative aspect of the paper overall.  Among smaller issues, if accepted, the paper would need proofreading before publication. There are also some typos (e.g. 3,34,600 on p. 4 should probably be 334,600).",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "ee97d6626909606f",
    "paper_id": "COLING_2020_49",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Problem definition and motivation is verbosely presented. Section 3 provides a great background on PCL (though I recommend the authors to add a few more examples while explaining what PCL is and what is not). The overall outcome of the work is a challenging NLP dataset which, if released publicly, could be a very valuable resource.",
    "weaknesses": "Since this is a dataset paper, the technical limitations are not judged. However, a discussion on what modification could be applied to the vanilla classifiers to obtain performance improvement is necessary. Perhaps, it is worth proposing an incremental version of BERT based model that is tailormade for the task.  A note on how to trivially and non-trivially extend the dataset to other domains and languages should be provided.  Minor: Report kappa scores for Step-2 as well (in section 4.2.2) and provide some examples of such spans.",
    "comments": "",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "57516f820c5446c8",
    "paper_id": "COLING_2020_4",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper consider the relation classification problem in the presence of unseen (novel) relations in a zero-shot setting. Compared with existing approach leveraging external artificial descriptive information, the proposed approach considers external knowledge graph embedding and further the logical rule mining to improve relation representation. The approach is novel, and extensive ablation studies show the effectiveness of proposed approach.\nThe idea of leveraging knowledge is novel. The paper reads good, however it might be difficult for readers new to zero shot learning.   Comments, - Zero-shot as the backbone of the approach is not clearly described. Despite the description of related works in Sec. 2. and math equations in Eq. (2) and Eq. (3), it is still hard to get the essence and motivation of zero-shot learning given the current manuscript. As a consequence, I am unsure about what is needed for zero shot learning and what is learnt  and what is the training objective function given Eq.(2) or Eq.(3) during the training phase.\n- It might be better to provide comparison with the existing approaches (Levy et al 2017 and Obamuyide and Vlachos, 2018) leveraging different external materials given the same training set.\nSome typos, - Fig. 1 is squeezed - There is no space between ‘(’ and the preceding letter in Sec. 3.2 - Some symbols above Eq.(5) is not in math environment.",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "19b3da4ecaf8f422",
    "paper_id": "COLING_2020_4",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This submission proposes to address the relation classification task in a zero-shot manner, i.e., for relations unseen during training time. The proposed method relies on word, graph, and rule embeddings, and their combinations. The results show that combining rule and graph embeddings yields best performance. The analysis shows that while this holds for most relations that benefit from the graph structure, it does not hold for relations that are sparse in the graph.\nStrong points: (S1) This paper tackles a timely challenge, of classifying relations without training data.\n(S2) The method is intuitive and well-described.  (S3) The paper is overall well-structured and understandable.\n(S4) The results clearly show the superiority over the compared baselines.\n(S5) The additional analysis and discussion gives welcome insights into where the method works better and where not.\nWeak points: (S1) My primary concern is that while indeed the 'unseen' relations are  not given during training, they can be assumed to be present in Wikipedia and in Wikidata, making them very accessible to all embedding methods. In addition, the data selection is biased towards the most populous 100 relations, the bottom 30 of which are considered 'unseen'. While that is true, it is also the case that they would be covered well in the background resources.\nWhile this strong dependency/assumption of the completeness of the underlying resource does not need to be problematic, in this case I find it to be a problem, in particular because its role in the results is not discussed/acknowledged in the paper.\n(W2) The related work on zero-shot RC seems quite sparse. Is this meant to be exhaustive? I'd suggest adding more recent work here, e.g. by Qin et al. (2020) on \"Generative Adversarial Zero-Shot Relational Learning for Knowledge Graphs\".\n(W3) The paper does not give an impression about the relevance/size of the zero-shot relation classification. How many of the relations are 'unseen' in a real use case? Would this be a long tail of many infrequent relation? Even if there is no exhaustive study on this, it would help the reader understand the significance of this work.\n(W4) Some important decisions should be motivated better. For instance, \"We drop relations from the cluster where all relations’ instance number is less than 500 with the assumption that there is no support from related seen labels.\" - why is this a good/reasonable assumption? Please elaborate Other comments: - in the introduction, what are 'traditional' methods?\n- In multiple occasions, the paper uses unsupported claims. For instance, the introduction states at least three times that various decisions of the approach are 'natural', but it is not clear why that is: this should be defended more clearly. Similarly, in section 3.2 \"PCNNs has been proven to be effective in RC.\" needs support/citation.\n- in section 3: what does this mean: \"Meanwhile, connections of relations are harvested.\"\n- In 3.1 - I think you mean \"relation instances\" (or \"occurrences\") - as a single relation might occur multiple times in the corpus - The citation style is not always correct. For instance, \"(Norouzi et al., 2013) proposes\" should become \"Norouzi et al., (2013) propose\" - Wikidata 2020 has much more than 594 relations - which version is used in the paper?\n- The reason why 'mother' is better captured in Wikipedia is probably because this information is mostly stated inline and as such is not extracted into Wikidata. So it is reasonable that the word embeddings can capture it better.",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "3da1f78e7f93de78",
    "paper_id": "COLING_2020_50",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- Proposed method is intuitive and well motivated, and explained clearly for the most part. There are several moving parts, but they are all tied together well.\n- Experimental results are very promising, and the provided examples illuminate the advantages of the proposed solution well.\n- In addition to automatic evaluation, the authors also took the effort to perform human evaluation and other attention-based analyses of the model.\n- Paper is well-structured and easy to follow.",
    "weaknesses": "1) There's very little dataset analysis in this paper, which makes it hard to know exactly how challenging the problems posed by these datasets really are. The original SAMSum dataset paper doesn't have any analysis either, which makes it all the more important to have some kind of analysis here in this paper. The other dataset used here (the Automobile Master Corpus) has neither an analysis nor a citation, and there are also no examples whatsoever of this dataset. Some questions that would be important to answer, at the very least, are:    a) What are the length distributions of the summaries and the dialogues respectively, and what's the relationship between the length of a dialogue and the length of its summary? \n  b) How many topic words does each dialogue have? How many of those actually occur in the final summary? \n  c) What are some interesting discourse phenomena that need to be correctly modeled in order to generate an accurate summary? \n2) There's no analysis of examples that this model performs poorly on, or other gaps that need to be addressed. \n3) While it's great that human evaluation was performed, it's lacking in a couple of different ways:   a) The human evaluation metrics should be described in further detail. What exactly do \"relevance\" and \"readability\" encompass? What were the guidelines given to the evaluators to rate these? \n  b) Were the examples double/triple reviewed in any way? \n  c) Related to (a) - another important aspect of a good summary is its completeness. Did either of the human evaluation metrics encompass completeness? \n  d) Human evaluation wasn't conducted on the gold summaries, which is unfortunate because it would provide a sense of an appropriate ceiling for the human evaluation numbers. \n  4) Some of the claims in the paper don't really seem to follow from the results of the experiments. For example, the paper makes the following claim from the human evaluation results:      \"As we can see, Pointer Generator suffers from repetition and generates many trivial facts. For Fast Abs RL Enhanced model, it successfully concentrates on the salient information, however, the dialogue structure is not well constructed. By introducing the topic word information and coverage mechanism, our TGDGA model avoids repetitive problems and better extracts the core information in the dialogue.\" \n   However, I don't know if any of these claims can be directly inferred from the results of the human evaluation (unless there are aspects of the human evaluation that were not described).\n5) For a model as complex as the one proposed here, I would have liked to have seen some kind of ablation analysis that shows the importance of each of the moving parts. While the proposed approach makes sense intuitively, there's not enough convincing experimental evidence to show that each of its parts is crucial (even though the results section seems to claim this).\nOther",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "a804d8709c7db648",
    "paper_id": "COLING_2020_51",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "1. Even though the paper contains several typos and unclearness, it is well-written and easy to follow.\n\t2. The proposed mechanisms make sense. The EGA provides a context-aware entity encoder and the CAT is a form of negative sampling for mitigating difficult cases. Both contributes to the final model for this task.\n\t3. The study of How to Gate and When to Gate is interesting and worth deeper analysis.",
    "weaknesses": "1. The main weakness is that the result on the test set is NOT reported. I can understand that since the FewRel test set is not publicly available and they asks task participants to submit their models to get the test result, it is hard to control the turnaround time. However, the competitive result on the validation set cannot convince everyone that their model",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "f2ffd4153ab09c80",
    "paper_id": "COLING_2020_52",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper focuses on lexical simplification of text, which is a well known task. \nThe paper specifically focuses on multi-word simplification, which is a relatively new focus. \nThis covers cases where the original phrasing and/or the replacement can be one or more words (up to 3 in this paper). \nThe research is well motivated. \nNotably, most of the recent (and cited) research focuses on complexity of words/MWEs for non-native English speakers, and the current paper does not mention whether it is in the same 'game' - this should be explicitly stated. \n(complexity for native speakers is a related but quite a different story).\nThe authors developed a new corpus of manually annotated lexical simplifications, which is a good contribution to the field.\nThe paper describes corpus creation, and development of a lexical simplification system, which is evaluated on the corpus data.\nCorpus Data for the corpus came  from the same sources as the annotated corpus of Shardlow et al. (2020). \n10K sentences were selected from each source.   Identification of complex words (potential targets for replacement) is a crucial step. \nFor corpus preparation, the authors used a CWI system from Gooding&Kochmar (2019). \nThen 1K sentences (per each source) were selected from that data for manual annotation. \nDid you try to verify the automatic CWI annotation?  The crowdsourcing section needs some more detail. \nDid you use only native English speakers for annotation (as Shardlow did) ? \nbecause workers could be from English-speaking countries but still non-native English speakers. \nSo this aspect needs clarification (and justification if non-native speakers were used).\nAlthough some measures of work-quality were taken, I am absolutely not convinced about the annotation is pure and gold-quality. \nI tend to accept that turkers working in good faith tried to provide rephrasings that were close to original meaning  (as they understood it). However, the problematic point is to what extent their rephrasing are simplifications, and not just equivalent complexity or even introducing more complexity. \nI would expect some validation measure, to show that most of the rephrasings are indeed simplifications. \nYou could give a sample to teachers for evaluation. Educators (teachers) are skilled at this. \n As it stands, I see no guarantee that the MWLS1 data set contains only simplifications, it might be slightly 'contaminated'. The low annotator agreement may be related to this. \nSince the corpus  is used for evaluation, validation is a crucial step.  It  is lacking.\nThe simplification system, Plainifier, includes several steps. \nSection 4 describes it and already makes an important omission. \nThe description directly goes into candidate generation. \nBut how does the system identify potential targets, how is CWI performed? \nIs the CWI system from Gooding and Kochmar used / included in Plainifier? \nOr some other module?  Does Plainifier include CWI at all? \nQiang et al. (2020)  also omit this aspect. But for a full system, this must be described.\nCandidates for target-replacement are generated by using BERT, following Qiang et al., but with a twist for MWE targets and for generating MWE candidates as well. \nCandidates are then ranked using three aspects: 1) probability in context (how well they fit with the surrounding context) - obtained from the BERT LM 2) preservation of original meaning (via semantic similarity using embeddings) 3) familiarity - using frequency in Google Books ngrams as a proxy. \nThis again closely follows the methods from Qiang et al. Evaluation is carried out on the MWLS1 corpus. \nThe results look promising. \nSeveral measures are used for evaluation, and this is really good. \nI am happy you didn't use BLEU, it is a bad measure  (see paper \"BLEU is Not Suitable for the Evaluation of Text Simplification\" in EMNLP 2018).  One specific aspect is not well covered. \nAccording to table 2, about 500 instances in MWLS1 corpus are one-word targets (about 1/3 of the corpus). \nTheir  replacements  are on average 1.2 words long (so mostly one-word). \nSince the paper focuses on MWEs, it would nice to see how the system performs on just MWEs separately, and the 500 separately. \nThis would provide a better understanding on how well it works for MWEs.  Another aspect. \nAs stated above, CWI in Plainifier is not explained. \nWhat did you use for evaluations? \nIt seems no CWI was used in evaluation (annotated segments were taken directly as complexity sites?) \nPlease state it explicitly or explain.   Interestingly, tuning results in section 5.2 show that similarity and familiarity are more 'important' than fit with context, and that preservation of meaning (similarity) is most important by far. Since candidates are coming from BERT, this is not surprising. If candidates were coming from a synonym-resource, contextual-fit and familiarity would be more important.\nI scored \"Originality\" as 4. Much of the work is based on ideas in Qiang et al., but the provision of the corpus covinced me to go for 4.\nOverall, nice work!",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "844c9a88c7d7e69a",
    "paper_id": "COLING_2020_53",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper studies review generation given a set of attribute identifiers such as user ID, product ID and rating. As this information is scarce, the authors propose to select more information related to these attributes to enrich the generation process. The evaluation results, obtained by comparison with other approaches and human judgments, are promising.\nThe paper is appropriate for the conference, well-written and easy to follow. The addressed problem is interesting, and the approach and obtained results are promising. The idea of \"enriching\" the input information by collecting other reviews that are related to the input is interesting and solves the problem of having almost no information at the beginning of the process.\nIn the SL-Reflect part, it is said that a ground-truth review Y is obtained. This Y review is very important for the rest of the process, but how is it obtained? Would it be available if this is the first review of a particular product or by a specific user?\nI am also curious about how this technology could be used in the real world... As a user who reads reviews when choosing to buy something, do I want to rely on \"false\" reviews that could not be accurate or express the real characteristics of the product? On the other hand, I think this kind of approach could be very useful to give a user the first draft of his/her review, which can later be corrected in a short time and be more beneficial for other users...",
    "overall_score": "4",
    "confidence": "2"
  },
  {
    "review_id": "3031c033f83e4aab",
    "paper_id": "COLING_2020_54",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The motivation of this paper is clear, i.e., to solve the problem that \"What would be the best generic ALD ...\", as described at the beginning of paragraph 2, section 1. The generic abusive language typology is categorised as two aspects, i.e., target aspect and content aspect, and multi-aspect embedding layer considers embeddings of both target and content, followed by cross-attention gate flow to refine four types of embeddings. The proposed model outperforms baselines on all the seven datasets. Detailed ablation studies have been given in section 5 as well.",
    "weaknesses": "For the structure of the paper, section 4 can be integrated into section 5 as a sub-section.\nthe description of baselines in section 4 is too detailed, which should be refined and shortened appropriately.\nThe paragraph 3, section 5.1 can be titled as a sub-section called \"case study\" since this paragraph analyzes some prediction examples in Table 3.",
    "comments": "",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "e4fe14b878c45979",
    "paper_id": "COLING_2020_55",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The authors propose to use LSTM models for age prediction for sentences and texts. Extensive experiments show that the best model outperforms the SOTA results (especially) at sentence level. It deserves further consideration to design better solution for text-level age prediction in the future. Overall, the paper is clearly written and well organized.\nSome typos or grammar errors: 1. section 2, \"70/17/16%\" => \"67/17/16%\", \"67/15/15%\" => \"70/15/15%\"; 2. section 2, \"all sentences of a given A text\"; 3. page 3, paragraph 1, \"The idea it gets more ...\"; 4. page 3, paragraph 2, \"The first one CONSIST in ...\"; 5. the information of reference (Schwarm and Ostendorf, 2005) is not complete.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "3abf4f2012fe8860",
    "paper_id": "COLING_2020_56",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- Instead of focusing on \"what models can be used for evaluating\", the paper studies \"what aspects need to be evaluated\". The questions investigated are interesting and meaningful for the community.\n- A considerable amount of experiments have been carried out to verify the authors' hypotheses, and relatively good results have been achieved to support the hypotheses.",
    "weaknesses": "- The authors used USR (which is USL-A in this paper, [Mehri and Eskenazi, 2020]) as a main baseline, compared to which configurability is not novel as it has been proposed in USR.\n- The proposed USL-H differs from USL-A in that it composes three scores in a hierarchy. I consider such hierarchy as the main novelty. In order to verify the validity of the hierarchy (Section 6.1), the authors ought to compare three score composition methods (A. Human vanilla, B. Human USL-A, C. Human USL-L). However, only A and C have been compared (Figure 3).\n- The proposed USL-H achieves the best correlations according to Table 4, but it only achieves marginal improvement over the second best metric, BERT-NUP.  - Some terms are not explained: 1) What is the difference between PONE and EN-PONE? 2) What is the difference between \"weighted\" version and \"unweighted\" version of USL_S-A/H in Table 4?",
    "comments": "",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "77fbe56ec66dbd44",
    "paper_id": "COLING_2020_57",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- This is an interesting evaluation with various attempts to explain the observed results. Conclusions/recommendations are provided as well. Data quality tends to be a problem that needs to be addressed, either because collected data is too simple/easy/naively collected or because it contains errors.\n- The article is well-written.",
    "weaknesses": "- Experiments used mostly code released by the authors of the original papers. There are some guesses/default values used here. It would be good include a note on how the performance reported here for single target datasets compares to the numbers presented by the system developers -- especially since this is done on SimpleQuestions, which is not greatly altered by the filtering of multiple-triple questions outside of FB2M.",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "40d85283ba19fcb7",
    "paper_id": "COLING_2020_58",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "of the study are more significant and I'd recommend accepting this paper at the conference. However, I can do that only with limited confidence, since while I found the technical part very clear and easy to follow, I'm not familiar with the application area here, which seems to contain quite a few previous articles judging by the provided bibliography.",
    "weaknesses": "",
    "comments": "",
    "overall_score": "4",
    "confidence": "2"
  },
  {
    "review_id": "4d5cd3e9027f2516",
    "paper_id": "COLING_2020_58",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper focusses on the analysis and identification of press release documents that deviate from the original observational study that they refer to, in the sense of exaggerating results. More specifically they focus on health topics and propose a pipeline for identifying press releases that propose a causal relation while the corresponding publication proposes a correlational relation.\nThe task is interesting, and the corpus would be a useful addition to the community, allowing for the development of further systems addressing misinformation. I have some concerns on the strategy and annotation process: More specifically: -- I am wondering if there was some manual estimation while annotating sentences of whether, in the case of matching/contrasting classes between the press release and the scientific paper, were actually referring to the same thing. I understand that this was performed on the article level, but was there some sentence-level evaluation as well? \n-- It would be useful to present the process of deciding whether a scientific paper is correlational or causal, in the case where the conclusion contains multiple sentences, classified under different taxonomical classes, e.g., direct causal + correlational. Are only papers containing solely correlational and non-claim sentences considered correlational? You seem to be hinting at the latter later on in the paper, but it should be better clarified at the part where the classification strategies are described (section 5.1).\nThe paper is well written and the authors explain in detail the procedure for selecting the final documents in the corpus. Some minor re-organisation would benefit the paper since some information is mentioned in some section and revisited-expanded in some later section leaving open questions in between and obstructing the reading (see comments below).\nI am missing the details of fine-tuning the BERT models on the task: What parameters were used? Likewise, for the use of augmented data, the pre-processing details should be explained either in the main paper or the appendix. These are important for reproducibility. Furthermore, I am wondering if the authors considered other models apart from BERT that have shown to outperform it such as XLNet, ELECTRA, etc. These additions would further aid in the interpretation of results and appreciation of the task.\nFinally, for the interpretation of results, it would be good to know how fig.3a was calculated. I assume that the rate was calculated as #of exaggerated press releases over the total number of press releases referring to papers with correlational conclusions? Or was it over the total number of press releases? In either case, it would be useful to visualise the evolution of the rate of press releases referring to correlational versus direct causal conclusions, in order to be able to better interpret the results. As it is currently presented I find it hard to properly interpret the presented results and claims in the discussion.\nOther comments: Section 1- Introduction: -- \"Furthermore, 14,426 observational studies contained structured abstracts, in which the sentences in the conclusion subsection were used as main statements in research papers.\" -- > While it is made clear later on that these constitute the final set of papers used, it is not properly explained here. It is better to clarify early on. \n-- Provide either a link or a citation for EurekAlert Section 2 - Related Work: -- \"missing of information\" --> missing information -- it is better to use consistently either the numerical or the word form of numbers (e.g., you mention '16 questions', and then '10 criteria' in the next sentence. \n-- 'Causal relation can be broadly defined ... ' --> 'Causal relations ...? or 'A causal relation... -- 'Causal relation can be broadly defined as any type of cause-effect relations, such as in the NLP task' --> it would be better to provide an example here.\nSection 3 Corpus Construction: -- Perhaps precision would be a more suitable metric compared to accuracy for the LightGBM classifier, as I believe the important task is to avoid a high number of false negatives (regardless,  accuracy does seem sufficiently high). \n-- 'conclusion subsections' refers to both conclusions from the abstract and conclusions from the main body? It is a bit unclear here. I understand based on the conclusions section of your own paper that you probably take only the abstract conclusions, but it would be better to state this clearly and earlier on. \n-- In our taxonomy, “can + causal cue” belongs to the category of “direct causal”. -- > I am a bit uncertain of what is implied in this sentence. Is this rule in the taxonomy wrong based on the authors' error analysis, or is it the case that there are more examples that satisfy the rule rather than those that contradict it? In either case, is there a way to revise the markers in the taxonomy in ways that better capture the task at hand? \n-- The error analysis examples are very interesting! I am wondering if it would be possible to group them and show what percentage of errors do they represent in the results? \n-- Figure 2: Claim aggregation algorithm for press releases --> for clarity I would change to: \"Claim aggregation algorithm for press releases that correspond to papers identified as correlational\"",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "eec4304e9d51f5ec",
    "paper_id": "COLING_2020_59",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper is about named entity (genes, diseases) recognition of Chinese biomedical texts. The authors present a large collection of unannotated patents, and a small subset (5k sentences, 21 documents) manually annotated. The paper also includes experiments with named entity recognition, and relation extraction (between the named entities of interest).\nThe data set presented (released publicly) is likely to be useful for others; the NER method follows a rather standard, but reasonable, approach; and the relation extraction experiments, although preliminary, may be of interest to NLP community as well. The paper is generally well structured, and well written (see below for a few detailed comments).\nMy only (minor) reservation of the paper is that it may be slightly too specific and technical for COLING. The paper mainly is about a practical NLP problem, involving rather little (computational) linguistics.\nI have some detailed comments/suggestions below.\n- I could not fully understand the averaging. In particular, does the   averaged scores on Table 4, include 'O' label? It seems not (good),    but it would be best to explicitly state it. Related to this, is   there a reason for reporting micro-averaged scores? Clearly there is   no clear preference towards recognizing majority class(es) better   than the minority class. \n   - The NER results presented show really large variation over the cross    validation folds. With the variation at hand, I think it is very   difficult to decide the \"best\" model with certainty. Hence, I   suggest: (1) softer conclusions, and (2) maybe trying model   averaging or a similar method to reduce the variation. I definitely   like that authors reported the variation in the scores, the   suggestions should not be understood as artificially reducing it   (e.g., testing on a single validation split), or definitely not    reporting the variation. \n   - The authors report that they reserve one of the documents as 'dev   set'. However, the results presented are 5-fold CV results, and   there is no report of additional hyperparameter tuning. Hence, it   sis not clear what the purpose of the dev set is.\n- The 'mutual F1 score' used for IAA is not defined, nor there is a   reference in the paper.\n- More a question than a criticism: the authors present code-mixing as    a difficulty. Isn't code-mixing helpful for NER? I'd expect more   code mixing in named entity boundaries, hence, overall I expect it   to be helpful (although, it may be specific to the domain/task).\n- I suggest also specifying the sources (BC/HG) of the annotated data    set in Table 2 (like in Table 3).\n- Typos:     - I suggest use of more descriptive words or abbreviations in       table headings. \n    - Section 3.2, paragraph 3: \"entity type type\" -> \"entity type\"     - Suggest using (or not using) punctuation after captions       consistently. \n    - The references contain some lowercase \"named entities\", like       'bert', 'chinese' (likely BibTeX normalization issues), I       recommend a through check.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "c27d9f95f680f34b",
    "paper_id": "COLING_2020_5",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper presents a new variant of hypernymy discovery (SemEval 2018 task) similar to a single step in taxonomy induction: given a target WordNet synset, predict its direct hypernym. The suggested method is trained as a sequence generation task, which, given a synset, predicts the entire hypernym chain. The method is compared with a version that only predicts the direct hypernym, which performs slightly worse.  The paper is well written. The authors did a good job comparing to a diverse set of baselines (from KB completion, taxonomy induction and hypernymy discovery literature). There is an interesting error analysis, and throughout the paper, design choices are well supported. For example, I was happy to see the distinction between instance and class hypernyms, and between nouns and verbs (which is often ignored). There is a good level of technical details.  Questions and comments: - I have a bit of an issue with the generic name for the task because it doesn't differentiate it from the previous tasks. E.g. is it defined differently from \"taxonomy induction\"? Is it a variant of \"hypernym discovery\" (SemEval 2018), except for direct hypernyms within a taxonomy?\n- I was a bit worried about test set leakage when I saw that the hypo2path model performed better than hypo2hyper. If I understand correctly, this is not the case (as explained in 4.1). I would like to see it more clearly explained: how exactly did you make sure that paths are not shared between the data sets? And in that case, what explains the success of the path-based model? Is it like multi-task learning or data augmentation?\n- Please elaborate on “Luong-style attention” - For the discriminative model (“path encoder”), how were the distractors selected?\n- How did you adapt the CRIM model to be synset based (rather than lemma based?)  - While I agree with your view on why it’s more general not to rely on definitions, I think this comment would have been better supported if your method was evaluated on a taxonomy that doesn’t contain definitions. Otherwise, if the method is only evaluated on WordNet, what difference does it make? You might as well use the definitions.\n- I don’t understand the claim before footnote 6.  - I like it that you have an additional metric other than a strict accuracy metric, but I wonder whether WuP is a good metric considering that it measures *similarity* which is different from *hypernymy*. Wouldn’t it reward models for predicting a similar synset such as a co-hyponym? I'm curious whether it's not the case that models typically err on predicting a similar but incorrect node, in which case they would score highly on this metric despite being fuzzy and imprecise.\n- For the camera-ready version, will you be able to include the ablation results?\n- Are the embeddings of synsets without pretrained embeddings, which were initialized randomly, still frozen?\n- With respect to the fact that it’s easier to predict instance hypernyms, this was also one of the findings in the SemEval 2018 task: “systems tended to perform better with entities than with concepts. This is probably due to the fact that entities contain many hypernyms which appear often (e.g. person, company)”.\n- “...and has a number of problems described in Richens (2008).” - like what?\n- Which relations did M3GM perform well on?\n- Regarding “path validity” in the error analysis - does this indicate memorization? How would your method work on a larger taxonomy?\n- I would separate the error of predicting indirect hypernyms (somewhat valid, depending on how far they are) and siblings (invalid). WordNet is inconsistent with the distance between hypo-hyper pairs. For instance, dog and mammal have 5 edges apart but mammal is a valid hypernym for dog (despite not technically being the direct one) while cat (its sibling) definitely isn't.\n- Regarding polysemous lemmas, this is interesting because the model is expected to memorize the synset ID associated with each sense, which doesn't make much sense to me. A more natural setup would be to find hypernyms for words in context, by combining the task with WSD. But I understand it's outside the scope of this paper.\n- Figure 1 - Consider replacing with vector graphics for better quality (or enlarging the figure).",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "3f1d5d7f2b96d783",
    "paper_id": "COLING_2020_5",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The motivation part does not feel fully connected the the main goal of the paper, and is somewhat unnecessary. I need to know what hyonymy is, and that it is hard for embeddings, but if (not the case) I don't think it matters, it is probably because I don't care about semantic relations, and claiming is-a is more important than other relations won't convince me.\nThe connection between the first two sentences is confusing. It is implied that what makes IS-A important is its usability. I believe it is commonly used because it is simple and well defined, not because it is more important. \nOn a similar notion, the finding that \"it is more difficult to predict hypernymy than other lexical relations\" is exaggerated, this is not a main claim of the paper and it is more difficult than some other lexical relations, in the specific context there, but it might be easier for other methods, and there are other possible relations which are not discussed there.   \"There has been much recent work on\" - requires citations (survey if exists, otherwise some dominant papers) Noun path of length 5 is comparable to the whole path. Is path of length 2 comparable too? If so, what does that mean about the usefulness of the path? \nWhere is the path expected to have an impact (e.g. in verbs and instance nouns it does not)? It seems the reasons for the path improvement are not trivial and worth exploring a bit more.\nRegarding path validity, are any of the paths ones that were not seen as a full path in the training? If not, then this finding is much less surprising, It worth noting either way.\nThe error analysis is really nice.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "4ac76ffb7ed4ca65",
    "paper_id": "COLING_2020_60",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- as far as I know, the proposed method is a new approach for solving the target problem whereas all the previous post-processing methods (retrofitting, counter-fitting, attract-repel) are more or less built on the same schema.\n- this new method offers the possibility to control more directly the injection of semantic knowledge into the embeddings by applying linear algebra operations while existing approaches are based on the definition of objective functions between opposite terms that are optimized through SGD, which does not offer a clear view of the applied changes.\n- the article proposes a simple but interesting way, called thesauri contagion, to extent the initial set of synonyms and antonyms.\n- HRSWE is able to outperform Attract-Repel in some experiments and seems to be globally more robust than Attract-Repel. However, the main difference between the two method concerns their speed: HRSWE is much faster than Attract-Repel.\n-",
    "weaknesses": "- the main weakness of the paper concerns the results of the proposed method, compared to those of the reference method, Attract-Repel. This weakness is twofold. First, HRSWE is far from outperforming Attract-Repel in all the experiments and moreover, the differences between the two methods are always small. Since no indication is given about the application of statistical tests, we do not know if these differences are significant or not. For instance, a difference of 1.4 points for Spearman correlation on SimVerb-3500 is not necessary a significant difference. Hence, it is difficult from these results to state that one method is actually better than the other. Moreover, they have the same number of hyperparameters. It should also be noted that while HRSWE-3 generally obtains the best results among the different versions of HRSWE, this is not true in the case of lexical simplification, which also contributes a little bit to blur the interpretation of results.\nBut the main issue concerning the evaluations relies on the use of thesauri contagion. As mentioned above, this is an interesting way to expand the reference resource but it could also be applied to Attract-Repel for enlarging the number of the relations it exploits. As a consequence, comparing Attract-Repel without thesauri contagion with HRSWE with thesauri contagion is not actually fair. The paper gives the results of HRSWE without thesauri contagion but that case, except for the lexical simplification task, Attract-Repel is always the best method (we do not know for robustness).\n- the idea of comparing HRSWE and Attract-Repel in terms of robustness is interesting as this kind of aspect is important but rarely addressed. However, the global objective of this evaluation is not very clear since the target perturbation concerns possible errors about synonyms that are falsely considered as antonyms and vice versa. This case is not likely to happen with reference resources since synonyms and antonyms are generally not ambiguous for humans. In that case, ambiguities between synonyms and hypernyms for instance could be more interesting to consider but are out of the scope of the proposed method. The situation would be different with relations automatically extracted from text since distributional approaches for instance are known to have difficulties for distinguishing synonyms and antonyms. But in that case, words that are neither synonyms nor antonyms should also be taken into account as possible perturbations. This is not the experimented settings for antonyms, which come from BabelNet, something that should be mentioned together with the main characteristics of the reference resource (even if it is described in (Mrkšić et al., 2017)). The synonyms are more likely to raise such issue since they come from PPDB but the acquisition method in that case is not prone to confuse synonyms and antonyms. Finally, the robustness evaluation could also be justified by the results of thesauri contagion but nothing is said about whether this mechanism produces false relations and how many.\n- HRSWE is much faster that Attract-Repel in the experiments of the paper but since these experiments only focus on the relations involving the vocabulary of the datasets, the number of relations is not very high for each experiment (this number should be indicated). But as HRSWE is based on operations such as eigen decomposition, it is probably less scalable for dealing with large vocabularies, even with the use of randomized algorithms, than methods such as Attract-Repel. Finally, even if HRSWE is faster than Attract-Repel, the durations in both cases are not very high and are not actually problematic.\n-- Specific remarks - related work: the part about post-processing methods is fairly limited, with missing references.\nOf course, the article about retrofitting, which is not cited whereas the word \"retrofitting\" is used: Manaal Faruqui, Jesse Dodge, Sujay Kumar Jauhar, Chris Dyer, Eduard Hovy, and Noah A. Smith. 2015. Retrofitting Word Vectors to Semantic Lexicons. In 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2015), pages 1606–1615, Denver, Colorado.\nCounter-fitting is a previous work of the authors of Attract-Repel that takes into account both synonyms and antonyms: Nikola Mrkšić, Diarmuid Ó Séaghdha, Blaise Thomson, Milica Gašić, Lina M. Rojas-Barahona, PeiHao Su, David Vandyke, Tsung-Hsien Wen, and Steve Young. 2016. Counter-fitting Word Vectors to Linguistic Constraints. In 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL HLT 2016), pages 142–148, San Diego, California.\nIt also could have been a method to test in the evaluations.\nParagram is the method that was extended by Attract-Repel for integrating antonyms: John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. 2015. From Paraphrase Database to Compositional Paraphrase Model and Back. Transactions of the Association for Computational Linguistics, 3:345–358 - hyperparameters for Attract-Repel Intervals of tested values are given for each parameter but the best value is never provided. Moreover, Mrkšić et al. (2017) adopted a batch size of 50 while the minimal value is equal here to 64, with the exception of a value of 32 for lexical simplification. The upper bound for the number of epochs could also be higher.\n- hyperparameters for HRSWE The best values of hyperparameters are not given for the lexical simplification task.",
    "comments": "",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "7dfacbd9d54a6266",
    "paper_id": "COLING_2020_61",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper describes (1) new corpus resources for the under-resourced Kinyarwanda and Kirundi languages, (2) preliminary experiments on genre classification using these corpora. The resources are described thoroughly, and a useful survey of related work on these languages is presented. A variety of models are used in the experiments, and strong baseline results on this task are achieved, including experiments on transfer learning from the better-resourced Kinyarwanda to Kirundi; an approach likely to play an important role in scaling NLP to the Bantu language family, which has a small number of reasonably-resourced languages, e.g. Swahili, Lingala, Chichewa. Overall the paper should be of interest to COLING attendees.\nGeneral comments: Abstract: \"datasets... for multi-class classification\".  It would be good to note here and in the introductions that this is specifically a genre or subject classification task.\nIntroduction: \"has made access to information more easily\" => \"has made access to information easier\" Introduction, p.2 \"In this family, they are...\" => \"In this family, there are...\" Introduction: \"fourteen classes... twelve classes\". Again, as in the abstract, should make clear what these classes are!\nLast line of p. 2 \"who have not been\" => \"which have not been\" Related work.  You might also note Jackson Muhirwe's PhD work at Makerere; some of which was published here: Muhirwe J. (2010) Morphological Analysis of Tone Marked Kinyarwanda Text. In: Yli-Jyrä A., Kornai A., Sakarovitch J., Watson B. (eds) Finite-State Methods and Natural Language Processing. FSMNLP 2009. Lecture Notes in Computer Science, vol 6062. Springer, Berlin, Heidelberg. https://doi.org/10.1007/978-3-642-14684-8_6 3.3 Dataset cleaning.  I know it's just a change in perspective, but I'd prefer viewing the cleaning and stopword removal as standard pre-processing steps; suggesting distributing these as tools vs. distributing the corpora with these steps applied.  Classifiers should work on un-preprocessed text in any case.   3.4 I don't understand how the cleaning steps you described could reduce the vocabulary from 370K to 300K.  Please clarify.\n4.1 In training the word embeddings, you say \"removing stopwords\".  Does that mean removed from the corpus before training?  I'm not sure I see the value in doing so, and wonder if it negatively impacts the quality of the embeddings.\n4.1 Given the morphological complexity of these languages, I wonder whether results might be improved by working at the subword level (syllables, or morphemes... cf. Muhirwe's work above). This could conceivably help is the cross-lingual training as well. You do have Char-CNN experiments but there may not be enough data to get competitive results at the character level.\n4.3.2 \"different epochs and number of features... different train sets\"; this is fine, but you should refer to the table where these choices are actually laid out 4.4.1 Had the Char-CNN converged at 20 epochs?",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "8950d977b777eb3e",
    "paper_id": "COLING_2020_62",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "of automated reference-based metrics. \nAs you are comparing different systems on relatively small test sets, significance levels for at least some of the metrics should be provided, especially when you are using subsets of only 100 sentences.\nDetails: - Put the figures and tables nearer to where they are discussed in the text.\n- The violin plots seem to suggest a scale of below 0 to above 100, which should not be possible according to the description of the annotations.\n- Are the annotators native English speakers? If not, then it might be difficult for them to judge fluency.\n- Provide a url or reference to LDC2015E86 and LDC2017T10 - FORGe in section 4.1 is not mentioned anywhere else.",
    "comments": "",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "c39647a8dd484dd5",
    "paper_id": "COLING_2020_63",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "On the example of domain adaptation for machine translation using a transformer network, this paper analyzes and visualizes the impact of different model components and specific neurons w.r.t. both adaptation performance and catastrophic forgetting, i.e. general-domain performance. Each component is investigated by either freezing only the component or all other components. Individual parameters are evaluated with respect to their importance by approximating the impact on the loss function by their removal.\nI find that the authors tackle an important problem, the methodology is described in good detail, the experiments well conducted and the analyses insightful. Some of the methods could be more generally applied for interpreting networks. \nMy main concern is that the work seems incomplete without what the authors state as future work: \"Inspired by our findings, we can freeze part of those important parameters during continual training to avoid catastrophic forgetting. Besides, we can retrain those unimportant parameters to further improve the in-domain translation\". Without those experiments it is pretty much impossible to draw conclusions from the parameter-wise analysis, leaving the following questions open: - If we freeze the most important parameters, will the network find a way to replace them with other parameters when fine-tuning?  - Will that mitigate catastrophic forgetting?  - What does the importance matrix look like after fine tuning, if the previously most important parameters are frozen? \nIt seems to me that adding those results will easily fit into the scope of the paper.\nSo I am somewhat ambivalent about accepting the paper. I did gain some interesting insights, but at the same time I wish the authors would get a chance to complete their work before publishing.\nMinor comments: - Fig. 1: Please state the task, model and domain which the graph represents - 2) - notation: ground truth sequence and translation can have different lengths, so you should use different variables (e.g. I and I*) - 4.3.2 - it is unclear on which training examples t you are computing Eq. 7 before and after fine-tuning. I'm guessing it is the general domain training corpus before fine-tuning, and the in-domain training corpus after fine-tuning, but that should be explicitly stated.\n- Eq. 8 - it seems that you should be iterating over individual scalar parameters, rather than modules i. Otherwise the sum over matrices with different dimensions is undefined (assuming that the square and sqrt are computed element-wise) - Fig. 6 - does the x-axis denote importance before or after fine-tuning?\nMissing reference: - \"Compact Personalized Models for Neural Machine Translation\", EMNLP 2018, also investigates the impact of freezing subnetworks on adaptation performance.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "36d61631251d50e0",
    "paper_id": "COLING_2020_64",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- interesting and comprehensive comparison of the new dataset with existing ones - dataset is much larger than previous ones, covering many topics",
    "weaknesses": "Clarity - Introduction is unclear. What \"unsupervised datasets\" are is not explained? the four requirements are presented but not clearly justified - When desiderata are described in detail they refer to 'unsupervised data-to-text models,' that have not been discussed in any ways. They are only briefly described too late in the paper,  at the end in the evaluation section. They should be presented upfront so that the desiderata would be justified more clearly - the first requirement then is very confusing \"Text and graphs should have similar contents, such as a Wikipedia article and a knowledge graph of the same article.\" Isn't this what you need for supervised graph2text?\n- the second one \"To fit for recent unsupervised models...\"  this is an example of why recent unsupervised models should have been explained upfront to justify this requirement.\n- Specific questions on  Sec4  \"and use the titles of these articles to query their corresponding knowledge graphs from DBPedia\" so it is only a matter of retrieving an already existing graph?\n- \"relations that are unlikely to be described in text such as “latitude” and “longitude.”\" how many relations are of this type? what is the accuracy of this heuristic?\n- Step 4 sounds very heuristics. why 10? Was it tested in any ways? ( see more",
    "comments": "",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "a27390e7c7583af1",
    "paper_id": "COLING_2020_64",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "In this paper, the authors present a new dataset for unsupervised data-to-text generation compiled from existing manually and automatically annotated datasets. The paper is clearly written, and the description of the dataset is extensive and clearly detailed. The dataset is likely to be of great value for the community.\nThe authors also evaluate their dataset by testing some off-the-shelf generators and show that some systems achieve high scores in terms of automatic evaluations. It would be a great addition to provide a human assessment of the outputs in order to fully demonstrate the usefulness (and limitations) of the presented data.\n- Intro: Even though it looks obvious, explain clearly why the need to have non-parallel data.\n- 4.1 Step1: Do you have any insights with respect to the quality of the tokenization, i.e. the type of errors observed in the processed data?\n- 4.1 Step2: Is it possible to provide a full list of the relations that are kept as supplementary material?\n- 4.1 Step3: See the paper of Castro Ferreira et al, who performed a similar task manually; https://www.aclweb.org/anthology/W18-6521.pdf - 4.1 Step4: Is there a mechanism to ensure that the text does not express more than the input triples? What are the specific challenges here?\n- 4.3: According to the WebNLG 2020 website, there were 15 categories  in 2017:  “The 10 seen categories used in 2017: Airport, Astronaut, Building, City, ComicsCharacter, Food, Monument, SportsTeam, University, and WrittenWork. The 5 unseen categories of 2017, which will now be part of the seen data: Athlete, Artist, CelestialBody, MeanOfTransportation, Politician.” \nAlso, the number of reported data-text pairs is over 25,000 (https://www.aclweb.org/anthology/W17-3518.pdf) Typos/style: - DBPedia -> DBpedia - Conclusions: “which implies that even WebNLG and GenWiki” -> even though?",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "538e665324ddc3e6",
    "paper_id": "COLING_2020_65",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper presents a dynamic memory network for multi-turn dialog generation. The aim is to enable dialog systems to keep track of dialog history in case of long dialog and stay coherent over time. \nThe paper present a clear analysis of the problem in regards of existing technics, a clear description of the model as well as the experimental setup. The evaluation of the model is particularly of a high quality: using 3 public datasets, standard evaluation based on datasets & human evaluation & ablation test to investigate the effectiveness of each module & error analysis",
    "overall_score": "5",
    "confidence": "4"
  },
  {
    "review_id": "28037c79c81912cd",
    "paper_id": "COLING_2020_66",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Very well written, clear and compelling. \nAn extensive introduction to the problem and the approach. \nRevealing and intriguing results",
    "weaknesses": "The authors refer to Gulordova et al. (2018) for the LSTM architecture but I miss a brief overview of the architecture in this paper.\nEdits: domain",
    "comments": "",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "d62c39f8f7e82640",
    "paper_id": "COLING_2020_66",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This is an interesting and original paper. The main claim is that the way in which LSTMs reproduce the kinds of syntactic dependencies found in natural language is different from the way humans do this, because the LSTMs are equally suited to reproducing other \"unnatural\" kinds of syntactic dependencies which humans cannot. Some analysis of the internal structure of the trained models also suggests that they are not generalizing in the way we usually assume humans do.\nFollowing much other work, the reported experiments assess the LSTM's ability to predict the correct singular/plural agreeing form of a verb, in the context of three unnatural/artificial variants of natural language: one involving reversal, one involving a string of \"dummy\" words, and one involving random shuffling of lexical items. Table 1 shows that the accuracy of agreement predictions is essentially unaffected by these distortions. Analysis of the internal structure of the model trained on original/\"real\" natural language (Figure 1) suggests it has not identified a unifying concept of \"singular/plural agreement\", instead learning a many separate patterns.\nOverall, I like the paper and I think it contributes something important. There are a few aspects of the results and analysis section that should be tightened up though, in my opinion.\nA couple of points the logic of the crucial gradient-similarity calculations. First, the middle of page 6 introduces this concept as \"gradients in weight space of any output value\". My first impression was that this \"output value\" would be something like the logit for a plural verb form, but this would not make sense, I don't think: it's crucial that the gradients that are actually used are gradients of the *difference* between the singular and plural forms (as in (1) and (2)). If I'm understanding correctly, the difference in (1) is, up to a constant, equivalent to some sort of error/loss function -- and this is what makes the connection between the gradients discussed here and the gradients used in optimization of parameters. This connection needs to be stepped through much more carefully though. ( As first it seemed to me that the logic relied on an equivocation on the term \"gradient\".)\nNext: let's just grant the assumption that the gradients like (2) correspond to the gradients used in learning; I still have some questions about exactly why this kind of gradient \"tells us which weights were important in producing that output\". One intuition is that, for a weight to be important in this sense, the predicted output (say, the value in (1)) should \"vary a lot\" as a function of varying this weight; it should *not* be the case that, no matter what value you choose for this weight, the output is the same. But these are properties of the output-as-a-function-of-weight, considered \"globally\" across all the possible values of that weight; a weight which is very important, i.e. a weight which varies a lot when we consider the full range of possible values for it, might nonetheless have large chunks of its range where the objective function is flat. The gradient, however, is a \"local\" property of how much the objective function is varying, in the neighborhood of one particular value. So it's not obvious to me that, in general, large components of the gradient correspond to weights that \"vary a lot\" in the sense that makes them important. Is the idea perhaps just that, in the long run, if you collect up a large number of values from the gradient of a function that \"varies a lot\" in the global sense, they will tend to be larger magnitude, even though each one is only a local measure of how much it varies? Or is there a more direct/analytic connection here I'm missing?\nSome more minor points:  - Understanding exactly which pairs contributed a similarity data point to Figures 1a and 1b takes some work. An example or equation (with Cartesian products) could help here. Am I understanding right that each point in Figure 1b basically corresponds to one particular time when the model needed to decide between a singular or plural verb form, in the original training corpus?\n - I was a bit confused about the point being made by Figure 2c. Is the comparison against the \"baseline similarities\" even meaningful? And am I understanding right that \"same prefix\" here means \"same prefix with the reversal applied to the part of it that occurs after the marker\"?\n - The first paragraph of section 3.2 (and perhaps one or two other places) makes an assumption that the patterns observed in *attested* natural languages correspond closely/exactly to those that are learnable in principle. But typology can arguably be skewed by other factors besides learnability, e.g. historical accidents. ( In this sense, AGL experiments are more informative than typological surveys, at least to the extent that those experiments engage the actual language-learning machinery.)\n - I think it's misleading to call the repetitions pattern a \"lack of dependencies\". A language with a lack of dependencies would be completely unconstrained. In fact the repetitions pattern has a very strict, but boring, system of dependencies: those 23 terminals have to be identical. Of course the fact remains that this is a kind of dependency pattern that we assume is \"unnatural\".\n - The mention of brain regions at the top of page 9 could be interpreted as going one step too far.",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "5a3d20262ffbd2b2",
    "paper_id": "COLING_2020_67",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "in this work. What would be human annotation guidelines for such data set creation? How would one assess agreement if one were to create such data with human experts?\nTherefore, it appears that the actual task taken on by this paper is that of learning the latent decisions behind the pull-quote identification of *these particular* publications.\nHaving said that, the approach and analysis undertaken by the paper is very insightful. While the task can be construed as learning to extract pull-quotes in a manner similar to that of these selected publications, the methodical approach taken in the paper is commendable. It was enjoyable to see the paper build from hand-crafted features used in a traditional ML classifier to more recent deep learning models with character and word-based features, to cross-task of approaches used in similar tasks (headline, clickbait, summarization).\nThe observations and conclusions from the experiments are perceptive, and readers of the paper would certainly learn about interesting linguistic characteristics that are useful in identifying noteworthy sentences in any given text.\nIt was great to see the human evaluation in Section 5.5 of the paper. This really helped to see the impact of the pull-quotes on human readers. It would have been neat to see such an analysis of the data as part of the task definition early on... to perhaps help more clearly define what a human reader (or a human writer) is expecting to highlight as quotable. ( a.k.a., crowd-sourcing pull-quote extraction?)",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "9bfeab97c9c80594",
    "paper_id": "COLING_2020_68",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "+ The idea of combining different distance metrics in metric learning is interesting. Illustrations of measures are welcome.   In figure 2, there is a mismatch between captions (b) and (c) and their corresponding matrices + Strategy to identify positive and negative examples + Experiments assess the effectiveness of the approach and some analysis are provided",
    "weaknesses": "The main weakness of the paper relies on very insufficient literature review. It does impact the positioning of the contribution (i.e., novelty) as well as its evaluation through suitable baselines. \nAll the literature about neural information retrieval is missing: 1. Metric learning has already been used in neural IR (DRMM by Guo et al 2016, DeepMatch Lu and LI 2013, ...). Il would be also interesting to mention other neural IR models (Yang et al 2019, Guo et al 2019, … and different tutorial nn4ir at sigir) Please compare your work with these references.\n2. I do not agree with the following statement : “It is remarkable that although the biomedical domain has  plenty of structured knowledge as biomedical terminology databases and ontologies, most of the approaches have not made use of these resources (Majdoubi et al., 2009). An exception is the work presented by (Bhogal et al., 2007), where ontologies are used to expand query terms. Structured resources offer information that is complementary to textual information and that can be used to alleviate problems as polysemy or synonymy disambiguation”   Check more recent references such as :  Tymoshenko, K., Moschitti, A., & Severyn, A. (2014, April). Encoding semantic resources in syntactic structures for passage reranking. In Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics (pp. 664-672). \nXu, B., Lin, H., & Lin, Y. (2018). Learning to refine expansion terms for biomedical information retrieval using semantic resources. IEEE/ACM Transactions on Computational Biology and Bioinformatics, 16(3), 954-966. \nL. Tamine, L. Soulier, G.‑H. Nguyen, N. Souf : “Offline versus Online Representation Learning of Documents Using External Knowledge”, ACM Transactions on Information Systems, vol. 37 (4), pp. 42:1-42:34, (Association for Computing Machinery) (2019) G.‑H. Nguyen, L. Soulier, L. Tamine, N. Bricon‑Souf : “DSRIM: A Deep Neural Information Retrieval Model Enhanced by a Knowledge Resource Driven Representation of Documents”, The 3rd ACM International Conference on the Theory of Information Retrieval, Amsterdam, Netherlands (2017) The authors should rework the positioning of their paper according to these references as well as the choice of baselines.\nIn addition, we can add the following remarks: All notation used in equations and figures should be clearly defined mainly in formula (1) and figure 1.  : alpha, w, N, disc(q,p+), etc by analyzing table 3 and table 4, the three representations have, individually, led to a MAPs < 0,150, however, the MAP is doubled when the three representations are combined. It is an interesting result, however I am curious to know what is the result of combining these representations by paires, for example : (w2vec -terms), (w2vec -concepts) or  (terms-concepts) having influenced the results or not.   Some edits  Section 1 comparing against →  comparing with It is important to highlighted →  to highlight Section 2  (Feng et al., 2015) were a metric learning approach… →  where the convolutional neural netowork learns to represent →  network  Section 3 The proposed architecture is describe →  is described The model implementation is publicity available → publicly  Section 5 The following results à the following study (experiment)",
    "comments": "",
    "overall_score": "1",
    "confidence": "4"
  },
  {
    "review_id": "8fe34372cb461bc9",
    "paper_id": "COLING_2020_69",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "of the paper is the clarity: the paper is clearly presented and well-reasoned. It is easy to follow and possibly easy to replicate. To this end, the authors have set up a GitHub link, but since it is anonymised it cannot be browsed.  I would suggest the following, if the paper is accepted for publication:  1) Replace \"n't\" with full form: academic papers are still a quite formal genre. \n2) Replace the section title \"Summary\" with \"Conclusion\" and expand it. For example, say in which way the presented results can benefit the community, whether future experiments of the same kind are planned on another task or using other datasets or for another downstream task. Fine-tuning of pre-trained models is a pervasive problem: is there a way to generalize on the results presented on the paper? Or are these task- and dataset-specific? etc.   3) Update the preliminary results in Table 3. \n4) Add a table or an appendix where the hardware characteristics (computer resources) are shown (eg. CPU, GPU, Clock Speed, Ram etc). \n5) Add a table or an appendix where the the processing time of the performance (first row in Table 4) is reported for each model.",
    "weaknesses": "",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "f3077c788b0c062b",
    "paper_id": "COLING_2020_6",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper explores the application of active learning to modern NLP architectures based on the BERT Transformer paradigm. The goal of AL in this context is to validate if AL pooling strategies allow training those models in use cases with scarce availability of labelled examples. \nTo demonstrate that AL helps on training in such scenarios in terms of accuracy and stability, they implement two architectures: one bare BERT and another more complex that includes stacking all inner representations of BERT and applying a CNN before the output layers. \nThe paper includes experiments applied to some GLUE tasks, including text entailment or sentiment analysis. They compare two strategies for acquiring new data to label: random selection and BALD, an acquisition function based on MC dropout that explores model uncertainty associated with each unlabelled sample.\nBeyond analysing the role of AL, the article analyses if freezing some layers of the pre-trained model during the fine-tuning process allows improving the accuracy. By freezing some layers at lower and higher stages of the BERT model, they compare the accuracies obtained for different GLUE tasks.\nEven though it is true that it might novel to study it when applied to BERT models, I recommend the authors to take a look to \"Practical Obstacles to Deploying Active Learning\", by David Lowell et al, where they analyse the role of AL in NLP tasks. In this paper, they complain about the lack of robustness of AL methods in NLP tasks. In the present article, I miss the most basic acquisition function used in AL: uncertainty sampling, that does not need any further forward pass. It would be interesting to compare the performance of such a simple heuristic for selecting the elements to query. It might provide an explanation for the qualitative analysis included based on the ambiguity of the neutral examples.\nBeyond this metric, I would also encourage the authors to review other methods of AL that take into account the diversity of the selected elements, especially in methods like BERT that are batch trained. In those methods, they try to ensure that elements included in each batch are diverse enough to enrich the training process: Zhdanov, Fedor. “ Diverse mini-batch Active Learning.” arXiv preprint arXiv:1901.05954 (2019). \n[1] Du, Bo, et al. “Exploring representativeness and informativeness for active learning.” IEEE transactions on cybernetics 47.1 (2015): 14–26. \nOne thing that is quite confusing is the analysis of AL results in section 4.2. It is unclear if the size of the subset U was optimized for obtaining the best results when applying BALD as the acquisition function. How did you perform this optimization?\nIn general terms, results for both the AL and the layer freezing experiments present a high variance and depend a lot on the task.  When looking at NLI tasks, it is hard to tell that using BALD is beneficial enough for the higher complexity that it adds to the training process. Why did you not include more tasks to show that the conclusions extracted truly generalize?",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "bdfe32c46c23a450",
    "paper_id": "COLING_2020_70",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The authors provide a model (Lin) that uses syntax (dependency based) and semantic (VerbNet based) features to determine tasks. They focus not just on whether a sentence contains a task but also which verb encodes the task. Their approach is unsupervised and they compare across two datasets which they prepared that are built on existing NLP datasets (email and chat).\nWill the authors make the datasets available for research purposes? These would be particularly valuable since they are based on top of existing NLP datasets.\nSyntactic features in section 2.1: Are there not enough imperatives (\"Send the minutes out next week\") to make them a useful class? I expect that most of these will be captured by the tense rule that identifies VB POS, but if the authors have a way of directly identifying imperatives, that could be very helpful. As a minor point, for 5 verb association, it would be better to focus on an example where the COMP was a valid task to contrast with dependents which are not valid tasks (or ideally include an example of both; there should be room to include this by judicious trimming).\nSection 3 is really about the dataset, not the experimental setup. It would be helpful to know in this section whether all of the annotated data was part of the test/eval set or whether some of it was used for training. ( Later it is mentioned that the authors use 5-fold cross validation, which is fine, but if this section is about experimental setup, it should describe that.) To save some space, section 3 could be merged into section 4.\nIn the results discussion, the authors state that Lin only performs slightly better than BERT. If the difference is statistically significant, then a couple of percentage points is a reasonable improvement, although not as significant as one might expect given the features that Lin uses. If there is room, it would be great to have more discussion of (1) why full Lin is so much better than the syntax and semantics separately (even if these are just hypotheses for future work) and (2) why BERT comes so close to Lin (building on the one sentence the authors provide about this). ( 1) appears some in the qualitative analysis below, but the density of the discussion makes it hard to understand without rereading several times.\nIn the qualitative analysis, it was took a couple of readings to understand what the authors meant by \"low recall\" since at first glance the description is about a low precision issue (marking too many things as tasks when they are not). It would help if the authors did *not* include the bad precision example of \"You should thank me for this\" and instead focused on the later example \"I think we can send the details tomorrow\". Given how frequent the phrase \"I think\" is in emails and chats (it is a form of politeness to soften an order or strong opinion), making mistakes on this could have a significant impact on performance for many datasets.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "2feed061d38ab734",
    "paper_id": "COLING_2020_71",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper is a resource paper. The resource it introduces is a corpus of Russian texts, annotated with semantic change judgements, following the DURel framework. The authors describe the annotation procedure and the corpus, and also provide some baseline experiments on the automatic detection of semantic change. The corpus is promised to be published upon acceptance.\nThis is a very good paper and should definitely be accepted. Since it follows an already established annotation framework, it is not innovative in that sense, but having corpora in different languages annotated according to the same framework is a big win for the community.  The only real issue with the paper that I see is that the pre-Sowiet era is much longer than the other two. It is conceivable that this early data set is inconsistent in itself, and it would be good if the authors comment on this.\nMinor things: - There are some missing articles in the text (4.1: „loses old sense“ → „loses an old sense“; „naturally captures the type of meaning change“ → „naturally captures the following types of meaning changes“, ...). Critical proofreading would be good for the final version.\n- The authors state that the corpus will be published unter a „permissive“ license, which is surprisingly vague. I‘d assume that the authors know which license can be used by now. The wish of course would be to use a creative commons license.",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "d62f5fc7b6674836",
    "paper_id": "COLING_2020_72",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper presents a complicated matter in a remarkably clear fashion, which makes it easy to follow, even for non-specialists. The extension of NLP procedures to languages with less resources is welcome, and the approach taken here is well argued, justified and evaluated. I cannot comment on the mathematical details, though.\nThe term \"backtranslation\" is mentioned in Section 2, but is explicated in detail as late as in 6.3. Consider adding a short sentence in Section 2 explaining the nature and motivation of a back-translation approach.\nIn section 3, a reference is needed for the reader to look up the history of the Arabic language(s). The following chapter might be sufficient (or references therein): van Putten. Marijn. 2020. Classical and Modern Standard Arabic. In Christopher Lucas & Stefano Manfredi (eds.), Arabic and contact-induced change, 57–82. Berlin: Language Science Press. https://langsci-press.org/catalog/book/235 A couple of references use \"(Smith 2000)\" where \"Smith (2000)\" would have been appropriate. Please check all references.  The literature entries for Ferguson, Guzman, Habash have problems with upper/lower case letters. Check these and also the remaining entries.  Appendices D & E  would profit from lines with transliteration. I can read some Arabic, but not as fast as I would need to make the appendix a worthwhile addition for a reader like me.",
    "overall_score": "4",
    "confidence": "2"
  },
  {
    "review_id": "426b46e52813af10",
    "paper_id": "COLING_2020_72",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper presents a research aiming to create larger training and evaluation sets of data in translation enterprises that have English on one side and dialects of Arabic (Egyptian and Levantine), known as resource-poor, on the other. Supplementary parallel data are generated by a bootstrapping technique applied to an original set of English sentences, acquired from more sources, and their human translations in Modern Standard Arabic (MSA) and two of its dialects. To this original set, a Byte-Pair Encoding with a carefully chosen fixed vocabulary is applied and then the training corpus is augmented via a bootstrapping approach. The paper is important as an example of successful adaptation of known MT techniques for pairs of languages in which one is resource-poor and for offering to the research community a 4-way benchmark dataset between Egyptian, Levantine, MSA and English.\nTechnical details are clean and prove an intimate knowledge of the fields of NN and MT. English is accurate.  The plot displaying the correlation between the size of the vocabulary and the BLEU score of translations, combined with the authors’ comments related to the dimension of the training set, suggest the use of a 3-dimensional plot.  As far as I understand, bootstrapping original texts can occasionally lead to syntactically incorrect sentences. If this is the case, care should be taken in using the BLUE measure, because comparing syntactically incorrect data with their translations is not what we usually want, since it may result in slightly too optimistic conclusions.  Incomplete reference with wrong year: “Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015. Neural machine translation of rare words with subword units.” The correct reference is the following: “Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016), Berlin, Germany.”\nOther incomplete references:  - “Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising pre-training for neural machine translation.”\n- “Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need.”\nAlso, pages are missing for a number of references.",
    "overall_score": "4",
    "confidence": "2"
  },
  {
    "review_id": "92e1f51db53bf433",
    "paper_id": "COLING_2020_73",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "I see is that the baseline methods compared to Hier-*-BERT are somehow weak(Table2). \nHave you checked the main reasons/challenges in misclassifying the explicit and implicit aspects?\nI suggest placing the related work section at the beginning of the paper.",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "e91b87bc53fe1409",
    "paper_id": "COLING_2020_74",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- interesting idea; - going against the mainstream approaches to contextualized meaning representations; - the choices are well-motivated; - convincing error analysis; - well-motivated, solid background in lexical semantics; Limitations: The method is currently limited to nouns, and the constructed representations are 6-dimensional (for coarse supersenses). That clearly does not have enough expressive power for lexical semantics of all nouns, and is confirmed by the fact that MLP was able to make better use of more training data with FlauBERT. There is definitely room for improvement if the SLICE embeddings were more fine-grained, but the authors should discuss where the data for such improvements would come from. The choice of coarse senses was already motivated by data availability.\n- focus on monosemous words is necessary for pseudo-annotation, but arguably the classifiers were never taught to deal with polysemous words, and then tasked with WSD; - The description for Table 2 could be made more clearer; it is not obvious what the letters in the columns R, L, and C stand for; - Table 2: while the overall pattern for the linear model and MLP are the same, MLP reaches relatively high accuracy in all configurations. What does that say about the configurations themselves?\n- It would be great to include a section outlining how this could possibly be extended to other parts of speach. A big problem for interpretable representations is scoring words on inapplicable categories (e.g. \"animacy\" for prepositions); what's the plan for that?",
    "weaknesses": "",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "a24f22640c3b57be",
    "paper_id": "COLING_2020_74",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper proposes a hybrid model for the representation of contextual embeddings (BERT) that aims to disambiguate French nouns at supersense level. The model's performance does not improve the BERT state of the art performance on the same corpus (FlauBERT). However, it has the advantage of making embeddings interpretable, helping in the error analysis. In the error analysis section, the main finding is that errors are due to the polysemous nature of nouns: it is not surprising since polysemy is the primary source of errors for all the WSD systems. The paper does not propose how to improve on that aspect - just using a different corpus is mentioned as a possibility, but not changes in the methodology are tried or suggested -. In other words, even if the methodology is interesting and it is true that makes the embeddings less opaque, at the moment it is not able to guarantee improvements in the results because, from error analysis, it is not possible to get insights on how to improve the hybrid embedding representations of polysemous words.",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "87a3e229e569e6a8",
    "paper_id": "COLING_2020_75",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper presents experiments on modeling the language and writing style employed by twitter users in posts and descriptions, as representations used to predict whether news pieces shared by them are real or fake. The idea is well motivated and novel, and the paper reads very well. Experiments and modeling choices are described with adequate details and well justified and I particularly appreciated the further linguistic analysis and interpretation of the model (section 7).  My main concern refers to the lack of comparison with other work. Indeed the comparison with the baseline and the ablation tests adequately confirm the positive impact of user representations based on the language they use of fake news prediction. However, it would be further convincing and helpful to other researchers if there was a comparison with other work that does not use user linguistic representation and could put this approach into perspective.  Also, I am a bit concerned/curious regarding the data preprocessing and the impact of the authors' choices on the user selection. I am not sure how large the set of users excluded due to oversharing (>1 piece of news) is, but I am wondering if \"over-sharers\" of fake news would impact the identified linguistic features and the learned models. While the reasoning for excluding them is well explained and motivated, I would like to see a break down of the percentage of users excluded in the different preprocessing steps (perhaps as an extension to Table 1?) and even some comparative tests. I am not entirely sure that having users sharing more than one piece of news would be an issue, so long as these news pieces wouldn't be split across training and test data.\nFinally, as a minor comment, it is not clear how the topics for n-grams are chosen. are they attributed manually? It should be clarified in the main text.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "d1388d463fa663c3",
    "paper_id": "COLING_2020_75",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This work built a fake news prediction model using both news and user representation from user-generated texts. Experimental results showed that the user text information contributed to predicting the fake. Moreover, the paper showed linguistic analysis to show typical expressions by users in real and fake news. Cosine similarities between users are calculated using proposed user vectors to confirm the echo chamber effect. \n  Introducing vectors of news spreading users sounds an interesting idea. The paper's investigation, the user vector made from linguistic features contribute, is interesting and important. The results of active topics by users for both real and fake is also impressive. \n  There are some ways to build user vectors not only from their timeline and profiles but also from their tweets itself (e.g., Persona chat model). Does the proposed method have a clear advantage to such models?",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "d082a6bc04471482",
    "paper_id": "COLING_2020_76",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "In the present paper, the authors describe the results of a quantitative analysis of various genres in terms of coreference. They analyse a number of coreference-related features and compare the genres from the point of view of their distribution. The aim is to find the differences between spoken and written genres. The work is interesting and useful, as the number of studies of coreference in spoken texts is limited. The paper address a number of important issues in both coreference analysis (e.g. how should the distance be measured) and the analysis of spoken language. As the authors use a number of existing resources, they also assure the comparability of the categories used. Here, it would be interesting to know, if there were any problems, e.g. if there were still some incompatible categories that the authors came across.\nSpecific comments I like the discussion about the variation in distance measured by different means at the beginning of section 2. Specifically, in a cross-lingual task, token-based measure is a problem. However, there could be differences across studies using various metrics. If measured in sentences or clauses, the distance may vary depending on a genre, if there is a variation in sentence length in terms of words (in spoken texts, there could be shorter sentences, etc.). The question is, if the distance should be measured in characters, but I believe that the decision depends on the conceptual background on what one wants to find out.\nAnother point in Section 2 in the discussion of the diverging results could be variation within spoken and written texts various authors use in their analysis. There could be further dimensions that have an impact on the choice of referring expressions in a language, e.g. narrativeness, if there are dialogues or monologues, etc.\nConcerning related work, Kunz et al. (2017) point out that coreference devices (especially personal pronouns) and some features of coreference chains (chain length and number of chains) contribute to the distinction between written and spoken registers.\nThere are several works concerned with lexicogrammar suggesting that distinction between written vs. spoken, and also between formal vs. colloquial are weak in English  (Mair 2006: 183).\nTable 1: the statistics on different parts of OntoNotes and the total number in OntoNotes are given in one table in the same column formatting, which is slightly misleading. \n  4.1: large NP spans vs. shot NP spans – sometimes only heads of  nouns or full NPs are considered.\nReferences to examples: 1→ (1), etc.\nPersonal pronouns: 1st and 2nd person pronouns are not considered in the analysis of coreference in some frameworks. The authors should verify which cases they include into their analysis.\nThe finding about NPs being more dominant is not surprising (and was also expected by the authors) and has also something to do with the fact that spoken texts reveal a reduced information density if compared to the written ones.\nThe discussion about the results on spoken vs. written is good and important. Even within written text, there could be a continuum, e.g. political speeches, which are written to be spoken or fictional texts that contain dialogues (as the authors point out themselves), could be closer to further spoken texts. At the same time, academic speeches or TED talks that contain less interaction with the audience (depending on a speaker’s style) could be closer to written texts, also in terms of referring expressions – we would expect them to contain more NPs, and probably complex Nps describing some notions.\nOverall, it is interesting to know if there are more dimensions than just the difference between spoken and written in the data, e.g.  narrativeness (narrative vs. non-narrative) or dialogicity (dialogic vs. monologic), etc. In fact, genre classification can and should be sometimes more fine-grained than just drawing a rigid line between texts that are considered to be spoken and those that are considered to be written.\nTextual problems: Page 2, Section 2: interfering mentions. - These → There are some typographical problems in the text.\nPage 3, Section 3: I am not sure if the abbreviation Sct. is allowed by the Coling style. \nIn the reference list, the authors should check spelling of the some entries, e.g. english→ English in Berfin et al. (2019), Zeldes (2018). There is an empty space in Godfrey et al. (1992).\nCited references: Kunz, Kerstin and Degaetano-Ortlieb, Stefania and Lapshinova-Koltunski, Ekaterina and Menzel, Katrin and Steiner, Erich (2017).  GECCo -- an empirically-based comparison of English-German cohesion. In De Sutter, Gert and Lefer, Marie-Aude and Delaere, Isabelle (eds.), Empirical Translation Studies: New Methodological and Theoretical Traditions. Mouton de Gruyter, pages 265–312.\n@BOOK{Mair2006,   title = {Twentieth-Century English: History, Variation and Standardization},   publisher = {Cambridge University Press},   year = {2006},   author = {Mair, Christian},   address = {Cambridge} }",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "d17520c5626fdc7a",
    "paper_id": "COLING_2020_76",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper presents a corpus study of coreferences comparing different genres (news, blogs, conversations) and media (written, transcribed speech, microblogging) based on the Ontonotes and Switchboard corpora and a dataset from Twitter sub-threads. The analysed factors include the use of pronouns and noun phrases, the characteristics of such NP mentions (syntactic complexity) and various distances measured between mentions of the same entity.  This is an interesting study, and could be potentially useful for models trying to do domain adaptation, as coreferecen systems for written text perform poorly on conversations and microblogging. \nOverall it seems the contributions are only moderately significant however, for the following reasons: (1) the paper builds on two papers:  (Aktas et al., 2018), where the twitter data was collected and described, and (Aktas et al., 2019) which described coreferences in  Ontonotes sub-corpora/genres in what I assume is a similar manner (the paper is not freely available, only the abstract). It is not clear how the present paper adds to these papers, and should be made more explicit. \n(2) the interest for coreference model is rather vaguely described, and it would have been interesting to have a more detailed descriptions of how the knowledge derived from the study could be used in such models. The paper mentions experiments using models trained on written texts applied to other genres/media, how hard would it have been to experiment training on other data, or to combine them ? \nThis seems too preliminary to assess the real interest for automated models.  More minor points: - the introduction is rather strangely constructed, and almost reads as a post-introduction section/a related work section already. The context should be made clearer and a few examples wouldn't hurt.\n- i'm not sure I understand the term \"coreference strategies\", which seem to imply an intentionality in the way coreferences are produced in different contexts. A lot of what is shown in the paper could be attributed to more general aspects of the genres/media (longer sentences for purely written text, more context available, etc) and some of the properties of coreferences could just be by-product of that. The use of specific personal pronouns (1st/2nd/3rd) is another example.  - there is zero descriptions of the statistical tests used, and of the assumptions made, if a parametric model was used. This should be addressed. Also some conclusions are based on multiple-testing, which should include some kind of correction (it might have been done, but again, there is zero details about this).  - some technical details are presented a little vaguely, which could be understood given size constraints, but sometimes it is a bit too much: for instance, instead of explaining what hierarchical clustering method was applied, the paper only mentions using some R implementation with default settings, which is rather uninformative.  - about the clustering, why not cluster on all the dimensions at the same time ? ( with some normalization of features of course) Details: - Tables/figures have rather cursory captions. For instance table 1 coud recall the meanings of abbreviations for all sub-corpora, especially from Ontonotes. It is also not a good idea to have ontonotes as a whole *and* all the subcorpora without making it clear.  - section 3.1, the paper mentions the use of a sentence splitter from (Proisl and Uhrig, 2016) which is a German sentence splitter ?\n- table 2: why not give relative (to corpus size) frequency instead of absolute frequency ? this would make it easier to interpret.",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "37f5cdda9053e0c5",
    "paper_id": "COLING_2020_77",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "From a general point of view, the paper describes the effect of the teacher-forcing technique employed in training a recurrent neural network model on a sequence-to-sequence task. The teacher-forcing technique (i.e. the use of the gold-pattern to be used as input at time t) increases the exposure bias of the model (indeed the training condition is different with respect to the test condition, where the output prediction at time (t-1) is used as input at time t, hereinafter the \"student-forcing\" technique), making the model less prone to correctly predict unseen sequences. Low-resource settings perform better with the \"student-forcing\" technique, while high-resource settings benefit of the \"teacher-forcing\" technique.\nThe authors argues that the teacher-forcing technique may cause the overfitting on the training data, due to an increase of the exposure bias especially in low-resource settings. The authors tested such hypothesis on a morphological inflection task, modelled as sequence-to-sequence task at character level and employing a mixture of the two forcing methods during training.\nThe paper is clear and interesting, my main concern being the fact that the choice between the teacher and the student forcing technique remains open and highly dependant on the specific task and the size of the training data.",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "44d30e06334cd663",
    "paper_id": "COLING_2020_78",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The topic of the paper is grammatical error detection (GED) in speech. While GED in written texts has been subject of many studies, GED in speech has been less well studied and the results so far have been disappointing: due to relatively short duration of most spoken responses and multiple disfluencies typical for spontaneous speech, especially non-native, conventional methods of GED that worked well on written text generally tended to fail on speech.\nThis paper presents a new set of annotations of grammatical errors in English native and non-native speech obtained via crowdsourcing. The authors promise to publicly release these annotations. The first part of the paper is devoted to the detailed description of data processing including transcription QC and error annotation. The second part of the paper describes the training and evaluation of GED system based on GloVe and BERT. The authors also tried numerous other features including prosody but disappointingly these have not led to further improvement.  The paper is very well written and the methodology is sound. All evaluations are done with 10-fold CV and 10 random seeds with the evaluations reported based on means across these 100 runs. All analyses are done using open-source tools and could be reproduced. The authors promise to release the data partitioning to further aide reproducibility.  I believe it will be a great addition to COLING program.  I only have a few minor comments and suggestions for references: - The overall precision and recall remain relatively low, which is consistent with previous work. Anther metrics that you might consider in future is how well your system predicts the overall grammaticality score/total number of grammatical errors. While less useful for targeted feedback, this could still be a very useful measure in the context of language assessment.\n- Section 2: Privacy concerns is another reason speech corpora are less common. Since voice recordings are considered PII, releasing spoken corpora requires a different level of participant consent and additional legal considerations.\n- It would be helpful to have some information about English language proficiency level of the non-native crowdworkers.   - This reference might be useful for your transcription analysis: Evanini, K., Higgins, D., & Zechner, K. (2010). Using Amazon Mechanical Turk for transcription of non-native speech. In CSLDAMT ’10 Proceedings of the NAACL HLT 2010 Workshop on Creating Speech and Language Data with Amazon’s Mechanical Turk (pp. 53–56).   - 3.1 In addition to total number of recordings, it would be useful to also report total duration in hours or minutes.  - This volume: https://benjamins.com/catalog/scl.23 might be a useful resource for speech vs. text comparison in the context of ELL assessment.\n- 4.1 Should \"M^2\" be a number?",
    "overall_score": "5",
    "confidence": "4"
  },
  {
    "review_id": "0a225f3783ade8a3",
    "paper_id": "COLING_2020_79",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper is outside of my area of expertise, and my comments should be taken accordingly. It describes an improved system for word segmentation of \"Chinese\" (I guess this means Standard Written Chinese, but this isn't specified). Obviously, even small improvements on performance of word segmentation for Chinese could have significant positive impacts given the number of users of Chinese language resources. So, given that the paper reports improved performance over what it identifies as the previous state-of-the-art, it seems like a good paper for COLING. ( I have no reason to doubt that the comparison is against the actual state-of-the-art. However, I lack the expertise to know if this is the case or not.)\nSome potential issues I noted about this work are as follows: 1. I didn't see any indication as to whether the resources used in this work would be made public or not, including the new annotation guidelines. The impact of this work would be significantly greater if these resources would be made widely available, and that should be a factor in its acceptance, in my view. My low score on \"Reproducibility\" is due to this specific concern. The underlying algorithms seem clear enough, at least.\n2. I believe that the paper is, at present, aimed at a relatively small audience already familiar with previous work in this area. It would be hard for someone who is not aware of the literature on this topic, both in terms of work on Chinese and the parsing algorithms, to follow, in my view.\n3. The paper could benefit from more explicit example of the challenges that Chinese data poses for word segmentation for readers who are not familiar with this. This would, in particular, clarify the extent to which the results of this paper could be applied to other languages. ( This seems likely, especially for man Southeast Asian languages.)\n4. The paper categorizes Chinese as \"polysynthetic\", which is really bizarre. Most sources treat it as the \"opposite\" of polysynthetic, i.e., as analytic or isolating. Perhaps one can argue that the standard categorization is wrong, but no reasonable definition of polysynthetic would include Chinese.\n5. The term \"multi-grained word segmentation\" is not standard, and never appears to be defined in this paper. It would be good for it to be defined.",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "bb5de4892066ada8",
    "paper_id": "COLING_2020_7",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The use of 2 accuracy-based metrics to evaluate the quality of the methods proposed and post-hoc analyses to help understand the strengths and limitations of the proposed models (impact of the difficult sentence length and impact of the number of words typed).\nMain",
    "weaknesses": "The work is fantastic ! I found no limitation.",
    "comments": "",
    "overall_score": "5",
    "confidence": "4"
  },
  {
    "review_id": "82a668a23172852c",
    "paper_id": "COLING_2020_80",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "which if addressed would lead to an even stronger camera-ready version: - The paper does not discuss linguistic aspects in detail. In particular, I'd appreciate some more discussion on why decisions on transcripts should lead to improved translations. This paper gives empirical evidence, and justifies the approach from an engineer's perspective only. To make this suggestion more precise: In the intro, please consider elaborating further on this sentence: \"We believe that these are two edge design choices and that a tighter coupling of ASR and MT is desirable for future end-to-end ST applications.\"\n- The focus seems to be on improving translations only (and probably lead to the choice of alpha=0.3 for the training objective). This is a reasonable choice, but should be stated more explicitly, given that one could also target improvements in both BLEU score *and* WER. In fact, the proposed model seems to experience a trade-off between translation accuracy and transcription accuracy, which in itself is a very interesting observation. It might be worth citing a highly relevant, concurrent work that goes the other direction, assuming that both translation and transcript are of equal importance: https://arxiv.org/pdf/2007.12741.pdf  . Another related work to cite might be https://ieeexplore.ieee.org/document/5947637 who have reported on BLEU/WER tradeoff quite a few years ago.\n- On \"chained decoders\": are these conceptually related / identical to http://arxiv.org/abs/1802.06655 ?\n- Evaluation: please do not say \"significant\" unless you have actually formally verified statistical significance, in which case it would be necessary to report the details of the stat. significance check. Also, the standard nowadays is to use sacreBLEU to compute comparable BLEU scores (please see https://www.aclweb.org/anthology/W18-6319/ on why it is impossible to compare BLEU scores when the tokenization details are not known or not consistent).\n- typo: \"weekly tight\" -> \"weakly tied\"",
    "comments": "",
    "overall_score": "5",
    "confidence": "5"
  },
  {
    "review_id": "3c1c4e64213d23e6",
    "paper_id": "COLING_2020_81",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- Mapping existing Thai corpora to the UD scheme is a useful contribution for Thai NLP, as the UD scheme has become popular for multilingual work, and this will allow Thai NLP to profit better from advances in this area.\n- Syllable segmentation is shown to be advantageous over characters or BPE, suggesting a potential research direction for improving future Thai NLP models.\nMain",
    "weaknesses": "(for each of them, see detailed",
    "comments": "",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "f9f68885d116f6ef",
    "paper_id": "COLING_2020_81",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper presents a conversion from a \"traditional\" POS tag set to UD tagset for Thai, and presents POS tagging experiments with a variety of models that differ in two dimensions: (1) the type of sub-word units, and (2) and type of pre-training. The authors conclude that sub-word units are beneficial for the task as well as language-specific pre-training.\nThe problem investigated is a standard problem, but the fact that the paper is on a relatively less-studied language increases the value of the paper. The conversion of a language-specific tagset to a multi-lingual, \"universal\", tagset is interesting, and the methods presented, although standard/straightforward, makes sense. However, the paper leaves many questions unanswered, it should probably be a long paper with further information, analyses and discussion. I will list some of the issues with the paper.\nThe conversion of ORCHID to UD has multiple issues: - First, there is no clear motivation for this conversion.  From Table   1, this seems this is a lossy conversion.  Given that there is not   strong motivation (e.g., creating a UD compliant treebank), the    purpose of the conversion is also unclear, similar to what authors   mean by \"manageable tagset\" (p.1, last paragraph).\n- There is little information on conversion process. It seems there    wasn't enough attention put into the conversion process.  In a   proper conversion to UD, one should also map some of the sub-types   to morphological features.   - The conversion also makes the results incomparable to the earlier   studies on this data set. Irrespective of the comparison to other    studies, it would have been interesting to see the results of the   systems tested with both tagsets. \n   - It is also surprising to see no mention of the existing Thai UD   treebank (Thai PUD). The treebank includes both coarse (UD) and fine    (presumably ORCHID) POS tags. A comparison of conversion used in    this treebank and in the present paper is needed for better   understanding the value of the conversion presented. \n   - Additional information, e.g., tag distribution (before and after   conversion), on the data set would be beneficial for interpretation   of the results discussed. \n   Investigating the effects of use of sub-word units is interesting. \nHowever, I have difficulties interpreting the results for two reasons: - No discussion, or analysis, of the results. We are given a number of   performance metrics, announcement of the best models, and no insight   into why these differences should be observed, or are there anything   contrary to expectations. For example, the failure of multi-lingual   BERT (even in OoV words) needs some explanation discussion. \n  Pre-trained model not only \"does not benefit from the cross lingual   transfer\", but hurts the performance of the model compared much   simpler ones. \n   - The models presented typically have large variation due to effects   like random initialization and test set split. As a result it is   difficult get a good sense of which model is better without having   an indication of how much the results vary. Furthermore, the authors   seem to have missed the fact that in the results they present in    Table 4, syllable-based (not pre-trained) model performs as well as   BERT on in-vocabulary words. \n   In general, given current presentation, I do not think results are conclusive.\nThere are also frequent typos and language mistakes. A thorough  proofreading is recommended. Here are a few examples: - title: \"Pre-trained Language Model\" -> \"Pre-trained Language Models\" - abstract: \"syllables representations\" -> \"syllable representations\" - Intro, paragraph  1: \"Modern approaches include\" -> \"Modern approaches includes\" - In general quite a few agreement mistakes, a through proofreading   would be good.\n- Intro, paragraph  1: multiple citations should be placed in a single   pair of parentheses, and citations should not be after the sentence final   punctuation.\n- 'related work', paragraph 1 (and other places): when using a   citation as part of the sentence, it should not be in parentheses. \n  \"(Akbik et al., 2018) proposed\" -> \"Akbik et al. (2018) proposed\"   (see conference style/guidelines for more information) - Most tables are not referenced from the main text. The reader needs   to know what to look at in those tables, and in what way they   support the description or argumentation.\n- There are proper names or abbreviations in the references (likely   BibTeX normalization issues): \"thai\", \"bilstm\", ...",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "aad3d6f1569faa42",
    "paper_id": "COLING_2020_82",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "• Clear hypothesis and argumentation • Strong result which shows the contributions of the individual components",
    "weaknesses": "• Could use better motivation of specifically why the BiLSTM model with attention was chosen. Why not use BERT, since this has been shown to be very effective on this task? \n• Would be nice to see some followup analysis of what the model gets wrong. Where is the next improvement going to come from?",
    "comments": "",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "1092ba4012c277d6",
    "paper_id": "COLING_2020_83",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper describes a new collection for multi-hop Question Answering (QA). The new collection represents an important contribution to the community given that: 1) there are few datasets of this type, 2) other datasets are smaller and 3) this dataset contains explanations about how to obtain the answer. Besides, the process for obtaining the dataset is mostly automatic (it only requires to create some logical rules). So, I think this paper would help to advance the current state of the art in multi-hop QA systems.\nI only miss a deeper analysis of errors in Section 5.3. I think it is important to manually analyze the errors in order to detect why human performance is below 90. The authors claim that it could be because some questions could be unanswerable given the supporting information. I think it is important to know this feature of the dataset before releasing it to the research community.\nI also think that some of the inferred relations can be explicit in some text. For example, a text could contain that A is the grandmother/grandchildren of B (e.g. grandchildren of Queen Victoria). How do you check that a multi-hop inference is required? With the experiments using single-hop?\nI give below some details comment per section: Section 2.2: - Please, include an example for each type of question (or at least for type 4) Section 3.1: - \"entity. We used only a summary from each Wikipedia article as a paragraph that describes the entity\": How do you obtain such a summary?\nSection 4: - \"the system understand several logical rules.\" -- > \"the system understandS several logical rules.\"\nSection 5.3: - \"which might be because the mismatch information between Wikipedia and Wikidata makes questions unanswerable\": Before publishing the paper, the authors should check this information. It is important to know if come questions are unanswerable.\n- \"We conjecture that there are two main reasons why the score of the evidence generation task was low.\" : Again, I think you should perform a deeper analysis beyond these conjectures for the camera-ready version.\nSection 6:  - Include the main disadvantages of datasets for bullet \"Multi-hop questions in MRC domain\"  (they are already given in the Intro, so maybe you can refer to that section) - Appendix A6 seems to be empty. Maybe you should include a reference to algorithm 1 in such section.",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "36fe80bed0946ea8",
    "paper_id": "COLING_2020_84",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "--------- - Generating good distractors for multiple choice questions is a task with real-world applications that would greatly support teachers.\n- The paper is clearly written. In particular, the proposed model is quite complicated, but the general ideas are relatively well explained and the intentions behind technical decisions are articulated.",
    "weaknesses": "---------- I was somewhat underwhelmed by the evaluation. While the numbers show that the proposed method outperforms the state of the art, I did not get a good sense what the generated distractors really look like or whether they would be useful in the creation of actual exams.\n- The evaluation does not directly assess the aspects of distractor generation that the authors want to improve on. In the introduction the authors say that they are specifically interested in generating distractors that are incorrect, but plausible. The proposed method is designed to improve those aspects. However, neither the automatic evaluation nor the human evaluation tries to directly assess those aspects.\n- While the evaluation compares the proposed approach to a number of other recently proposed approaches to distractor generation, how exactly the proposed model differs from these prior approaches could be explained more explicitly and with a bit more detail.\n- I wonder about the variety of distractors that can be generated by this approach in comparison to the variety among human generated distractors. It seems that the proposed approach would always generate distractors which draw on information from other parts of the text passage. However, there may be questions where the answer as well as any potential distractors do not so much rely on specific parts of the reading passage. For example, questions that ask for the meaning of words or phrases may rely more on the test takers prior knowledge. Or the distractors for question 2 from Figure 1, which asks for a title, seem to not be tied to closely to a specific part of the given passage either.\n- I wish the paper would give more examples (than just one) of generated distractors. And I wish there were some examples of distractors generated by some of the prior approaches for comparison.\nOther questions and",
    "comments": "",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "e90565a81657b82b",
    "paper_id": "COLING_2020_85",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "- First line of page 2, this bit does not seem very fluent, maybe something is missing, please check and amend if/as needed: \"... enabling us to easily create large number of coherent test examples...\" - I am dubious about the orthodoxy of using the image with the pussycat's face in the title as well as to replace the name of the project/system in the running text of the paper: while it may look cute (to some), I suspect that other readers (like this innovation-averse reviewer) may find it annoying, if not outright inappropriate. If the paper is accepted (as incidentally I think it should be), I think that the author(s) would be well advised to consult with the conference chairs and proceedings editors to check that this rather unusual practice is deemed acceptable and technically feasible for the proceedings.\n- Similar to the previous point, I wonder if the deliberately eccentric heading of the very short and merely descriptive section 3 (i.e. \"Do Androids Dream of Coreference Translation Pipelines?\", which consists of merely 14 lines) is entirely justified and appropriate for a paper to be published in conference proceedings, although I accept that to some extent these are matters of personal taste.\n- Possible typo on page 2, please check and edit as required: \"follows this work, but _create_ the challenge set in an automatic way.\" >>> should it be 'creates'?\n- The last paragraph of Section 3 on page 3 seems to include some typos such as the following, so please check and amend the passage as appropriate: \"The coreference steps resembles\" (one of the last two words is incorrect), \"the rules-based approach\" ('rule-based'?), \" each of these phenomenon\" ('phenomena'?).\n- The caption of Figure 1 on page 4 seems too (and unnecessarily) long: a caption is a caption; data analysis and comments should be given in the running text of the paper, referring to the figure whose data is being illustrated and discussed.\n- Line 3 on page 5: \"This _filters_ our subset to 4,580 modified examples.\" >>> For clarity, should 'filters' be 'reduced' (or something similar) instead?",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "fc4493ef6f6a54e0",
    "paper_id": "COLING_2020_85",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper provides a thorough analysis of coreference resolution capabilities of a simple context-aware NMT system on the basis of two datasets: (1) an adaptation of the existing ContraPro test suite, and (2) a new test suite called ContraCAT consisting of examples automatically generated from templates. The results from the adapted ContraPro shows that NMT systems are easily distracted by the addition of irrelevant phrases. The results from ContraCAT show that NMT performs much better at certain steps of a hypothesized coreference resolution pipeline than on others. Finally, the authors perform some targeted training data augmentation, but I found it hard to fully understand that part without resorting to the appendix.\nThe paper is clearly written and in large parts easy to follow. The results are insightful, but are hampered by the general problem of using artificially created sentences (rather than naturally occurring ones -- how often do you see apple-eating cats in OpenSubtitles?) and by the small number of evaluated models (see below).\nQuestions and suggestions: - There are a few details missing in the adversarial attack generation: how many distinct phrases are used for phrase addition? Do you randomly choose one phrase for a given ContraPro example, or do you combine every example with every phrase? Also, your claim that \"it is true\" necessarily introduces an event reference may not hold (which you acknowledge later on).\n- Your work assumes that context-aware NMT models should follow the coreference resolution pipeline depicted in Table 1. How \"universally accepted\" is this pipeline in the CR field? Couldn't some end-to-end model provide equivalent CR performance than a pipeline?\n- I understand that the emphasis of this paper is on the resource construction and its analysis, but I would find it instructive to have two additional systems in your analysis: (1) a pure CR system that only operates on the source side, and (2) a somewhat more sophisticated context-aware NMT model than the concatenation one. Your resources are well adapted to compare different approaches to document-level NMT, and it would be a pity not to take advantage of that.\n- How well are the vocabulary items of your templates represented in the training data? Did you check this, and did you find any influence on the results?\n- In the category \"MT test suites with animal names\", there is also MuCoW (Raganato et al., LREC 2020), but it might be thematically too far away to warrant citation... Typos, presentation and style: - I find the first half of the Abstract somewhat incoherent, please consider revising it.\n- §1: Your consideration applies to NMT models in general, not only those based on Transformers, I would say.\n- Table 1: Is it on purpose that cat and actor are swapped between Step 2 and 3?\n- It would be practical if the categories were named the same way in Table 2 and Figure 2.\n- As mentioned above, I was not able to follow your data augmentation strategy properly. Where do you take your data from, and what exactly do you generate as additional instances?",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "1056f20ab2e317b4",
    "paper_id": "COLING_2020_86",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The manuscript first investigates whether readability/complexity linguistic features of a given sentence can predict eye tracking fixations when reading the sentence. They find a positive result, and then move on to see whether eye tracking features can be used to improve in the prediction of sentence readability when combined to linguistic features. The authors conduct a study using a Brazilian Portuguese sentence readability corpus, and report experiments where they \"solve\" the dataset. They will release the code to reproduce their results.\nIn Section 3.1 (and anywhere else): use \"\\paragraph{PSS2 Corpus} (...)\" instead of \"\\textbf{PSS2 Corpus} \\\\ (...)\".\n\"None had participated in the survey (...)\" You did not mention any surveys before. Is the XYZ Corpus going to be published in a separate article? If not, I would suggest only including the part of the corpus that is relevant for this work: the eye-tracking data from 30 students who read all 50 paragraphs. Will you share the XYZ corpus publicly with the research community?\nI suggest the authors write down the training objectives of their models. E.g. in Section 4.2 Single-task, they simply state that the input to the model are \"(...) linguistic features for each sentence of the Simple-Complex pair, and the output was a ranking from the simplest to the more complex.\" How is this ranking implemented? Is the input all sentences in the dataset at the same time? In other words, is the ranking learning over the entire dataset of over random minibatches? The actual architecture can be included in the appendix, whereas the description of the model and the loss/objective should be included in the main text.\nCould you clearly describe what is accuracy (e.g. in Table 5)? I suppose it means that the model can correctly predict XX% of the time which sentence is the most complex in a pair of held-out sentences. Is that right?\nIn the Multi-task model, it reads \"Half of the 4,962 pairs were randomly inverted for training and testing, resulting in 50% simple-complex and 50% complex-simple pairs.\" What are the splits for the single task model? What is the size of the training/validation/test splits? The multi-task model is more clearly described.\nIs the sequential transfer learning model (the one in the right side of Figure 1) also trained using multi-task learning to predict (1) the three eye tracking measures plus (2) whether sentence A is more complex than sentence B? Do you need to have eye tracking features in the sequential transfer model (right side of Figure 1), or after transfer it works without these features?\nWhat happens if you use a different variant of the corpus? You mentioned you use PSS2, what if you use PSS1 or PSS3 instead, would results change much?\nYour model uses pairwise ranking and it is evaluated basically by how well can it rank sentences from simple to complex. How \"easy\" is this task, compared to (for example) text simplification?",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "262c82da5b4e0958",
    "paper_id": "COLING_2020_8",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper describes a model for morphological segmentation. \nThe model is a neural network that takes as input a representation of the word to segment and a representation of the context. \nThe model is trained and tested on mongolian data and reaches good performances (98% f measure). \nThe work is sound and well conducted but lacks a well fromulated scientific question. \nThis question could be \"is context important for morphological analysis\" and the answer will be yes, with a quanfication of the role of context. \nBut such a question is not really novel.\nIn theory, the model could be applied on other languages, it would have been interesting to see the performances on different languages offering different morphological systems.\nthe paper is generally well written, there are some typos and some formulations are not very natural  some of the typos and/or questionable formulations: p1 close related -> closely related p1 several segmentation results -> different segmentation results ? \np2 artificial linguistics ?? what is this ?? \np2 LTMS -> LSTM p3 For Self-attention -> Self-attention p5 following Vaswani -> follows Vaswani p6 we used has annotated -> we used has been annotated p7 but there is the principle a confound -> not clear what that means p9 our-word -> ??",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "21e82addf86f0429",
    "paper_id": "COLING_2020_9",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "Related to a recent notion of Visual Dialog tasks, the paper introduces and evaluates a sophisticated neural architecture to answer queries related to images through a dialog (a sequence of several queries and answers). \nThere are several components in the system but the paper focuses on two of them, namely VTA to map visual features to textual features found in the dialog history and current query; and VGAT to build graphs from these visual-textual pairs An evaluation is done on the VisDial dataset for 5 metrics and with comparison with several SOTA systems covering different approaches. The proposed system performs best for all metrics. Ablation seems to confirm the interest of both components (VTA and VGAT), even if the results are maybe not so significative. A few exemples are provided for illustration. \nThe task and the proposed architecture are interesting. However, it is difficult to follow the details of the model, also because of (maybe) some errors. There are also a lot a parameters and some hypothesis that may be discussed. For instance, it is unclear why a graph representation is really needed instead of some ranking of the visual-textual pairs and how exactly the graph is exploited, a priori (from Fig. 4) the top-5 strongest connections in the graph More specific remarks: - in 3.1, is the number k of visual features a fixed parameter (and then which value is used for the experiments) or a parameter depending on the image being processed ?\n- In eq. (1), (2), and maybe (3) and (4), I wonder if i should range from 1 to h rather than from 1 to k ?\n- in 3.3, I don't understand what you mean by \"homogenous information\" - in 3.3, what do you mean by two textual operations (with different colors) - the construction of the sequence of graphs in 3.3 is really unclear; in eq (10), do you mean G(i>0) rather than G(i) ? If I understand correctly, G(i=0) and G(i>0) are a kind of serialization of the graphs but it is unclear if e(i>0)=e(i) ? In eq (6), are you using new multiheads or are they related to those defined in 3.2 ? Why k iteration steps ? Is it to identify at each step (i) the most interesting neighbor nodes for node (i) ?\n- in 3.1: how if build the list of 100 answers for each query ? Are some false answers randomly added to the right answer, or a specific set of 100 answers is provided for each query ? It is a single set of 100 answers for each dialog ?",
    "overall_score": "4",
    "confidence": "3"
  }
]