[
  {
    "review_id": "f3077c788b0c062b",
    "paper_id": "COLING_2020_6",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper explores the application of active learning to modern NLP architectures based on the BERT Transformer paradigm. The goal of AL in this context is to validate if AL pooling strategies allow training those models in use cases with scarce availability of labelled examples. \nTo demonstrate that AL helps on training in such scenarios in terms of accuracy and stability, they implement two architectures: one bare BERT and another more complex that includes stacking all inner representations of BERT and applying a CNN before the output layers. \nThe paper includes experiments applied to some GLUE tasks, including text entailment or sentiment analysis. They compare two strategies for acquiring new data to label: random selection and BALD, an acquisition function based on MC dropout that explores model uncertainty associated with each unlabelled sample.\nBeyond analysing the role of AL, the article analyses if freezing some layers of the pre-trained model during the fine-tuning process allows improving the accuracy. By freezing some layers at lower and higher stages of the BERT model, they compare the accuracies obtained for different GLUE tasks.\nEven though it is true that it might novel to study it when applied to BERT models, I recommend the authors to take a look to \"Practical Obstacles to Deploying Active Learning\", by David Lowell et al, where they analyse the role of AL in NLP tasks. In this paper, they complain about the lack of robustness of AL methods in NLP tasks. In the present article, I miss the most basic acquisition function used in AL: uncertainty sampling, that does not need any further forward pass. It would be interesting to compare the performance of such a simple heuristic for selecting the elements to query. It might provide an explanation for the qualitative analysis included based on the ambiguity of the neutral examples.\nBeyond this metric, I would also encourage the authors to review other methods of AL that take into account the diversity of the selected elements, especially in methods like BERT that are batch trained. In those methods, they try to ensure that elements included in each batch are diverse enough to enrich the training process: Zhdanov, Fedor. “ Diverse mini-batch Active Learning.” arXiv preprint arXiv:1901.05954 (2019). \n[1] Du, Bo, et al. “Exploring representativeness and informativeness for active learning.” IEEE transactions on cybernetics 47.1 (2015): 14–26. \nOne thing that is quite confusing is the analysis of AL results in section 4.2. It is unclear if the size of the subset U was optimized for obtaining the best results when applying BALD as the acquisition function. How did you perform this optimization?\nIn general terms, results for both the AL and the layer freezing experiments present a high variance and depend a lot on the task.  When looking at NLI tasks, it is hard to tell that using BALD is beneficial enough for the higher complexity that it adds to the training process. Why did you not include more tasks to show that the conclusions extracted truly generalize?",
    "overall_score": "2",
    "confidence": "4"
  }
]