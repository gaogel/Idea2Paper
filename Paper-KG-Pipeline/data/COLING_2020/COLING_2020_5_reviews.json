[
  {
    "review_id": "c27d9f95f680f34b",
    "paper_id": "COLING_2020_5",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper presents a new variant of hypernymy discovery (SemEval 2018 task) similar to a single step in taxonomy induction: given a target WordNet synset, predict its direct hypernym. The suggested method is trained as a sequence generation task, which, given a synset, predicts the entire hypernym chain. The method is compared with a version that only predicts the direct hypernym, which performs slightly worse.  The paper is well written. The authors did a good job comparing to a diverse set of baselines (from KB completion, taxonomy induction and hypernymy discovery literature). There is an interesting error analysis, and throughout the paper, design choices are well supported. For example, I was happy to see the distinction between instance and class hypernyms, and between nouns and verbs (which is often ignored). There is a good level of technical details.  Questions and comments: - I have a bit of an issue with the generic name for the task because it doesn't differentiate it from the previous tasks. E.g. is it defined differently from \"taxonomy induction\"? Is it a variant of \"hypernym discovery\" (SemEval 2018), except for direct hypernyms within a taxonomy?\n- I was a bit worried about test set leakage when I saw that the hypo2path model performed better than hypo2hyper. If I understand correctly, this is not the case (as explained in 4.1). I would like to see it more clearly explained: how exactly did you make sure that paths are not shared between the data sets? And in that case, what explains the success of the path-based model? Is it like multi-task learning or data augmentation?\n- Please elaborate on “Luong-style attention” - For the discriminative model (“path encoder”), how were the distractors selected?\n- How did you adapt the CRIM model to be synset based (rather than lemma based?)  - While I agree with your view on why it’s more general not to rely on definitions, I think this comment would have been better supported if your method was evaluated on a taxonomy that doesn’t contain definitions. Otherwise, if the method is only evaluated on WordNet, what difference does it make? You might as well use the definitions.\n- I don’t understand the claim before footnote 6.  - I like it that you have an additional metric other than a strict accuracy metric, but I wonder whether WuP is a good metric considering that it measures *similarity* which is different from *hypernymy*. Wouldn’t it reward models for predicting a similar synset such as a co-hyponym? I'm curious whether it's not the case that models typically err on predicting a similar but incorrect node, in which case they would score highly on this metric despite being fuzzy and imprecise.\n- For the camera-ready version, will you be able to include the ablation results?\n- Are the embeddings of synsets without pretrained embeddings, which were initialized randomly, still frozen?\n- With respect to the fact that it’s easier to predict instance hypernyms, this was also one of the findings in the SemEval 2018 task: “systems tended to perform better with entities than with concepts. This is probably due to the fact that entities contain many hypernyms which appear often (e.g. person, company)”.\n- “...and has a number of problems described in Richens (2008).” - like what?\n- Which relations did M3GM perform well on?\n- Regarding “path validity” in the error analysis - does this indicate memorization? How would your method work on a larger taxonomy?\n- I would separate the error of predicting indirect hypernyms (somewhat valid, depending on how far they are) and siblings (invalid). WordNet is inconsistent with the distance between hypo-hyper pairs. For instance, dog and mammal have 5 edges apart but mammal is a valid hypernym for dog (despite not technically being the direct one) while cat (its sibling) definitely isn't.\n- Regarding polysemous lemmas, this is interesting because the model is expected to memorize the synset ID associated with each sense, which doesn't make much sense to me. A more natural setup would be to find hypernyms for words in context, by combining the task with WSD. But I understand it's outside the scope of this paper.\n- Figure 1 - Consider replacing with vector graphics for better quality (or enlarging the figure).",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "3f1d5d7f2b96d783",
    "paper_id": "COLING_2020_5",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The motivation part does not feel fully connected the the main goal of the paper, and is somewhat unnecessary. I need to know what hyonymy is, and that it is hard for embeddings, but if (not the case) I don't think it matters, it is probably because I don't care about semantic relations, and claiming is-a is more important than other relations won't convince me.\nThe connection between the first two sentences is confusing. It is implied that what makes IS-A important is its usability. I believe it is commonly used because it is simple and well defined, not because it is more important. \nOn a similar notion, the finding that \"it is more difficult to predict hypernymy than other lexical relations\" is exaggerated, this is not a main claim of the paper and it is more difficult than some other lexical relations, in the specific context there, but it might be easier for other methods, and there are other possible relations which are not discussed there.   \"There has been much recent work on\" - requires citations (survey if exists, otherwise some dominant papers) Noun path of length 5 is comparable to the whole path. Is path of length 2 comparable too? If so, what does that mean about the usefulness of the path? \nWhere is the path expected to have an impact (e.g. in verbs and instance nouns it does not)? It seems the reasons for the path improvement are not trivial and worth exploring a bit more.\nRegarding path validity, are any of the paths ones that were not seen as a full path in the training? If not, then this finding is much less surprising, It worth noting either way.\nThe error analysis is really nice.",
    "overall_score": "4",
    "confidence": "4"
  }
]