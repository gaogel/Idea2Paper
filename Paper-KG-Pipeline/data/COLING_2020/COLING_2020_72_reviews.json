[
  {
    "review_id": "d62f5fc7b6674836",
    "paper_id": "COLING_2020_72",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper presents a complicated matter in a remarkably clear fashion, which makes it easy to follow, even for non-specialists. The extension of NLP procedures to languages with less resources is welcome, and the approach taken here is well argued, justified and evaluated. I cannot comment on the mathematical details, though.\nThe term \"backtranslation\" is mentioned in Section 2, but is explicated in detail as late as in 6.3. Consider adding a short sentence in Section 2 explaining the nature and motivation of a back-translation approach.\nIn section 3, a reference is needed for the reader to look up the history of the Arabic language(s). The following chapter might be sufficient (or references therein): van Putten. Marijn. 2020. Classical and Modern Standard Arabic. In Christopher Lucas & Stefano Manfredi (eds.), Arabic and contact-induced change, 57–82. Berlin: Language Science Press. https://langsci-press.org/catalog/book/235 A couple of references use \"(Smith 2000)\" where \"Smith (2000)\" would have been appropriate. Please check all references.  The literature entries for Ferguson, Guzman, Habash have problems with upper/lower case letters. Check these and also the remaining entries.  Appendices D & E  would profit from lines with transliteration. I can read some Arabic, but not as fast as I would need to make the appendix a worthwhile addition for a reader like me.",
    "overall_score": "4",
    "confidence": "2"
  },
  {
    "review_id": "426b46e52813af10",
    "paper_id": "COLING_2020_72",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper presents a research aiming to create larger training and evaluation sets of data in translation enterprises that have English on one side and dialects of Arabic (Egyptian and Levantine), known as resource-poor, on the other. Supplementary parallel data are generated by a bootstrapping technique applied to an original set of English sentences, acquired from more sources, and their human translations in Modern Standard Arabic (MSA) and two of its dialects. To this original set, a Byte-Pair Encoding with a carefully chosen fixed vocabulary is applied and then the training corpus is augmented via a bootstrapping approach. The paper is important as an example of successful adaptation of known MT techniques for pairs of languages in which one is resource-poor and for offering to the research community a 4-way benchmark dataset between Egyptian, Levantine, MSA and English.\nTechnical details are clean and prove an intimate knowledge of the fields of NN and MT. English is accurate.  The plot displaying the correlation between the size of the vocabulary and the BLEU score of translations, combined with the authors’ comments related to the dimension of the training set, suggest the use of a 3-dimensional plot.  As far as I understand, bootstrapping original texts can occasionally lead to syntactically incorrect sentences. If this is the case, care should be taken in using the BLUE measure, because comparing syntactically incorrect data with their translations is not what we usually want, since it may result in slightly too optimistic conclusions.  Incomplete reference with wrong year: “Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015. Neural machine translation of rare words with subword units.” The correct reference is the following: “Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (ACL 2016), Berlin, Germany.”\nOther incomplete references:  - “Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising pre-training for neural machine translation.”\n- “Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need.”\nAlso, pages are missing for a number of references.",
    "overall_score": "4",
    "confidence": "2"
  }
]