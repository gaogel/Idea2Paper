[
  {
    "review_id": "b3063b5f07883cf1",
    "paper_id": "ACL_2017_104",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- Outperforms ALIGN in supervised entity linking task which suggests that the proposed framework improves representations of text and knowledge that are learned jointly.\n- Direct comparison with closely related approach using very similar input data.\n- Analysis of the smoothing parameter provides useful analysis since impact of popularity is a persistent issue in entity linking.",
    "weaknesses": "- Comparison with ALIGN could be better. ALIGN used content window size 10 vs this paper's 5, vector dimension of 500 vs this paper's 200. Also its not clear to me whether N(e_j) includes only entities that link to e_j. The graph is directed and consists of wikipedia outlinks, but is adjacency defined as it would be for an undirected graph? For ALIGN, the context of an entity is the set of entities that link to that entity. If N(e_j) is different, we cannot tell how much impact this change has on the learned vectors, and this could contribute to the difference in scores on the entity similarity task.  - It is sometimes difficult to follow whether \"mention\" means a string type, or a particular mention in a particular document. The phrase \"mention embedding\" is used, but it appears that embeddings are only learned for mention senses.\n- It is difficult to determine the impact of sense disambiguation order without comparison to other unsupervised entity linking methods.",
    "comments": "",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "916436a27bd07320",
    "paper_id": "ACL_2017_104",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper addresses the problem of disambiguating/linking textual entity mentions into a given background knowledge base (in this case, English Wikipedia).  (Its title and introduction are a little overblown/misleading, since there is a lot more to bridging text and knowledge than the EDL task, but EDL is a core part of the overall task nonetheless.)  The method is to perform this bridging via an intermediate layer of representation, namely mention senses, thus following two steps: (1) mention to mention sense, and (2) mention sense to entity.  Various embedding representations are learned for the words, the mention senses, and the entities, which are then jointly trained to maximize a single overall objective function that maximizes all three types of embedding equally.   Technically the approach is fairly clear and conforms to the current deep processing fashion and known best practices regarding embeddings; while one can suggest all kinds of alternatives, it’s not clear they would make a material difference.  Rather, my comments focus on the basic approach.  It is not explained, however, exactly why a two-step process, involving the mention senses, is better than a simple direct one-step mapping from word mentions to their entities.  (This is the approach of Yamada et al., in what is called here the ALIGN algorithm.)  Table 2 shows that the two-step MPME (and even its simplification SPME) do better.  By why, exactly?  What is the exact difference, and additional information, that the mention senses have compare4ed to the entities?  To understand, please check if the following is correct (and perhaps update the paper to make it exactly clear what is going on).   For entities: their profiles consist of neighboring entities in a relatedness graph.                    This graph is built (I assume) by looking at word-level relatedness of the entity definitions (pages in Wikipedia).  The profiles are (extended skip-gram-based) embeddings.   For words: their profiles are the standard distributional semantics approach, without sense disambiguation.   For mention senses: their profiles are the standard distributional semantics approach, but WITH sense disambiguation.  Sense disambiguation is performed using a sense-based profile (‘language model’) from local context words and neighboring mentions, as mentioned briefly just before Section 4, but without details.  This is a problem point in the approach.  How exactly are the senses created and differentiated?  Who defines how many senses a mention string can have?  If this is done by looking at the knowledge base, then we get a bijective mapping between mention senses and entities -– that is, there is exactly one entity for each mention sense (even if there may be more entities). \n In that case, are the sense collection’s definitional profiles built starting with entity text as ‘seed words’?                    If so, what information is used at the mention sense level that is NOT used at the entity level?  Just and exactly the words in the texts that reliably associate with the mention sense, but that do NOT occur in the equivalent entity webpage in Wikipedia?  How many such words are there, on average, for a mention sense?                    That is, how powerful/necessary is it to keep this extra differentiation information in a separate space (the mention sense space) as opposed to just loading these additional words into the Entity space (by adding these words into the Wikipedia entity pages)?   If the above understanding is essentially correct, please update Section 5 of the paper to say so, for (to me) it is the main new information in the paper.   It is not true, as the paper says in Section 6, that “…this is the first work to deal with mention ambiguity in the integration of text and knowledge representations, so there is no exact baselines for comparison”.  The TAC KBP evaluations for the past two years have hosted EDL tasks, involving eight or nine systems, all performing exactly this task, albeit against Freebase, which is considerably larger and more noisy than Wikipedia.  Please see http://nlp.cs.rpi.edu/kbp/2016/ .   On a positive note: I really liked the idea of the smoothing parameter in Section 6.4.2.\nPost-response: I have read the authors' responses.  I am not really satisfied with their reply about the KBP evaluation not being relevant, but that they are interested in the goodness of the embeddings instead.  In fact, the only way to evaluate such 'goodness' is through an application.  No-one really cares how conceptually elegant an embedding is, the question is: does it perform better?",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "464ed08037befba6",
    "paper_id": "ACL_2017_104",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Good ideas, simple neural learning, interesting performance (altough not striking) and finally large set of applications.",
    "weaknesses": "amount of novel content. Clarity in some sections.  The paper presents a neural learning method for entity disambiguation and linking. It introduces a good idea to integrate entity, mention and sense modeling within the smame neural language modeling technique. The simple training procedure connected with the modeling allows to support a large set of application.\nThe paper is clear formally, but the discussion is not always at the same level of the technical ideas.\nThe empirical evaluation is good although not striking improvements of the performance are reported. Although it seems an extension of (Yamada et al., CoNLL 2016), it adds novel ideas and it is of a releant interest.\nThe weaker points of the paper are: - The prose is not always clear. I found Section 3 not as clear. Some details of Figure 2 are not explained and the terminology is somehow redundant: for example, why do you refer to the dictionary of mentions? or the dictionary of entity-mention pairs? are these different from text anchors and types for annotated text anchors?\n- Tha paper is quite close in nature to Yamada et al., 2016) and the authors should at least outline the differences.\nOne general observation on the current version is: The paper tests the Multiple Embedding model against entity linking/disambiguation tasks. However, word embeddings are not only used to model such tasks, but also some processes not directly depending on entities of the KB, e.g. parsing, coreference or semantic role labeling. \nThe authors should show that the word embeddings provided by the proposed MPME method are not weaker wrt to simpler wordspaces in such other semantic tasks, i.e. those involving directly entity mentions.\nI did read the author's response.",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "b263bd1912b05e3c",
    "paper_id": "ACL_2017_105",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The idea of hard monotonic attention is new and substantially different from others.",
    "weaknesses": "The experiment results on morphological inflection generation is somewhat mixed. The proposed model is effective if the amount of training data is small (such as CELEX). It is also effective if the alignment is mostly monotonic and less context sensitive (such as Russian, German and Spanish).",
    "comments": "The authors proposed a novel neural model for morphological inflection generation which uses \"hard attention\", character alignments separately obtained by using a Bayesian method for transliteration. It is substantially different from the previous state of the art neural model for the task which uses \"soft attention\", where character alignment and conversion are solved jointly in the probabilistic model.\nThe idea is novel and sound. The paper is clearly written. The experiment is comprehensive. The only concern is that the proposed method is not necessarily the state of the art in all conditions. It is suitable for the task with mostly monotonic alignment and with less context sensitive phenomena. The paper would be more convincing if it describe the practical merits of the proposed method, such as the ease of implementation and computational cost.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "ac306cecb711d2d4",
    "paper_id": "ACL_2017_105",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "A new encoder-decoder model is proposed that explicitly takes  into account monotonicity.",
    "weaknesses": "Maybe the model is just an ordinary BiRNN with alignments de-coupled. \nOnly evaluated on morphology, no other monotone Seq2Seq tasks.",
    "comments": "The authors propose a novel encoder-decoder neural network architecture with \"hard monotonic attention\". They evaluate it on three morphology datasets.\nThis paper is a tough one. One the one hand it is well-written, mostly very clear and also presents a novel idea, namely including monotonicity in morphology tasks.  The reason for including such monotonicity is pretty obvious: Unlike machine translation, many seq2seq tasks are monotone, and therefore general encoder-decoder models should not be used in the first place. That they still perform reasonably well should be considered a strong argument for neural techniques, in general. The idea of this paper is now to explicity enforce a monotonic output character generation. They do this by decoupling alignment and transduction and first aligning input-output sequences monotonically and then training to generate outputs in agreement with the monotone alignments. \nHowever, the authors are unclear on this point. I have a few questions: 1) How do your alignments look like? On the one hand, the alignments seem to be of the kind 1-to-many (as in the running example, Fig.1), that is, 1 input character can be aligned with zero, 1, or several output characters. However, this seems to contrast with the description given in lines 311-312 where the authors speak of several input characters aligned to 1 output character. That is, do you use 1-to-many, many-to-1 or many-to-many alignments?\n2) Actually, there is a quite simple approach to monotone Seq2Seq. In a first stage, align input and output characters monotonically with a 1-to-many constraint (one can use any monotone aligner, such as the toolkit of Jiampojamarn and Kondrak). Then one trains a standard sequence tagger(!) to predict exactly these 1-to-many alignments. For example, flog->fliege (your example on l.613): First align as in \"f-l-o-g / f-l-ie-ge\". Now use any tagger (could use an LSTM, if you like) to predict \"f-l-ie-ge\" (sequence of length 4) from \"f-l-o-g\" (sequence of length 4). Such an approach may have been suggested in multiple papers, one reference could be [*, Section 4.2] below. \nMy two questions here are:  2a) How does your approach differ from this rather simple idea?\n2b) Why did you not include it as a baseline?\nFurther issues: 3) It's really a pitty that you only tested on morphology, because there are many other interesting monotonic seq2seq tasks, and you could have shown your system's superiority by evaluating on these, given that you explicitly model monotonicity (cf. also [*]).\n4) You perform \"on par or better\" (l.791). There seems to be a general cognitive bias among NLP researchers to map instances where they perform worse to \"on par\" and all the rest to \"better\". I think this wording should be corrected, but otherwise I'm fine with the experimental results.\n5) You say little about your linguistic features: From Fig. 1, I infer that they include POS, etc.  5a) Where did you take these features from?\n5b) Is it possible that these are responsible for your better performance in some cases, rather than the monotonicity constraints?\nMinor points: 6) Equation (3): please re-write $NN$ as $\\text{NN}$ or similar 7) l.231 \"Where\" should be lower case 8) l.237 and many more: $x_1\\ldots x_n$. As far as I know, the math community recommends to write $x_1,\\ldots,x_n$ but $x_1\\cdots x_n$. That is, dots should be on the same level as surrounding symbols.\n9) Figure 1: is it really necessary to use cyrillic font? I can't even address your example here, because I don't have your fonts.\n10) l.437: should be \"these\" [*]  @InProceedings{schnober-EtAl:2016:COLING,    author    = {Schnober, Carsten  and  Eger, Steffen  and  Do Dinh, Erik-L\\^{a}n  and  Gurevych, Iryna},   title     = {Still not there? Comparing Traditional Sequence-to-Sequence Models to Encoder-Decoder Neural Networks on Monotone String Translation Tasks},   booktitle = {Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers},   month     = {December},   year                                                      = {2016},   address   = {Osaka, Japan},   publisher = {The COLING 2016 Organizing Committee},   pages     = {1703--1714},   url                                               = {http://aclweb.org/anthology/C16-1160} } AFTER AUTHOR RESPONSE Thanks for the clarifications. I think your alignments got mixed up in the response somehow (maybe a coding issue), but I think you're aligning 1-0, 0-1, 1-1, and later make many-to-many alignments from these. \nI know that you compare to  Nicolai, Cherry and Kondrak (2015) but my question would have rather been: why not use 1-x (x in 0,1,2) alignments as in  Schnober et al. and then train a neural tagger on these (e.g. BiLSTM). I wonder how much your results would have differed from such a rather simple baseline. ( A tagger is a monotone model to start with and given the monotone alignments, everything stays monotone. In contrast, you start out with a more general model and then put hard monotonicity constraints on this ...) NOTES FROM AC Also quite relevant is Cohn et al. (2016), http://www.aclweb.org/anthology/N16-1102 .\nIsn't your architecture also related to methods like the Stack LSTM, which similarly predicts a sequence of actions that modify or annotate an input?   Do you think you lose anything by using a greedy alignment, in contrast to Rastogi et al. (2016), which also has hard monotonic attention but sums over all alignments?",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "cf6fb7acdd0fa21c",
    "paper_id": "ACL_2017_107",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper presents several weakly supervised methods for developing NERs. The methods rely on some form of projection from English into another language. The overall approach is not new and the individual methods proposed are improvements of existing methods. For an ACL paper I would have expected more novel approaches.\nOne of the contributions of the paper is the data selection scheme. The formula used to calculate the quality score is quite straightforward and this is not a bad thing. However, it is unclear how the thresholds were calculated for Table 2. The paper says only that different thresholds were tried. Was this done on a development set? There is no mention of this in the paper. The evaluation results show clearly that data selection is very important, but one may not know how to tune the parameters for a new data set or a new language pair.  Another contribution of the paper is the combination of the outputs of the two systems developed in the paper. I tried hard to understand how it works, but the description provided is not clear.  The paper presents a number of variants for each of the methods proposed. Does it make sense to combine more than two weakly supervised systems? Did the authors try anything in this direction.\nIt would be good to know a bit more about the types of texts that are in the \"in-house\" dataset.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "091722b846bda325",
    "paper_id": "ACL_2017_107",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper describes a model for cross-lingual named entity recognition (NER). \nThe authors employ conditional random fields, maximum entropy Markov, and neural network-based NER methods. In addition, authors propose two methods to combine the output of those methods (probability-based and ranking-based), and a method to select the best training instances from cross-lingual comparable corpora. The cross-lingual projection is done using a variant of Mikolov’s proposal. In general, the paper is easy to follow, well-structured, and the English quality is also correct. The results of the combined annotations are interesting.\nDetailed comments: I was wondering which is the motivation behind proposing a Continuous Bag-of-word (CBOW) model variation. You don’t give much details about this (or the parameters employed). Was the original model (or the Continuous Skip-gram model) offering low results? I suggest to include also the results with the CBOW model, so readers can analyse the improvements of your approach. \nSince you use a decay factor for the surrounding embeddings, I suggest to take a look to the exponential decay used in [1].\nSimilarly to the previous comment, I would like to look at the differences between the original Mikolov’s cross-lingual projections and your frequency weighted projections. These contributions are more valuable if readers can see that your method is really superior.\n“the proposed data selection scheme is very effective in selecting good-quality projection-labeled data and the improvement is significant” ← Have you conducted a test of statistical significance? I would like to know if the differences between result in this work are significant.  I suggest to integrate the text of Section 4.4 at the beginning of Section 4.2. \nIt would look cleaner. I also recommend to move the evaluation of Table 2 to the evaluation section.\nI miss a related work section. Your introduction includes part of that information. I suggest to divide the introduction in two sections.\nThe evaluation is quite short (1.5 pages with conclusion section there). You obtain state-of-the-art results, and I would appreciate more discussion and analysis of the results.\nSuggested references: [1] Iacobacci, I., Pilehvar, M. T., & Navigli, R. (2016). Embeddings for word sense disambiguation: An evaluation study. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Vol. 1, pp. 897-907).",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "3b5f6fd5e10d6d48",
    "paper_id": "ACL_2017_108",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "the paper is well-written, except for a few places as described below. The problem the paper tackles is useful. The proposed approach, multigraph-based model, is a variant of MH. The empirical result is solid.",
    "weaknesses": "Clarification is needed in several places.\n1. In section 3, in addition to the description of the previous model, MH, you need point out the issues of MH which motivate you to propose a new model.\n2. In section 4, I don't see the reason why separators are introduced. what additional info they convene beyond T/I/O?\n3. section 5.1 does not seem to provide useful info regarding why the new model is superior.\n4. the discussion in section 5.2 is so abstract that I don't get the insights why the new model is better than MH. can you provide examples of spurious structures?",
    "comments": "The paper presents a new model for detecting overlapping entities in text. The new model improves the previous state-of-the-art, MH, in the experiments on a few benchmark datasets. But it is not clear why and how the new model works better.",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "009c94991db35f13",
    "paper_id": "ACL_2017_108",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The problem itself could be rather interesting especially for crossing entities to decide which one might actually be mentioned in some text. The technique seems to work although the empirically results do not show some \"dramatic\" effect. I like that some words are spent on efficiency compared to a previous system. The paper in general is well-written but also needs some further polishing in some details (see minor remarks below).",
    "weaknesses": "The problem itself is not really well motivated. Why is it important to detect China as an entity within the entity Bank of China, to stay with the example in the introduction? I do see a point for crossing entities but what is the use case for nested entities? This could be much more motivated to make the reader interested. As for the approach itself, some important details are missing in my opinion: What is the decision criterion to include an edge or not? In lines 229--233 several different options for the I^k_t nodes are mentioned but it is never clarified which edges should be present!\nAs for the empirical evaluation, the achieved results are better than some previous approaches but not really by a large margin. I would not really call the slight improvements as \"outperformed\" as is done in the paper. What is the effect size? Does it really matter to some user that there is some improvement of two percentage points in F_1? What is the actual effect one can observe? How many \"important\" entities are discovered, that have not been discovered by previous methods? Furthermore, what performance would some simplistic dictionary-based method achieve that could also be used to find overlapping things? And in a similar direction: what would some commercial system like Google's NLP cloud that should also be able to detect and link entities would have achieved on the datasets. Just to put the results also into contrast of existing \"commercial\" systems.\nAs for the result discussion, I would have liked to see some more emphasis on actual crossing entities. How is the performance there? This in my opinion is the more interesting subset of overlapping entities than the nested ones. How many more crossing entities are detected than were possible before? Which ones were missed and maybe why? Is the performance improvement due to better nested detection only or also detecting crossing entities? Some",
    "comments": "",
    "overall_score": "2",
    "confidence": "3"
  },
  {
    "review_id": "f66f713d2aa11aaf",
    "paper_id": "ACL_2017_117",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper addresses a relevant topic: learning the mapping between natural language and KB relations, in the context of QA (where we have only partial information for one of the arguments), and in the case of having a very large number of possible target relations.\nThe proposal consists in a new method to combine two different representations of the input text: a word level representation (i.e. with segmentation of the target relation names and also the input text), and relations as a single token (i.e without segmentation of relation names nor input text).  It seems, that the main contribution in QA is the ability to re-rank entities after the Entity Linking step.\nResults show an improvement compared with the state of the art.",
    "weaknesses": "The approach has been evaluated in a limited dataset.",
    "comments": "I think, section 3.1 fits better inside related work, so the 3.2 can become section 3 with the proposal. Thus, new section 3 can be splitted more properly.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "d2d3f1de569b06d7",
    "paper_id": "ACL_2017_122",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper proposes a novel approach for dialogue state tracking that benefits from representing slot values with pre-trained embeddings and learns to compose them into distributed representations of user utterances and dialogue context. \nExperiments performed on two datasets show consistent and significant improvements over the baseline of previous delexicalization based approach. \nAlternative approaches (i.e., XAVIER, GloVe, Program-SL999) for pre-training word embeddings have been investigated.",
    "weaknesses": "Although one of the main motivations for using embeddings is to generalize to more complex dialogue domains where delexicalization may not scale for, the datasets used seem limited.    I wonder how the approach would compare with and without a separate slot tagging component on more complex dialogues. For example, when computing similarity between the utterance and slot value pairs, one can actually limit the estimation to the span of the slot values. This should be applicable even when the values do not match.\nI think the examples in the intro is misleading, shouldn’t the dialogue state also include “restaurant_name=The House”? This brings another question, how does resolution of coreferences impact this task?",
    "comments": "On the overall, use of pre-trained word embeddings is a great idea, and the specific approach for using them is exciting.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "cb2a8f8eb36370fc",
    "paper_id": "ACL_2017_122",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper presents a neural network-based framework for dialogue state tracking. \nThe main contribution of this work is on learning representations of user utterances, system outputs, and also ontology entries, all of which are based on pre-trained word vectors. \nParticularly for the utterance representation, the authors compared two different neural network models: NBT-DNN and NBT-CNN. \nThe learned representations are combined with each other and finally used in the downstream network to make binary decision for a given slot value pair. \nThe experiment shows that the proposed framework achieved significant performance improvements compared to the baseline with the delexicalized approach.\nIt's generally a quality work with clear goal, reasonable idea, and improved results from previous studies. \nBut the paper itself doesn't seem to be very well organized to effectively deliver the details especially to readers who are not familiar with this area.\nFirst of all, more formal definition of DST needs to be given at the beginning of this paper. \nIt is not clear enough and could be more confusing after coupling with SLU. \nMy suggestion is to provide a general architecture of dialogue system described in Section 1 rather than Section 2, followed by the problem definition of DST focusing on its relationships to other components including ASR, SLU, and policy learning.\nAnd it would also help to improve the readability if all the notations used throughout the paper are defined in an earlier section. \nSome symbols (e.g. t_q, t_s, t_v) are used much earlier than their descriptions.\nBelow are other comments or questions: - Would it be possible to perform the separate SLU with this model? If no, the term 'joint' could be misleading that this model is able to handle both tasks.\n- Could you please provide some statistics about how many errors were corrected from the original DSTC2 dataset? \nIf it is not very huge, the experiment could include the comparisons also with other published work including DSTC2 entries using the same dataset.\n- What do you think about using RNNs or LSTMs to learn the sequential aspects in learning utterance representations? \nConsidering the recent successes of these recurrent networks in SLU problems, it could be effective to DST as well.\n- Some more details about the semantic dictionary used with the baseline would help to imply the cost for building this kind of resources manually.\n- It would be great if you could give some samples which were not correctly predicted by the baseline but solved with your proposed models.",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "e14b62347ed69ee4",
    "paper_id": "ACL_2017_128",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper proposes a neural network architecture that represent structural linguistic knowledge in a memory network for sequence tagging tasks (in particular, slot-filling of the natural language understanding unit in conversation systems). Substructures (e.g. a node in the parse tree) is encoded as a vector (a memory slot) and a weighted sum of the substructure embeddings are fed in a RNN at each time step as additional context for labeling.\n-----Strengths----- I think the main contribution of this paper is a simple way to \"flatten\" structured information to an array of vectors (the memory), which is then connected to the tagger as additional knowledge. The idea is similar to structured / syntax-based attention (i.e. attention over nodes from treeLSTM); related work includes Zhao et al on textual entailment, Liu et al. on natural language inference, and Eriguchi et al. for machine translation. The proposed substructure encoder is similar to DCNN (Ma et al.): each node is embedded from a sequence of ancestor words. The architecture does not look entirely novel, but I kind of like the simple and practical approach compared to prior work.\n-----Weaknesses----- I'm not very convinced by the empirical results, mostly due to the lack of details of the baselines. Comments below are ranked by decreasing importance.\n-  The proposed model has two main parts: sentence embedding and substructure embedding. In Table 1, the baseline models are TreeRNN and DCNN, they are originally used for sentence embedding but one can easily take the node/substructure embedding from them too. It's not clear how they are used to compute the two parts.\n- The model uses two RNNs: a chain-based one and a knowledge guided one. The only difference in the knowledge-guided RNN is the addition of a \"knowledge\" vector from the memory in the RNN input (Eqn 5 and 8). It seems completely unnecessary to me to have separate weights for the two RNNs. The only advantage of using two is an increase of model capacity, i.e. more parameters. \nFurthermore, what are the hyper-parameters / size of the baseline neural networks? They should have comparable numbers of parameters.\n- I also think it is reasonable to include a baseline that just input additional knowledge as features to the RNN, e.g. the head of each word, NER results etc.\n- Any comments / results on the model's sensitivity to parser errors?\nComments on the model: - After computing the substructure embeddings, it seems very natural to compute an attention over them at each word. Is there any reason to use a static attention for all words? I guess as it is, the \"knowledge\" is acting more like a filter to mark important words. Then it is reasonable to include the baseline suggest above, i.e. input additional features.\n- Since the weight on a word is computed by inner product of the sentence embedding and the substructure embedding, and the two embeddings are computed by the same RNN/CNN, doesn't it means nodes / phrases similar to the whole sentence gets higher weights, i.e. all leaf nodes?\n- The paper claims the model generalizes to different knowledge but I think the substructure has to be represented as a sequence of words, e.g. it doesn't seem straightforward for me to use constituent parse as knowledge here.\nFinally, I'm hesitating to call it \"knowledge\". This is misleading as usually it is used to refer to world / external knowledge such as a knowledge base of entities, whereas here it is really just syntax, or arguably semantics if AMR parsing is used.\n-----General Discussion----- This paper proposes a practical model which seems working well on one dataset, but the main ideas are not very novel (see comments in Strengths). I think as an ACL paper there should be more takeaways. More importantly, the experiments are not convincing as it is presented now. Will need some clarification to better judge the results.\n-----Post-rebuttal----- The authors did not address my main concern, which is whether the baselines (e.g. TreeRNN) are used to compute substructure embeddings independent of the sentence embedding and the joint tagger. Another major concern is the use of two separate RNNs which gives the proposed model more parameters than the baselines. Therefore I'm not changing my scores.",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "2d9859b8e9b6826f",
    "paper_id": "ACL_2017_12",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "It's well written and clearly presented. The rules are motivated by empirical observations of the data, and seems to be well justified as evidenced by the evaluation.",
    "weaknesses": "There are some underspecification in the paper that makes it difficult to reproduce the results. See below for details.",
    "comments": "- Section 4.1: what are there 5 seasons? What about things such as Ramadan month or Holiday Season?\n- Section 5.1: \"two benchmark datasets\" => \"three datasets\"?\n- Section 5.2: an example without time token will be helpful.\n- Section 5.2: given this approach is close to the ceiling of performance since 93% expressions contain time token, and the system has achieved 92% recall, how do you plan to improve further?\n- Is there any plan to release the full set of rules/software used?",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "2ab98ab5559060a5",
    "paper_id": "ACL_2017_12",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper proposes a method to recognize time expressions from text. It is a simple rule-based method, which is a strong advantage as an analysis tool since time expression recognition should be a basic process in applications. \nExperiments results show that the proposed method outperforms the state-of-the-art rule-based methods and machine learning based method for time expression recognition.  It is great, but my concern is generality of the method. The rules in the method were designed based on observations of corpora that are used for evaluation as well. Hence I’m afraid that the rules over-fit to these corpora. Similarly, domains of these corpora may have affected the rule design. \nThere is no statistic nor discussion to show overlaps in time expressions in the observed corpora. If it was shown that time expressions in these corpora are mostly overlap, the fact should have supported generality of the rules.  Anyway, it was better that the experiments have been conducted using a new corpus that was distinct from rule design process in order to show that the proposed method is widely effective.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "1a6f943baa3fa784",
    "paper_id": "ACL_2017_130",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper proposes to apply NLP to speech transcripts (narratives and descriptions) in order to identify patients with MCI (mild cognitive impairment, ICD-10 code F06.7). The authors claim that they were able to distinguish between healthy control participants and patients with MCI (lines 141-144). However in the conclusion, lines 781-785, they say that “… accuracy ranging from 60% to 85% …. means that it is not easy to distinguish between healthy subjects and those with cognitive impairments”. So the paper beginning is more optimistic than the conclusion but anyway the message is encouraging and the reader becomes curious to see more details about what has been actually done.\nThe corpus submitted in the dataset is constructed for 20 healthy patients and 20 control participants only (20+20), and it is non-understandable for people who do not speak Portuguese. It would be good to incorporate more technological details in the article and probably to include at least one example of a short transcript that is translated to English, and eventually a (part of a) sample network with embeddings for this transcript.",
    "weaknesses": "The paper starts with a detailed introduction and review of relevant work. Some of the cited references are more or less NLP background so they can be omitted e.g. (Salton 1989) in section 4.2.3. Other references are not directly related to the topic e.g. “sentiment classification” and “pedestrian detection in images”, lines 652-654, and they can be omitted too. In general lines 608-621, section 4.2.3 can be shortened as well etc. etc. The suggestion is to compress the first 5 pages, focusing the review strictly on the paper topic, and consider the technological innovation in more detail, incl. samples of English translations of the ABCD and/or Cindarela narratives.\nThe relatively short narratives in Portuguese esp. in ABCD dataset open the question how the similarities between words have been found, in order to construct word embeddings. In lines 272-289 the authors explain that they generate word-level networks from continuous word representations. What is the source for learning the continuous word representations; are these the datasets ABCD+Cinderella only, or external corpora were used? In lines 513-525 it is written that sub-word level (n-grams) networks were used to generate word embeddings. Again, what is the source for the training? Are we sure that the two kinds of networks together provide better accuracy? And what are the “out-of-vocabulary words” (line 516), from where they come?",
    "comments": "It is important to study how NLP can help to discover cognitive impairments; from this perspective the paper is interesting. Another interesting aspect is that it deals with NLP for Portuguese, and it is important to explain how one computes embeddings for a language with relatively fewer resources (compared to English).  The text needs revision: shortening sections 1-3, compressing 4.1 and adding more explanations about the experiments. Some clarification about the NURC/SP N. 338 EF and 331 D2 transcription norms can be given.\nTechnical comments: Line 029: ‘… as it a lightweight …’ -> shouldn’t this be ‘… as in a lightweight …’ Line 188: PLN -> NLP Line 264: ‘out of cookie out of the cookie’ – some words are repeated twice  Table 3, row 2, column 3: 72,0 -> 72.0 Lines 995-996: the DOI number is the same as the one at lines 1001-1002; the link behind the title at lines 992-993 points to the next paper in the list",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "d3b7ca1115e7d716",
    "paper_id": "ACL_2017_130",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper explores is problem of identifying patients with Mild Cognitive Impairment (MCI) by analyzing speech transcripts available from three different datasets. A graph based method leveraging co-occurrence information between words found in transcripts is described. Features are encoded using different characteristics of the graph lexical, syntactic properties, and many others. \nResults are reported using 5 fold cross validation using a number of classifiers. Different models exhibit different performance across the three datasets. This work targets a well defined problem and uses appropriate datasets.",
    "weaknesses": "The paper suffers from several drawbacks 1. The paper is hard to read due to incorrect usage of English. The current manuscript would benefit a  lot from a review grammar and spellings. \n2. The main machine learning problem being addressed is poorly described. What was a single instance of classification? It seems every transcripts was classified as MCI or No MCI. If this is the case, the dataset descriptions should describe the numbers at a transcript level. Tables 1,2, and 3 should describe the data not the study that produced the transcripts. The age of the patients is irrelevant for the classification task. A lot of text (2 pages) is consumed in simply describing the datasets with details that do not affect the end classification task. Also, I was unsure why numbers did not add up. For e.g.: in section 4.1.1 the text says 326 people were involved. But the total number of males and females in Table 1 are less than 100? \n3. What is the motivation behind enriching the graph? Why not represent each word by a node in the graph and connect them by the similarity between their vectors, irrespective of co-occurrence? \n4. The datsets are from a biomedical domain. No domain specific tools have been leveraged. \n5. Since dataset class distribution is unclear, it is unclear to determine if accuracy is a good measure for evaluation. In either case, since it is a binary classification task, F1 would have been a desirable metric. \n6. Results are reported unto 4 decimal places on very small datasets (43 transcripts) without statistical tests over increments. Therefore, it is unclear if the gains are significant.",
    "comments": "",
    "overall_score": "2",
    "confidence": "5"
  },
  {
    "review_id": "2daeeefb0dfe3c54",
    "paper_id": "ACL_2017_130",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper describes a novel application of mostly existing representations, features sets, and methods: namely, detecting Mild Cognitive Impairment (MCI)  in speech narratives. The nature of the problem, datasets, and domain are thoroughly described. While missing some detail, the proposed solution and experiments sound reasonable. Overall, I found the study interesting and informative.\nIn terms of drawbacks, the paper needs some considerable editing to improve readability. Details on some key concepts appear to be missing. For example,  details on the multi-view learning used are omitted; the set of “linguistic features” needs to be clarified; it is not entirely clear what datasets were used to generate the word embeddings (presumably the 3 datasets described in the paper, which appear to be too small for that purpose…). It is also not clear why disfluencies (filled pauses, false starts, repetitions, etc.) were removed from the dataset. One might suggest that they are important features in the context of MCI. It is also not clear why the most popular tf-idf weighting scheme was not used for the BoW classifications. In addition, tests for significance are not provided to substantiate the conclusions from the experiments. Lastly, the related work is described a bit superficially.  Detailed comments are provided below: Abstract: The abstract needs to be shortened. See detailed notes below.\nLines 22,23 need rephrasing.            “However, MCI disfluencies produce agrammatical speech impacting in parsing results” → impacting the parsing results?\nLines 24,25: You mean correct grammatical errors in transcripts manually? It is not clear why this should be performed, doesn’t the fact that grammatical errors are present indicate MCI? … Only after reading the Introduction and Related Work sections it becomes clear what you mean. Perhaps include some examples of disfluencies.\nLines 29,30 need rephrasing: “as it a lightweight and language  independent representation” Lines 34-38 need rephrasing: it is not immediately clear which exactly are the 3 datasets. Maybe: “the other two: Cinderella and … “             Line 70: “15% a year” → Not sure what exactly “per year” means… Line 73 needs rephrasing.\nLines 115 - 117: It is not obvious why BoW will also have problems with disfluencies, some explanation will be helpful.\nLines 147 - 149: What do you mean by “the best scenario”?\nLine 157: “in public corpora of Dementia Bank” → a link or citation to Dementia Bank will be helpful.  Line 162: A link or citation describing the “Picnic picture of the Western Aphasia Battery” will be helpful.\nLine 170: An explanation as to what the WML subtest is will be helpful.\nLine 172 is missing citations.\nLines 166 - 182: This appears to be the core of the related work and it is described a bit superficially. For example, it will be helpful to know precisely what methods were used to achieve these tasks and how they compare to this study.\nLine 185: Please refer to the conference citation guidelines. I believe they are something along these lines: “Aluisio et al. (2016)  used…” Line 188: The definition of “PLN” appears to be missing.\nLines 233 - 235 could you some rephrasing. Lemmatization is not necessarily a last step in text pre-processing and normalization, in fact there are also additional common normalization/preprocessing steps omitted.  Lines 290-299: Did you create the word embeddings using the MCI datasets or external datasets?\nLine 322: consisted of → consist of Lines 323: 332 need to be rewritten. ... “ manually segmented of the DementiaBank and Cinderella” →  What do you mean by segmented, segmented into sentences? Why weren’t all datasets automatically segmented?; “ ABCD” is not defined; You itemized the datasets in i) and ii), but subsequently  you refer to 3 dataset, which is a bit confusing. Maybe one could explicitly name the datasets, as opposed to referring to them as “first”, “second”, “third”.\nTable 1 Caption: The demographic information is present, but there are no any additional statistics of the dataset, as described.\nLines 375 - 388:  It is not clear why filled pauses, false starts, repetitions, etc. were removed. One might suggest that they are important features in the context of MCI ….\nLine 399: … multidisciplinary team with psychiatrists ... → consisting of psychiatrists… Lines 340-440: A link or citation describing the transcription norms will be helpful.\nSection 4.2.1. It is not clear what dataset was used to generate the word embeddings.  Line 560. The shortest path as defined in feature 6?\nSection “4.2.2 Linguistic Features” needs to be significantly expanded for clarity. Also, please check the conference guidelines regarding additional pages (“Supplementary Material”).\nLine 620: “In this work term frequency was …” → “In this work, term frequency was …” Also, why not tf-idf, as it seems to be the most common weighting scheme?  The sentence on lines 641-645 needs to be rewritten.\nLine 662: What do you mean by “the threshold parameter”? The threshold for the word embedding cosine distance?\nLine 735 is missing a period.\nSection 4.3 Classification Algorithms: Details on exactly what scheme of multi-view learning was used are entirely omitted. Statistical significance of result differences is not provided.",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "ddcbedee9e3fe0a3",
    "paper_id": "ACL_2017_134",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper is well-written and easy to understand. The methods and results are interesting.",
    "weaknesses": "The evaluation and the obtained results might be problematic (see my comments below).",
    "comments": "This paper proposes a system for end-to-end argumentation mining using neural networks. The authors model the problem using two approaches: (1) sequence labeling (2) dependency parsing. The paper also includes the results of experimenting with a multitask learning setting for the sequence labeling approach. The paper clearly explains the motivation behind the proposed model. \nExisting methods are based on ILP, manual feature engineering and manual design of ILP constraints. However, the proposed model avoids such manual effort. \nMoreover, the model jointly learns the subtasks in argumentation mining and therefore, avoids the error back propagation problem in pipeline methods. \nExcept a few missing details (mentioned below), the methods are explained clearly.\nThe experiments are substantial, the comparisons are performed properly, and the results are interesting. My main concern about this paper is the small size of the dataset and the large capacity of the used (Bi)LSTM-based recurrent neural networks (BLC and BLCC). The dataset includes only around 320 essays for training and 80 essays for testing. The size of the development set, however, is not mentioned in the paper (and also the supplementary materials). This is worrying because very few number of essays are left for training, which is a crucial problem. The total number of tags in the training data is probably only a few thousand. Compare it to the standard sequence labeling tasks, where hundreds of thousands (sometimes millions) of tags are available. For this reason, I am not sure if the model parameters are trained properly. The paper also does not analyze the overfitting problem. It would be interesting to see the training and development \"loss\" values during training (after each parameter update or after each epoch). The authors have also provided some information that can be seen as the evidence for overfitting: Line 622 \"Our explanation is that taggers are simpler local models, and thus need less training data and are less prone to overfitting\".\nFor the same reason, I am not sure if the models are stable enough. Mean and standard deviation of multiple runs (different initializations of parameters) need to be included. Statistical significance tests would also provide more information about the stability of the models and the reliability of results. \nWithout these tests, it is hard to say if the better results are because of the superiority of the proposed method or chance.\nI understand that the neural networks used for modeling the tasks use their regularization techniques. However, since the size of the dataset is too small, the authors need to pay more attention to the regularization methods. The paper does not mention regularization at all and the supplementary material only mentions briefly about the regularization in LSTM-ER. This problem needs to be addressed properly in the paper.\nInstead of the current hyper-parameter optimization method (described in supplementary materials) consider using Bayesian optimization methods.\nAlso move the information about pre-trained word embeddings and the error analysis from the supplementary material to the paper. The extra one page should be enough for this.\nPlease include some inter-annotator agreement scores. The paper describing the dataset has some relevant information. This information would provide some insight about the performance of the systems and the available room for improvement.\nPlease consider illustrating figure 1 with different colors to make the quality better for black and white prints.\nEdit: Thanks for answering my questions. I have increased the recommendation score to 4. Please do include the F1-score ranges in your paper and also report mean and variance of different settings. I am still concerned about the model stability. \nFor example, the large variance of Kiperwasser setting needs to be analyzed properly. Even the F1 changes in the range [0.56, 0.61] is relatively large. \nIncluding these score ranges in your paper helps replicating your work.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "e1839f809dbf7faf",
    "paper_id": "ACL_2017_134",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The work describes a joint neural approach to argumentation mining. There are several approaches explored including:  1) casting the problem as a dependency parsing problem (trying several different parsers)  2) casting the problem as a sequence labeling problem 3) multi task learning (based on sequence labeling model underneath) 4) an out of the box neural model for labeling entities and relations (LSTM-ER) 5) ILP based state-of-the art models All the approaches are evaluated using F1 defined on concepts and relations. \nDependency based solutions do not work well, seq. labeling solutions are effective. \nThe out-of-the-box LSTM-ER model performs very well. Especially on paragraph level. \nThe Seq. labeling and LSTM-ER models both outperform the ILP approach. \nA very comprehensive supplement was given, with all the technicalities of training the models, optimizing hyper-parameters etc. \nIt was also shown that sequence labeling models can be greatly improved by the multitask approach (with the claim task helping more than the relation task). \nThe aper  is a very thorough investigation of neural based approaches to end-to-end argumentation mining.\n- Major remarks     - my one concern is with the data set, i'm wondering if it's a problem that essays in the train set and in the test set might    be on the same topics, consequently writers might use the same or similar arguments in both essays, leading to information    leakage from the train to the test set. In turn, this might give overly optimistic performance estimates. Though, i think the same    issues are present for the ILP models, so your model does not have an unfair advantage. Still, this may be something to discuss.\n  - my other concern is that one of your best models LSTM-ER is acutally just a an out-of-the box application of a model from related     work. However, given the relative success of sequence based models and all the experiments and useful lessons learned, I think this      work deserves to be published.\n- Minor remarks and questions: 222 - 226 - i guess you are arguing that it's possible to reconstruct the full graph once you get a tree as output? Still, this part is not quite clear. \n443-444 The ordering in this section is seq. tagging -> dependency based -> MTL using seq. tagging, it would be much easier to follow if the order of the first two were                   reversed (by the time I got here i'd forgotten what STag_T stood for) 455 - What does it mean that it de-couples them but jointly models them (isn't coupling them required to jointly model them?) \n         - i checked Miwa and Bansal and I couldn't find it 477 - 479 -  It's confusing when you say your system de-couples relation info from entity info, my best guess is that you mean it                         learns some tasks as \"the edges of the tree\" and some other tasks as \"the labels on those edges\", thus decoupling them. \n                        In any case,  I recommend you make this part clearer Are the F1 scores in the paragraph and essay settings comparable? In particular for the relation tasks. I'm wondering if paragraph based  models might miss some cross paragraph relations by default, because they will never consider them.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "06aaaa1eed2ebec2",
    "paper_id": "ACL_2017_145",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Overall a very strong paper.",
    "weaknesses": "The comparison against similar approaches could be extended.",
    "comments": "The main focus of this paper is the introduction of a new model for learning multimodal word distributions formed from Gaussian mixtures for multiple word meanings. i. e. representing a word by a set of many Gaussian distributions. \nThe approach, extend the model introduced by Vilnis and McCallum (2014) which represented word as unimodal Gaussian distribution. By using a multimodal, the current approach attain the problem of polysemy.\nOverall, a very strong paper, well structured and clear. The experimentation is correct and the qualitative analysis made in table 1 shows results as expected from the approach.  There’s not much that can be faulted and all my comments below are meant to help the paper gain additional clarity.  Some comments:  _ It may be interesting to include a brief explanation of the differences between the approach from Tian et al. 2014 and the current one. Both split single word representation into multiple prototypes by using a mixture model.  _ There are some missing citations that could me mentioned in related work as : Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space Neelakantan, A., Shankar. J. Passos, A., McCallum. EMNLP 2014 Do Multi-Sense Embeddings Improve Natural Language Understanding? Li and Jurafsky, EMNLP 2015 Topical Word Embeddings. Liu Y., Liu Z., Chua T.,Sun M. AAAI 2015 _ Also, the inclusion of the result from those approaches in tables 3 and 4 could be interesting.  _ A question to the authors: What do you attribute the loss of performance of w2gm against w2g in the analysis of SWCS?\nI have read the response.",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "878c1121f1c33c69",
    "paper_id": "ACL_2017_145",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The problem is clearly motivated and defined. Gaussian mixtures are much more expressive than deterministic vector representations. It can potentially capture different word meanings by its modes, along with probability mass and uncertainty around those modes. This work represents an important contribution to word embedding.  This work propose a max-margin learning objective with closed-form similarity measurement for efficient training.\nThis paper is mostly well written.",
    "weaknesses": "See below for some questions.",
    "comments": "In the Gaussian mixture models, the number of gaussian components (k) is usually an important parameter. In the experiments of this paper, k is set to 2. What is your criteria to select k? Does the increase of k hurt the performance of this model? What does the learned distribution look like for a word that only has one popular meaning?\nI notice that you use the spherical case in all the experiments (the covariance matrix reduces to a single number). Is this purely for computation efficiency? \nI wonder what's the performance of using a general diagonal covariance matrix. \nSince in this more general case, the gaussian mixture defines different degrees of uncertainty along different directions in the semantic space, which seems more interesting.\nMinor comments: Table 4 is not referred to in the text. \nIn reference, Luong et al. lacks the publication year.\nI have read the response.",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "d95411c0842382dd",
    "paper_id": "ACL_2017_148",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- this article puts two fields together: text readability for humans and machine comprehension of texts",
    "weaknesses": "- The goal of your paper is not entirely clear. I had to read the paper 4 times and I still do not understand what you are talking about!\n- The article is highly ambiguous what it talks about - machine comprehension or text readability for humans - you miss important work in the readability field - Section 2.2. has completely unrelated discussion of theoretical topics.\n- I have the feeling that this paper is trying to answer too many questions in the same time, by this making itself quite weak. Questions such as “does text readability have impact on RC datasets” should be analyzed separately from all these prerequisite skills.",
    "comments": "- The title is a bit ambiguous, it would be good to clarify that you are referring to machine comprehension of text, and not human reading comprehension, because “reading comprehension” and “readability” usually mean that.\n- You say that your “dataset analysis suggested that the readability of RC datasets does not directly affect the question difficulty”, but this depends on the method/features used for answer detection, e.g. if you use POS/dependency parse features.\n- You need to proofread the English of your paper, there are some important omissions, like “the question is easy to solve simply look..” on page 1.\n- How do you annotate datasets with “metrics”??\n- Here you are mixing machine reading comprehension of texts and human reading comprehension of texts, which, although somewhat similar, are also quite different, and also large areas.\n- “readability of text” is not “difficulty of reading contents”. Check this: DuBay, W.H. 2004. The Principles of Readability. Costa Mesa, CA: Impact information.  - it would be good if you put more pointers distinguishing your work from readability of questions for humans, because this article is highly ambiguous. \nE.g. on page 1 “These two examples show that the readability of the text does not necessarily correlate with the difficulty of the questions” you should add “for machine comprehension” - Section 3.1. - Again: are you referring to such skills for humans or for machines? If for machines, why are you citing papers for humans, and how sure are you they are referring to machines too?\n- How many questions the annotators had to annotate? Were the annotators clear they annotate the questions keeping in mind machines and not people?",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "1c5f7e7cbe5d82c5",
    "paper_id": "ACL_2017_150",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The authors present a novel adaptation of encoder-decoder neural MT using an approach that starts and ends with characters, but in between works with representations of morphemes and characters.  The authors release both their code as well as their final learned models for fr-en, cs-en, and en-cs. This is helpful in validating their work, as well as for others looking to replicate and extends this work.\nThe system reported appears to produce translation results of reasonable quality even after the first training epoch, with continued progress in future epochs.\nThe system appears to learn reasonable morphological tokenizations, and appears able to handle previously unseen words (even nonce words) by implicitly backing off to morphemes.",
    "weaknesses": "In the paper, the authors do not explicitly state which WMT test and dev sets their results are reported on. This is problematic for readers wishing to compare the reported results to existing work (for example, the results at matrix.statmt.org). The only way this reviewer found to get this information was to look in the README of the code supplement, which indicates that the test set was newstest2015 and the dev test was newstest2013. This should have been explicitly described in the paper.\nThe instructions given in the software README are OK, but not great. The training and testing sections each could be enhanced with explicit examples of how to run the respective commands. The software itself should respond to a --help flag, which it currently does not.\nThe paper describes a 6-level architecture, but the diagram in Figure 2 appears to show fewer than 6 layers. What's going on? The caption should be more explicit, and if this figure is not showing all of the layers, then there should be a figure somewhere (even if it's in an appendix) showing all of the layers.\nThe results show comparison to other character-based neural systems, but do not show state-of-the-art results for other types of MT system. WMT (and matrix.statmt.org) has reported results for other systems on these datasets, and it appears that the state-of-the-art is much higher than any of the results reported in this paper. That should be acknowledged, and ideally should be discussed.\nThere are a handful of minor English disfluencies, misspellings, and minor LaTeX issues, such as reverse quotation marks. These should be corrected.",
    "comments": "Paper is a nice contribution to the existing literature on character-based neural MT.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "b5e71cd06094cfb3",
    "paper_id": "ACL_2017_150",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "In general, the paper is well structured and clear. It is possible to follow most of the explanation, the ideas presented are original and the results obtained are quite interesting.",
    "weaknesses": "I have some doubts about the interpretation of the results. In addition, I think that some of the claims regarding the capability of the method proposed to learn morphology are not propperly backed by scientific evidence.",
    "comments": "This paper explores a complex architecture for character-level neural machine translation (NMT). The proposed architecture extends a classical encoder-decoder architecture by adding a new deep word-encoding layer capable of encoding the character-level input into sub-word representations of the source-language sentence. In the same way, a deep word-decoding layer is added to the output to transform the target-language sub-word representations into a character sequence as the final output of the NMT system. The objective of such architecture is to take advantage of the benefits of character-level NMT (reduction of the size of the vocabulary and flexibility to deal with unseen words) and, at the same time, improving the performance of the whole system by using an intermediate representation of sub-words to reduce the size of the input sequence of characters. In addition, the authors claim that their deep word-encoding model is able to learn morphology better than other state-of-the-art approaches.\nI have some concerns regarding the evaluation. The authors compare their approach to other state-of-the-art systems taking into account two parameters: training time and BLEU score. However, I do not clearly see the advantage of the model proposed (DCNMT) in front of other approaches such as bpe2char. The difference between both approaches as regards BLEU score is very small (0.04 in Cs-En and 0.1 in En-Cs) and it is hard to say if one of them is outperforming the other one without statistical significance information: has statistical significance been evaluated? As regards the training time, it is worth mentioning that the bpe2char for Cs-En takes 8 days less than DCNMT. For En-Cs training time is not provided (why not?) and for En-Fr bpe2char is not evaluated. I think that a more complete comparison with this system should be carried out to prove the advantages of the model proposed.\nMy second concern is on the 5.2 Section, where authors start claiming that they investigated about the ability of their system to learn morphology. However, the section only contains a examples and some comments on them. Even though these examples are very well chosen and explained in a very didactic way, it is worth noting that no experiments or formal evaluation seem to have been carried out to support the claims of the authors. I would definitely encourage authors to extend this very interesting part of the paper that could even become a different paper itself. On the other hand, this Section does not seem to be a critical point of the paper, so for the current work I may suggest just to move this section to an appendix and soften some of the claims done regarding the capabilities of the system to learn morphology.\nOther comments, doubts and suggestions:  - There are many acronyms that are used but are not defined (such as LSTM, HGRU, CNN or PCA) or which are defined after starting to use them (such as RNN or BPE). Even though some of these acronyms are well known in the field of deep learning, I would encourage the authors to defined them to improve clearness.\n - The concept of energy is mentioned for the first time in Section 3.1. Even though the explanation provided is enough at that point, it would be nice to refresh the idea of energy in Section 5.2 (where it is used several times) and providing some hints about how to interpret it: a high energy on a character would be indicating that the current morpheme should be split at that point? In addition, the concept of peak (in Figure 5) is not described.\n - When the acronym BPE is defined, capital letters are used, but then, for the rest of mentions it is lower cased; is there a reason for this?\n - I am not sure if it is necessary to say that no monolingual corpus is used in Section 4.1.\n - It seems that there is something wrong with Figure 4a since the colours for the energy values are not shown for every character.\n - In Table 1, the results for model (3) (Chung et al. 2016) for Cs-En were not taken from the papers, since they are not reported. If the authors computed these results by themselves (as it seems) they should mention it.\n - I would not say that French is morphologically poor, but rather that it is not that rich as Slavic languages such as Czech.\n - Why a link is provided for WMT'15 training corpora but not for WMT'14?\n - Several references are incomplete Typos:   - \"..is the bilingual, parallel corpora provided...\" -> \"..are the bilingual, parallel corpora provided...\"   - \"Luong and Manning (2016) uses\" -> \"Luong and Manning (2016) use\"   - \"HGRU (It is\" -> \"HGRU (it is\"   - \"coveres\" -> \"covers\"   - \"both consists of two-layer RNN, each has 1024\" -> \"both consist of two-layer RNN, each have 1024\"   - \"the only difference between CNMT and DCNMT is CNMT\" -> \"the only difference between CNMT and DCNMT is that CNMT\"",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "c567b986c859ecb4",
    "paper_id": "ACL_2017_169",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Useful application for teachers and learners; supports fine-grained comparison of GEC systems.",
    "weaknesses": "Highly superficial description of the system; evaluation not satisfying.",
    "comments": "The paper presents an approach of automatically enriching the output of GEC systems with error types. This is a very useful application because both teachers and learners can benefit from this information (and many GEC systems only output a corrected version, without making the type of error explicit). It also allows for finer-grained comparison of GEC systems, in terms of precision in general, and error type-specific figures for recall and precision.\nUnfortunately, the description of the system remains highly superficial. The core of the system consists of a set of (manually?) created rules but the paper does not provide any details about these rules. The authors should, e.g., show some examples of such rules, specify the number of rules, tell us how complex they are, how they are ordered (could some early rule block the application of a later rule?), etc. -- Instead of presenting relevant details of the system, several pages of the paper are devoted to an evaluation of the systems that participated in CoNLL-2014. Table 6 (which takes one entire page) list results for all systems, and the text repeats many facts and figures that can be read off the table.  The evaluation of the proposed system is not satisfying in several aspects. \nFirst, the annotators should have independently annotated a gold standard for the 200 test sentences instead of simply rating the output of the system. Given a fixed set of tags, it should be possible to produce a gold standard for the rather small set of test sentences. It is highly probable that the approach taken in the paper yields considerably better ratings for the annotations than comparison with a real gold standard (see, e.g., Marcus et al. (1993) for a comparison of agreement when reviewing pre-annotated data vs. annotating from scratch). \nSecond, it is said that \"all 5 raters individually considered at least 95% of our rule-based error types to be either “Good” or “Acceptable”\". \nMultiple rates should not be considered individually and their ratings averaged this way, this is not common practice. If each of the \"bad\" scores were assigned to different edits (we don't learn about their distribution from the paper), 18.5% of the edits were considered \"bad\" by some annotator -- this sounds much worse than the average 3.7%, as calculated in the paper. \nThird, no information about the test data is provided, e.g. how many error categories they contain, or which error categories are covered (according to the cateogories rated as \"good\" by the annotators). \nForth, what does it mean that \"edit boundaries might be unusual\"? A more precise description plus examples are at need here. Could this be problematic for the application of the system?\nThe authors state that their system is less domain dependent as compared to systems that need training data. I'm not sure that this is true. E.g., I suppose that Hunspell's vocabulary probably doesn't cover all domains in the same detail, and manually-created rules can be domain-dependent as well -- and are completely language dependent, a clear drawback as compared to machine learning approaches. Moreover, the test data used here (FCE-test, CoNLL-2014) are from one domain only: student essays.\nIt remains unclear why a new set of error categories was designed. One reason for the tags is given: to be able to search easily for underspecified categories (like \"NOUN\" in general). It seems to me that the tagset presented in Nicholls (2003) supports such searches as well. Or why not using the CoNLL-2014 tagset? Then the CoNLL gold standard could have been used for evaluation.\nTo sum up, the main motivation of the paper remains somewhat unclear. Is it about a new system? But the most important details of it are left out. Is it about a new set of error categories? But hardly any motivation or discussion of it is provided. Is it about evaluating the CoNLL-2014 systems? But the presentation of the results remains superficial.\nTypos: - l129 (and others): c.f. - > cf.\n- l366 (and others): M2 -> M^2 (= superscribed 2) - l319: 50-70 F1: what does this mean? 50-70%?\nCheck references for incorrect case - e.g. l908: esl -> ESL - e.g. l878/79: fleiss, kappa",
    "overall_score": "2",
    "confidence": "3"
  },
  {
    "review_id": "12c131c278312454",
    "paper_id": "ACL_2017_169",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- The proposed evaluation is an important stepping stone for    analyzing GEC system behavior. \n - The paper includes evaluation for a variety of systems. \n - The approach has several advantages over previous work:    - it computes precision by error type    - it is independent of manual error annotation    - it can assess the performance on multi token errors  - The automatically selected error tags for pre-computed error spans    are mostly approved of by human experts",
    "weaknesses": "- A key part – the rules to derive error types – are not described. \n - The classifier evaluation lacks a thorough error analysis and based    upon that it lacks directions of future work on how to improve the    classifier. \n - The evaluation was only performed for English and it is unclear how    difficult it would be to use the approach on another language.\nClassifier and Classifier Evaluation ==================================== It is unclear on what basis the error categories were devised. Are they based on previous work?\nAlthough the approach in",
    "comments": "the results in Table 6 with respect to whether the systems were designed to handle the error type. For some error types, there is a straight-forward mapping between error type in the gold standard and in the auto reference, for example for word order error. It remains unclear whether the systems failed completely on specific error types or were just not designed to correct them (CUUI for example is reported with precision+recall=0.0, although it does not target word order errors). In the CUUI case (and there are probably similar cases), this also points at an error in the classification which is neither analyzed nor discussed.\nPlease report also raw values for TP, FP, TN, FN in the appendix for Table 6. This makes it easier to compare the systems using other measures. Also, it seems that for some error types and systems the results in Table 6 are based only on a few instances. This would also be made clear when reporting the raw values.\nYour write \"All but 2 teams (IITB and IPN) achieved the best score in at least 1 category, which suggests that different approaches to GEC complement different error types.\" ( 606) It would be nice to mention here that this is in line with previous research.\nMulti-token error analysis is helpful for future work but the result needs more interpretation: Some systems are probably inherently unable to correct such errors but none of the systems were trained on a parallel corpus of learner data and fluent (in the sense of Sakaguchi et al, 2016) corrections.\nOther ===== - The authors should have mentioned that for some of the GEC   approaches, it was not impossible before to provide error   annotations, e.g. systems with submodules for one error type each. \n  Admittedly, the system would need to be adapted to include the   submodule responsible for a change in the system output. Still, the   proposed approach enables to compare GEC systems for which producing   an error tagged output is not straightforward to other systems in a   unified way.\n- References: Some titles lack capitalizations. URL for Sakaguchi et   al. (2016) needs to be wrapped. Page information is missing for   Efron and Tibshirani (1993).\nAuthor response =============== I agree that your approach is not \"fatally flawed\" and I think this review actually points out quite some positive aspects. The approach is good, but the paper is not ready.\nThe basis for the paper are the rules for classifying errors and the lack of description is a major factor.        This is not just a matter about additional examples. If the rules are not seen as a one-off implementation, they need to be described to be replicable or to adapt them.\nGeneralization to other languages should not be an afterthought.  It would be serious limitation if the approach only worked on one language by design.  Even if you don't perform an adaption for other languages, your approach should be transparent enough for others to estimate how much work such an adaptation would be and how well it could reasonably work.  Just stating that most research is targeted at ESL only reinforces the problem.\nYou write that the error types certain systems tackle would be \"usually obvious from the tables\".  I don't think it is as simple as that -- see the CUUI example mentioned above as well as the unnecessary token errors.  There are five systems that don't correct them (Table 5) and it should therefore be obvious that they did not try to tackle them. However, in the paper you write that \"There is also no obvious explanation as to why these teams had difficulty with this error type\".",
    "overall_score": "2",
    "confidence": "5"
  },
  {
    "review_id": "816ecc058afdbf5e",
    "paper_id": "ACL_2017_16",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper tries to use the information from arguments, which is usually ignored yet actually quite important, to improve the performance of event detection. The framework is clear and simple. With the help of the supervised attention mechanism, an important method that has been used in many tasks such as machine translation, the performance of their system outperforms the baseline significantly.",
    "weaknesses": "The attention vector is simply the summation of two attention vectors of each part. Maybe the attention vector could be calculated in a more appropriate approach. For the supervised attention mechanism, two strategies are proposed. \nBoth of them are quite straightforward. Some more complicated strategies can work better and can be tried.",
    "comments": "Although there are some places that can be improved, this paper proposed a quite effective framework, and the performance is good. The experiment is solid. It can be considered to be accepted.",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "fac12643c7576785",
    "paper_id": "ACL_2017_173",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- Weaknesses: Many grammar errors, such as the abstract - General Discussion:",
    "weaknesses": "Many grammar errors, such as the abstract",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "a875c09bd2511d76",
    "paper_id": "ACL_2017_173",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Introduces  a new document clustering approach and compares it to several established methods, showing that it improves results in most cases. \nThe analysis is very detailed and thorough--quite dense in many places and requires careful reading.\nThe presentation is organized and clear, and I am impressed by the range of comparisons and influential factors that were considered. Argument is convincing and the work should influence future approaches.",
    "weaknesses": "The paper does not provide any information on the availability of the software described.",
    "comments": "Needs some (minor) editing for English and typos--here are just a few: Line 124: regardless the size > regardless of the size Line 126: resources. Because > resources, because Line 205: consist- ing mk > consisting of mk Line 360: versionand > version and",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "0949bf76c9e2cb32",
    "paper_id": "ACL_2017_178",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper describes an extension of word embedding methods to also provide representations for phrases and concepts that correspond to words.  The method works by fixing an identifier for groups of phrases, words and the concept that all denote this concept, replace the occurrences of the phrases and words by this identifier in the training corpus, creating a \"tagged\" corpus, and then appending the tagged corpus to the original corpus for training.  The concept/phrase/word sets are taken from an ontology.  Since the domain of application is biomedical, the related corpora and ontologies are used.  The researchers also report on the generation of a new test dataset for word similarity and relatedness for real-world entities, which is novel.\nIn general, the paper is nicely written.  The technique is pretty natural, though not a very substantial contribution. The scope of the contribution is limited, because of focused evaluation within the biomedical domain.\nMore discussion of the generated test resource could be useful.  The resource could be the true interesting contribution of the paper.\nThere is one small technical problem, but that is probably just a matter of mathematical expression than implementation.\nTechnical problem: Eq. 8: The authors want to define the MAP calculation.                          This is a good idea, thought I think that a natural cut-off could be defined, rather than ranking the entire vocabulary.                          Equation 8 does not define a probability; it is quite easy to show this, even if the size of the vocabulary is infinite.  So you need to change the explanation (take out talk of a probability).\nSmall corrections: line: 556: most concepts has --> most concepts have",
    "overall_score": "2",
    "confidence": "3"
  },
  {
    "review_id": "47e9a46f2e60d0db",
    "paper_id": "ACL_2017_178",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The key question addressed by the paper is that phrases that are not lexically similar can be semantically close and, furthermore, not all phrases are compositional in nature. To this end, the paper proposes a plausible model to train phrase embeddings. The trained embeddings are shown to be competitive or better at identifying similarity between concepts.\nThe software released with the paper could be useful for biomedical NLP researchers.\n-",
    "weaknesses": "The primary weakness of the paper is that the model is not too novel. It is essentially a tweak to skip-gram.  Furthermore, the full model presented by the paper doesn't seem to be the best one in the results (in Table 4). On the two Mayo datasets, the Choi baseline is substantially better. A similar trend seems to dominate Table 6 too. On the larger UMNSRS data, the proposed model is at best competitive with previous simpler models (Chiu).",
    "comments": "The paper says that it is uses known phrases as distant supervision to train embeddings. However, it is not clear what the \"supervision\" here is. If I understand the paper correctly, every occurrence of a phrase associated with a concept provides the context to train word embeddings. But this is not supervision in the traditional sense (say for identifying the concept in the text or other such predictive tasks). So the terminology is a bit confusing.\n The notation introduced in Section 3.2 (E_W, etc) is never used in the rest of the paper.\nThe use of \\beta to control for compositionality of phrases by words is quite surprising. Essentially, this is equivalent to saying that there is a single global constant that decides \"how compositional\" any phrase should be. The surprising part here is that the actual values of \\beta chosen by cross validation from Table 3 are odd. For PM+CL and WikiNYT, it is zero, which basically argues against compositionality.  The experimental setup for table 4 needs some explanation. The paper says that the data labels similarity/relatedness of concepts (or entities). However, if the concepts-phrases mapping is really many-to-many, then how are the phrase/word vectors used to compute the similarities? It seems that we can only use the concept vectors.\nIn table 5, the approximate phr method (which approximate concepts with the average of the phrases in them) is best performing. So it is not clear why we need the concept ontology. Instead, we could have just started with a seed set of phrases to get the same results.",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "24123c381443cb99",
    "paper_id": "ACL_2017_178",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- The proposed joint embedding model is straightforward and makes reasonable sense to me. Its main value in my mind is in reaching a (configurable) middle ground between treating phrases as atomic units on one hand to considering their compositionallity on the other. The same approach is applied to concepts being ‘composed’ of several representative phrases.\n-  The paper describes a decent volume of work, including model development, an additional contribution in the form of a new evaluation dataset, and several evaluations and analyses performed.",
    "weaknesses": "- The evaluation reported in this paper includes only intrinsic tasks, mainly on similarity/relatedness datasets. As the authors note, such evaluations are known to have very limited power in predicting the utility of embeddings in extrinsic tasks. Accordingly, it has become recently much more common to include at least one or two extrinsic tasks as part of the evaluation of embedding models.\n- The similarity/relatedness evaluation datasets used in the paper are presented as datasets recording human judgements of similarity between concepts. However, if I understand correctly, the actual judgements were made based on presenting phrases to the human annotators, and therefore they should be considered as phrase similarity datasets, and analyzed as such.\n- The medical concept evaluation dataset, ‘mini MayoSRS’ is extremely small (29 pairs), and its larger superset ‘MayoSRS’ is only a little larger (101 pairs) and was reported to have a relatively low human annotator agreement. The other medical concept evaluation dataset, ‘UMNSRS’, is more reasonable in size, but is based only on concepts that can be represented as single words, and were represented as such to the human annotators. This should be mentioned in the paper and makes the relevance of this dataset questionable with respect to representations of phrases and",
    "comments": "",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "255bcc64aada1b6c",
    "paper_id": "ACL_2017_180",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper presents a new dataset with annotations of products coming from online cybercrime forums. The paper is clear and well-written and the experiments are good. Every hypothesis is tested and compared to each other.\nHowever, I do have some concerns about the paper: 1. The authors took the liberty to change the font size and the line spacing of the abstract, enabling them to have a longer abstract and to fit the content into the 8 pages requirement.\n2. I don't think this paper fits the tagging, chunking, parsing area, as it is more an information extraction problem.\n3. I have difficulties to see why some annotations such as sombody in Fig. 1 are related to a product.\n4. The basic results are very basic indeed and - with all the tools available nowadays in NLP -, I am sure that it would have been possible to have more elaborate baselines without too much extra work.\n5. Domain adaptation experiments corroborate what we already know about user-generated data where two forums on video games, e.g., may have different types of users (age, gender, etc.) leading to very different texts. So this does not give new highlights on this specific problem.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "4f8c69df5e9cd7e0",
    "paper_id": "ACL_2017_182",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- well-written - extensive experiments - good results -",
    "weaknesses": "- nothing ground-breaking, application of existing technologies - code not available - results are as could be expected",
    "comments": "- why didn't you use established audio features such as MFCCs?\n- Minor Details: - L155 and other places: a LSTM -> an LSTM - L160, L216 and other Places: why are there hyphens (-) after the text?\n- L205: explanation of convolution is not clear - Table1 should appear earlier, on page 2 already cited - L263: is 3D-CNN a standard approach in video processing? alternatives?\n- L375, 378: the ^ should probably positioned above the y - L380: \"to check overfitting\" -> did you mean \"to avoid\"?\n- L403, 408..: put names in \" \" or write them italic, to make it easier to recognize them - L420: a SVM -> an SVM - L448: Output ... are -> wrong numerus, either \"Outputs\", or use \"is\"  - L489: superflous whitespace after \"layer\" - L516, 519: \"concatenation\" should not be in a new line - L567: why don't you know the exact number of persons?\n- L626: remove comma after Since - L651: doesnt -> does not  - L777: insert \"hand, the\" after other - References: need some cleanup: L823 superflous whitespace, L831 Munich, L860 what is ACL(1)?, L888 superflous ), L894 Volume, L951 superflous new lines, L956 indent Linguistics properly",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "4e76c4ab1a34054c",
    "paper_id": "ACL_2017_18",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "-- A well-motivated approach, with a clear description and solid results.",
    "weaknesses": "-- Nothing substantial other than the comments below.",
    "comments": "The paper describes a new method called attention-over-attention for reading comprehension. First layers of the network compute a vector for each query word and document word, resulting in a |Q|xK matrix for the query and a |D|xK for the document. Since the answer is a document word, an attention mechanism is used for assigning weights to each word, depending on their interaction with query words. In this work, the authors deepen a traditional attention mechanism by computing a weight for each query word through a separate attention and then using that to weight the main attention over document words. Evaluation is properly conducted on benchmark datasets, and various insights are presented through an analysis of the results as well as a comparison to prior work. I think this is a solid piece of work on an important problem, and the method is well-motivated and clearly described, so that researchers can easily reproduce results and apply the same techniques to other similar tasks.\n- Other remarks: -- p4, Equation 12: I am assuming i is iterating over training set and p(w) is referring to P(w|D,Q) in the previous equation? Please clarify to avoid confusion.\n-- I am wondering whether you explored/discussed initializing word embeddings with existing vectors such as Google News or Glove? Is there a reason to believe the general-purpose word semantics would not be useful in this task?\n-- p6 L589-592: It is not clear what the authors are referring to when they say 'letting the model explicitly learn weights between individual attentions'? Is this referring to their own architecture, more specifically the GRU output indirectly affecting how much attention will be applied to each query and document word? Clarifying that would be useful. Also, I think the improvement on validation is not 4.1, rather 4.0 (72.2-68.2).\n-- p7 Table 5: why do you think the weight for local LM is relatively higher for the CN task while the benefit of adding it is less? Since you included the table, I think it'll be nice to provide some insights to the reader.\n-- I would have liked to see the software released as part of this submission.\n-- Typo p2 L162 right column: \"is not that effective than expected\" --> \"is not as effective as expected\"?\n-- Typo p7 L689 right column: \"appear much frequent\" --> \"appears more frequently\"?\n-- Typo p8 L719-721 left column: \"the model is hard to\" --> \"it is hard for the model to\"? & \" hard to made\" --> \"hard to make\"?",
    "overall_score": "5",
    "confidence": "4"
  },
  {
    "review_id": "854c787d7d555d13",
    "paper_id": "ACL_2017_193",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper introduces UCCA as a target representation for semantic parsing and also describes a quite successful transition-based parser for inference into that representation. I liked this paper a lot. I believe there is a lot of value simply in the introduction of UCCA (not new, but I believe relatively new to this community), which has the potential to spark new thinking about semantic representations of text. I also think the model was well thought out. \nWhile the model itself was fairly derivative of existing transition-based schemes, the extensions the authors introduced to make the model applicable in this domain were reasonable and well-explained, at what I believe to be an appropriate level of detail.\nThe empirical evaluation was pretty convincing -- the results were good, as compared to several credible baselines, and the authors demonstrated this performance in multiple domains. My biggest complaint about this paper is the lack of multilingual evaluation, especially given that the formalism being experimented with is exactly one that is supposed to be fairly universal. I'm reasonably sure multilingual UCCA corpora exist (in fact, I think the \"20k leagues\" corpus used in this paper is one such), so it would be good to see results in a language other than English.\nOne minor point: in section 6, the authors refer to their model as \"grammarless\", which strikes me as not quite correct. It's true that the UCCA representation isn't derived from linguistic notions of syntax, but it still defines a way to construct a compositional abstract symbolic representation of text, which to me, is precisely a grammar. ( This is clearly a quibble, and I don't know why it irked me enough that I feel compelled to address it, but it did.)\nEdited to add: Thanks to the authors for their response.",
    "overall_score": "5",
    "confidence": "4"
  },
  {
    "review_id": "c1f3f3285cacbec5",
    "paper_id": "ACL_2017_193",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper presents the first parser to UCCA, a recently proposed meaning representation. The parser is transition based, and uses a new transition set designed to recover challenging discontinuous structures with reentrancies. \nExperiments demonstrate that the parser works well, and that it is not easy to build these representation on top of existing parsing approaches.  This is a well written and interesting paper on an important problem. The transition system is well motivated and seems to work well for the problem. The authors also did a very thorough experimental evaluation, including both varying the classifier for the base parser (neural, linear model, etc.) and also comparing to the best output you could get from other existing, but less expressive, parsing formulations. This paper sets a strong standard to UCCA parsing, and should also be interesting to researchers working with other expressive meaning representations or complex transition systems.  My only open question is the extent to which this new parser subsumes all of the other transition based parsers for AMR, SDP, etc. Could the UCCA transition scheme be used in these cases (which heuristic alignments if necessary), and would it just learn to not use the extra transitions for non-terminals, etc. \nWould it reduce to an existing algorithm, or perhaps work better? Answering this question isn’t crucial, the paper is very strong as is, but it would add to the overall understanding and could point to interesting areas for future work.\n---- I read the author response and agree with everything they say.",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "f9eabc13e50ba72d",
    "paper_id": "ACL_2017_193",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "========================= The paper presents quite solid work, with state-of-the art transition-based techniques, and machine learning for parsing techniques.\nIt is very well written, formal and experimental aspects are described in a very precise way, and the authors demonstrate a very good knowledge of the related work, both for parsing techniques and for shallow semantic representations.\n=========================",
    "weaknesses": "========================= Maybe the weakness of the paper is that the originality lies mainly in the targeted representations (UCCA), not really in the proposed parser.\n========================= - More detailed comments and clarification questions: ========================= Introduction Lines 46-49: Note that \"discontinuous nodes\" could be linked to non-projectivity in the dependency framework. So maybe rather state that the difference is with phrase-structure syntax not dependency syntax.\nSection 2: In the UCCA scheme's description, the alternative \"a node (or unit) corresponds to a terminal or to several sub-units\" is not very clear. Do you mean something else than a node is either a terminal or a non terminal? Can't a non terminal node have one child only (and thus neither be a terminal nor have several sub-units) ?\nNote that \"movement, action or state\" is not totally appropriate, since there are processes which are neither movements nor actions (e.g. agentless transformations). \n(the UCCA guidelines use these three terms, but later state the state/process dichotomy, with processes being an \"action, movement or some other relation that evolves in time\").\nlines 178-181: Note that the contrast between \"John and Mary's trip\" and \"John and Mary's children\" is not very felicitous. The relational noun \"children\" evokes an underlying relation between two participants (the children and John+Mary), that has to be accounted for in UCCA too.\nSection 4: Concerning the conversion procedures: - While it is very complete to provide the precise description of the conversion procedure in the supplementary material, it would ease reading to describe it informally in the paper (as a variant of the constituent-to-dependency conversion procedure à la Manning, 95). Also, it would be interesting to motivate the priority order used to define the head of an edge.\n- How l(u) is chosen in case of several children with same label should be made explicit (leftmost ?).\n- In the converted graphs in figure 4, some edges seem inverted (e.g. the direction between \"John\" and \"moved\" and between \"John\" and \"gave\" should be the same).\n- Further, I am confused as to why the upper bound for remote edges in bilexical approximations is so low. The current description of the conversions do not allow to get an quick idea of which kind of remote edges cannot be handled.\nConcerning the comparison to other parsers: It does not seem completely fair to tune the proposed parser, but to use default settings for the other parsers.\nSection 5 Line 595: please better motivate the claim \"using better input encoding\" Section 6 I am not convinced by the alledged superiority of representations with non-terminal nodes. Although it can be considered more elegant not to choose a head for some constructions, it can be noted that formally co-head labels can be used in bilexical dependencies to recover the same information.",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "3552359a3003a501",
    "paper_id": "ACL_2017_19",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper introduced a novel method to improve zero pronoun resolution performance.. The main contributions of this papers are: 1) proposed a simple method to automatically generate a large training set for zero pronoun resolution task; 2) adapted a two step learning process to transfer knowledge from large data set to the specific domain data; 3) differentiate unknown words using different tags. In general, the paper is well written. Experiments are thoroughly designed.",
    "weaknesses": "But I have a few questions regarding finding the antecedent of a zero pronoun: 1. How will an antecedent be identified, when the prediction is a pronoun? The authors proposed a method by matching the head of noun phrases. It’s not clear how to handle the situation when the head word is not a pronoun. \n2. What if the prediction is a noun that could not be found in the previous contents? \n3. The system achieves great results on standard data set. I’m curious is it possible to evaluate the system in two steps? The first step is to evaluate the performance of the model prediction, i.e. to recover the dropped zero pronoun into a word; the second step is to evaluate how well the systems works on finding an antecedent.\nI’m also curious why the authors decided to use attention-based neural network. A few sentences to provide the reasons would be helpful for other researchers.\nA minor comment: In figure 2, should it be s1, s2 … instead of d1, d2 ….?",
    "comments": "Overall it is a great paper with innovative ideas and solid experiment setup.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "2a6828e899b5cdf0",
    "paper_id": "ACL_2017_19",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The approach is novel and the results are very promising, beating state-of-the-art.",
    "weaknesses": "The linguistic motivation behind the paper is troublesome (see below). I feel that the paper would benefit a lot from a more thoughtful interpretation of the results.",
    "comments": "This paper presents an approach for Zero Pronoun Resolution in Chinese. The authors advocate a novel procedure for generating large amount of relevant data from unlabeled documents. These data are then integrated smartly in an NN-based architecture at a pre-training step. The results improve on state-of-the-art.\nI have mixed feelings about this study. On the one hand, the approach seems sound and shows promising results, beating very recent systems (e.g., Chen&Ng 2016). On the other hand, the way the main contribution is framed is very disturbing from the linguistic point of view. In particular, (zero) pronoun resolution is, linguistically speaking, a context modeling task, requiring accurate interpretation of discourse/salience, semantic and syntactic clues. It starts from the assumption that (zero) pronouns are used in specific contexts, where full NPs shouldn't normally be possible. From this perspective, generating ZP data via replacing nominal with zeroes (\"blank\") doesn't sound very convincing. And indeed, as the authors themselves show, the pre-training module alone doesn't achieve a reasonable performance. To sum it up, i don't think that these generated pseudo-data can be called AZP data. It seems more likely that they encode some form of selectional preferences (?). It would be nice if the authors could invest some effort in better understanding what exactly the pre-training module learns -- and then reformulate the corresponding sections.  The paper can benefit from a proofreading by a native speaker of English -- for example, the sentence on lines 064-068 is not grammatical.\n-- other points -- lines 78-79: are there any restrictions on the nouns and especially pronouns? \nfor example, do you use this strategy for very common pronouns (as English \"it\")? if so, how do you guarantee that the two occurrences of the same token  are indeed coreferent?\nline 91: the term antecedent is typically used to denote a preceding mention coreferent with the anaphor, which is not what you mean here line 144: OntoNotes (typo) lines 487-489: it has been shown that evaluation on gold-annotated data does not provide reliable estimation of performance. and, indeed, all the recent studies of coreference evaluate on system mentions. for example, the studies of Chen&Ng you are citing, provide different types of evaluation, including those on system mentions. please consider rerunning your experiments to get a more realistic evaluation setup line 506: i don't understand what the dagger over the system's name means. is your improvement statistically significant on all the domains? including bn and tc??\nline 565: learn (typo) section 3.3: in this section you use the abbreviation AZP instead of ZP without introducing it, please unify the terminology references -- please double-check for capitalization",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "6c3e12ba2ce07576",
    "paper_id": "ACL_2017_201",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper presents a 2 x 2 x 3 x 10 array of accuracy results based on systematically changing the parameters of embeddings models: (context type, position sensitive, embedding model, task), accuracy - context type ∈ {Linear, Syntactic} - position sensitive ∈ {True, False} - embedding model ∈ {Skip Gram, BOW, GLOVE} - task ∈ {Word Similarity, Analogies, POS, NER, Chunking, 5 text classific. \ntasks} The aim of these experiments was to investigate the variation in performance as these parameters are changed. The goal of the study itself is interesting for the ACL community and similar papers have appeared before as workshop papers and have been well cited, such as Nayak et al.'s paper mentioned below.",
    "weaknesses": "Since this paper essentially presents the effect of systematically changing the context types and position sensitivity, I will focus on the execution of the investigation and the analysis of the results, which I am afraid is not  satisfactory.\nA) The lack of hyper-parameter tuning is worrisome. E.g.    - 395 Unless otherwise notes, the number of word embedding dimension is set to 500. \n   - 232 It still enlarges the context vocabulary about 5 times in practice. \n   - 385 Most hyper-parameters are the same as Levy et al' best configuration.\n  This is worrisome because lack of hyperparameter tuning makes it difficult to make statements like method A is better than method B. E.g. bound methods may perform better with a lower dimensionality than unbound models, since their effective context vocabulary size is larger.\nB) The paper sometimes presents strange explanations for its results. E.g.    - 115 \"Experimental results suggest that although it's hard to find any  universal insight, the characteristics of different contexts on different models are concluded according to specific tasks.\"\n   What does this sentence even mean?     - 580 Sequence labeling tasks tend to classify words with the same syntax  to the same category. The ignorance of syntax for word embeddings which  are learned by bound representation becomes beneficial.     These two sentences are contradictory, if a sequence labeling task    classified words with \"same syntax\" to same category then syntx becomes    a ver valuable feature. Bound representation's ignorance of syntax    should cause a drop in performance just like other tasks which does not    happen.\nC) It is not enough to merely mention Lai et. al. 2016 who have also done a    systematic study of the word embeddings, and similarly the paper     \"Evaluating Word Embeddings Using a Representative Suite of Practical    Tasks\", Nayak, Angeli, Manning. appeared at the repeval workshop at     ACL 2016. should have been cited. I understand that the focus of Nayak    et al's paper is not exactly the same as this paper, however they    provide recommendations about hyperparameter tuning and experiment    design and even provide a web interface for automatically running    tagging experiments using neural networks instead of the \"simple linear    classifiers\" used in the current paper.\nD) The paper uses a neural BOW words classifier for the text classification tasks    but a simple linear classifier for the sequence labeling tasks. What is    the justification for this choice of classifiers? Why not use a simple    neural classifier for the tagging tasks as well? I raise this point,    since the tagging task seems to be the only task where bound    representations are consistently beating the unbound representations,    which makes this task the odd one out.",
    "comments": "Finally, I will make one speculative suggestion to the authors regarding the analysis of the data. As I said earlier, this paper's main contribution is an analysis of the following table. \n(context type, position sensitive, embedding model, task, accuracy) So essentially there are 120 accuracy values that we want to explain in terms of the aspects of the model. It may be beneficial to perform factor analysis or some other pattern mining technique on this 120 sample data.",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "14cd3f86ede87479",
    "paper_id": "ACL_2017_201",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Evaluating bag of words and \"bound\" contexts from either dependencies or sentence ordering is important, and will be a useful reference to the community. The experiments were relatively thorough (though some choices could use further justification), and the authors used downstream tasks instead of just intrinsic evaluations.",
    "weaknesses": "The authors change the objective function of GBOW from p(c|\\sum w_i) to p(w|\\sum c_i). This is somewhat justified as dependency-based context with a bound representation only has one word available for predicting the context, but it's unclear exactly why that is the case and deserves more discussion. \nPresumably the non-dependency context with a bound representation would also suffer from this drawback? If so, how did Ling et al., 2015 do it? Unfortunately, the authors don't compare any results against the original objective, which is a definite weakness. In addition, the authors change GSG to match GBOW, again without comparing to the original objective. Adding results from word vectors trained using the original GBOW and GSG objective functions would justify these changes (assuming the results don't show large changes). \nThe hyperparameter settings should be discussed further. This played a large role in Levy et al. (2015), so you should consider trying different hyperparameter values. These depend pretty heavily on the task, so simply taking good values from another task may not work well.\nIn addition, the authors are unclear on exactly what model is trained in section 3.4. They say only that it is a \"simple linear classifier\". In section 3.5, they use logistic regression with the average of the word vectors as input, but call it  a Neural Bag-of-Words model. Technically previous work also used this name, but I find it misleading, since it's just logistic regression (and hence a linear model, which is not something I would call \"Neural\"). It is important to know if the model trained in section 3.4 is the same as the model trained in 3.5, so we know if the different conclusions are the result of the task or the model changing.",
    "comments": "This paper evaluates context taken from dependency parses vs context taken from word position in a given sentence, and bag-of-words vs tokens with relative position indicators. This paper is useful to the community, as they show when and where researchers should use word vectors trained using these different decisions.  - Emphasis to improve: The main takeaway from this paper that future researchers will use is given at the end of 3.4 and 3.5, but really should be summarized at the start of the paper. Specifically, the authors should put in the abstract that for POS, chunking, and NER, bound representations outperform bag-of-words representations, and that dependency contexts work better than linear contexts in most cases. In addition, for a simple text classification model, bound representations perform worse than bag-of-words representations, and there seemed to be no major difference between the different models or context types.\n- Small points of improvement:  Should call \"unbounded\" context \"bag of words\". This may lead to some confusion as one of the techniques you use is Generalized Bag-Of-Words, but this can be clarified easily. \n043: it's the \"distributional hypothesis\", not the \"Distributed Hypothesis\". \n069: citations should have a comma instead of semicolon separating them. \n074: \"DEPS\" should be capitalized consistently throughout the paper (usually it appears as \"Deps\"). Also should be introduced as something like dependency parse tree context (Deps). \n085: typo: \"How different contexts affect model's performances...\" Should have the word \"do\".",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "fce9d4f647bc7160",
    "paper_id": "ACL_2017_201",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper systematically investigated how context types (linear vs dependency-based) and representations (bound word vs unbound word) affect word embedding learning. They experimented with three models (Generalized Bag-Of-Words, Generalized Skip-Gram and Glove) in multiple different tasks (word similarity, word analogy, sequence labeling and text classification). \nOverall,  1)            It is well-written and structured. \n2)            The experiments are very thoroughly evaluated. The analysis could help researchers to choose different word embeddings or might even motivate new models. \n3)            The attached software can also benefit the community.",
    "weaknesses": "The novelty is limited.",
    "comments": "For the dependency-based context types, how does the dependency parsing affect the overall performance? Is it fair to compare those two different context types since the dependency-based one has to rely on the predicted dependency parsing results (in this case CoreNLP) while the linear one does not?",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "101fc4ed938a299a",
    "paper_id": "ACL_2017_214",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper proposed a macro discourse structure scheme. The authors carried out a pilot study annotating a corpus consisting of 97 news articles from Chinese treebank 8.0. They then built a model to recognize the primary-secondary relations and 5 discourse relations (joint, elaboration, sequence, background, cause-result) in this corpus.\nThe paper is poorly written and I have difficulties to follow it. I strongly suggest that the authors should find a native English speaker to carefully proofread the paper. Regarding the content, I have several concerns:  1 The logic of the paper is not clear and justifiable:  1) what are \"logical semantics\" and \"pragmatic function\"(line 115-116)? I'd prefer the authors to define them properly.\n2) macro discourse structure: there are some conflicts of the definition between macro structure and micro structure. Figure 4 demonstrates the combination of macro discourse structure and micro discourse structure. There, the micro discourse structure is presented *within paragraphs*. However, in the specific example of micro discourse structure shown in Figure 6, the micro-level discourse structure is *beyond the paragraph boundary* and captures the discourse relations across paragraphs. This kind of micro-level discourse structure is indeed similar to the macro structure proposed by the authors in Figure 5, and it's also genre independent. So, why can't we just use the structure in Figure 6? What's the advantage of macro discourse structure proposed in Figure 5? For me, it's genre dependent and doesn't provide richer information compared to Figure 6.\nBy the way, why sentence 6 and sentence 15 are missing in Figure 5? Is it because they are subtitles? But sentence 12 which is a subtitle is present there.\n2 Corpus construction (section 4) is not informative enough: without a detailed example, it's hard to know the meaning of \"discourse topic, lead, abstract, paragraph topics (line 627-629)\". And you were saying you \"explore the relationships between micro-structure and macro-structure\", but I can't find the correspondent part.\nTable 4 is about agreement study The authors claimed \"Its very difficult to achieve high consistence because the judgments of relation and structure are very subjective. Our measurement data is only taken on the layer of leaf nodes.\"--------> First, what are the leaf nodes? In the macro-level, they are paragraphs; in the micro-level, they are EDUs. Should we report the agreement study for macro-level and micro-level separately? Second, it seems for me that the authors only take a subset of data to measure the agreement. This doesn't reflect the overall quality of the whole corpus, i.e., high agreement on the leaf nodes annotation doesn't ensure that we will get high agreement on the non-leaf nodes annotation.\nSome other unclear parts in section 4: Table 4: \"discourse structure, discourse relation\" are not clear, what is discourse structure and what is discourse relation? \nTable 5: \"amount of macro discourse relations\", still not clear to me, you mean the discourse relations between paragraphs? But in Figure 6, these relations can exist both between sentences and between paragraphs.\n3 Experiments: since the main purpose of the paper is to provide richer discourse structure (both on macro and micro level), I would expect to see some initial results in this direction. The current experiment is not very convincing: a) no strong baselines; b) features are not clearly described and motivated; c) I don't understand why only a sub set of discourse relations from Table 6 is chosen to perform the experiment of discourse relation recognition.\nIn general, I think the paper needs major improvement and currently it is not ready for acceptance.",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "503134b0b94abd88",
    "paper_id": "ACL_2017_214",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper presents a unified annotation that combines macrostructures and RST structure in Chinese news articles. Essentially, RST structure is adopted for each paragraph and macrostructure is adopted on top of the paragraphs. \nWhile the view that nuclearity should not depend on the relation label itself but also on the context is appealing, I find the paper having major issues in the annotation and the experiments, detailed below: - The notion of “primary-secondary” relationship is advocated much in the paper, but later in the paper that it became clear this is essentially the notion of nuclearity, extended to macrostructure and making it context-dependent instead of relation-dependent. Even then, the status nuclear-nuclear, nuclear-satellite, satellite-nuclear are “redefined” as new concepts.\n- Descriptions of established theories in discourse are often incorrect. For example, there is rich existing work on pragmatic functions of text but it is claimed to be something little studied. There are errors in the related work section, e.g., treating RST and the Chinese Dependency Discourse Treebank as different as coherence and cohesion; the computational approach subsection lacking any reference to work after 2013; the performance table of nuclearity classification confusing prior work for sentence-level and document-level parsing.\n- For the annotation, I find the macro structure annotation description confusing; furthermore, statistics for the macro labels are not listed/reported. The agreement calculation is also problematic; the paper stated that \"Our measurement data is only taken on the layer of leaf nodes\". I don't think this can verify the validity of the annotation. There are multiple mentions in the annotation procedure that says “prelim experiments show this is a good approach”, but how? Finally it is unclear how the kappa values are calculated since this is a structured task; is this the same calculation as RST discourse treebank?\n- It is said in the paper that nuclearity status closely associates with the relation label itself. So what is the baseline performance that just uses the relation label? Note that some features are not explained at all (e.g., what are “hierarchical characteristics”?)\n- The main contribution of the paper is the combination of macro and micro structure. However, in the experiments only relations at the micro level are evaluated; even so, only among 5 handpicked ones. I don't see how this evaluation can be used to verify the macro side hence supporting the paper.\n- The paper contains numerous grammatical errors. Also, there is no text displayed in Figure 7 to illustrate the example.",
    "overall_score": "1",
    "confidence": "4"
  },
  {
    "review_id": "e3e260a2c568fa10",
    "paper_id": "ACL_2017_214",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The macro discourse structure is a useful complement to micro structures like RST. The release of the dataset would be helpful to a range of NLP applications.",
    "weaknesses": "1. Providing more comparisons with the existed CDTB will be better. \n2. The “primary-secondary” relationship is mentioned a lot in this paper, however, its difference with the nuclearity is unclear and not precisely defined. \n3. The experiment method is not clearly described in the paper.",
    "comments": "",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "1fe960f8047257c2",
    "paper_id": "ACL_2017_216",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "1. The idea of assigning variable-length document segments with dependent topics is novel. This prior knowledge is worth incorporated in the LDA-based framework. \n2. Whereas we do not have full knowledge on recent LDA literature, we find the part of related work quite convincing. \n3. The method proposed for segment sampling with O(M) complexity is impressive. \nIt is crucial for efficient computation.",
    "weaknesses": "1. Compared to Balikas COLING16's work, the paper has a weaker visualization (Fig 5), which makes us doubt about the actual segmenting and assigning results of document. It could be more convincing to give a longer exemplar and make color assignment consistent with topics listed in Figure 4. \n2. Since the model is more flexible than that of Balikas COLING16, it may be underfitting, could you please explain this more?",
    "comments": "The paper is well written and structured. The intuition introduced in the Abstract and again exemplified in the Introduction is quite convincing. The experiments are of a full range, solid, and achieves better quantitative results against previous works. If the visualization part is stronger, or explained why less powerful visualization, it will be more confident. Another concern is about computation efficiency, since the seminal LDA work proposed to use Variational Inference which is faster during training compared to MCMC, we wish to see the author’s future development.",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "cd0b054ec0ba5e3e",
    "paper_id": "ACL_2017_216",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- Well-written, well-organized - Incorporate topical segmentation to copula LDA to enable the joint learning of segmentation and latent models - Experimental setting is well-designed and show the superiority of the proposed method from several different indicators and datasets ###",
    "weaknesses": "- No comparison with \"novel\" segmentation methods ###",
    "comments": "- In line 105, \"latent radom topics\" -> \"latent random topics\"",
    "overall_score": "4",
    "confidence": "2"
  },
  {
    "review_id": "63e2913e817231e2",
    "paper_id": "ACL_2017_21",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper is clearly written, and the claims are well-supported.  The Related Work in particular is very thorough, and clearly establishes where the proposed work fits in the field.\nI had two main questions about the method: (1) phrases are mentioned in section 3.1, but only word representations are discussed.  How are phrase representations derived? \n(2) There is no explicit connection between M^+ and M^- in the model, but they are indirectly connected through the tanh scoring function.  How do the learned matrices compare to one another (e.g., is M^- like -1*M^+?)?  Furthermore, what would be the benefits/drawbacks of linking the two together directly, by enforcing some measure of dissimilarity?\nAdditionally, statistical significance of the observed improvements would be valuable.\nTypographical comments: - Line 220: \"word/phase pair\" should be \"word/phrase pair\" - Line 245: I propose an alternate wording: instead of \"entities are translated to,\" say \"entities are mapped to\".  At first, I read that as a translation operation in the vector space, which I think isn't exactly what's being described.\n- Line 587: \"slightly improvement in F-measure\" should be \"slight improvement in F-measure\" - Line 636: extraneous commas in citation - Line 646: \"The most case\" should be \"The most likely case\" (I'm guessing) - Line 727: extraneous period and comma in citation",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "9c7cd097159e03ea",
    "paper_id": "ACL_2017_21",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "1. Interesting research problem 2. The method in this paper looks quite formal. \n3. The authors have released their dataset with the submission. \n4. The design of experiments is good.",
    "weaknesses": "1. The advantage and disadvantage of the transductive learning has not yet discussed.",
    "comments": "In this paper, the authors introduce a transductive learning approach for Chinese hypernym prediction, which is quite interesting problem. The authors establish mappings from entities to hypernyms in the embedding space directly, which sounds also quite novel. This paper is well written and easy to follow. \nThe first part of their method, preprocessing using embeddings, is widely used method for the initial stage. But it's still a normal way to preprocess the input data. The transductive model is an optimization framework for non-linear mapping utilizing both labeled and unlabeled data. The attached supplementary notes about the method makes it more clear. The experimental results have shown the effectiveness of the proposed method in this paper. The authors also released dataset, which contributes to similar research for other researchers in future.",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "cd8df66c0142a4b0",
    "paper_id": "ACL_2017_220",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Great paper: Very well-written, interesting results, creative method, good and enlightening comparisons with earlier approaches. In addition, the corpus, which is very carefully annotated, will prove to be a valuable resource for other researchers. I appreciated the qualitative discussion in section 5. Too many ML papers just give present a results table without much further ado, but the discussion in this paper really provides insights for the reader.",
    "weaknesses": "In section 4.1, the sentence \"The rest of the model’s input is set to zeroes...\" is quite enigmatic until you look at Figure 2. Some extra sentence here explaining what is going on would be helpful. Furthermore, in Figure 2, in the input layers to the LSTMs it says \"5*Embeddings(50D)\" also for the networks taking dependency labels as input. Surely this is wrong? ( Or if it is correct, please explain what you mean).",
    "comments": "Concerning the comment in 4.2 \"LSTMs are excellent at modelling language sequences ... which is why we use this type of model.\". This comment seems strange to me. This is not a sequential problem in that sense. \nFor each datapoint, you feed the network all 5 words in an example in one go, and the next example has nothing to do with the preceding one. The LSTM architecture could still be superior, of course, but not for the reason you state. Or have I misunderstood something? I'd be interested to hear the authors' comments on this point.",
    "overall_score": "5",
    "confidence": "4"
  },
  {
    "review_id": "2cd35d9fd87c3247",
    "paper_id": "ACL_2017_222",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "When introducing the task, the authors use illustrative examples as well as the contributions of this paper. \nRelated Works section covers the state of the art, at the same time pointing similarities and differences between related Works and the proposed method. \nThe presentation of the method is very clear, since the authors separate the tagging scheme and the end-to-end model. \nAnother strong point of this work is the baselines used to compare the proposed methods with several classical triplet extraction methods. \nAt last, the presentation of examples from dataset used to illustrate the advantages and disadvantages of the methods was very important. These outputs complement the explanation of tagging and evaluation of triplets.",
    "weaknesses": "One of the main contributions of this paper is a new tagging scheme described in Section 3.1, however there are already other schemes for NER and RE being used, such as IO, BIO and BILOU. \nDid the authors perform any experiment using other tagging scheme for this method? \nRegarding the dataset, in line 14, page 5, the authors cite the number of relations (24), but they do not mention the number or the type of named entities. \nIn Section 4.1, the evaluation criteria of triplets are presented. These criteria were based on previous work? As I see it, the stage of entity identification is not complete if you consider only the head of the entity. \nRegarding example S3, shown in Table 3, the output of the LSTM-LSTM-Bias was considered correct? The text states that the relation role is wrong, although it is not clear if the relation role is considered in the evaluation.",
    "comments": "This paper proposes a novel tagging scheme and investigates the end-to-end models to jointly extract entities and relations. \nThe article is organized in a clear way and it is well written, which makes it easy to understand the proposed method.",
    "overall_score": "5",
    "confidence": "5"
  },
  {
    "review_id": "2e4134d5da8883f9",
    "paper_id": "ACL_2017_226",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "1) This paper proposed a semi-automated framework (human generation -> auto expansion -> human post-editing) to construct a compositional semantic similarity evaluation data set.\n2) The proposed framework is used to create a Polish compositional semantic similarity evaluation data set which is useful for future work in developing Polish compositional semantic models.",
    "weaknesses": "1) The proposed framework has only been tested on one language. It is not clear whether the framework is portable to other languages. For example, the proposed framework relies on a dependency parser which may not be available in some languages or in poor performance in some other languages.\n2) The number of sentence pairs edited by leader judges is not reported so the correctness and efficiency of the automatic expansion framework can not be evaluated. The fact that more than 3% (369 out of 10k) of the post-edited pairs need further post-editing is worrying.  3) There are quite a number of grammatical mistakes. Here are some examples but not the complete and exhaustive list: line 210, 212, 213: \"on a displayed image/picture\" -> \"in a displayed image/picture\" line 428: \"Similarly as in\" -> \"Similar to\" A proofread pass on the paper is needed.",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "83caf4c25f1103f5",
    "paper_id": "ACL_2017_237",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "S1) Tackles a challenging problem of unsupervised sentiment analysis.\n S2) Figure 2, in particular, is a nice visualisation.\n#",
    "weaknesses": "W1) The experiments, in particular, are very thin. I would recommend also measuring F1 performance and expanding the number of techniques compared.\n W2) The methodology description needs more organisation and elaboration. The ideas tested are itemised, but insufficiently justified.   W3) The results are quite weak in terms of the reported accuracy and depth of analysis. Perhaps this work needs more development, particularly with validating the central assumption that the Distributional Hypothesis implies that opposite words, although semantically similar, are separated well in the vector space?",
    "comments": "",
    "overall_score": "1",
    "confidence": "2"
  },
  {
    "review_id": "efb6aa99976db889",
    "paper_id": "ACL_2017_237",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper deals with the issue of finding word polarity orientation in an unsupervised manner, using word embeddings.",
    "weaknesses": "The paper presents an interesting and useful idea, however, at this moment, it is not applied to any test case. The ideas on which it is based are explained in an \"intuitive\" manner and not thoroughly justified.",
    "comments": "This is definitely interesting work. The paper would benefit from more experiments being carried out, comparison with other methods (for example, the use of the Normalized Google Distance by authors such as (Balahur and Montoyo, 2008) - http://ieeexplore.ieee.org/abstract/document/4906796/) and the application of the knowledge obtained to a real sentiment analysis scenario. At this point, the work, although promising, is in its initial phase.",
    "overall_score": "2",
    "confidence": "5"
  },
  {
    "review_id": "c9fc8a3f351f6858",
    "paper_id": "ACL_2017_239",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- The transfer learning / data efficiency motivation is an interesting one, as it directly relates to the idea of using embeddings as a simple \"semi-supervised\" approach.",
    "weaknesses": "- A good evaluation approach would be one that propagates to end tasks. \nSpecifically, if the approach gives some rank R for a set of embeddings, I would like it to follow the same rank for an end task like text classification, parsing or machine translation. However, the approach is not assessed in this way so it is difficult to trust the technique is actually more useful than what is traditionally done.\n- The discussion about injective embeddings seems completely out-of-topic and does not seem to add to the paper's understanding.\n- The experimental section is very confusing. Section 3.7 points out that the analysis results in answers to questions as \"is it worth fitting syntax specific embeddings even when supervised datset is large?\" but I fail to understand where in the evaluation the conclusion was made.\n- Still in Section 3.7, the manuscript says \"This hints, that purely unsupervised large scale pretraining might not be suitable for NLP applications\". This is a very bold assumption and I again fail to understand how this can be concluded from the proposed evaluation approach.\n- All embeddings were obtained as off-the-shelf pretrained ones so there is no control over which corpora they were trained on. This limits the validity of the evaluation shown in the paper.\n- The manuscript needs proofreading, especially in terms of citing figures in the right places (why Figure 1, which is on page 3, is only cited in page 6?).",
    "comments": "",
    "overall_score": "2",
    "confidence": "3"
  },
  {
    "review_id": "77957bfdfd8c136f",
    "paper_id": "ACL_2017_239",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper proposed an interesting and important metric for evaluating the quality of word embeddings, which is the \"data efficiency\" when it is used in other supervised tasks.\nAnother interesting point in the paper is that the authors separated out three questions: 1) whether supervised task offers more insights to evaluate embedding quality; 2) How stable is the ranking vs labeled data set size; 3) The benefit to linear vs non-linear models.\nOverall, the authors presented comprehensive experiments to answer those questions, and the results see quite interesting to know for the research community.",
    "weaknesses": "The overall result is not very useful for ML practioners in this field, because it merely confirms what has been known or suspected, i.e. it depends on the task at hand, the labeled data set size, the type of the model, etc. So, the result in this paper is not very actionable. The reviewer noted that this comprehensive analysis deepens the understanding of this topic.",
    "comments": "The paper's presentation can be improved. Specifically:  1) The order of the figures/tables in the paper should match the order they are mentioned in the papers. Right now their order seems quite random.\n2) Several typos (L250, 579, etc). Please use a spell checker.\n3) Equation 1 is not very useful, and its exposition looks strange. It can be removed, and leave just the text explanations.\n4) L164 mentions the \"Appendix\", but it is not available in the paper.\n5) Missing citation for the public skip-gram data set in L425.\n6) The claim in L591-593 is too strong. It must be explained more clearly, i.e. when it is useful and when it is not.\n7) The observation in L642-645 is very interesting and important. It will be good to follow up on this and provide concrete evidence or example from some embedding. Some visualization may help too.\n8) In L672 should provide examples of such \"specialized word embeddings\" and how they are different than the general purpose embedding.\n9) Figuer 3 is too small to read.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "26bdb4f52678ee7a",
    "paper_id": "ACL_2017_251",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper delves into the mathematical properties of the skip-gram model, explaining the reason for its success on the analogy task and for the general superiority of additive composition models. It also establishes a link between skip-gram and Sufficient Dimensionality Reduction.\nI liked the focus of this paper on explaining the properties of skip-gram, and generally found it inspiring to read. I very much appreciate the effort to understand the assumptions of the model, and the way it affects (or is affected by) the composition operations that it is used to perform. In that respect, I think it is a very worthwhile read for the community.\nMy main criticism is however that the paper is linguistically rather naive. The authors' use of 'compositionality' (as an operation that takes a set of words and returns another with the same meaning) is extremely strange. Two words can of course be composed and produce a vector that is a) far away from both; b) does not correspond to any other concept in the space; c) still has meaning (productivity wouldn't exist otherwise!) Compositionality in linguistic terms simply refers to the process of combining linguistic constituents to produce higher-level constructs. It does not assume any further constraint, apart from some vague (and debatable) notion of semantic transparency. The paper's implication (l254) that composition takes place over sets is also wrong: ordering matters hugely (e.g. 'sugar cane' is not 'cane sugar'). This is a well-known shortcoming of additive composition.  Another important aspect is that there are pragmatic factors that make humans prefer certain phrases to single words in particular contexts (and the opposite), naturally changing the underlying distribution of words in a large corpus. For instance, talking of a 'male royalty' rather than a 'king' or 'prince' usually has implications with regard to the intent of the speaker (here, perhaps highlighting a gender difference). This means that the equation in l258 (or for that matter the KL-divergence modification) does not hold, not because of noise in the data, but because of fundamental linguistic processes. \nThis point may be addressed by the section on SDR, but I am not completely sure (see my comments below).\nIn a nutshell, I think the way that the authors present composition is flawed, but the paper convinces me that this is indeed what happens in skip-gram, and I think this is an interesting contribution.  The part about Sufficient Dimensionality Reduction seems a little disconnected from the previous argument as it stands. I'm afraid I wasn't able to fully follow the argument, and I would be grateful for some clarification in the authors' response. If I understand it well, the argument is that skip-gram produces a model where a word's neighbours follow some exponential parametrisation of a categorical distribution, but it is unclear whether this actually reflects the distribution of the corpus (as opposed to what happens in, say, a pure count-based model). The fact that skip-gram performs well despite not reflecting the data is that it implements some form of SDR, which does not need to make any assumption about the underlying form of the data. But then, is it fair to say that the resulting representations are optimised for tasks where geometrical regularities are important, regardless of the actual pattern of the data? I.e. there some kind of denoising going on?\nMinor comments: - The abstract is unusually long and could, I think, be shortened.\n- para starting l71: I think it would be misconstrued to see circularity here. \nFirth observed that co-occurrence effects were correlated with similarity judgements, but those judgements are the very cognitive processes that we are trying to model with statistical methods. Co-occurrence effects and vector space word representations are in some sense 'the same thing', modelling an underlying linguistic process we do not have direct observations for. So pair-wise similarity is not there to break any circularity, it is there because it better models the kind of judgements humans known to make.\n- l296: I think 'paraphrase' would be a better word than 'synonym' here, given that we are comparing a set of words with a unique lexical item.\n- para starting l322: this is interesting, and actually, a lot of the zipfian distribution (the long tail) is fairly uniform.\n- l336: it is probably worth pointing out that the analogy relation does not hold so well in practice and requires to 'ignore' the first returned neighbour of the analogy computation (which is usually one of the observed terms).\n- para starting l343: I don't find it so intuitive to say that 'man' would be a synonym/paraphrase of anything involving 'woman'. The subtraction involved in the analogy computation is precisely not a straightforward composition operation, as it involves an implicit negation.  - A last, tiny general comment. It is usual to write p(w|c) to mean the probability of a word given a context, but in the paper 'w' is actually the context and 'c' the target word. It makes reading a little bit harder... Perhaps change the notation?\nLiterature: The claim that Arora (2016) is the only work to try and understand vector composition is a bit strong. For instance, see the work by Paperno & Baroni on explaining the success of addition as a composition method over PMI-weighted vectors: D. Paperno and M. Baroni. 2016. When the whole is less than the sum of its parts: How composition affects PMI values in distributional semantic vectors. \nComputational Linguistics 42(2): 345-350.\n*** I thank the authors for their response and hope to see this paper accepted.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "182c5955fd74c440",
    "paper_id": "ACL_2017_256",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "Review, ACL 2017, paper 256: This paper extends the line of work which models generation in dialogue as a sequence to sequence generation problem, where the past N-1 utterances (the ‘dialogue context’) are encoded into a context vector (plus potential other, hand-crafted features), which is then decoded into a response: the Nth turn in the dialogue. As it stands, such models tend to suffer from lack of diversity, specificity and local coherence in the kinds of response they tend to produce when trained over large dialogue datasets containing many topics (e.g. Cornell, Opensubtitles, Ubuntu, etc.). Rather than attempting to produce diverse responses using the decoder, e.g. through word-by-word beam search (which has been shown not to work very well, even lose crucial information about grammar and valid sequences), or via a different objective function (such as in Li et. al.’s work) the authors introduce a latent variable, z, over which a probability distribution is induced as part of the network. At prediction time, after encoding utterances 1 to k, a context z is sampled, and the decoder is greedily used to generate a response from this. The evaluation shows small improvements in BLEU scores over a vanilla seq2seq model that does not involve learning a probability distribution over contexts and sampling from this.\nThe paper is certainly impressive from a technical point of view, i.e. in the application of deep learning methods, specifically conditioned variational auto encoders, to the problem of response generation, and its attendant difficulties in training such models. Their use of Information-Retrieval techniques to get more than one reference response is also interesting.  I have some conceptual comments on the introduction and the motivations behind the work, some on the model architecture, and the evaluation which I write below in turn: Comments on the introduction and motivations….  The authors seem not fully aware of the long history of this field, and its various facets, whether from a theoretical perspective, or from an applied one.\n1. “[ the dialogue manager] typically takes a new utterance and the dialogue context as input, and generates discourse level decisions.”          This is not accurate. Traditionally at least, the job of the dialogue manager is to select actions (dialogue acts) in a particular dialogue context. \nThe                    action chosen is then passed to a separate generation module for realisation. Dialogue management is usually done in the context of task-based systems which are goal driven. The dialogue manager is to choose actions which are optimal in some sense, e.g. reach a goal (e.g. book a restaurant) in as few steps as possible. See publications from Lemon & Pietquin, 2012, Rieser, Keizer and colleagues, and various publications from Steve Young, Milica Gasic and colleagues for an overview of the large literature on Reinforcement Learning and MDP models for task-based dialogue systems.\n2. The authors need to make a clear distinction between task-based, goal-oriented dialogue, and chatbots/social bots, the latter being usually no more than a language model, albeit a sophisticated one (though see Wen et. al. 2016). What is required from these two types of system is usually distinct. \nWhereas the former is required to complete a task, the latter is, perhaps only required to keep the user engaged. Indeed the data-driven methods that have been used to build such systems are usually very different. \n3. The authors refer to ‘open-domain’ conversation. I would suggest that there is no such thing as open-domain conversation - conversation is always in the context of some activity and for doing/achieving something specific in the world. And it is this overarching goal, the overarching activity, this overarching genre, which determines the outward shape of dialogues and determines what sorts of dialogue structure are coherent. Coherence itself is activity/context-specific. Indeed a human is not capable of open-domain dialogue: if they are faced with a conversational topic or genre that they have never participated in, they would embarrass themselves with utterances that would look incoherent and out of place to others already familiar with it. \n(think of a random person on the street trying to follow the conversations at some coffee break at ACL). This is the fundamental problem I see with systems that attempt to use data from an EXTREMELY DIVERSE, open-ended set of conversational genres (e.g. movie subtitles) in order to train one model, mushing everything together so that what emerges at the other end is just very good grammatical structure. Or very generic responses.  Comments on the model architecture: Rather than generate from a single encoded context, the authors induce a distribution over possible contexts, sample from this, and generate greedily with the decoder. It seems to me that this general model is counter intuitive, and goes against evidence from the Linguistic/Psycholinguistic literature on dialogue: this literature shows that people tend to resolve potential problems in understanding and acceptance very locally - i.e. make sure they agree on what the context of the conversation is - and only then move on with the rest of the conversation, so that at any given point, there is little uncertainty about the current context of the conversation. The massive diversity one sees results from the diversity in what the conversation is actually trying to achieve (see above), diversity in topics and contexts etc, so that in a given, fixed context, there is a multitude of possible next actions, all coherent, but leading the conversation down a different path.\nIt therefore seems strange to me at least to shift the burden of explaining diversity and coherence in follow-up actions to that of the linguistic/verbal/surface contexts in which they are uttered, though of course, uncertainty here can also arise as a result of mismatches in vocabulary, grammars, concepts, people’s backgrounds etc. But this probably wouldn’t explain much of the variation in follow-up response.  In fact, at least as far as task-based Dialogue systems are concerned, the challenge is to capture synonymy of contexts, i.e. dialogues that are distinct on the surface, but lead to the same or similar context, either in virtue of interactional and syntactic equivalence relations, or synonymy relations that might hold in a particular domain between words or sequences of words (e.g. “what is your destination?” = “where would you like to go?” in a flight booking domain). See e.g. Bordes & Weston, 2016; and Kalatzis, Eshghi & Lemon, 2016 - the latter use a grammar to cluster semantically similar dialogues.\nComments on the evaluation: The authors seek to show that their model can generate more coherent, and more diverse responses. The evaluation method, though very interesting, seems to address coherence but not diversity, despite what they say in section 5.2: The precision and recall metrics measure distance between ground truth utterances and the ones the model generates, but not that between the generated utterances themselves (unless I’m misunderstanding the evaluation method). \nSee e.g. Li et al. who measure diversity by counting the number distinct n-grams in the generated responses.\nFurthermore, I’m not sure that the increase in BLEU scores are meaningful: they are very small. In the qualitative assessment of the generated responses, one certainly sees more diversity, and more contentful utterances in the examples provided. But I can’t see how frequent such cases in fact are.\nAlso, it would have made for a stronger, more meaningful paper if the authors had compared their results with other work, (e.g. Li et. al) that use very different methods to promote diversity (e.g. by using a different objective function). The authors in fact do not mention this, or characterise it properly, despite actually referring to Li et. al. 2015.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "d4c713997129b494",
    "paper_id": "ACL_2017_256",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper presents a neural sequence-to-sequence model for encoding dialog contexts followed by decoding system responses in open-domain conversations. \nThe authors introduced conditional variational autoencoder (CVAE) which is a deep neural network-based generative model to learn the latent variables for describing responses conditioning dialog contexts and dialog acts. \nThe proposed models achieved better performances than the baseline based on RNN encoder-decoder without latent variables in both quantitative and qualitative evaluations.\nThis paper is well written with clear descriptions, theoretically sound ideas, reasonable comparisons, and also detailed analysis. \nI have just a few minor comments as follows: - Would it be possible to provide statistical significance of the results from the proposed models compared to the baseline in quantitative evaluation? The differences don't seem that much for some metrics.\n- Considering the importance of dialog act in kgCVAE model, the DA tagging performances should affect the quality of the final results. Would it be there any possibility to achieve further improvement by using better DA tagger? \nRecently, deep learning models have achieved better performances than SVM also in DA tagging.\n- What do you think about doing human evaluation as a part of qualitative analysis? It could be costly, but worth a try to analyze the results in more pragmatic perspective.\n- As a future direction, it could be also interesting if kgCVAE model is applied to more task-oriented human-machine conversations which usually have much richer linguistic features available than open conversation.\n- In Table 1, 'BLUE-1 recall' needs to be corrected to 'BLEU-1 recall'.",
    "overall_score": "5",
    "confidence": "4"
  },
  {
    "review_id": "cf85b5aefe85d2d2",
    "paper_id": "ACL_2017_266",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper addresses an important aspect of sentiment analysis, namely how to appropriately induce embeddings for training supervised classifers for polarity classification. The paper is well-structured and well-written. The major claims made by the authors are sufficiently supported by their experiments.",
    "weaknesses": "The outcome of the experiments is very predictable. The methods that are employed are very simple and ad-hoc. I found hardly any new idea in that paper. Neither are there any significant lessons that the reader learns about embeddings or sentiment analysis. The main idea (i.e. focusing on more task-specific data for training more accurate embeddings) was already published in the context of named-entity recognition by Joshi et al. (2015). The additions made in this paper are very incremental in nature.\nI find some of the experiments inconclusive as (apparently) no statistical signficance testing between different classifiers has been carried out. In Tables 2, 3 and 6, various classifier configurations produce very similar scores. In such cases, only statistical signficance testing can really give a proper indication whether these difference are meaningful. For instance, in Table 3 on the left half reporting results on RT, one may wonder whether there is a significant difference between \"Wikipedia Baseline\" and any of the combinations. Furthermore, one doubts whether there is any signficant difference between the different combinations (i.e. either using \"subj-Wiki\", \"subj-Multiun\" or \"subj-Europarl\") in that table. \nThe improvement by focusing on subjective subsets is plausible in general. \nHowever, I wonder whether in real life, in particular, a situation in which resources are sparse this is very helpful. Doing a pre-selection with OpinionFinder is some pre-processing step which will not be possible in most languages other than English. There are no equivalent tools or fine-grained datasets on which such functionality could be learnt. The fact that in the experiments for Catalan, this information is not considered proves that.  Minor details: - lines 329-334: The discussion of this dataset is confusing. I thought the task is plain polarity classification but the authors here also refer to \"opinion holder\" and \"opinion targets\". If these information are not relevant to the experiments carried out in this paper, then they should not be mentioned here.\n- lines 431-437: The variation of \"splicing\" that the authors explain is not very well motivated. First, why do we need this? In how far should this be more effective than simple \"appending\"?\n- lines 521-522: How is the subjective information isolated for these configurations? I assume the authors here again employ OpinionFinder? However, there is no explicit mention of this here.\n- lines 580-588: The definitions of variables do not properly match the formula (i.e. Equation 3). I do not find n_k in Equation 3.\n- lines 689-695: Similar to lines 329-334 it is unclear what precise task is carried out. Do the authors take opinion holders and targets in consideration?\n***AFTER AUTHORS' RESPONSE*** Thank you very much for these clarifying remarks. \nI do not follow your explanations regarding the incorporation of opinion holders and targets, though.\nOverall, I will not change my scores since I think that this work lacks sufficient novelty (the things the authors raised in their response are just insufficient to me). This submission is too incremental in nature.",
    "comments": "",
    "overall_score": "2",
    "confidence": "3"
  },
  {
    "review_id": "f87f4734a17c9f1f",
    "paper_id": "ACL_2017_266",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "An interesting and comprehensive study of the effect of using special-domain corpora for training word embeddings.  Clear explanation of the assumptions, contributions, methodology, and results.  Thorough evaluation of various aspects of the proposal.",
    "weaknesses": "Some conclusions are not fully backed up by the numerical results.  E.g., the authors claim that for Catalan, the improvements of using specific corpora for training word vectors is more pronounced than English.  I am not sure why this conclusion is made based on the results.  E.g., in Table 6, none of the combination methods outperform the baseline for the 300-dimension vectors.",
    "comments": "The paper presents a set of simple, yet interesting experiments that suggest word vectors (here trained using the skip-gram method) largely benefit from the use of relevant (in-domain) and subjective corpora. \nThe paper answers important questions that are of benefit to practitioners of natural language processing.  The paper is also very well-written, and very clearly organized.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "736c53646f637754",
    "paper_id": "ACL_2017_26",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "26: An End-to-End Model for Question Answering over Knowledge Base with Cross-Attention Combining Global Knowledge This paper presents an approach for factoid question answering over a knowledge graph (Freebase), by using a neural model that attempts to learn a semantic correlation/correspondence between various \"aspects\" of the candidate answer (e.g., answer type, relation to question entity, answer semantic, etc.) and a subset of words of the question. A separate correspondence component is learned for each \"aspect\" of the candidate answers. The two key contributions of this work are: (1) the creation of separate components to capture different aspects of the candidate answer, rather than relying on a single semantic representation, and (2) incorporating global context (from the KB) of the candidate answers.\nThe most interesting aspect of this work, in my opinion, is the separation of candidate answer representation into distinct aspects, which gives us (the neural model developer) a little more control over guiding the NN models towards information that would be more beneficial in its decision making. It sort of harkens to the more traditional algorithms that rely on feature engineering. But in this case the \"feature engineering\" (i.e., aspects) is more subtle, and less onerous. I encourage the authors to continue refining this system along these lines.\nWhile the high-level idea is fairly clear to a reasonably informed reader, the devil in the details would make it hard for some audience to immediately grasp key insights from this work. Some parts of the paper could benefit from more explanation... Specifically: (1) Context aspect of candidate answers (e_c) is not clearly explained in the paper. Therefore, the last two sentences of Section 3.2.2 seem unclear.\n(2) Mention of OOV in the abstract and introduction need more explanation. As such, I think the current exposition in the paper assumes a deep understanding of prior work by the reader.\n(3) The experiments conducted in this paper restrict comparison to IR-based system -- and the reasoning behind this decision is reasonable. But it is not clear then why the work of Yang et al. (2014) -- which is described to be SP-based -- is part of the comparison. While, I am all for including more systems in the comparison, there seem to be some inconsistencies in what should and should not be compared. Additionally, I see not harm in also mentioning the comparable performance numbers for the best SP-based systems.\nI observe in the paper that the embeddings are learned entirely from the training data. I wonder how much impact the random initialization of these embeddings has on the end performance. It would be interesting to determine (and list) the variance if any. Additionally, if we were to start with pre-trained embeddings (e.g., from word2vec) instead of the randomly initialized ones, would that have any impact?\nAs I read the paper, one possible direction of future work that occurred to me was to possibly include structured queries (from SP-based methods) as part of the cross-attention mechanism. In other words, in addition to using the various aspects of the candidate answers as features, one could include structured queries that generate the produce that candidate answer as an additional aspect of the candidate answer. An attention mechanism could then also focus on various parts of the structured query, and its (semantic) matches to the input question as an additional signal for the NN model. Just a thought.\nSome notes regarding the positioning of the paper: I hesitate to call the model proposed here \"attention\" models, because (per my admittedly limited understanding) attention mechanisms apply to \"encoder-decoder\" situations, where semantics expressed in one structured form (e.g., image, sentence in one language, natural language question, etc.) are encoded into an abstract representation, and then generated into another structured form (e.g., caption, sentence in another language, structured query, etc.). The attention mechanism allows the \"encoder\" to jump around and attend to different parts of the input (instead of sequentially) as the output is being generated by the decoder. This paper does not appear to fit this notion, and may be confusing to a broader audience.\n------ Thank you for clarifications in the author response.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "c129d7338fa83337",
    "paper_id": "ACL_2017_26",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper contributes to the field of knowledge base-based question answering (KB-QA), which is to tackle the problem of retrieving results from a structured KB based on a natural language question. KB-QA is an important and challenging task.\nThe authors clearly identify the contributions and the novelty of their work, provide a good overview of the previous work and performance comparison of their approach to the related methods.\nPrevious approaches to NN-based KB-QA represent questions and answers as fixed length vectors, merely as a bag of words, which limits the expressiveness of the models. And previous work also don’t leverage unsupervised training over KG, which potentially can help a trained model to generalize. \nThis paper makes two major innovative points on the Question Answering problem.\n1) The backbone of the architecture of the proposed approach is a cross-attention based neural network, where attention is used for capture different parts of questions and answer aspects. The cross-attention model contains two parts, benefiting each other. The A-Q attention part tries to dynamically capture different aspects of the question, thus leading to different embedding representations of the question. And the Q-A attention part also offer different attention weight of the question towards the answer aspects when computing their Q-A similarity score. \n2) Answer embeddings are not only learnt on the QA task but also modeled using TransE which allows to integrate more prior knowledge on the KB side. \nExperimental results are obtained on Web questions and the proposed approach exhibits better behavior than state-of-the-art end-to-end methods. The two contributions were made particularly clear by ablation experiment. Both the cross-attention mechanism and global information improve QA performance by large margins.\nThe paper contains a lot of contents. The proposed framework is quite impressive and novel compared with the previous works.",
    "weaknesses": "The paper is well-structured, the language is clear and correct. Some minor typos are provided below. \n1. Page 5, column 1, line 421:                                       re-read                      reread 2. Page 5, column 2, line 454: pairs be    pairs to be",
    "comments": "In Equation 2: the four aspects of candidate answer aspects share the same W and b. How about using separate W and b for each aspect? \nI would suggest considering giving a name to your approach instead of \"our approach\", something like ANN or CA-LSTM…(yet something different from Table 2).   In general, I think it is a good idea to capture the different aspects for question answer similarity, and cross-attention based NN model is a novel solution for the above task. The experimental results also demonstrate the effectiveness of the authors’ approach. Although the overall performance is weaker than SP-based methods or some other integrated systems, I think this paper is a good attempt in end-to-end KB-QA area and should be encouraged.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "d0132f4647bef69c",
    "paper_id": "ACL_2017_270",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper presents a purpose-built neural network architecture for textual entailment/NLI based on a three step process of encoding, attention-based matching, and aggregation. The model has two variants, one based on TreeRNNs and the other based on sequential BiLSTMs. The sequential model outperforms all published results, and an ensemble with the tree model does better still.\nThe paper is clear, the model is well motivated, and the results are impressive. Everything in the paper is solidly incremental, but I nonetheless recommend acceptance.  Major issues that I'd like discussed in the response: – You suggest several times that your system can serve as a new baseline for future work on NLI. This isn't an especially helpful or meaningful claim—it could be said of just about any model for any task. You could argue that your model is unusually simple or elegant, but I don't think that's really a major selling point of the model. \n– Your model architecture is symmetric in some ways that seem like overkill—you compute attention across sentences in both directions, and run a separate inference composition (aggregation) network for each direction. This presumably nearly doubles the run time of your model. Is this really necessary for the very asymmetric task of NLI? Have you done ablation studies on this?** \n– You present results for the full sequential model (ESIM) and the ensemble of that model and the tree-based model (HIM). Why don't you present results for the tree-based model on its own?**\nMinor issues: – I don't think the Barker and Jacobson quote means quite what you want it to mean. In context, it's making a specific and not-settled point about *direct* compositionality in formal grammar. You'd probably be better off with a more general claim about the widely accepted principle of compositionality. \n– The vector difference feature that you use (which has also appeared in prior work) is a bit odd, since it gives the model redundant parameters. Any model that takes vectors a, b, and (a - b) as input to some matrix multiplication is exactly equivalent to some other model that takes in just a and b and has a different matrix parameter. There may be learning-related reasons why using this feature still makes sense, but it's worth commenting on. \n– How do you implement the tree-structured components of your model? Are there major issues with speed or scalability there? \n\u001f– Typo: (Klein and D. Manning, 2003)  – Figure 3: Standard tree-drawing packages like (tikz-)qtree produce much more readable parse trees without crossing lines. I'd suggest using them.\n--- Thanks for the response! I still solidly support publication. This work is not groundbreaking, but it's novel in places, and the results are surprising enough to bring some value to the conference.",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "af62a9483b853d1e",
    "paper_id": "ACL_2017_270",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This approach outperforms several strong models previously proposed for the task. The authors have tried a large number of experiments, and clearly report the ones that did not work, and the hyperparameter settings of the ones that did. This paper serves as a useful empirical study for a popular problem.",
    "weaknesses": "Unfortunately, there are not many new ideas in this work that seem useful beyond the scope the particular dataset used. While the authors claim that the proposed network architecture is simpler than many previous models, it is worth noting that the model complexity (in terms of the number of parameters) is fairly high. Due to this reason, it would help to see if the empirical gains extend to other datasets as well. In terms of ablation studies, it would help to see 1) how well the tree-variant of the model does on its own and 2) the effect of removing inference composition from the model.\nOther minor issues: 1) The method used to enhance local inference (equations 14 and 15) seem very similar to the heuristic matching function used by Mou et al., 2015 (Natural Language Inference by Tree-Based Convolution and Heuristic Matching). You may want to cite them.\n2) The first sentence in section 3.2 is an unsupported claim. This either needs a citation, or needs to be stated as a hypothesis.\nWhile the work is not very novel, the the empirical study is rigorous for the most part, and could be useful for researchers working on similar problems. \nGiven these strengths, I am changing my recommendation score to 3. I have read the authors' responses.",
    "comments": "",
    "overall_score": "3",
    "confidence": "5"
  },
  {
    "review_id": "29f2e106080d11ca",
    "paper_id": "ACL_2017_276",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The contribution is quite well-written and easy to follow for the most part. \nThe model is exposed in sufficient detail, and the experiments are thorough within the defined framework. The benefits of introducing an auxiliary objective are nicely exposed.",
    "weaknesses": "The paper shows very limited awareness of the related work, which is extensive across the tasks that the experiments highlight. Tables 1-3 only show the three systems proposed by the contribution (Baseline, +dropout, and +LMcost), while some very limited comparisons are sketched textually.\nA contribution claiming novelty and advancements over the previous state of the art should document these improvements properly: at least by reporting the relevant scores together with the novel ones, and ideally through replication. \nThe datasets used in the experiments are all freely available, the previous results well-documented, and the previous systems are for the most part publicly available.\nIn my view, for a long paper, it is a big flaw not to treat the previous work more carefully.\nIn that sense, I find this sentence particularly troublesome: \"The baseline results are comparable to the previous best results on each of these benchmarks.\" The reader is here led to believe that the baseline system somehow subsumes all the previous contributions, which is shady on first read, and factually incorrect after a quick lookup in related work.\nThe paper states \"new state-of-the-art results for error detection on both FCE and CoNLL-14 datasets\". Looking into the CoNLL 2014 shared task report, it is not straightforward to discern whether the latter part of the claim does holds true, also as per Rei and Yannakoudakis' (2016) paper. The paper should support the claim by inclusion/replication of the related work.",
    "comments": "The POS tagging is left as more of an afterthought. The comparison to Plank et al. (2016) is at least partly unfair as they test across multiple languages in the Universal Dependencies realm, showing top-level performance across language families, which I for one believe to be far more relevant than WSJ benchmarking. How does the proposed system scale up/down to multiple languages, low-resource languages with limited training data, etc.? The paper leaves a lot to ask for in that dimension to further substantiate its claims.\nI like the idea of including language modeling as an auxiliary task. I like the architecture, and sections 1-4 in general. In my view, there is a big gap between those sections and the ones describing the experiments (5-8).\nI suggest that this nice idea should be further fleshed out before publication. \nThe rework should include at least a more fair treatment of related work, if not replication, and at least a reflection on multilinguality. The data and the systems are all there, as signs of the field's growing maturity. The paper should in my view partake in reflecting this maturity, and not step away from it. In faith that these improvements can be implemented before the publication deadline, I vote borderline.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "f99ca7aae6d6160a",
    "paper_id": "ACL_2017_276",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The article is well written; what was done is clear and straightforward. Given how simple the contribution is, the gains are substantial, at least in the error correction task.",
    "weaknesses": "The novelty is fairly limited (essentially, another permutation of tasks in multitask learning), and only one way of combining the tasks is explored. E.g., it would have been interesting to see if pre-training is significantly worse than joint training; one could initialize the weights from an existing RNN LM trained on unlabeled data; etc.",
    "comments": "I was hesitating between a 3 and a 4. While the experiments are quite reasonable and the combinations of tasks sometimes new, there's quite a bit of work on multitask learning in RNNs (much of it already cited), so it's hard to get excited about this work. I nevertheless recommend acceptance because the experimental results may be useful to others.\n- Post-rebuttal: I've read the rebuttal and it didn't change my opinion of the paper.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "242672a9a2cdba5c",
    "paper_id": "ACL_2017_288",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper analyzes the story endings (last sentence of a 5-sentence story) in the corpus built for the story cloze task (Mostafazadeh et al. 2016), and proposes a model based on character and word n-grams to classify story endings. \nThe paper also shows better performance on the story cloze task proper (distinguishing between \"right\" and \"wrong\" endings) than prior work.\nWhereas style analysis is an interesting area and you show better results than prior work on the story cloze task, there are several issues with the paper. \nFirst, how do you define \"style\"? Also, the paper needs to be restructured (for instance, your section \"Results\" actually mixes some results and new experiments) and clarified (see below for questions/comments): right now, it is quite difficult for the reader to follow what data is used for the different experiments, and what data the discussion refers to.\n(1) More details about the data used is necessary in order to assess the claim that \"subtle writing task [...] imposes different styles on the author\" (lines 729-732). How many stories are you looking at, written by how many different persons? And how many stories are there per person? From your description of the post-analysis of coherence, only pairs of stories written by the same person in which one was judged as \"coherent\" and the other one as \"neutral\" are chosen. Can you confirm that this is the case? So perhaps your claim is justified for your \"Experiment 1\". However my understanding is that in experiment 2 where you compare \"original\" vs. \"right\" or \"original\" vs. \"wrong\", we do not have the same writers. So I am not convinced lines 370-373 are correct.\n(2) A lot in the paper is simply stated without any justifications. For instance how are the \"five frequent\" POS and words chosen? Are they the most frequent words/POS? ( Also theses tables are puzzling: why two bars in the legend for each category?). Why character *4*-grams? Did you tune that on the development set? If these were not the most frequent features, but some that you chose among frequent POS and words, you need to justify this choice and especially link the choice to \"style\". How are these features reflecting \"style\"?\n(3) I don't understand how the section \"Design of NLP tasks\" connects to the rest of the paper, and to your results. But perhaps this is because I am lost in what \"training\" and \"test\" sets refer to here.\n(4) It is difficult to understand how your model differs from previous work. \nHow do we reconcile lines 217-219 (\"These results suggest that real understanding of text is required in order to solve the task\") with your approach?\n(5) The terminology of \"right\" and \"wrong\" endings is coming from Mostafazadeh et al., but this is a very bad choice of terms. What exactly does a \"right\" or \"wrong\" ending mean (\"right\" as in \"coherent\" or \"right\" as in \"morally good\")? \nI took a quick look, but couldn't find the exact prompts given to the Turkers. \nI think this needs to be clarified: as it is, the first paragraph of your section \"Story cloze task\" (lines 159-177) is not understandable.\nOther questions/comments: Table 1. Why does the \"original\" story differ from the coherent and incoherent one? From your description of the corpus, it seems that one Turker saw the first 4 sentences of the original story and was then ask to write one sentence ending the story in a \"right\" way (or did they ask to provide a \"coherent\" ending?) and one sentence ending the story in a \"wrong\" way (or did they ask to provide an \"incoherent\" ending)? I don't find the last sentence of the \"incoherent\" story that incoherent... If the only shoes that Kathy finds great are $300, I can see how Kathy doesn't like buying shoes ;-) This led me to wonder how many Turkers judged the coherence of the story/ending and how variable the judgements were. What criterion was used to judge a story coherent or incoherent? Also does one Turker judge the coherence of both the \"right\" and \"wrong\" endings, making it a relative judgement? Or was this an absolute judgement? This would have huge implications on the ratings.\nLines 380-383: What does \"We randomly sample 5 original sets\" mean?\nLine 398: \"Virtually all sentences\"? Can you quantify this?\nTable 5: Could we see the weights of the features?  Line 614: \"compared to ending an existing task\": the Turkers are not ending a \"task\" Line 684-686: \"made sure each pair of endings was written by the same author\" -> this is true for the \"right\"/\"wrong\" pairs, but not for the \"original\"-\"new\" pairs, according to your description.\nLine 694: \"shorter text spans\": text about what? This is unclear.\nLines 873-875: where is this published?",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "16859382c05865e8",
    "paper_id": "ACL_2017_288",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper has a promising topic (different writing styles in finishing a story) that could appeal to Discourse and Pragmatics area participants.",
    "weaknesses": "The paper suffers from a convincing and thorough discussion on writing style and implications of the experiments on discourse or pragmatics. \n(1) For example, regarding \"style\", the authors could have sought answers to the following questions: what is the implication of starting an incoherent end-of-story sentence with a proper noun (l. 582)? Is this a sign of topic shift? What is the implication of ending a story coherently with a past tense verb, etc. \n(2) It is not clear to me why studies on deceptive language are similar to short or long answers in the current study. I would have liked to see a more complete comparison here. \n(3) The use of terms such as \"cognitive load\" (l. 134) and \"mental states\" (l. 671) appears somewhat vague. \n(4) There is insufficient discussion on the use of coordinators (line 275 onwards); the paper would benefit from a more thorough discussion of this issue (e.g. what is the role of coordinators in these short stories and in discourse in general? Does the use of coordinators differ in terms of the genre of the story? How about the use of \"no\" coordinators?)   (5) The authors do not seem to make it sufficiently clear who the target readers of this research would be (e.g. language teachers? Crowd-sourcing experiment designers? etc.)  The paper needs revision in terms of organization (there are repetitions throughout the text).  Also, the abbreviations in Table 5 and 6 are not clear to me.",
    "comments": "All in all, the paper would have to be revised particularly in terms of its theoretical standpoint and implications to discourse and pragmatics.\n===== In their response to the reviewers' comments, the authors indicate their willingness to update the paper and clarify the issues related to what they have experimented with. However, I would have liked to see a stronger commitment to incorporating the implications of this study to the Discourse and Pragmatics area.",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "ed555a19d3b14006",
    "paper_id": "ACL_2017_288",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "*The paper is very well written *It shows how stylometric analysis can help in reasoning-like text classification *The results have important implications for design on NLP datasets *The results may have important implications for many text classification tasks",
    "weaknesses": "*I see few weaknesses in this paper. The only true one is the absence of a definition of style, which is a key concept in the paper",
    "comments": "This paper describes two experiments that explore the relationship between writing task and writing style. In particular, controlling for vocabulary and topic, the authors show that features used in authorship attribution/style analysis can go a long way towards distinguishing between 1) a natural ending of a story 2) an ending added by a different author and 3) a purposefully incoherent ending added by a different author.\nThis is a great and fun paper to read and it definitely merits being accepted. \nThe paper is lucidly written and clearly explains what was done and why. The authors use well-known simple features and a simple classifier to prove a non-obvious hypothesis. Intuitively, it is obvious that a writing task greatly constraints style. However, proven in such a clear manner, in such a controlled setting, the findings are impressive.\nI particularly like Section 8 and the discussion about the implications on design of NLP tasks. I think this will be an influential and very well cited paper. Great work.   The paper is a very good one as is. One minor suggestion I have is defining what the authors mean by “style” early on. The authors seem to mean “a set of low-level easily computable lexical and syntactic features”.  As is, the usage is somewhat misleading for anyone outside of computational stylometrics.  The set of chosen stylistic features makes sense. However, were there no other options? Were other features tried and they did not work? I think a short discussion of the choice of features would be informative.",
    "overall_score": "5",
    "confidence": "4"
  },
  {
    "review_id": "7b2bbebe32a6e73e",
    "paper_id": "ACL_2017_318",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This work showed that word representation learning can benefit from sememes when used in an appropriate attention scheme. Authors hypothesized that sememes can act as an essential regularizer for WRL and WSI tasks and proposed SE-WL model which detects word senses and learn representations simultaneously. \nThough experimental results indicate that WRL benefits, exact gains for WSI are unclear since a qualitative case study of a couple of examples has only been done. Overall, paper is well-written and well-structured.\nIn the last paragraph of introduction section, authors tried to tell three contributions of this work. ( 1) and (2) are more of novelties of the work rather than contributions. I see the main contribution of the work to be the results which show that we can learn better word representations (unsure about WSI) by modeling sememe information than other competitive baselines. ( 3) is neither a contribution nor a novelty.\nThe three strategies tried for SE-WRL modeling makes sense and can be intuitively ranked in terms of how well they will work. Authors did a good job explaining that and experimental results supported the intuition but the reviewer also sees MST as a fourth strategy rather than a baseline inspired by Chen et al. 2014 (many WSI systems assume one sense per word given a context). \nMST many times performed better than SSA and SAC. Unless authors missed to clarify otherwise, MST seems to be exactly like SAT with a difference that target word is represented by the most probable sense rather than taking an attention weighted average over all its senses. MST is still an attention based scheme where sense with maximum attention weight is chosen though it has not been clearly mentioned if target word is represented by chosen sense embedding or some function of it.\nAuthors did not explain the selection of datasets for training and evaluation tasks. Reference page to Sogou-T text corpus did not help as reviewer does not know Chinese language. It was unclear which exact dataset was used as there are several datasets mentioned on that page. Why two word similarity datasets were used and how they are different  (like does one has more rare words than another) since different models performed differently on these datasets. The choice of these datasets did not allow evaluating against results of other works which makes the reviewer wonder about next question.\nAre proposed SAT model results state of the art for Chinese word similarity? \nE.g. Schnabel et al. (2015) report a score of 0.640 on WordSim-353 data by using CBOW word embeddings.\nReviewer needs clarification on some model parameters like vocabulary sizes for words (Does Sogou-T contains 2.7 billion unique words) and word senses (how many word types from HowNet). Because of the notation used it is not clear if embeddings for senses and sememes for different words were shared. Reviewer hopes that is the case but then why 200 dimensional embeddings were used for only 1889 sememes. It would be better if complexity of model parameters can also be discussed.\nMay be due to lack of space but experiment results discussion lack insight into observations other than SAT performing the best. Also, authors claimed that words with lower frequency were learned better with sememes without evaluating on a rare words dataset.\nI have read author's response.",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "dc2295191c355393",
    "paper_id": "ACL_2017_318",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper proposes the use of HowNet to enrich embedings. The idea is interesting and gives good results.",
    "weaknesses": "The paper is interesting, but I am not sure the contibution is important enough for a long paper. Also, the comparision with other works may not be fair: authors should compare to other systems that use manually developed resources.\nThe paper is understandable, but it would help some improvement on the English.",
    "comments": "",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "cc75fce0c6e8f132",
    "paper_id": "ACL_2017_318",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "1. The proposed models are shown to lead to rather substantial and consistent improvements over reasonable baselines on two different tasks (word similarity and word analogy), which not only serves to demonstrate the effectiveness of the models but also highlights the potential utility of incorporating sememe information from available knowledge resources for improving word representation learning. \n2. The paper contributes to ongoing efforts in the community to account for polysemy in word representation learning. It builds nicely on previous work and proposes some new ideas and improvements that could be of interest to the community, such as applying an attention scheme to incorporate a form of soft word sense disambiguation into the learning procedure.",
    "weaknesses": "1. Presentation and clarity: important details with respect to the proposed models are left out or poorly described (more details below). Otherwise, the paper generally reads fairly well; however, the manuscript would need to be improved if accepted. \n2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).",
    "comments": "1. The authors stress the importance of accounting for polysemy and learning sense-specific representations. While polysemy is taken into account by calculating sense distributions for words in particular contexts in the learning procedure, the evaluation tasks are entirely context-independent, which means that, ultimately, there is only one vector per word -- or at least this is what is evaluated. Instead, word sense disambiguation and sememe information are used for improving the learning of word representations. This needs to be clarified in the paper. \n2. It is not clear how the sememe embeddings are learned and the description of the SSA model seems to assume the pre-existence of sememe embeddings. This is important for understanding the subsequent models. Do the SAC and SAT models require pre-training of sememe embeddings? \n3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work. \n4. A reasonable argument is made that the proposed models are particularly useful for learning representations for low-frequency words (by mapping words to a smaller set of sememes that are shared by sets of words). Unfortunately, no empirical evidence is provided to test the hypothesis. It would have been interesting for the authors to look deeper into this. This aspect also does not seem to explain the improvements much since, e.g., the word similarity data sets contain frequent word pairs. \n5. Related to the above point, the improvement gains seem more attributable to the incorporation of sememe information than word sense disambiguation in the learning procedure. As mentioned earlier, the evaluation involves only the use of context-independent word representations. Even if the method allows for learning sememe- and sense-specific representations, they would have to be aggregated to carry out the evaluation task. \n6. The example illustrating HowNet (Figure 1) is not entirely clear, especially the modifiers of \"computer\". \n7. It says that the models are trained using their best parameters. How exactly are these determined? It is also unclear how K is set -- is it optimized for each model or is it randomly chosen for each target word observation? Finally, what is the motivation for setting K' to 2?",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "0785427aec7d7c3a",
    "paper_id": "ACL_2017_31",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "Update after author response:  1. My major concern about the optimization of model's hyperparameter (which are numerous) has not been addressed. This is very important, considering that you report results from folded cross-validation.  2. The explanation that benefits of their method are experimentally confirmed with 2% difference -- while evaluating via 5-fold CV on 200 examples -- is quite unconvincing.\n======================================================================== Summary: In this paper authors present a complex neural model for detecting factuality of event mentions in text. The authors combine the following in their complex model:                          (1) a set of traditional classifiers for detecting event mentions, factuality sources, and source introducing predicates (SIPs), (2) A bidirectional attention-based LSTM model that learns latent representations for elements on different dependency paths used as input, (2) A CNN that uses representations from the LSTM and performs two output predictions (one to detect specific from underspecified cases and another to predict the actual factuality class).  From the methodological point of view, the authors are combining a reasonably familiar methods (att-BiLSTM and CNN) into a fairly complex model. However, this model does not take raw text (sequence of word embeddings) as input, but rather hand-crafted features (e.g., different dependency paths combining factuality concepts, e.g., sources, SIPs, and clues). The usage of hand-crafted features is somewhat surprising if coupled with complex deep model. The evaluation seems a bit tainted as the authors report the results from folded cross-validation but do not report how they optimized the hyperparameters of the model. Finally, the results are not too convincing -- considering the complexity of the model and the amount of preprocessing required (extraction of event mentions, SIPs, and clues), a 2% macro-average gain over the rule-based baseline and overall 44% performance seems modest, at best (looking at Micro-average, the proposed model doesn't outperform simple MaxEnt classifier).\nThe paper is generally well-written and fairly easy to understand. Altogether, I find this paper to be informative to an extent, but in it's current form not a great read for a top-tier conference.    Remarks: 1. You keep mentioning that the LSTM and CNN in your model are combined \"properly\" -- what does that actually mean? How does this \"properness\" manifest? What would be the improper way to combine the models?\n2. I find the motivation/justification for the two output design rather weak:      - the first argument that it allows for later addition of cues (i.e manually-designed features) kind of beats the \"learning representations\" advantage of using deep models. \n        - the second argument about this design tackling the imbalance in the training set is kind of hand-wavy as there is no experimental support for this claim.  3. You first motivate the usage of your complex DL architecture with learning latent representations and avoiding manual design and feature computation.  And then you define a set of manually designed features (several dependency paths and lexical features) as input for the model. Do you notice the discrepancy?  4. The LSTMs (bidirectional, and also with attention) have by now already become a standard model for various NLP tasks. Thus I find the detailed description of the attention-based bidirectional LSTM unnecessary. \n5. What you present as a baseline in Section 3 is also part of your model (as it generates input to your model). Thus, I think that calling it a baseline undermines the understandability of the paper.  6. The results reported originate from a 5-fold CV. However, the model contains numerous hyperparameters that need to be optimized (e.g., number of filters and filter sizes for CNNs). How do you optimize these values? Reporting results from a folded cross-validation doesn't allow for a fair optimization of the hypeparameters: either you're not optimizing the model's hyperparameters at all, or you're optimizing their values on the test set (which is unfair).  7. \" Notice that some values are non-application (NA) grammatically, e.g., PRu, PSu, U+/-\" -- why is underspecification in ony one dimension (polarity or certainty) not an option? I can easily think of a case where it is clear the event is negative, but it is not specified whether the absence of an event is certain, probable, or possible.  Language & style: 1. \" to a great degree\" -> \"great degree\" is an unusual construct, use either \"great extent\" or \"large degree\" 2. \" events that can not\" -> \"cannot\" or \"do not\" 3. \" describes out networks...in details shown in Figure 3.\" - > \"...shown in Figure 3 in details.\"",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "0864802cc7a42f92",
    "paper_id": "ACL_2017_31",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper is very clear in its presentation of a sophisticated model for factuality classification and of its evaluation.  It shows that the use of attentional features and BiLSTM clearly provide benefit over alternative pooling strategies, and that the model also exceeds the performance of a more traditional feature-based log-linear model.  Given the small amount of training data in FactBank, this kind of highly-engineered model seems appropriate. It is interesting to see that the BiLSTM/CNN model is able to provide benefit despite little training data.",
    "weaknesses": "My main concerns with this work regard its (a) apparent departure from the evaluation procedure in the prior literature; (b) failure to present prior work as a strong baseline; and (c) novelty.\nWhile I feel that the work is original in engineering deep neural nets for the factuality classification task, and that such work is valuable, its approach is not particularly novel, and \"the proposal of a two-step supervised framework\" (line 087) is not particularly interesting given that FactBank was always described in terms of two facets (assuming I am correct to interpret \"two-step\" as referring to these facets, which I may not be).\nThe work cites Saurí and Pustejovsky (2012), but presents their much earlier (2008) and weaker system as a baseline; nor does it consider Qian et al.'s (IALP 2015) work which compares to the former.              Both these works are developed on the TimeBank portion of FactBank and evaluated on a held-out ACQUAINT TimeBank section, while the present work does not report results on a held-out set.\nde Marneffe et al.'s (2012) system is also chosen as a baseline, but not all their features are implemented, nor is the present system evaluated on their PragBank corpus (or other alternative representations of factuality proposed in Prabhakaran et al. (*SEM 2015) and Lee et al. (EMNLP 2015)).  The evaluation is therefore somewhat lacking in comparability to prior work.\nThere were also important questions left unanswered in evaluation, such as the effect of using gold standard events or SIPs.\nGiven the famed success of BiLSTMs with little feature engineering, it is somewhat disappointing that this work does not attempt to consider a more minimal system employing deep neural nets on this task with, for instance, only the dependency path from a candidate event to its SIP plus a bag of modifiers to that path. The inclusion of heterogeneous information in one BiLSTM was an interesting feature, which deserved more experimentation: what if the order of inputs were permuted? what if delimiters were used in concatenating the dependency paths in RS instead of the strange second \"nsubj\" in the RS chain of line 456? What if each of SIP_path, RS_path, Cue_path were input to a separate LSTM and combined? The attentional features were evaluated together for the CNN and BiLSTM components, but it might be worth reporting whether it was beneficial for each of these components. Could you benefit from providing path information for the aux words? Could you benefit from character-level embeddings to account for morphology's impact on factuality via tense/aspect? \nProposed future work is lacking in specificity seeing as there are many questions raised by this model and a number of related tasks to consider applying it to.",
    "comments": "194: Into what classes are you classifying events?\n280: Please state which are parameters of the model.\n321: What do you mean by \"properly\"? You use the same term in 092 and it's not clear which work you consider improper nor why.\n353: Is \"the chain form\" defined anywhere? Citation? The repetition of nsubj in the example of line 456 seems an unusual feature for the LSTM to learn.\n356: It may be worth footnoting here that each cue is classified separately.\n359: \"distance\" -> \"surface distance\" 514: How many SIPs? Cues? Perhaps add to Table 3.\nTable 2. Would be good if augmented by the counts for embedded and author events. Percentages can be removed if necessary.\n532: Why 5-fold? Given the small amount of training data, surely 10-fold would be more useful and not substantially increase training costs.\n594: It's not clear that this benefit comes from PSen, nor that the increase is significant or substantial.  Does it affect overall results substantially?\n674: Is this significance across all metrics?\n683: Is the drop of F1 due to precision, recall or both?\n686: Not clear what this sentence is trying to say.\nTable 4: From the corpus sizes, it seems you should only report 2 significant figures for most columns (except CT+, Uu and Micro-A).\n711: It seems unsurprising that RS_path is insufficient given that the task is with respect to a SIP and other inputs do not encode that information. It would be more interesting to see performance of SIP_path alone.\n761: This claim is not precise, to my understanding. de Marneffe et al (2012) evaluates on PragBank, not FactBank.\nMinor issues in English usage: 112: \"non-application\" -> \"not applicable\" 145: I think you mean \"relevant\" -> \"relative\" 154: \"can be displayed by a simple source\" is unclear 166: Not sure what you mean by \"basline\". Do you mean \"pipeline\"?",
    "overall_score": "3",
    "confidence": "5"
  },
  {
    "review_id": "8e9f72d0d23955ec",
    "paper_id": "ACL_2017_31",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "] - The structure of the paper is (not perfectly but) well organized.\n- The empirical results show convincing (statistically significant) performance gains of the proposed model over strong baseline.\n[",
    "weaknesses": "] See below for details of the following weaknesses: - Novelties of the paper are relatively unclear.\n- No detailed error analysis is provided.\n- A feature comparison with prior work is shallow, missing two relevant papers.\n- The paper has several obscure descriptions, including typos.\n[",
    "comments": "is that the paper has several obscure descriptions, including typos, as shown below: - The explanations for features in Section 3.2 are somewhat intertwined and thus confusing.  The section would be more coherently organized with more separate paragraphs dedicated to each of lexical features and sentence-level features, by:   - (1) stating that the SIP feature comprises two features (i.e., lexical-level and sentence-level) and introduce their corresponding variables (l and c) *at the beginning*;   - (2) moving the description of embeddings of the lexical feature in line 280-283 to the first paragraph; and   - (3) presenting the last paragraph about relevant source identification in a separate subsection because it is not about SIP detection.\n- The title of Section 3 ('Baseline') is misleading.  A more understandable title would be 'Basic Factor Extraction' or 'Basic Feature Extraction', because the section is about how to extract basic factors (features), not about a baseline end-to-end system for event factuality identification.\n- The presented neural network architectures would be more convincing if it describes how beneficial the attention mechanism is to the task.\n- Table 2 seems to show factuality statistics only for all sources.  The table would be more informative along with Table 4 if it also shows factuality statistics for 'Author' and 'Embed'.\n- Table 4 would be more effective if the highest system performance with respect to each combination of the source and the factuality value is shown in boldface.\n- Section 4.1 says, \"Aux_Words can describe the *syntactic* structures of sentences,\" whereas section 5.4 says, \"they (auxiliary words) can reflect the *pragmatic* structures of sentences.\"  These two claims do not consort with each other well, and neither of them seems adequate to summarize how useful the dependency relations 'aux' and 'mark' are for the task.\n- S7 seems to be another example to support the effectiveness of auxiliary words, but the explanation for S7 is thin, as compared to the one for S6.  What is the auxiliary word for 'ensure' in S7?\n- Line 162: 'event go in S1' should be 'event go in S2'.\n- Line 315: 'in details' should be 'in detail'.\n- Line 719: 'in Section 4' should be 'in Section 4.1' to make it more specific.\n- Line 771: 'recent researches' should be 'recent research' or 'recent studies'.  'Research' is an uncountable noun.\n- Line 903: 'Factbank' should be 'FactBank'.",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "7318a1153650ca4d",
    "paper_id": "ACL_2017_323",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper introduces an extension of the entity grid model. A convolutional neural network is used to learn sequences of entity transitions indicating coherence, permitting better generalisation over longer sequences of entities than the direct estimates of transition probabilities in the original model.\nThis is a nice and well-written paper. Instead of proposing a fully neural approach, the authors build on existing work and just use a neural network to overcome specific issues in one step. This is a valid approach, but it would be useful to expand the comparison to the existing neural coherence model of Li and Hovy. The authors admit being surprised by the very low score the Li and Hovy model achieves on their task. This makes the reader wonder if there was an error in the experimental setup, if the other model's low performance is corpus-dependent and, if so, what results the model proposed in this paper would achieve on a corpus or task where the other model is more successful. A deeper investigation of these factors would strengthen the argument considerably.\nIn general the paper is very fluent and readable, but in many places definite articles are missing (e.g. on lines 92, 132, 174, 488, 490, 547, 674, 764 and probably more). I would suggest proofreading the paper specifically with article usage in mind. The expression \"...limits the model to do X...\", which is used repeatedly, sounds a bit unusual. Maybe \"limits the model's capacity to do X\" or \"stops the model from doing X\" would be clearer.\n-------------- Final recommendation adjusted to 4 after considering the author response. I agree that objective difficulties running other people's software shouldn't be held against the present authors. The efforts made to test the Li and Hovy system, and the problems encountered in doing so, should be documented in the paper. I would also suggest that the authors try to reproduce the results of Li and Hovy on their original data sets as a sanity check (unless they have already done so), just to see if that works for them.",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "9b8a6fbf29942574",
    "paper_id": "ACL_2017_323",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper proposes a convolutional neural network approach to model the coherence of texts. The model is based on the well-known entity grid representation for coherence, but puts a CNN on top of it.  The approach is well motivated and described, I especially appreciate the clear discussion of the intuitions behind certain design decisions (e.g. why CNN and the section titled 'Why it works').\nThere is an extensive evaluation on several tasks, which shows that the proposed approach beats previous methods. It is however strange that one previous result could not be reproduced: the results on Li/Hovy (2014) suggest an implementation or modelling error that should be addressed.\nStill, the model is a relatively simple 'neuralization' of the entity grid model. I didn't understand why 100-dimensional vectors are necessary to represent a four-dimensional grid entry (or a few more in the case of the extended grid). How does this help? I can see that optimizing directly for coherence ranking would help learn a better model, but the difference of transition chains for up to k=3 sentences vs. k=6 might not make such a big difference, especially since many WSJ articles may be very short.\nThe writing seemed a bit lengthy, the paper repeats certain parts in several places, for example the introduction to entity grids. In particular, section 2 also presents related work, thus the first 2/3 of section 6 are a repetition and should be deleted (or worked into section 2 where necessary). The rest of section 6 should probably be added in section 2 under a subsection (then rename section 2 as related work).\nOverall this seems like a solid implementation of applying a neural network model to entity-grid-based coherence. But considering the proposed consolidation of the previous work, I would expect a bit more from a full paper, such as innovations in the representations (other features?) or tasks.\nminor points: - this paper may benefit from proof-reading by a native speaker: there are articles missing in many places, e.g. '_the_ WSJ corpus' (2x), '_the_ Brown ... toolkit' (2x), etc.\n- p.1 bottom left column: 'Figure 2' -> 'Figure 1' - p.1 Firstly/Secondly -> First, Second - p.1 'limits the model to' -> 'prevents the model from considering ...' ?\n- Consider removing the 'standard' final paragraph in section 1, since it is not necessary to follow such a short paper.",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "bf6cd5cc7ad9671c",
    "paper_id": "ACL_2017_326",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The authors use established neural network methods (adversarial networks -- Goodfellow et al, NIPS-2014) to take advantage of 8 different Chinese work breaking test sets, with 8 different notions of what counts as a word in Chinese.\nThis paper could have implications for many NLP tasks where we have slightly different notions of what counts as correct.  We have been thinking of that problem in terms of adaptation, but it is possible that Goodfellow et al is a more useful way of thinking about this problem.",
    "weaknesses": "We need a name for the problem mentioned above.  How about: the elusive gold standard.  I prefer that term to multi-criteria.\nThe motivation seems to be unnecessarily narrow.  The elusive gold standard comes up in all sorts of applications, not just Chinese Word Segmentation.\nThe motivation makes unnecessary assumptions about how much the reader knows about Chinese.              When you don't know much about something, you think it is easier than it is.  Many non-Chinese readers (like this reviewer) think that Chinese is simpler than it is.              It is easy to assume that Chinese Word Segmentation is about as easy as tokenizing English text into strings delimited by white space.  But my guess is that IAA (inter-annotator agreement) is pretty low in Chinese.  The point you are trying to make in Table 1 is that there is considerable room for disagreement among native speakers of Chinese.\nI think it would help if you could point out that there are many NLP tasks where there is considerable room for disagreement.  Some tasks like machine translation, information retrieval and web search have so much room for disagreement that the metrics for those tasks have been designed to allow for multiple correct answers.  For other tasks, like part of speech tagging, we tend to sweep the elusive gold standard problem under a rug, and hope it will just go away.  But in fact, progress on tagging has stalled because we don't know how to distinguish differences of opinions from errors.  When two annotators return two different answers, it is a difference of opinion.  But when a machine returns a different answer, the machine is almost always wrong.\nThis reader got stuck on the term: adversary.  I think the NIPS paper used that because it was modeling noise under \"murphy's law.\"  It is often wise to assume the worst.\nBut I don't think it is helpful to think of differences of opinion as an adversarial game like chess.  In chess, it makes sense to think that your opponent is out to get you, but I'm not sure that's the most helpful way to think about differences of opinion.\nI think it would clarify what you are doing to say that you are applying an established method from NIPS (that uses the term \"adversarial\") to deal with the elusive gold standard problem.  And then point out that the elusive gold standard problem is a very common problem.  You will study it in the context of a particular problem in Chinese, but the problem is much more general than that.",
    "comments": "I found much of the paper unnecessarily hard going.  I'm not up on Chinese or the latest in NIPS, which doesn't help.  But even so, there are some small issues with English, and some larger problems with exposition.\nConsider Table 4.  Line 525 makes an assertion about the first block and depth of networks.  Specifically, which lines in Table 4 support that assertion.\nI assume that P and R refer to precision and recall, but where is that explained.  I assume that F is the standard F measure, and OOV is out-of-vocabulary, but again, I shouldn't have to assume such things.\nThere are many numbers in Table 4.  What counts as significance?  Which numbers are even comparable?  Can we compare numbers across cols?  Is performance on one collection comparable to performance on another?  Line 560 suggests that the adversarial method is not significant.  What should I take away from Table 4?  Line 794 claims that you have a significant solution to what I call the elusive gold standard problem.              But which numbers in Table 4 justify that claim?\nSmall quibbles about English: works --> work (in many places).  Work is a  mass noun, not a count noun (unlike \"conclusion\").              One can say one conclusion, two conclusions, but more/less/some work (not one work, two works).\nline 493: each dataset, not each datasets line 485: Three datasets use traditional Chinese (AS, CITY, CKIP) and the other five use simplified Chinese.\nline 509: random --> randomize",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "bbe421471d058780",
    "paper_id": "ACL_2017_326",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "1. Multi-criteria learning is interesting and promising. \n2. The proposed model is also interesting and achieves a large improvement from baselines.",
    "weaknesses": "1. The proposed method is not compared with other CWS models. The baseline model (Bi-LSTM) is proposed in [1] and [2]. However, these model is proposed not for CWS but for POS tagging and NE tagging. The description \"In this paper, we employ the state-of-the-art architecture ...\" (in Section 2) is misleading. \n2. The purpose of experiments in Section 6.4 is unclear. In Sec. 6.4, the purpose is that investigating \"datasets in traditional Chinese and simplified Chinese could help each other.\" However, in the experimental setting, the model is separately trained on simplified Chinese and traditional Chinese, and the shared parameters are fixed after training on simplified Chinese. What is expected to fixed shared parameters?",
    "comments": "The paper should be more interesting if there are more detailed discussion about the datasets that adversarial multi-criteria learning does not boost the performance.\n[1] Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional lstm-crf models for sequence tagging. arXiv preprint arXiv:1508.01991. \n[2] Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional lstm-cnns-crf. arXiv preprint arXiv:1603.01354 .",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "c8b05795ef1ddc6b",
    "paper_id": "ACL_2017_331",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Detailed guidelines and explicit illustrations.",
    "weaknesses": "The document-independent crowdsourcing annotation is unreliable.",
    "comments": "This work creates a new benchmark corpus for concept-map-based MDS. It is well organized and written clearly. The supplement materials are sufficient. I have two questions here. \n1)              Is it necessary to treat concept map extraction as a separate task? \nOn the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable. \n2)              How can you determine the importance of a concept independent of the documents? The definition of summarization is to reserve the main concepts of documents. Therefore, the importance of a concept highly depends on the documents. For example, in the given topic of coal mining accidents, assume there are two concepts: A) an instance of coal mining accidents and B) a cause of coal mining accidents. Then, if the document describes a series of coal mining accidents, A is more important than B. In comparison, if the document explores why coal mining accidents happen, B is more significant than A. Therefore, just given the topic and two concepts A&B, it is impossible to judge their relative importance.\nI appreciate the great effort spent by authors to build this dataset. However, this dataset is more like a knowledge graph based on common sense rather than summary.",
    "overall_score": "3",
    "confidence": "5"
  },
  {
    "review_id": "3941de5d2b14a0f2",
    "paper_id": "ACL_2017_331",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper presents an approach to creating concept maps using crowdsourcing. \nThe general ideas are interesting and the main contribution lies in the collection of the dataset. As such, I imagine that the dataset will be a valuable resource for further research in this field. Clearly a lot of effort has gone into this work.",
    "weaknesses": "Overall I felt this paper a bit overstated in placed. As an example, the authors claim a new crowdsourcing scheme as one of their contributions. This claims is quite strong though and it reads more like the authors are applying best practice in crowdsourcing to their work. This isn’t a novel methods then, it’s rather a well thought and sound application of existing knowledge.\nSimilarly, the authors claim that they develop and present a new corpus. This seems true and I can see how a lot of effort was invested in its preparation, but then Section 4.1 reveals that actually this is based on an existing dataset.  This is more a criticism of the presentation than the work though.",
    "comments": "",
    "overall_score": "3",
    "confidence": "2"
  },
  {
    "review_id": "ae658011474d2530",
    "paper_id": "ACL_2017_333",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The authors propose a selective encoding model as extension to the sequence-to-sequence framework for abstractive sentence summarization. The paper is very well written and the methods are clearly described. The proposed methods are evaluated on standard benchmarks and comparison to other state-of-the-art tools are presented, including significance scores.",
    "weaknesses": "There are some few details on the implementation and on the systems to which the authors compared their work that need to be better explained.",
    "comments": "- Major review: - I wonder if the summaries obtained using the proposed methods are indeed abstractive. I understand that the target vocabulary is build out of the words which appear in the summaries in the training data. But given the example shown in Figure 4, I have the impression that the summaries are rather extractive. \nThe authors should choose a better example for Figure 4 and give some statistics on the number of words in the output sentences which were not present in the input sentences for all test sets.\n- page 2, lines 266-272: I understand the mathematical difference between the vector hi and s, but I still have the feeling that there is a great overlap between them. Both \"represent the meaning\". Are both indeed necessary? Did you trying using only one of them.\n- Which neural network library did the authors use for implementing the system? \nThere is no details on the implementation.\n- page 5, section 44: Which training data was used for each of the systems that the authors compare to? Diy you train any of them yourselves?\n- Minor review: - page 1, line 44: Although the difference between abstractive and extractive summarization is described in section 2, this could be moved to the introduction section. At this point, some users might no be familiar with this concept.\n- page 1, lines 93-96: please provide a reference for this passage: \"This approach achieves huge success in tasks like neural machine translation, where alignment between all parts of the input and output are required.\"\n- page 2, section 1, last paragraph: The contribution of the work is clear but I think the authors should emphasize that such a selective encoding model has never been proposed before (is this true?). Further, the related work section should be moved to before the methods section.\n- Figure 1 vs. Table 1: the authors show two examples for abstractive summarization but I think that just one of them is enough. Further, one is called a figure while the other a table.\n- Section 3.2, lines 230-234 and 234-235: please provide references for the following two passages: \"In the sequence-to-sequence machine translation (MT) model, the encoder and decoder are responsible for encoding input sentence information and decoding the sentence representation to generate an output sentence\"; \"Some previous works apply this framework to summarization generation tasks.\"\n- Figure 2: What is \"MLP\"? It seems not to be described in the paper.\n- page 3, lines 289-290: the sigmoid function and the element-wise multiplication are not defined for the formulas in section 3.1.\n- page 4, first column: many elements of the formulas are not defined: b (equation 11), W (equation 12, 15, 17) and U (equation 12, 15), V (equation 15).\n- page 4, line 326: the readout state rt is not depicted in Figure 2 (workflow).\n- Table 2: what does \"#(ref)\" mean?\n- Section 4.3, model parameters and training. Explain how you achieved the values to the many parameters: word embedding size, GRU hidden states, alpha, beta 1 and 2, epsilon, beam size.\n- Page 5, line 450: remove \"the\" word in this line? \" SGD as our optimizing algorithms\" instead of \"SGD as our the optimizing algorithms.\"\n- Page 5, beam search: please include a reference for beam search.\n- Figure 4: Is there a typo in the true sentence? \" council of europe again slams french prison conditions\" (again or against?)\n- typo \"supper script\" -> \"superscript\" (4 times)",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "301195cd29a362f2",
    "paper_id": "ACL_2017_333",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper is very clear and well-written. It proposes a novel approach to abstractive sentence summarization; basically sentence compression that is not constrained to having the words in the output be present in the input.  - Excellent comparison with many baseline systems.  - Very thorough related work.",
    "weaknesses": "The criticisms are very minor: - It would be best to report ROUGE F-Score for all three datasets. The reasons for reporting recall on one are understandable (the summaries are all the same length), but in that case you could simply report both recall and F-Score.  - The Related Work should come earlier in the paper.  - The paper could use some discussion of the context of the work, e.g. how the summaries / compressions are intended to be used, or why they are needed.",
    "comments": "- ROUGE is fine for this paper, but ultimately you would want human evaluations of these compressions, e.g. on readability and coherence metrics, or an extrinsic evaluation.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "18720294016f8ded",
    "paper_id": "ACL_2017_333",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper presents a new neural approach for summarization. They build on a standard encoder-decoder with attention framework but add a network that gates every encoded hidden state based on summary vectors from initial encoding stages. Overall, the method seems to outperform standard seq2seq methods by 1-2 points on three different evaluation sets.\nOverall, the technical sections of the paper are reasonably clear. Equation 16 needs more explanation, I could not understand the notation. The specific contribution,  the selective mechanism, seems novel and could potentially be used in other contexts.  The evaluation is extensive and does demonstrate consistent improvement. One would imagine that adding an additional encoder layer instead of the selective layer is the most reasonable baseline (given the GRU baseline uses only one bi-GRU, this adds expressivity), and this seems to be implemented Luong-NMT. My one concern is LSTM/GRU mismatch. Is the benefit coming from just GRU switch?  The quality of the writing, especially in the intro/abstract/related work is quite bad. This paper does not make a large departure from previous work, and therefore a related work nearby the introduction seems more appropriate. In related work, one common good approach is highlighting similarities and differences between your work and previous work, in words before they are presented in equations. Simply listing works without relating them to your work is not that useful. Placement of the related work near the intro will allow you to relieve the intro of significant background detail and instead focus on more high level.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "673930d9f7da80b1",
    "paper_id": "ACL_2017_335",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This work describes a gated attention-based recurrent neural network method for reading comprehension and question answering. This method employs a self-matching attention technique to counterbalance the limited context knowledge of gated attention-based recurrent neural networks when processing passages. Finally, authors use pointer networks  with signals from the question attention-based vector to predict the beginning and ending of the answer. \nExperimental results with the SQuAD dataset offer state-of-the-art performance compared with several recent approaches.  The paper is well-written, structured and explained. As far as I know, the mathematics look also good. In my opinion, this is a very interesting work which may be useful for the question answering community.\nI was wondering if the authors have plans to release the code of this approach. \nFrom that perspective, I miss a bit of information about the technology used for the implementation (theano, CUDA, CuDNN...), which may be useful for readers.\nI would appreciate if authors could perform a test of statistical significance of the results. That would highlight even more the quality of your results.\nFinally, I know that the space may be a constraint, but an evaluation including some additional dataset would validate more your work.",
    "overall_score": "4",
    "confidence": "2"
  },
  {
    "review_id": "793815387121a6cf",
    "paper_id": "ACL_2017_335",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper clearly breaks the network into three component for descriptive purposes, relates each of them to prior work and mentions its novelties with respect to them. It does a sound empirical analysis by describing the impact of each component by doing an ablation study. This is appreciated.\nThe results are impressive!",
    "weaknesses": "The paper describes the results on a single model and an ensemble model. I could not find any details of the ensemble and how was it created. I believe it might be the ensemble of the character based and word based model. Can the authors please describe this in the rebuttal and the paper.",
    "comments": "Along with the ablation study, it would be nice if we can have a qualitative analysis describing some example cases where the components of gating, character embedding, self embedding, etc. become crucial ... where a simple model doesn't get the question right but adding one or more of these components helps. This can go in some form of appendix or supplementary.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "bc7ef4d4a1f54bbb",
    "paper_id": "ACL_2017_338",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- The related work is quite thorough and the comparison with the approach presented in this paper makes the hypothesis of the paper stronger. The evaluation section is also extensive and thus, the experiments are convincing.",
    "weaknesses": "- In Section 3 it is not clear what is exactly the dataset that you used for training the SVM and your own model. Furthermore, you only give the starting date for collecting the testing data, but there is no other information related to the size of the dataset or the time frame when the data was collected. This might also give some insight for the results and statistics given in Section 3.2. \n     - In Table 3 we can see that the number of reviewers is only slightly lower than the number of reviews posted (at least for hotels), which means that only a few reviewers posted more than one review, in the labeled dataset. How does this compare with the full dataset in Table 2? What is the exact number of reviewers in Table 2 (to know what is the percentage of labeled reviewers)? It is also interesting to know how many reviews are made by one person on average. \nIf there are only a few reviewers that post more than one review (i.e., not that much info to learn from), the results would benefit from a thorough discussion.",
    "comments": "This paper focuses on identifying spam reviews under the assumption that we deal with a cold-start problem, i.e., we do not have enough information to draw a conclusion. The paper proposes a neural network model that learns how to represent new reviews by jointly using embedded textual information and behaviour information. Overall, the paper is very well written and the results are compelling.\n- Typos and/or grammar:                                       - The new reviewer only provide us                                              - Jindal and Liu (2008) make the first step -> the work is quite old, you could use past tense to refer to it      - Usage of short form “can’t”, “couldn’t”, “what’s” instead of the prefered long form      - The following sentence is not clear and should be rephrased: “The new reviewer just posted one review and we have to filter it out immediately, there is not any historical reviews provided to us.“",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "dc094f6a17573512",
    "paper_id": "ACL_2017_338",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper is well organized and clearly written. The idea of jointly encoding texts and behaviors is interesting. The cold-start problem is actually an urgent problem to several online review analysis applications. In my knowledge, the previous work has not yet attempted to tackle this problem. This paper is meaningful and presents a reasonable analysis. And the results of the proposed model can also be available for downstream detection models.",
    "weaknesses": "In experiments, the author set the window width of the filters in the CNN module to 2. Did the author try other window widths, for example width `1' to extract unigram features, `3' to trigram, or use them together? \nThe authors may add more details about the previous work in the related work section. More specifically description would help the readers to understand the task clearly.\nThere are also some typos to be corrected: Sec 1: ``...making purchase decision...'' should be ``making a/the purchase decision'' Sec 1: ``...are devoted to explore... '' should be `` are devoted to exploring'' Sec 1: ``...there is on sufficient behaviors...'' should be “there are no sufficient behaviors'' Sec 1: ``...on business trip...'' should be ``on a business trip'' Sec 1: ``...there are abundant behavior information...'' should be ``there is abundant behavior'' Sec 3: ``The new reviewer only provide us...'' should be ``...The new reviewer only provides us...'' Sec 3: ``...features need not to take much...'' should be ``...features need not take much...'' Sec 4: ``...there is not any historical reviews...'' should be ``...there are not any historical reviews...'' Sec 4: ``...utilizing a embedding learning model...'' should be ``...utilizing an embedding learning model...'' Sec 5.2 ``...The experiment results proves...'' should be ``...The experiment results prove...''",
    "comments": "It is a good paper and should be accepted by ACL.",
    "overall_score": "5",
    "confidence": "5"
  },
  {
    "review_id": "16c2d45af5bd9401",
    "paper_id": "ACL_2017_33",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- Innovative idea: sentiment through regularization - Experiments appear to be done well from a technical point of view - Useful in-depth analysis of the model",
    "weaknesses": "- Very close to distant supervision - Mostly poorly informed baselines",
    "comments": "",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "ffd1a2e08cddcfff",
    "paper_id": "ACL_2017_33",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper proposes a nice way to combine the neural model (LSTM) with linguistic knowledge (sentiment lexicon, negation and intensity). The method is simple yet effective. It achieves the state-of-the-art performance on Movie Review dataset and is competitive against the best models on SST dataset.",
    "weaknesses": "Similar idea has also been used in (Teng et al., 2016). Though this work is  more elegant in the framework design and mathematical representation, the experimental comparison with (Teng et al., 2016) is not as convincing as the comparisons with the rest methods. The authors only reported the re-implementation results on the sentence level experiment of SST and did not report their own phrase-level results.\nSome details are not well explained, see discussions below.",
    "comments": "The reviewer has the following questions/suggestions about this work, 1. Since the SST dataset has phrase-level annotations, it is better to show the statistics of the times that negation or intensity words actually take effect. \nFor example, how many times the word \"nothing\" appears and how many times it changes the polarity of the context.\n2. In section 4.5, the bi-LSTM is used for the regularizers. Is bi-LSTM used to predict the sentiment label?\n3. The authors claimed that \"we only use the sentence-level annotation since one of our goals is to avoid expensive phrase-level annotation\". However, the reviewer still suggest to add the results. Please report them in the rebuttal phase if possible.\n4. \" s_c is a parameter to be optimized but could also be set fixed with prior knowledge.\"  The reviewer didn't find the specific definition of s_c in the experiment section, is it learned or set fixed?  What is the learned or fixed value?\n5. In section 5.4 and 5.5, it is suggested to conduct an additional experiment with part of the SST dataset where only phrases with negation/intensity words are included. Report the results on this sub-dataset with and without the corresponding regularizer can be more convincing.",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "5c93eeee4ad4ae73",
    "paper_id": "ACL_2017_343",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "i. Well organized and easy to understand ii. Provides detailed comparisons under various experimental settings and shows the state-of-the-art performances",
    "weaknesses": "i. In experiments, this paper compares previous supervised approaches, but the proposed method is the semi-supervised approach even if the training data is enough to train.",
    "comments": "This paper adopts a pre-training approach to improve Chinese word segmentation. \nBased on the transition-based neural word segmentation, this paper aims to pre-train incoming characters with external resources (punctuation, soft segmentation, POS, and heterogeneous training data) through multi-task learning. That is, this paper casts each external source as an auxiliary classification task. The experimental results show that the proposed method achieves the state-of-the-art performances in six out of seven datasets.   This paper is well-written and easy to understand. A number of experiments prove the effectiveness of the proposed method. However, there exist an issue in this paper. The proposed method is a semi-supervised learning that uses external resources to pre-train the characters. Furthermore, this paper uses another heterogeneous training datasets even if it uses the datasets only for pre-training. Nevertheless, the baselines in the experiments are based on supervised learning. In general, the performance of semi-supervised learning is better than that of supervised learning because semi-supervised learning makes use of plentiful auxiliary information. In the experiments, this paper should have compared the proposed method with semi-supervised approaches.\nPOST AUTHOR RESPONSE What the reviewer concerned is that this paper used additional “gold-labeled” dataset to pretrain the character embeddings. Some baselines in the experiments used label information, where the labels are predicted automatically by their base models as the authors pointed out. When insisting superiority of a method, all circumstances should be same. Thus, even if the gold dataset isn’t used to train the segmentation model directly, it seems to me that it is an unfair comparison because the proposed method used another “gold” dataset to train the character embeddings.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "f3b8eec6e81e94b8",
    "paper_id": "ACL_2017_350",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Improves over the state-of-the-art. Method might be applicable for other domains.",
    "weaknesses": "Not much novelty in method.  Not quite clear if data set is general enough for other domains.",
    "comments": "This paper describes a rule-based method for generating additional weakly labeled data for event extraction.  The method has three main stages.  First, it uses Freebase to find important slot fillers for matching sentences in Wikipedia (using all slot fillers is too stringent resulting in too few matches).  Next, it uses FrameNet to to improve reliability of labeling trigger verbs and to find nominal triggers.  Lastly, it uses a multi-instance learning to deal with the noisily generated training data.\nWhat I like about this paper is that it improves over the state-of-the-art on a non-trival benchmark.  The rules involved don't seem too obfuscated, so I think it might be useful for the practitioner who is interested to improve IE systems for other domains.  On the other hand, some some manual effort is still needed, for example for mapping Freebase event types to ACE event types (as written in Section 5.3 line 578).  This also makes it difficult for future work to calibrate apple-to-apple against this paper.              Apart from this, the method also doesn't seem too novel.\nOther comments: - I'm also concern with the generalizability of this method to other   domains.  Section 2 line 262 says that 21 event types are selected   from Freebase.  How are they selected?  What is the coverage on the 33 event types in the ACE data.\n- The paper is generally well-written although I have some   suggestions for improvement.              Section 3.1 line 316 uses \"arguments liked time, location...\".  If you mean roles or arguments, or maybe you want to use actual realizations of time and location as examples.  There are minor typos, for e.g. line 357 is missing a \"that\", but this is not a major concern I have for this paper.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "26502f3dc750cfbb",
    "paper_id": "ACL_2017_352",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper introduces new configurations and training objectives for neural sequence models in a multi-task setting. As the authors describe well, the multi-task setting is important because some tasks have shared information and in some scenarios learning many tasks can improve overall performance.\nThe methods section is relatively clear and logical, and I like where it ended up, though it could be slightly better organized. The organization that I realized after reading is that there are two problems: 1) shared features end up in the private feature space, and 2) private features end up in the  shared space. There is one novel method for each problem. That organization up front would make the methods more cohesive. In any case, they introduce one  method that keeps task-specific features out of shared representation (adversarial loss) and another to keep shared features out of task-specific representations (orthogonality constraints). My only point of confusion is the adversarial system. \nAfter LSTM output there is another layer, D(s^k_T, \\theta_D), relying on parameters U and b. This output is considered a probability distribution which is compared against the actual. This means it is possible it will just learn U and b that effectively mask task-specific information from  the LSTM outputs, and doesn't  seem like it can guarantee task-specific information is removed.\nBefore I read the evaluation section I wrote down what I hoped the experiments would look like and it did most of it. This is an interesting idea and there are  a lot more experiments one can imagine but I think here they have the basics to show the validity of their methods. It would be helpful to have best known results on these tasks.\nMy primary concern with this paper is the lack of deeper motivation for the  approach. I think it is easy to understand that in a totally shared model there will be problems due to conflicts in feature space. The extension to  partially shared features seems like a reaction to that issue -- one would  expect that the useful shared information is in the shared latent space and  each task-specific space would learn features for that space. Maybe this works and maybe it doesn't, but the logic is clear to me. In contrast, the authors seem to start from the assumption that this \"shared-private\" model has this issue. I expected the argument flow to be 1) Fully-shared obviously has this problem; 2) shared-private seems to address this; 3) in practice shared-private does not fully address this issue for reasons a,b,c.; 4) we introduce a method that more effectively constrains the spaces. \nTable 4 helped me to partially understand what's going wrong with shared-private and what your methods do; some terms are _usually_ one connotation or another, and that general trend can probably get them into the shared feature space. This simple explanation, an example, and a more logical argument flow would help the introduction and make this a really nice reading paper.\nFinally, I think this research ties into some other uncited MTL work [1], which does deep hierarchical MTL - supervised POS tagging at a lower level, chunking at the next level up, ccg tagging higher, etc. They then discuss at the end some of the qualities that make MTL possible and conclude that MTL only works \"when tasks are sufficiently similar.\" The ASP-MTL paper made me think of this previous work because potentially this model could learn what sufficiently similar is -- i.e., if two tasks are not sufficiently similar the shared model would learn nothing and it would fall back to learning two independent systems, as compared to a shared-private model baseline that might overfit and perform poorly.\n[1] @inproceedings{sogaard2016deep,   title={Deep multi-task learning with low level tasks supervised at lower layers},   author={S{\\o}gaard, Anders and Goldberg, Yoav},   booktitle={Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics},   volume={2},   pages={231--235},   year={2016},   organization={Association for Computational Linguistics} }",
    "overall_score": "4",
    "confidence": "2"
  },
  {
    "review_id": "39664d3a6ee69ed5",
    "paper_id": "ACL_2017_352",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- This is a nice set of ideas working well together. I particularly like the focus on explicitly trying to create useful shared representations. These have been quite successful in the CV community, but it appears that one needs to work quite hard to create them for NLP.\n- Sections 2, 3 and 4 are very clearly expressed.\n- The task-level cross-validation in Section 5.5 is a good way to evaluate the transfer.\n- There is an implementation and data.\n#",
    "weaknesses": "- There are a few minor typographic and phrasing errors. Individually, these are fine, but there are enough of them to warrant fixing: ** l:84 the “infantile cart” is slightly odd -- was this a real example from the data? \n** l:233 “are different in” -> “differ in” ** l:341 “working adversarially towards” -> “working against” or “competing with”? \n** l:434 “two matrics” -> “two matrices” ** l:445 “are hyperparameter” -> “are hyperparameters” ** Section 6 has a number of number agreement errors (l:745/746/765/766/767/770/784) and should be closely re-edited. \n** The shading on the final row of Tables 2 and 3 prints strangely… - There is mention of unlabelled data in Table 1 and semi-supervised learning in Section 4.2, but I didn’t see any results on these experiments. Were they omitted, or have I misunderstood?\n- The error rate differences are promising in Tables 2 and 3, but statistical significance testing would help make them really convincing. Especially between SP-MLT and ASP-MTL results to highlight the benefit of adversarial training. It should be pretty straightforward to adapt the non-parametric approximate randomisation test (see http://www.lr.pi.titech.ac.jp/~takamura/pubs/randtest.pdf for promising notes a reference to the Chinchor paper) to produce these.\n- The colours are inconsistent in the caption of Figure 5 (b). In 5 (a), blue is used for “Ours”, but this seems to have swapped for 5 (b). This is worth checking, or I may have misunderstood the caption.\n#",
    "comments": "",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "c9f6e4949ef66b7d",
    "paper_id": "ACL_2017_355",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper presents a sophisticated application of Grid-type Recurrent Neural Nets to the task of determining predicate-argument structures (PAS) in Japanese.  The approach does not use any explicit syntactic structure, and outperforms the current SOA systems that do include syntactic structure.  The authors give a clear and detailed description of the implementation and of the results.  In particular, they pay close attention to the performance on dropped arguments, zero pronouns, which are prevalent in Japanese and especially challenging with respect to PAS. Their multi-sequence model, which takes all of the predicates in the sentence into account, achieves the best performance for these examples.  The paper is detailed and clearly written.",
    "weaknesses": "I really only have minor comments. There are some typos listed below, the correction of which would improve English fluency. I think it would be worth illustrating the point about the PRED including context around the \"predicate\" with the example from Fig 6 where the accusative marker is included with the verb in the PRED string.  I didn't understand the use of boldface in Table 2, p. 7.",
    "comments": "Typos: p1 :  error propagation does not need a \"the\", nor does \"multi-predicate interactions\" p2: As an solution -> As a solution, single-sequence model -> a single-sequence model,                    multi-sequence model -> a multi-sequence model  p. 3 Example in Fig 4.                    She ate a bread -> She ate bread. \np. 4 assumes the independence -> assumed independence, the multi-predicate interactions -> multi-predicate interactions, the multi-sequence model -> a multi-sequence model p.7: the residual connections -> residual connections, the multi-predicate interactions -> multi-predicate interactions (twice) p8 NAIST Text Corpus -> the NAIST Text Corpus, the state-of-the-art result -> state-of-the-art results I have read the author response and am satisfied with it.",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "031a3aee8c94da35",
    "paper_id": "ACL_2017_355",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper is well-structured and well-motivated. \nThe proposed model obtains an improvement in accuracy compared with the current state of the art system. \nAlso, the model using Grid-RNNs achieves a slightly better performance than that of proposed single-sequential model, mainly due to the improvement on the detection of zero arguments, that is the focus of this paper.",
    "weaknesses": "To the best of my understanding, the main contribution of this paper is an extension of the single-sequential model to the multi-sequential model. The impact of predicate interactions is a bit smaller than that of (Ouchi et al., 2015). There is a previous work (Shibata et al., 2016) that extends the (Ouchi et al., 2015)'s model with neural network modeling. I am curious about the comparison between them.",
    "comments": "",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "427729fdc28263fc",
    "paper_id": "ACL_2017_355",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper proposes a joint neural modelling approach to PAS analysis in Japanese, based on Grid-RNNs, which it compares variously with a conventional single-sequence RNN approach.\nThis is a solidly-executed paper, targeting a well-established task from Japanese but achieving state-of-the-art results at the task, and presenting the task in a mostly accessible manner for those not versed in Japanese. Having said that, I felt you could have talked up the complexity of the task a bit, e.g. wrt your example in Figure 1, talking through the inherent ambiguity between the NOM and ACC arguments of the first predicate, as the NOM argument of the second predicate, and better describing how the task contrasts with SRL (largely through the ambiguity in zero pronouns). I would also have liked to have seen some stats re the proportion of zero pronouns which are actually intra-sententially resolvable, as this further complicates the task as defined (i.e. needing to implicitly distinguish between intra- and inter-sentential zero anaphors). One thing I wasn't sure of here: in the case of an inter-sentential zero pronoun for the argument of a given predicate, what representation do you use? Is there simply no marking of that argument at all, or is it marked as an empty argument? My reading of the paper is that it is the former, in which case there is no explicit representation of the fact that there is a zero pronoun, which seems like a slightly defective representation (which potentially impacts on the ability of the model to capture zero pronouns); some discussion of this would have been appreciated.\nThere are some constraints that don't seem to be captured in the model (which some of the ILP-based methods for SRL explicitly model, e.g.): (1) a given predicate will generally have only one argument of a given type (esp. NOM and ACC); and (2) a given argument generally only fills one argument slot for a given predicate. I would have liked to have seen some analysis of the output of the model to see how well the model was able to learn these sorts of constraints. More generally, given the mix of numbers in Table 3 between Single-Seq and Multi-Seq (where it is really only NOM where there is any improvement for Multi-Seq), I would have liked to have seen some discussion of the relative differences in the outputs of the two models: are they largely identical, or very different but about the same in aggregate, e.g.? In what contexts do you observe differences between the two models? Some analysis like this to shed light on the internals of the models would have made the difference between a solid and a strong paper, and is the main area where I believe the paper could be improved (other than including results for SRL, but that would take quite a bit more work).\nThe presentation of the paper was good, with the Figures aiding understanding of the model. There were some low-level language issues, but nothing major: l19: the error propagation -> error propagation l190: an solution -> a solution l264 (and Figure 2): a bread -> bread l351: the independence -> independence l512: the good -> good l531: from their model -> of their model l637: significent -> significance l638: both of -> both and watch casing in your references (e.g. \"japanese\", \"lstm\", \"conll\", \"ilp\")",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "7b9ee7fb63d57a3e",
    "paper_id": "ACL_2017_365",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "1)  Evidence of the attention-MTL connection is interesting   2)  Methods are appropriate, models perform well relative to state-of-the-art",
    "weaknesses": "1)  Critical detail is not provided in the paper   2)  Models are not particularly novel",
    "comments": "This paper presents a new method for historical text normalization.  The model performs well, but the primary contribution of the paper ends up being a hypothesis that attention mechanisms in the task can be learned via multi-task learning, where the auxiliary task is a pronunciation task.  This connection between attention and MTL is interesting.\nThere are two major areas for improvement in this paper.  The first is that we are given almost no explanation as to why the pronunciation task would somehow require an attention mechanism similar to that used for the normalization task. \n Why the two tasks (normalization and pronunciation) are related is mentioned in the paper: spelling variation often stems from variation in pronunciation. \nBut why would doing MTL on both tasks result in an implicit attention mechanism (and in fact, one that is then only hampered by the inclusion of an explicit attention mechanism?).                    This remains a mystery.  The paper can leave some questions unanswered, but at least a suggestion of an answer to this one would strengthen the paper.\nThe other concern is clarity.  While the writing in this paper is clear, a number of details are omitted.                    The most important one is the description of the attention mechanism itself.  Given the central role that method plays, it should be described in detail in the paper rather than referring to previous work.  I did not understand the paragraph about this in Sec 3.4.\nOther questions included why you can compare the output vectors of two models (Figure 4), while the output dimensions are the same I don't understand why the hidden layer dimensions of two models would ever be comparable.  Usually how the hidden states are \"organized\" is completely different for every model, at the very least it is permuted.                    So I really did not understand Figure 4.\nThe Kappa statistic for attention vs. MTL needs to be compared to the same statistic for each of those models vs. the base model.\nAt the end of Sec 5, is that row < 0.21 an upper bound across all data sets?\nLastly, the paper's analysis (Sec 5) seems to imply that the attention and MTL approaches make large changes to the model (comparing e.g. Fig 5) but the experimental improvements in accuracy for either model are quite small (2%), which seems like a bit of a contradiction.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "22bf60f04f22e06e",
    "paper_id": "ACL_2017_365",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "1) Novel application of seq2seq to historical text correction, although it has been applied recently to sentence grammatical error identification [1].  2) Showed that using grapheme-to-phoneme as an auxiliary task in a MTL setting improves text normalization accuracy.",
    "weaknesses": "1) Instead of arguing that the MTL approach replaces the attention mechanism, I think the authors should investigate why attention did not work on MTL, and perhaps modify the attention mechanism so that it would not harm performance.\n2) I think the authors should reference past seq2seq MTL work, such as [2] and [3]. The MTL work in [2] also worked on non-attention seq2seq models.\n3) This paper only tested on one German historical text data set of 44 documents. It would be interesting if the authors can evaluate the same approach in another language or data set.\nReferences: [1] Allen Schmaltz, Yoon Kim, Alexander M. Rush, and Stuart Shieber. 2016. \nSentence-level grammatical error identification as sequence-to-sequence correction. In Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications.\n[2] Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Lukasz Kaiser. Multi-task Sequence to Sequence Learning. ICLR’16.  [3] Dong, Daxiang, Wu, Hua, He, Wei, Yu, Dianhai, and Wang, Haifeng. \nMulti-task learning for multiple language translation. ACL'15 --------------------------- Here is my reply to the authors' rebuttal: I am keeping my review score of 3, which means I do not object to accepting the paper. However, I am not raising my score for 2 reasons: - the authors did not respond to my questions about other papers on seq2seq MTL, which also avoided using attention mechanism. So in terms of novelty, the main novelty lies in applying it to text normalization.\n- it is always easier to show something (i.e. attention in seq2seq MTL) is not working, but the value would lie in finding out why it fails and changing the attention mechanism so that it works.",
    "comments": "",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "2f665dd4c0f1eb96",
    "paper_id": "ACL_2017_365",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "well written, solid experimental setup and intriguing qualitative analysis",
    "weaknesses": "except for the qualitative analysis, the paper may belong better to the applications area, since the models are not particularly new but the application itself is most of its novelty",
    "comments": "This paper presents a \"sequence-to-sequence\" model with attention mechanisms and an auxiliary phonetic prediction task to tackle historical text normalization. None of the used models or techniques are new by themselves, but they seem to have never been used in this problem before, showing and improvement over the state-of-the-art.  Most of the paper seem like a better fit for the applications track, except for the final analysis where the authors link attention with multi-task learning, claiming that the two produce similar effects. The hypothesis is intriguing, and it's supported with a wealth of evidence, at least for the presented task. \nI do have some questions on this analysis though: 1) In Section 5.1, aren't you assuming that the hidden layer spaces of the two models are aligned? Is it safe to do so?\n2) Section 5.2, I don't get what you mean by the errors that each of the models resolve independently of each other. This is like symmetric-difference? That is, if we combine the two models these errors are not resolved anymore?\nOn a different vein, 3) Why is there no comparison with Azawi's model?\n======== After reading the author's response.\nI'm feeling more concerned than I was before about your claims of alignment in the hidden space of the two models. If accepted, I would strongly encourage the authors to make clear in the paper the discussion you have shared with us for why you think that alignment holds in practice.",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "09f28a174d85d2d9",
    "paper_id": "ACL_2017_367",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper addresses a long standing problem concerning automatic evaluation of the output of generation/translation systems.\nThe analysis of all the available metrics is thorough and comprehensive.\nThe authors demonstrate a new metric with a higher correlation with human judgements The bibliography will help new entrants into the field.",
    "weaknesses": "The paper is written as a numerical analysis paper, with very little insights to linguistic issues in generation, the method of generation, the differences in the output from a different systems and human generated reference.\nIt is unclear if the crowd source generated references serve well in the context of an application that needs language generation.",
    "comments": "Overall, the paper could use some linguistic examples (and a description of the different systems) at the risk of dropping a few tables to help the reader with intuitions.",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "e8afc5b554c20df2",
    "paper_id": "ACL_2017_369",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper details a method of achieving translation from morphologically impoverished languages (e.g. Chinese) to morphologically rich ones (e.g. Spanish) in a two-step process. First, a system translates into a simplified version of the target language. Second, a system chooses morphological features for each generated target word, and inflects the words based on those features.\nWhile I wish the authors would apply the work to more than one language pair, I believe the issue addressed by this work is one of the most important and under-addressed problems with current MT systems. The approach taken by the authors is very different than many modern approaches based on BPE and character-level models, and instead harkens back to approaches such as \"Factored Translation Models\" (Koehn and Hoang, 2007) and \"Translating into Morphologically Rich Languages with Synthetic Phrases\" (Chahuneau et a. 2013), both of which are unfortunately uncited.\nI am also rather suspicious of the fact that the authors present only METEOR results and no BLEU or qualitative improvements. If BLEU scores do not rise, perhaps the authors could argue why they believe their approach is still a net plus, and back the claim up with METEOR and example sentences.\nFurthermore, the authors repeatedly talk about gender and number as the two linguistic features they seek to correctly handle, but seem to completely overlook person. Perhaps this is because first and second person pronouns and verbs rarely occur in news, but certainly this point at least merits brief discussion. I would also like to see some discussion of why rescoring hurts with gender. If the accuracy is very good, shouldn the reranker learn to just keep the 1-best?\nFinally, while the content of this paper is good overall, it has a huge amount of spelling, grammar, word choice, and style errors that render it unfit for publication in its current form. Below is dump of some errors that I found.\nOverall, I would like to this work in a future conference, hopefully with more than one language pair, more evaluation metrics, and after further proofreading.\nGeneral error dump: Line 062: Zhand --> Zhang Line 122: CFR --> CRF Whole related work section: consistent use of \\cite when \\newcite is appropriate It feels like there's a lot of filler: \"it is important to mention that\", \"it is worth mentioning that\", etc Line 182, 184: \"The popular phrase-based MT system\" = moses? or PBMT systems in general? \nLine 191: \"a software\" Line 196: \"academic and commercial level\" -- this should definitely be pluralized, but are these even levels? \nLine 210: \"a morphology-based simplified target\" makes it sound like this simplified target uses morphology. Perhaps the authors mean \"a morphologically simplified target\"? \nLine 217: \"decide on the morphological simplifications\"? \nTable 1: extra space in \"cuestión\" on the first line and \"titulado\" in the last line. \nTable 1: Perhaps highlight differences between lines in this table somehow? \nHow is the simplification carried out? Is this simplifier hand written by the authors, or does it use an existing tool? \nLine 290: i.e. --> e.g. Line 294: \"train on\" or \"train for\" Line 320: \"our architecture is inspired by\" or \"Collobert's proposal inspires our architecture\" Line 324: drop this comma Line 338: This equation makes it look like all words share the same word vector W Line 422: This could also be \"casas blancas\", right? How does the system choose between the sg. and pl. forms? Remind the reader of the source side conditioning here. \nLine 445: This graph is just a lattice, or perhaps more specifically a \"sausage lattice\" Line 499: Insert \"e.g.\" or similiar: (e.g. producirse) Line 500: misspelled \"syllable\" Line 500/503: I'd like some examples or further clarity on what palabras llanas and palabras estrújulas are and how you handle all three of these special cases. \nLine 570: \"and sentences longer than 50 words\" Line 571: \"by means of zh-seg\" (no determiner) or \"by means of the zh-seg tool\" Line 574: are you sure this is an \"and\" and not an \"or\"? \nLine 596: \"trained for\" instead of \"trained on\" Line 597: corpus --> copora Line 604: size is --> sizes are Line 613: would bigger embedding sizes help? 1h and 12h are hardly unreasonable training times. \nLine 615: \"seven and five being the best values\" Line 617: Why 70? Increased from what to 70? \nTable 3: These are hyperparameters and not just ordinary parameters of the model Line 650: \"coverage exceeds 99%\"? \nLine 653: \"descending\" Line 666: \"quadratic\" Line 668: space before \\cites Line 676: \"by far\" or \"by a large margin\" instead of \"by large\" Line 716: below Line 729: \"The standard phrase-based ...\" zh-seg citation lists the year as 2016, but the tool actually was released in 2009",
    "overall_score": "1",
    "confidence": "4"
  },
  {
    "review_id": "98f642fe1318a9fb",
    "paper_id": "ACL_2017_369",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper describes a method for improving two-step translation using deep learning. Results are presented for Chinese->Spanish translation, but the approach seems to be largely language-independent.\nThe setting is fairly typical for two-step MT. The first step translates into a morphologically underspecified version of the target language. The second step then uses machine learning to fill in the missing morphological categories and produces the final system output by inflecting the underspecified forms (using a morphological generator). The main novelty of this work is the choice of deep NNs as classifiers in the second step. The authors also propose a rescoring step which uses a LM to select the best variant.\nOverall, this is solid work with good empirical results: the classifier models reach a high accuracy (clearly outperforming baselines such as SVMs) and the improvement is apparent even in the final translation quality.\nMy main problem with the paper is the lack of a comparison with some straightforward deep-learning baselines. Specifically, you have a structured prediction problem and you address it with independent local decisions followed by a rescoring step. ( Unless I misunderstood the approach.) But this is a sequence labeling task which RNNs are well suited for. How would e.g. a bidirectional LSTM network do when trained and used in the standard sequence labeling setting? After reading the author response, I still think that baselines (including the standard LSTM) are run in the same framework, i.e. independently for each local label. If that's not the case, it should have been clarified better in the response. This is a problem because you're not using the RNNs in the standard way and yet you don't justify why your way is better or compare the two approaches.\nThe final re-scoring step is not entirely clear to me. Do you rescore n-best sentences? What features do you use? Or are you searching a weighted graph for the single optimal path? This needs to be explained more clearly in the paper. \n(My current impression is that you produce a graph, then look for K best paths in it, generate the inflected sentences from these K paths and *then* use a LM -- and nothing else -- to select the best variant. But I'm not sure from reading the paper.) This was not addressed in the response.\nYou report that larger word embeddings lead to a longer training time. Do they also influence the final results?\nCan you attempt to explain why adding information from the source sentence hurts? This seems a bit counter-intuitive -- does e.g. the number information not get entirely lost sometimes because of this? I would appreciate a more thorough discussion on this in the final version, perhaps with a couple of convincing examples.\nThe paper contains a number of typos and the general level of English may not be sufficient for presentation at ACL.\nMinor corrections: context of the application of MT -> context of application for MT In this cases, MT is faced in two-steps -> In this case, MT is divided into two steps markov -> Markov CFR -> CRF task was based on a direct translation -> task was based on direct translation task provided corpus -> task provided corpora the phrase-based system has dramatically -> the phrase-based approach... investigated different set of features -> ...sets of features words as source of information -> words as the source... correspondant -> corresponding Classes for gender classifier -> Classes for the... for number classifier -> for the... This layer's input consists in -> ...consists of to extract most relevant -> ...the most... Sigmoid does not output results in [-1, 1] but rather (0, 1). A tanh layer would produce (-1, 1).\ninformation of a word consists in itself -> ...of itself this $A$ set -> the set $A$ empty sentences and longer than 50 words -> empty sentences and sentences longer than... classifier is trained on -> classifier is trained in aproximately -> approximately coverage raises the 99% -> coverage exceeds 99% (unless I misunderstand) in descendant order -> in descending order cuadratic -> quadratic (in multiple places) but best results -> but the best results Rescoring step improves -> The rescoring step... are not be comparable -> are not comparable",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "91fa4135d7a83c7e",
    "paper_id": "ACL_2017_369",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper presents a method for generating morphology, focusing on gender and number, using deep learning techniques. From a morphologically simplified Spanish text, the proposed approach uses a classifier to reassign the gender and number for each token, when necessary. The authors compared their approach with other learning algorithms, and evaluated it in machine translation on the Chinese-to-Spanish (Zh->Es) translation direction.\nRecently, the task of generating gender and number has been rarely tackled, morphology generation methods usually target, and are evaluated on, morphologically-rich languages like German or Finnish. \nHowever, calling the work presented in this paper “morphology generation“ is a bit overselling as the proposed method clearly deals only with gender and number. And given the fact that some rules are handcrafted for this specific task, I do not think this method can be straightforwardly applied to do more complex morphology generation for morphologically-rich languages.\nThis paper is relatively clear in the sections presenting the proposed method. \nA lot of work has been done to design the method and I think it can have some interesting impact on various NLP tasks. However the evaluation part of this work is barely understandable as many details of what is done, or why it is done, are missing. From this evaluation, we cannot know if the proposed method brings improvements over state-of-the-art methods while the experiments cannot be replicated. Furthermore, no analysis of the results obtained is provided. Since half a page is still available, there was the possibility to provide more information to make more clear the evaluation. This work lacks of motivation. Why do you think deep learning can especially improve gender and number generation over state-of-the-art methods?\nIn your paper, the word “contribution“ should be used more wisely, as it is now in the paper, it is not obvious what are the real contributions (more details below).  abstract: what do you mean by unbalanced languages?\nsection 1: You claim that your main contribution is the use of deep learning. Just the use of deep learning in some NLP task is not a contribution.\nsection 2: You claim that neural machine translation (NMT), mentioned as “neural approximations“,  does not achieve state-of-the-art results for Zh->Es. I recommend to remove this claim from the paper, or to discuss it more, since Junczys-Dowmunt et al. (2016), during the last IWSLT, presented some results for Zh->Es with the UN corpus, showing that NMT outperforms SMT by around 10 BLEU points.\nsection 5.1: You wrote that using the Zh->Es language pair is one of your main contributions. Just using a language pair is not a contribution. Nonetheless, I think it is nice to see a paper on machine translation that does not focus of improving machine translation for English. \nThe numbers provided in Table 2 were computed before or after preprocessing? \nWhy did you remove the sentences longer than 50 tokens? \nPrecise how did you obtain development and test sets, or provide them. Your experiments are currently no replicable especially because of that.\nsection 5.2: You wrote that you used Moses and its default parameters, but the default parameters of Moses are not the same depending on the version, so you should provide the number of the version used.\nsection 5.3: What do you mean by “hardware cost“? \nTable 3: more details should be provided regarding how did you obtain these values. You chose these values given the classifier accuracy, but how precisely and on what data did you train and test the classifiers? On the same data used in section 6? \nIf I understood the experiments properly, you used simplified Spanish. But I cannot find in the text how do you simplify Spanish. And how do you use it to train the classifier and the SMT system?  section 6: Your method is better than other classification algorithms, but it says nothing about how it performs compared to the state-of-the-art methods. You should at least precise why you chose these classifications algorithms for comparison. Furthermore, how your rules impact these results? And more generally, how do you explain such a high accuracy for you method? \nDid you implement all these classification algorithms by yourselves? If not, you must provide the URL or cite the framework you used. \nFor the SMT experiments, I guess you trained your phrase table on simplified Spanish. You must precise it. \nYou chose METEOR over other metrics like BLEU to evaluate your results. You must provide some explanation for this choice. I particularly appreciate when I see a MT paper that does not use BLEU for evaluation, but if you use METEOR, you must mention which version you used. METEOR has largely changed since 2005. \nYou cited the paper of 2005, did you use the 2005 version? Or did you use the last one with paraphrases? \nAre your METEOR scores statistically significant?\nsection 7: As future work you mentioned “further simplify morphology“. In this paper, you do not present any simplification of morphology, so I think that choosing the word “further“ is misleading.\nsome typos: femenine ensambling cuadratic style: plain text citations should be rewritten like this: “(Toutanova et al, 2008) built “ should be “Toutanova et al. (2008) built “ place the caption of your tables below the table and not above, and with more space between the table and its caption. \nYou used the ACL 2016 template. You must use the new one prepared for ACL 2017. \nMore generally, I suggest that you read again the FAQ and the submission instructions provided on the ACL 2017 website. It will greatly help you to improve the paper. There are also important information regarding references: you must provide DOI or URL of all ACL papers in your references.\n----------------------- After authors response: Thank you for your response.\nYou wrote that rules are added just as post-processing, but does it mean that you do not apply them to compute your classification results? Or if you do apply them before computing these results, I'm still wondering about their impact on these results.\nYou wrote that Spanish is simplified as shown in Table 1, but it does not answer my question: how did you obtain these simplifications exactly? ( rules? \nsoftware? etc.) The reader need to now that to reproduce your approach.\nThe classification algorithms presented in Table 5 are not state-of-the-art, or if they are you need to cite some paper. Furthermore, this table only tells that deep learning gives the best results for classification, but it does not tell at all if your approach is better than state-of-the-art approach for machine translation. You need to compare your approach with other state-of-the-art morphology generation approaches (described in related work) designed for machine translation. If you do that your paper will be much more convincing in my opinion.",
    "overall_score": "2",
    "confidence": "5"
  },
  {
    "review_id": "5161130c7a6544c3",
    "paper_id": "ACL_2017_371",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The  idea to incorporate phrasal information into the task is interesting.",
    "weaknesses": "- The description is hard to follow. Proof-reading by an English native speaker would benefit the understanding - The evaluation of the approach has several weaknesses",
    "comments": "- In Equation 1 and 2 the authors mention a phrase representation give a fix-length word embedding vector. But this is not used in the model. The representation is generated based on an RNN. What the propose of this description?\n- Why are you using GRU for the Pyramid and LSTM for the sequential part? Is the combination of two architectures a reason for your improvements?\n- What is the simplified version of the GRU? Why is it performing better? How is it performing on the large data set?\n- What is the difference between RNNsearch (groundhog) and RNNsearch(baseline) in Table 4?\n-  What is the motivation for only using the ending phrases and e.g. not using the starting phrases?\n- Did you use only the pyramid encoder? How is it performing? That would be a more fair comparison since it normally helps to make the model more complex.\n- Why did you run RNNsearch several times, but PBNMT only once?\n- Section 5.2: What is the intent of this section",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "09a87b6eb2dda3cd",
    "paper_id": "ACL_2017_371",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "==== A new phrasal architecture.",
    "weaknesses": "==== **Technical**:  It's unclear whether there is a limit set  on the phrase length of the pRNN. \nMaybe I've missed this in the paper, if there is, please be more explicit about it because it affects the model quite drastically if for every sentence the largest phrase length is the sentence length.   - It's because if the largest phrase length is the sentence length, then model can be simplified into a some sort of convolution RNN where the each state of the RNN goes through some convolution layer before a final softmax and attention.   - If there is a limit set on the phrase length of pRNN, then it makes the system more tractable. But that would also mean that the phrases are determined by token ngrams which produces a sliding window of the \"pyramid encoders\" for each sentence where there are instance where the parameter for these phrases will be set close to zero to disable the phrases and these phrases would be a good intrinsic evaluation of the pRNN in addition to evaluating it purely on perplexity and BLEU extrinsically.  The usage of attention mechanism without some sort of pruning might be problematic at the phrasal level. The author have opted for some sort of greedy pruning as described in the caption of figure 4. But I support given a fixed set of phrase pairs at train time, the attention mechanism at the phrasal level can be pre-computed but at inference (apply the attention on new data at test time), this might be kind of problematic when the architecture is scaled to a larger dataset.  **Empirical**:  One issue with the language modelling experiment is the choice of evaluation and train set. Possibly a dataset like common crawl or enwiki8 would be more appropriate for language modelling experiments.  The main issue of the paper is in the experiments and results reporting, it needs quite a bit of reworking.   - The evaluation on PTB (table 2) isn't a fair one since the model was trained on a larger corpus (FBIS) and then tested on PTB. The fact that the previous study reported a 126 perplexity baseline using LSTM and the LSTM's perplexity of 106.9 provided by the author showed that the FBIS gives an advantage to computing the language model's perplexity when tested on PTB.\n - Also, regarding section 3.3, please cite appropriate publications the \"previous work\" presented in the tables. And are the previous work using the same training set?  - Additionally, why isn't the the GRU version of pRNNv reported in the FBIS evaluation in Table 3?\nThe result section cannot be simply presenting a table without explanation:  - Still on the result sections, although it's clear that BLEU and perplexity are objective automatic measure to evaluate the new architecture. It's not really okay to put up the tables and show the perplexity and BLEU scores without some explanation. E.g. in Table 2, it's necessary to explain why the LSTM's perplexity from previous work is higher than the author's implementation. Same in Table 3.  The result presented in Table 4 don't match the description in Section 4.3:  - It's not true that the pRNN outperforms both PBSMT and Enc-Dec model. The authors should make it clear that on different evaluation sets, the scores differs. And it's the averaged test scores that pRNN performs better - Please also make it clear whether the \"Test Avg.\" is a micro-average (all testsets are concatenated and evaluated as one set) or macro-average (average taken across the scores of individual test sets) score.  For table 4, please also include the significance of the BLEU improvement made by the pRNN with respect to the the baseline, see https://github.com/jhclark/multeval",
    "comments": "",
    "overall_score": "2",
    "confidence": "3"
  },
  {
    "review_id": "8d1d9dd3050bc1b8",
    "paper_id": "ACL_2017_375",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The model leverages both the network and the text to construct the latent representations, and the mutual attention approach seems sensible.\nA relatively thorough evaluation is provided, with multiple datasets, baselines, and evaluation tasks.",
    "weaknesses": "Like many other papers in the \"network embedding\" literature, which use neural network techniques inspired by word embeddings to construct latent representations of nodes in a network, the previous line of work on statistical/probabilistic modeling of networks is ignored.  In particular, all \"network embedding\" papers need to start citing, and comparing to, the work on the latent space model of Peter Hoff et al., and subsequent papers in both statistical and probabilistic machine learning publication venues: P.D. Hoff, A.E. Raftery, and M.S. Handcock. Latent space approaches to social network analysis. J. Amer. Statist. Assoc., 97(460):1090–1098, 2002.\nThis latent space network model, which embeds each node into a low-dimensional latent space, was written as far back as 2002, and so it far pre-dates neural network-based network embeddings.\nGiven that the aim of this paper is to model differing representations of social network actors' different roles, it should really cite and compare to the mixed membership stochastic blockmodel (MMSB): Airoldi, E. M., Blei, D. M., Fienberg, S. E., & Xing, E. P. (2008). Mixed membership stochastic blockmodels. Journal of Machine Learning Research.\nThe MMSB allows each node to randomly select a different \"role\" when deciding whether to form each edge.",
    "comments": "The aforementioned statistical models do not leverage text, and they do not use scalable neural network implementations based on negative sampling, but they are based on well-principled generative models instead of heuristic neural network objective functions and algorithms.  There are more recent extensions of these models and inference algorithms which are more scalable, and which do leverage text.\nIs the difference in performance between CENE and CANE in Figure 3 statistically insignificant? ( A related question: were the experiments repeated more than once with random train/test splits?)\nWere the grid searches for hyperparameter values, mentioned in Section 5.3, performed with evaluation on the test set (which would be problematic), or on a validation set, or on the training set?",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "51ca0b8309a48b3e",
    "paper_id": "ACL_2017_376",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Useful modeling contribution, and potentially useful annotated data, for an important problem -- event extraction for the relationships between countries as expressed in news text.",
    "weaknesses": "Many points are not explained well in the paper.",
    "comments": "This work tackles an important and interesting event extraction problem -- identifying positive and negative interactions between pairs of countries in the world (or rather, between actors affiliated with countries).  The primary contribution is an application of supervised, structured neural network models for sentence-level event/relation extraction.  While previous work has examined tasks in the overall area, to my knowledge there has not been any publicly availble sentence-level annotated data for the problem -- the authors here make a contribution as well by annotating some data included with the submission; if it is released, it could be useful for future researchers in this area.\nThe proposed models -- which seem to be an application of various tree-structured recursive neural network models -- demonstrate a nice performance increase compared to a fairly convincing, broad set of baselines (if we are able to trust them; see below).  The paper also presents a manual evaluation of the inferred time series from a news corpus which is nice to see.\nI'm torn about this paper.  The problem is a terrific one and the application of the recursive models seems like a contribution to this problem. \nUnfortunately, many aspects of the models, experimentation, and evaluation are not explained very well.  The same work, with a more carefully written paper, could be really great.\nSome notes: - Baselines need more explanation.  For example, the sentiment lexicon is not explained for the SVM.                    The LSTM classifier is left highly unspecified (L407-409) -- there are multiple different architectures to use an LSTM for classification.  How was it trained?  Is there a reference for the approach? \nAre the authors using off-the-shelf code (in which case, please refer and cite, which would also make it easier for the reader to understand and replicate if necessary)?  It would be impossible to replicate based on the two-line explanation here.   - (The supplied code does not seem to include the baselines, just the recursive NN models.  It's great the authors supplied code for part of the system so I don't want to penalize them for missing it -- but this is relevant since the paper itself has so few details on the baselines that they could not really be replicated based on the explanation in the paper.)\n- How were the recursive NN models trained?\n- The visualization section is only a minor contribution; there isn't really any innovation or findings about what works or doesn't work here.\nLine by line: L97-99: Unclear. Why is this problem difficult?  Compared to what? ( also the sentence is somewhat ungrammatical...) L231 - the trees are binarized, but how?\nFootnote 2 -- \"the tensor version\" - needs citation to explain what's being referred to.\nL314: How are non-state verbs defined?                    Does the definition of \"event word\"s here come from any particular previous work that motivates it?                    Please refer to something appropriate or related.\nFootnote 4: of course the collapsed form doesn't work, because the authors aren't using dependency labels -- the point of stanford collapsed form is to remove prepositions from the dependeny path and instead incorporate them into the labels.\nL414: How are the CAMEO/TABARI categories mapped to positive and negative entries?  Is performance sensitive to this mapping?  It seems like a hard task (there are hundreds of those CAMEO categories....) Did the authors consider using the Goldstein scaling, which has been used in political science, as well as the cited work by O'Connor et al.?  Or is it bad to use for some reason?\nL400-401: what is the sentiment lexicon and why is it appropriate for the task?\nL439-440: Not clear.  \"We failed at finding an alpha meeting the requirements for the FT model.\"  What does that mean? What are the requirements? What did the authors do in their attempt to find it?\nL447,L470: \"precision and recall values are based on NEG and POS classes\". \nWhat does this mean?  So there's a 3x3 contingency table of gold and predicted (POS, NEU, NEG) classes, but this sentence leaves ambiguous how precision and recall are calculated from this information.\n5.1 aggregations: this seems fine though fairly ad-hoc.  Is this temporal smoothing function a standard one?  There's not much justification for it, especially given something simpler like a fixed window average could have been used.\n5.2 visualizations: this seems pretty ad-hoc without much justification for the choices.  The graph visualization shown does not seem to illustrate much. \nShould also discuss related work in 2d spatial visualization of country-country relationships by Peter Hoff and Michael Ward.\n5.3 L638-639: \"unions of countries\" isn't a well defined concept.  mMybe the authors mean \"international organizations\"?\nL646-648: how were these 5 strong and 5 weak peaks selected?  In particular, how were they chosen if there were more than 5 such peaks?\nL680-683: This needs more examples or explanation of what it means to judge the polarity of a peak.  What does it look like if the algorithm is wrong?                    How hard was this to assess?  What was agreement rate if that can be judged?\nL738-740: The authors claim Gerrish and O'Connor et al. have a different \"purpose and outputs\" than the authors' work.  That's not right.  Both those works try to do both (1) extract time series or other statistical information about the polarity of the relationships between countries, and *also* (2) extract topical keywords to explain aspects of the relationships.  The paper here is only concerned with #1 and less concerned with #2, but certainly the previous work addresses #1.  It's fine to not address #2 but this last sentence seems like a pretty odd statement.\nThat raises the question -- Gerrish and O'Connor both conduct evaluations with an external database of country relations developed in political science (\"MID\", military interstate disputes).              Why don't the authors of this work do this evaluation as well?  There are various weaknesses of the MID data, but the evaluation approach needs to be discussed or justified.",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "18115dc58f41d975",
    "paper_id": "ACL_2017_37",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Relatively clear description of context and structure of proposed approach. \nRelatively complete description of the math. Comparison to an extensive set of alternative systems.",
    "weaknesses": "Weak results/summary of \"side-by-side human\" comparison in Section 5. Some disfluency/agrammaticality.",
    "comments": "The article proposes a principled means of modeling utterance context, consisting of a sequence of previous utterances. Some minor issues: 1. Past turns in Table 1 could be numbered, making the text associated with this table (lines 095-103) less difficult to ingest. Currently, readers need to count turns from the top when identifying references in the authors' description, and may wonder whether \"second\", \"third\", and \"last\" imply a side-specific or global enumeration.\n2. Some reader confusion may be eliminated by explicitly defining what \"segment\" means in \"segment level\", as occurring on line 269. Previously, on line 129, this seemingly same thing was referred to as \"a sequence-sequence [similarity matrix]\". The two terms appear to be used interchangeably, but it is not clear what they actually mean, despite the text in section 3.3. It seems the authors may mean \"word subsequence\" and \"word subsequence to word subsequence\", where \"sub-\" implies \"not the whole utterance\", but not sure.\n3. Currently, the variable symbol \"n\" appears to be used to enumerate words in an utterance (line 306), as well as utterances in a dialogue (line 389). The authors may choose two different letters for these two different purposes, to avoid confusing readers going through their equations.\n4. The statement \"This indicates that a retrieval based chatbot with SMN can provide a better experience than the state-of-the-art generation model in practice.\" at the end of section 5 appears to be unsupported. The two approaches referred to are deemed comparable in 555 out of 1000 cases, with the baseline better than the proposed method in 238 our of the remaining 445 cases. \nThe authors are encouraged to assess and present the statistical significance of this comparison. If it is weak, their comparison permits to at best claim that their proposed method is no worse (rather than \"better\") than the VHRED baseline.\n5. The authors may choose to insert into Figure 1 the explicit \"first layer\", \"second layer\" and \"third layer\" labels they use in the accompanying text.\n6.  Their is a pervasive use of \"to meet\" as in \"a response candidate can meet each utterace\" on line 280 which is difficult to understand.\n7. Spelling: \"gated recurrent unites\"; \"respectively\" on line 133 should be removed; punctuation on line 186 and 188 is exchanged; \"baseline model over\" -> \"baseline model by\"; \"one cannot neglects\".",
    "overall_score": "4",
    "confidence": "2"
  },
  {
    "review_id": "e0eb98bbe4263593",
    "paper_id": "ACL_2017_382",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper presents a step in the direction of developing more challenging corpora for training sentence planners in data-to-text NLG, which is an important and timely direction.",
    "weaknesses": "It is unclear whether the work reported in this paper represents a substantial advance over Perez-Beltrachini et al.'s (2016) method for selecting content. \nThe authors do not directly compare the present paper to that one. It appears that the main novelty of this paper is the additional analysis, which is however rather superficial.\nIt is good that the authors report a comparison of how an NNLG baseline fares on this corpus in comparison to that of Wen et al. (2016).  However, the BLEU scores in Wen et al.'s paper appear to be much much higher, suggesting that this NNLG baseline is not sufficient for an informative comparison.",
    "comments": "The authors need to more clearly articulate why this paper should count as a substantial advance over what has been published already by Perez-Beltrachini et al, and why the NNLG baseline should be taken seriously.  In contrast to LREC, it is not so common for ACL to publish a main session paper on a corpus development methodology in the absence of some new results of a system making use of the corpus.\nThe paper would also be stronger if it included an analysis of the syntactic constructions in the two corpora, thereby more directly bolstering the case that the new corpus is more complex.  The exact details of how the number of different path shapes are determined should also be included, and ideally associated with the syntactic constructions.\nFinally, the authors should note the limitation that their method does nothing to include richer discourse relations such as Contrast, Consequence, Background, etc., which have long been central to NLG. In this respect, the corpora described by Walker et al. JAIR-2007 and Isard LREC-2016 are more interesting and should be discussed in comparison to the method here.\nReferences Marilyn Walker, Amanda Stent, François Mairesse, and Rashmi Prasad. 2007. Individual and domain adaptation in sentence planning for dialogue. Journal of Artificial Intelligence Research (JAIR), 30:413–456.\nAmy Isard, 2016. “ The Methodius Corpus of Rhetorical Discourse Structures and Generated Texts” , Proceedings of the Tenth Conference on Language Resources and Evaluation (LREC 2016), Portorož, Slovenia, May 2016.\n--- Addendum following author response: Thank you for the informative response.  As the response offers crucial clarifications, I have raised my overall rating.  Re the comparison to Perez-Beltrachini et al.: While this is perhaps more important to the PC than to the eventual readers of the paper, it still seems to this reviewer that the advance over this paper could've been made much clearer.  While it is true that Perez-Beltrachini et al. \"just\" cover content selection, this is the key to how this dataset differs from that of Wen et al.  There doesn't really seem to be much to the \"complete methodology\" of constructing the data-to-text dataset beyond obvious crowd-sourcing steps; to the extent these steps are innovative or especially crucial, this should be highlighted.  Here it is interesting that 8.7% of the crowd-sourced texts were rejected during the verification step; related to Reviewer 1's concerns, it would be interesting to see some examples of what was rejected, and to what extent this indicates higher-quality texts than those in Wen et al.'s dataset.  Beyond that, the main point is really that collecting the crowd-sourced texts makes it possible to make the comparisons with the Wen et al. corpus at both the data and text levels (which this reviewer can see is crucial to the whole picture).\nRe the NNLG baseline, the issue is that the relative difference between the performance of this baseline on the two corpora could disappear if Wen et al.'s substantially higher-scoring method were employed.  The assumption that this relative difference would remain even with fancier methods should be made explicit, e.g. by acknowledging the issue in a footnote.  Even with this limitation, the comparison does still strike this reviewer as a useful component of the overall comparison between the datasets.\nRe whether a paper about dataset creation should be able to get into ACL without system results:  though this indeed not unprecedented, the key issue is perhaps how novel and important the dataset is likely to be, and here this reviewer acknowledges the importance of the dataset in comparison to existing ones (even if the key advance is in the already published content selection work).\nFinally, this reviewer concurs with Reviewer 1 about the need to clarify the role of domain dependence and what it means to be \"wide coverage\" in the final version of the paper, if accepted.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "a21fbcc961435869",
    "paper_id": "ACL_2017_382",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- Potentially valuable resource - Paper makes some good points",
    "weaknesses": "- Awareness of related work (see below) - Is what the authors are trying to do (domain-independent microplanning) even possible (see below) - Are the crowdsourced texts appropriate (see below)",
    "comments": "This is an interesting paper which presents a potentially valuable resource, and I in many ways I am sympathetic to it.  However, I have some high-level concerns, which are not addressed in the paper.  Perhaps the authors can address these in their response.\n(1) I was a bit surprised by the constant reference and comparison to Wen 2016, which is a fairly obscure paper I have not previously heard of.  It would be better if the authors justified their work by comparison to well-known corpora, such as the ones they list in Section 2. Also, there are many other NLG projects that looked at microplanning issue when verbalising DBPedia, indeed there was a workshop in 2016 with many papers on NLG and DBPedia (https://webnlg2016.sciencesconf.org/  and http://aclweb.org/anthology/W/W16/#3500); see also previous work by Duboue and Kutlak.  I would like to see less of a fixation on Wen (2016), and more awareness of other work on NLG and DBPedia.\n(2) Microplanning tends to be very domain/genre dependent.  For example, pronouns are used much more often in novels than in aircraft maintenance manuals.   This is why so much work has focused on domain-dependent resources. \n  So there are some real questions about whether it is possible even in theory to train a \"wide-coverage microplanner\".  The authors do not discuss this at all; they need to show they are aware of this concern.\n(3) I would be concerned about the quality of the texts obtained from crowdsourcing.              A lot of people dont write very well, so it is not at all clear to me that gathering example texts from random crowdsourcers is going to produce a good corpus for training microplanners.  Remember that the ultimate goal of microplanning is to produce texts that are easy to *read*.  Imitating human writers (which is what this paper does, along with most learning approaches to microplanning) makes sense if we are confident that the human writers have produced well-written easy-to-read texts.              Which is a reasonable assumption if the writers are professional journalists (for example), but a very dubious one if the writers are random crowdsourcers.\nFrom a presentational perspective, the authors should ensure that all text in their paper meets the ACL font size criteria.  Some of the text in Fig 1 and (especially) Fig 2 is tiny and very difficult to read; this text should be the same font size as the text in the body of the paper.\nI will initially rate this paper as borderline.  I look forward to seeing the author's response, and will adjust my rating accordingly.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "bca6a0572a276e9c",
    "paper_id": "ACL_2017_384",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- the model if theoretically solid and motivated by formal semantics.",
    "weaknesses": "- The paper is about is-a relation extraction but the majority of literature about taxonomization is not referenced in the paper, inter alia: Flati Tiziano, Vannella Daniele, Pasini Tommaso, Navigli Roberto. \n2016. MultiWiBi: The multilingual Wikipedia bitaxonomy project.\nSoren Auer, Christian Bizer, Georgi Kobilarov, Jens ¨ Lehmann, Richard Cyganiak, and Zachary Ive. \n2007. DBpedia: A nucleus for a web of open data.\nGerard de Melo and Gerhard Weikum. 2010. MENTA: Inducing Multilingual Taxonomies from Wikipedia.\nZornitsa Kozareva and Eduard H. Hovy. 2010. A Semi-Supervised Method to Learn and Construct Taxonomies Using the Web.  Vivi Nastase, Michael Strube, Benjamin Boerschinger, Caecilia Zirn, and Anas Elghafari. 2010. WikiNet: A Very Large Scale Multi-Lingual Concept Network.\nSimone Paolo Ponzetto and Michael Strube. 2007. \nDeriving a large scale taxonomy from Wikipedia.\nSimone Paolo Ponzetto and Michael Strube. 2011. \nTaxonomy induction based on a collaboratively built knowledge repository.  Fabian M. Suchanek, Gjergji Kasneci, and Gerhard Weikum. 2008. YAGO: A large ontology from Wikipedia and WordNet.  Paola Velardi, Stefano Faralli, and Roberto Navigli. \n2013. OntoLearn Reloaded: A graph-based algorithm for taxonomy induction.   - Experiments are poor, they only compare against \"Hearst patterns\" without taking into account the works previously cited.",
    "comments": "The paper is easy to follow and the supplementary material is also well written and useful, however the paper lack of references of is a relation extraction and taxonomization literature. The same apply for the experiments. \nIn fact no meaningful comparison is performed and the authors not even take into account the existence of other systems (more recent than hearst patterns).\nI read authors answers but still i'm not convinced that they couldn't perform more evaluations. I understand that they have a solid theoretical motivation but still, i think that comparison are very important to asses if the theoretical intuitions of the authors are confirmed also in practice. While it's true that all the works i suggested as comparison build taxonomies, is also true that a comparison is possible considering the edges of a taxonomy.\nAnyway, considering the detailed author answer and the discussion with the other reviewer i can rise my score to 3 even if i still think that this paper is poor of experiments and does not frame correctly in the is-a relation extraction / taxonomy building literature.",
    "overall_score": "3",
    "confidence": "5"
  },
  {
    "review_id": "c4ec7bcb0011d622",
    "paper_id": "ACL_2017_384",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper presents an approach for fine-grained IsA extraction by learning modifier interpretations. The motivation of the paper is easy to understand and this is an interesting task. In addition, the approach seems solid in general and the experimental results show that the approach increases in the number of fine-grained classes that can be populated.",
    "weaknesses": "Some parts of the paper are hard to follow. It is unclear to me why D((e, p, o)) is multiplied by w in Eq (7) and why the weight for e in Eq. (8) is explained as the product of how often e has been observed with some property and the weight of that property for the class MH. In addition, it also seems unclear how effective introducing compositional models itself is in increasing the coverage. I think one of the major factors of the increase of the coverage is the modifier expansion, which seems to also be applicable to the baseline 'Hearst'. It would be interesting to see the scores 'Hearst' with modifier expansion.",
    "comments": "Overall, the task is interesting and the approach is generally solid. However, since this paper has weaknesses described above, I'm ambivalent about this paper.\n- Minor comment: I'm confused with some notations. For example, it is unclear for me what 'H' stands for. It seems that 'H' sometimes represents a class such as in (e, H) (- O, but sometimes represents a noun phrase such as in (H, p, N, w) (- D. Is my understanding correct?\nIn Paragraph \"Precision-Recall Analysis\", why the authors use area under the ROC curve instead of area under the Precision-Recall curve, despite the paragraph title \"Precision-Recall Analysis\"?\n- After reading the response: Thank you for the response. I'm not fully satisfied with the response as to the modifier expansion. I do not think the modifier expansion can be applied to Hearst as to the proposed method. However, I'm wondering whether there is no way to take into account the similar modifiers to improve the coverage of Hearst. I'm actually between 3 and 4, but since it seems still unclear how effective introducing compositional models itself is, I keep my recommendation as it is.",
    "overall_score": "3",
    "confidence": "2"
  },
  {
    "review_id": "aa911f5c64a55fd6",
    "paper_id": "ACL_2017_384",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This is a novel approach to modeling the compositional structure of complex categories that maintains a set theoretic interpretation of common nouns and modifiers, while also permitting a distributional interpretation of head modification. The approach is well motivated and clearly defined and the experiments show that show that this decomposed representation can improve upon the Hearst-pattern derived IsA relations upon which it is trained in terms of coverage.",
    "weaknesses": "The experiments are encouraging. However, it would be nice to see ROC curves for the new approach alone, not in an ensemble with Hearst patterns. Table 5 tells us that Mods_I increases coverage at the cost of precision and Figure 2 tells us that Mods_I matches Hearst pattern precision for the high precision region of the data. However, neither of these tell us whether the model can distinguish between the high and low precision regions, and the ROC curves (which would tell us this) are only available for ensembled models.\nI believe that Eqn. 7 has an unnecessary $w$ since it is already the case that $w=D(\\rangle e, p, o \\langle)$. - discussion Overall, this is a nice idea that is well described and evaluated. I think this paper would be a good addition to ACL.",
    "comments": "",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "a2a0e65c41e0a094",
    "paper_id": "ACL_2017_387",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper tackles an interesting problem and provides a (to my knowledge) novel and reasonable way of learning and combining cognitive features with textual features for sentiment analysis and irony detection. The paper is  clearly written and organized, and the authors provided a lot of useful detail and informative example and plots. Most of the results are convincing, and the authors did a good job comparing their approach and results with previous work.",
    "weaknesses": "1. Just from the reading abstract, I expected that the authors' approach would significantly outperform previous methods, and that using both the eye-gaze and textual features consistently yields the best results. Upon reading the actual results section, however, it seems like the findings were more mixed. I think it would be helpful to update the abstract and introduction to reflect this. \n2. When evaluating the model on dataset 1 for sentiment analysis, were the sarcastic utterances included? Did the model do better on classifying the non-sarcastic utterances than the sarcastic ones? \n3. I understand why the eye-movement data would be useful for sarcasm detection, but it wasn't as obvious to me why it would be helpful for (non-sarcastic) sentiment classification beyond the textual features.",
    "comments": "This paper contains a lot of interesting content, and the approach seems solid and novel to me. The results were a little weaker than I had anticipated from the abstract, but I believe would still be interesting to the larger community and merits publication.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "197c6f2257ee9385",
    "paper_id": "ACL_2017_387",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "(1) A deep CNN framework is proposed to extract and combine cognitive features with textual features for sentiment analysis and sarcasm detection.  (2) The ideas is interesting and novelty.",
    "weaknesses": "(1) Replicability would be an important concern. Researchers cannot replicate the system/method for improvement due to lack of data for feature extraction.",
    "comments": "Overall, this paper is well written and organized. The experiments are conducted carefully for comparison with previous work and the analysis is reasonable. I offer some comments as follows.\n(1)           Does this model be suitable on sarcastic/non-sarcastic utterances? \nThe authors should provide more details for further analysis.  (2)           Why the eye-movement data would be useful for sarcastic/non-sarcastic sentiment classification beyond the textual features? The authors should provide more explanations.",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "5e67d3af7f9ea4f7",
    "paper_id": "ACL_2017_388",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper describes interesting and ambitious work: the automated conversion of Universal Dependency grammar structures into [what the paper calls] semantic logical form representations.  In essence, each UD construct is assigned a target construction in logical form, and a procedure is defined to effect the conversion, working ‘inside-out’ using an intermediate form to ensure proper nesting of substructures into encapsulating ones.  Two evaluations are carried out: comparing the results to gold-standard lambda structures and measuring the effectiveness of the resulting lambda expressions in actually delivering the answers to questions from two QA sets.   It is impossible to describe all this adequately in the space provided.  The authors have taken some care to cover all principal parts, but there are still many missing details.  I would love to see a longer version of the paper! \nParticularly the QA results are short-changed; it would have been nice to learn which types of question are not handled, and which are not answered correctly, and why not.  This information would have been useful to gaining better insight into the limitations of the logical form representations.   That leads to my main concern/objection.  This logical form representation is not in fact a ‘real’ semantic one.                          It is, essentially, a rather close rewrite of the dependency structure of the input, with some (good) steps toward ‘semanticization’, including the insertion of lambda operators, the explicit inclusion of dropped arguments (via the enhancement operation), and the introduction of appropriate types/units for such constructions as eventive adjectives and nouns like “running horse” and “president in 2009”.  But many (even simple) aspects of semantic are either not present (at least, not in the paper) and/or simply wrong.  Missing: quantification (as in “every” or “all”); numbers (as in “20” or “just over 1000”); various forms of reference (as in “he”, “that man”, “what I said before”); negation and modals, which change the semantics in interesting ways; inter-event relationships (as in the subevent relationship between the events in “the vacation was nice, but traveling was a pain”; etc. etc.  To add them one can easily cheat, by treating these items as if they were just unusual words and defining obvious and simple lambda formulas for them.  But they in fact require specific treatment; for example, a number requires the creation of a separate set object in the representation, with its own canonical variable (allowing later text to refer to “one of them” and bind the variable properly).  For another example, Person A’s model of an event may differ from Person B’s, so one needs two representation symbols for the event, plus a coupling and mapping between them.  For another example, one has to be able to handle time, even if simply by temporally indexing events and states.  None of this is here, and it is not immediately obvious how this would be added.  In some cases, as DRT shows, quantifier and referential scoping is not trivial.   It is easy to point to missing things, and unfair to the paper in some sense; you can’t be expected to do it all.  But you cannot be allowed to make obvious errors.  Very disturbing is the assignment of event relations strictly in parallel with the verb’s (or noun’s) syntactic roles.  No-one can claim seriously that “he broke the window” and “the window broke” has “he” and “the window” filling the same semantic role for “break”. \nThat’s simply not correct, and one cannot dismiss the problem, as the paper does, to some nebulous subsequent semantic processing.                          This really needs adequate treatment, even in this paper.  This is to my mind the principal shortcoming of this work; for me this is the make-or-break point as to whether I would fight to have the paper accepted in the conference.  (I would have been far happier if the authors had simply acknowledged that this aspect is wrong and will be worked on in future, with a sketch saying how: perhaps by reference to FrameNet and semantic filler requirements.)                           Independent of the representation, the notation conversion procedure is reasonably clear.  I like the facts that it is rather cleaner and simpler than its predecessor (based on Stanford dependencies), and also that the authors have the courage of submitting non-neural work to the ACL in these days of unbridled and giddy enthusiasm for anything neural.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "c812e6be4d31b271",
    "paper_id": "ACL_2017_395",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper is generally well written, presents most of its ideas clearly and makes apt comparisons to related work where required. The experiments are well structured and the results are overall good, though not outstanding. However, there are several problems with the paper that prevent me from endorsing it completely.",
    "weaknesses": "My main concern with the paper is the magnification of its central claims, beyond their actual worth.\n1) The authors use the term \"deep\" in their title and then several times in the paper. But they use a skip-gram architecture (which is not deep). This is misrepresentation.\n2) Also reinforcement learning is one of the central claims of this paper. \nHowever, to the best of my understanding, the motivation and implementation lacks clarity. Section 3.2 tries to cast the task as a reinforcement learning problem but goes on to say that there are 2 major drawbacks, due to which a Q-learning algorithm is used. This algorithm does not relate to the originally claimed policy.\nFurthermore, it remains unclear how novel their modular approach is. Their work seems to be very similar to EM learning approaches, where an optimal sense is selected in the E step and an objective is optimized in the M step to yield better sense representations. The authors do not properly distinguish their approach, nor motivative why RL should be preferred over EM in the first place.\n3) The authors make use of the term pure-sense representations multiple times, and claim this as a central contribution of their paper. I am not sure what this means, or why it is beneficial.\n4) They claim linear-time sense selection in their model. Again, it is not clear to me how this is the case. A highlighting of this fact in the relevant part of the paper would be helpful.  5) Finally, the authors claim state-of-the-art results. However, this is only on a single MaxSimC metric. Other work has achieved overall better results using the AvgSimC metric. So, while state-of-the-art isn't everything about a paper, the claim that this paper achieves it - in the abstract and intro - is at least a little misleading.",
    "comments": "",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "ba08c1da876f043e",
    "paper_id": "ACL_2017_395",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper describes a novel approach for learning multi-sense word representations using reinforcement learning. A CBOW-like architecture is used for sense selection, computing a score for each sense based on the dot product between the sum of word embeddings in the current context and the corresponding sense vector. A second module based on the skip-gram model is used to train sense representations, given results from the sense selection module. In order to train these two modules, the authors apply Q-Learning, where the Q-value is provided by the CBOW-based sense selection module. The reward is given by the skip-gram negative sampling likelihood. Additionally, the authors propose an approach for determining the number of senses for each word non-parametrically, by creating new senses when the Q-values for existing scores have a score under 0.5.\nThe resulting approach achieves good results under the \"MaxSimC\" metric, and results comparable to previous approaches under \"AvgSimC\". The authors suggest that their approach could be used to improve the performance for downstream tasks by replacing word embeddings with their most probable sense embedding. It would have been nice to see this claim explored, perhaps in a sequential labeling task such as POS-tagging or NER, especially in light of previous work questioning the usefulness of multi-sense representations in downstream tasks. \nI found it somewhat misleading to suggest that relying on MaxSimC could reduce overhead in a real world application, as the sense disambiguation step (with associated parameters) would still be required, in addition to the sense embeddings. A clustering-based approach using a weighted average of sense representations would have similar overhead. The claims about improving over word2vec using 1/100 of the data are also not particularly surprising on SCWS. \nThese are misleading contributions, as they do not advance/differ much from previous work.\nThe modular quality of their approach results in a flexibility that I think could have been explored further. The sense disambiguation module uses a vector averaging (CBOW) approach. A positive aspect of their model is that they should be able to substitute other context composition approaches (using alternative neural architecture composition techniques) relatively easily.\nThe paper applies an interesting approach to a problem that has been explored now in many ways. The results on standard benchmarks are comparable to previous work, but not particularly surprising/interesting. However, the approach goes beyond a simple extension of the skip-gram model for multi-sense representation learning by providing a modular framework based on reinforcement learning. \nIdeally, this aspect would be explored further. But overall, the approach itself may be interesting enough on its own to be considered for acceptance, as it could help move research in this area forward.\n- There are a number of typos that should be addressed (line 190--representations*, 331--selects*, 492--3/4th*).\nNOTE: Thank you to the authors for their response.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "9bdb08d414ed253e",
    "paper_id": "ACL_2017_395",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper propose DRL-Sense model that shows a marginal improvement on SCWS dataset and a significant improvement on ESL-50 and RD-300 datasets.",
    "weaknesses": "The technical aspects of the paper raise several concerns: Could the authors clarify two drawbacks in 3.2? The first drawback states that optimizing equation (2) leads to the underestimation of the probability of sense. As I understand, eq(2) is the expected reward of sense selection, z_{ik} and z_{jl} are independent actions and there are only two actions to optimize. \nThis should be relatively easy. In NLP setting, optimizing the expected rewards over a sequence of actions for episodic-task has been proven doable (Sequence Level Training with Recurrent Neural Networks, Ranzato 2015) even in a more challenging setting of machine translation where the number of actions ~30,000 and the average sequence length ~30 words. The DRL-Sense model has maximum 3 actions and it does not have sequential nature of RL. This makes it hard to accept the claim about the first drawback.\nThe second drawback, accompanied with the detail math in Appendix A, states that the update formula is to minimize the likelihood due to the log-likelihood is negative. Note that most out-of-box optimizers (Adam, SGD, Adadelta, …) minimize a function f, however, a common practice when we want to maximize f we just minimize -f. Since the reward defined in the paper is negative, any standard optimizer can be use on the expected of the negative reward, which is always greater than 0. This is often done in many modeling tasks such as language model, we minimize negative log-likelihood instead of maximizing the likelihood. The authors also claim that when “the log-likelihood reaches 0, it also indicates that the likelihood reaches infinity and computational flow on U and V” (line 1046-1049). Why likelihood→infinity? Should it be likelihood→1?\nCould the authors also explain how DRL -Sense is based on Q-learning? The horizon in the model is length of 1. There is no transition between state-actions and there is not Markov-property as I see it (k, and l are draw independently). I am having trouble to see the relation between Q-learning and DRL-Sense.  In (Mnih et al., 2013), the reward is given from the environment whereas in the paper, the rewards is computed by the model. What’s the reward in DRL-Sense? Is it 0, for all the (state, action) pairs or the cross-entropy in eq(4)?   Cross entropy is defined as H(p, q) = -\\sum_{x} q(x)\\log q(x), which variable do the authors sum over in (4)? I see that q(C_t, z_{i, k}) is a scalar (computed in eq(3)), while Co(z_{ik}, z_{jl}) is a distribution over total number of senses eq(1). These two categorial variables do not have the same dimension, how is cross-entropy H in eq(4) is computed then?\nCould the authors justify the dropout exploration? Why not epsilon-greedy exploration? Dropout is often used for model regularization, preventing overfitting. How do the authors know the gain in using dropout is because of exploration but regularization?\nThe authors states that Q-value is a probabilistic estimation (line 419), can you elaborate what is the set of variables the distribution is defined? When you sum over that set of variable, do you get 1? I interpret that Q is a distribution over senses per word, however  definition of q in eq(3) does not contain a normalizing constant, so I do not see q is a valid distribution. This also related to the value 0.5 in section 3.4 as a threshold for exploration. \nWhy 0.5 is chosen here where q is just an arbitrary number between (0, 1) and the constrain \\sum_z q(z) = 1 does not held? Does the authors allow the creation of a new sense in the very beginning or after a few training epochs? I would image that at the beginning of training, the model is unstable and creating new senses might introduce noises to the model.  Could the authors",
    "comments": "on that?\nGeneral discussion What’s the justification for omitting negative samples in line 517? Negative sampling has been use successfully in word2vec due to the nature of the task: learning representation. Negative sampling, however does not work well when the main interest is modeling a distribution p() over senses/words. Noise contrastive estimation is often preferred when it comes to modeling a distribution. The DRL-Sense, uses collocation likelihood to compute the reward, I wonder how the approximation presented in the paper affects the learning of the embeddings.\nWould the authors consider task-specific evaluation for sense embeddings as suggested in recent research [1,2] [1] Evaluation methods for unsupervised word embeddings. Tobias Schnabel, Igor Labutov, David Mimno and Thorsten Joachims.\n[2] Problems With Evaluation of Word Embeddings Using Word Similarity Tasks . \nManaal Faruqui, Yulia Tsvetkov, Pushpendre Rastogi, Chris Dyer --- I have read the response.",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "ef7db2258c80200c",
    "paper_id": "ACL_2017_419",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper introduces a simple and effective method for morphological paradigm completion in low-resource settings. The method uses a character-based seq2seq model trained on a mix of examples in two languages: a resource-poor language and a closely-related resource-rich language; each training example is annotated with a paradigm properties and a language ID. Thus, the model enables transfer learning across languages when the two languages share common characters and common paradigms. While the proposed multi-lingual solution is not novel (similar architectures have been explored in syntax, language modeling, and MT), the novelty of this paper is to apply the approach to morphology. Experimental results show substantial improvements over monolingual baselines, and include a very thorough analysis of the impact of language similarities on the quality of results. The paper is interesting, very clearly written, I think it’ll be a nice contribution to the conference program.  Detailed comments:  — My main question is why the proposed general multilingual methodology was limited to pairs of languages, rather than to sets of similar languages? For example, all Romance languages could be included in the training to improve Spanish paradigm completion, and all Slavic languages with Cyrillic script could be mixed to improve Ukrainian. It would be interesting to see the extension of the models from bi-lingual to multilingual settings.  — I think Arabic is not a fair (and fairly meaningless) baseline, given how different is its script and morphology from the target languages. A more interesting baseline would be, e.g., a language with a partially shared alphabet but a different typology. For example, a Slavic language with Latin script could be used as a baseline language for Romance languages. If Arabic is excluded, and if we consider a most distant language in the same the same family as a baseline, experimental results are still strong.  — A half-page discussion of contribution of Arabic as a regularizer also adds little to the paper; I’d just remove Arabic from all the experiments and would add a regularizer (which, according to footnote 5, works even better than adding Arabic as a transfer language).               — Related work is missing a line of work on “language-universal” RNN models that use basically the same approach: they learn shared parameters for inputs in multiple languages, and add a language tag to the input to mediate between languages. Related studies include a multilingual parser (Ammar et al., 2016), language models (Tsvetkov et al., 2016), and machine translation (Johnson et al., 2016 ) Minor:  — I don’t think that the claim is correct in line 144 that POS tags are easy to transfer across languages. Transfer of POS annotations is also a challenging task.   References:  Waleed              Ammar, George Mulcaire, Miguel Ballesteros, Chris Dyer, and Noah A. Smith. \" Many languages, one parser.” TACL 2016.  Yulia Tsvetkov, Sunayana Sitaram, Manaal Faruqui, Guillaume Lample, Patrick Littell, David Mortensen, Alan W. Black, Lori Levin, and Chris Dyer. \" Polyglot neural language models: A case study in cross-lingual phonetic representation learning.” NAACL 2016.\nMelvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat et al. \"Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation.\" arXiv preprint arXiv:1611.04558 2016.\n-- Response to author response:  Thanks for your response & I'm looking forward to reading the final version!",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "e2ec0be7eb025d1e",
    "paper_id": "ACL_2017_433",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- They obtain good results and their experimental setup appears to be solid.\n - They perform many careful analyses and explore the influence on many parameters of their model.\n - They provide a small Singlish treebank annotated according to the Universal Dependencies v1.4 guidelines.\n - They propose very sound guidelines on how to analyze common Singlish constructions in UD.\n - Their method is linguistically informed and they nicely exploit similarity between standard English and the creole Singaporean English.\n - The paper presents methods for a low-resource language.\n - They are not just applying an existing English method to another language but instead present a method that can be potentially used for other closely related language pairs.\n - They use a well-motivated method for selecting the sentences to include in their treebank.\n - The paper is very well written and easy to read.",
    "weaknesses": "- The annotation quality seems to be rather poor. They performed double annotation of 100 sentences and their inter-annotator agreement is just 75.72% in terms of LAS. This makes it hard to assess how reliable the estimate of the LAS of their model is, and the LAS of their model is in fact slightly higher than the inter-annotator agreement.  UPDATE: Their rebuttal convincingly argued that the second annotator who just annotated the 100 examples to compute the IAA didn't follow the annotation guidelines for several common constructions. Once the second annotator fixed these issues, the IAA was reasonable, so I no longer consider this a real issue.",
    "comments": "I am a bit concerned about the apparently rather poor annotation quality of the data and how this might influence the results, but overall, I liked the paper a lot and I think this would be a good contribution to the conference.\n- Questions for the authors:  - Who annotated the sentences? You just mention that 100 sentences were annotated by one of the authors to compute inter=annotator agreement but you don't mention who annotated all the sentences.\n - Why was the inter-annotator agreement so low? In which cases was there disagreement? Did you subsequently discuss and fix the sentences for which there was disagreement?\n - Table A2: There seem to be a lot of discourse relations (almost as many as dobj relations) in your treebank. Is this just an artifact of the colloquial language or did you use \"discourse\" for things that are not considered \"discourse\" in other languages in UD?\n - Table A3: Are all of these discourse particles or discourse + imported vocab? If the latter, perhaps put them in separate tables, and glosses would be helpful.\n- Low-level comments:  - It would have been interesting if you had compared your approach to the one by Martinez et al. (2017, https://arxiv.org/pdf/1701.03163.pdf). Perhaps you should mention this paper in the reference section.\n - You use the word \"grammar\" in a slightly strange way. I think replacing \"grammar\" with syntactic constructions would make it clearer what you try to convey. ( e.g., line 90)  - Line 291: I don't think this can be regarded as a variant of it-extraposition. But I agree with the analysis in Figure 2, so perhaps just get rid of this sentence.\n - Line 152: I think the model by Dozat and Manning (2016) is no longer state-of-the art, so perhaps just replace it with \"very high performing model\" or something like that.\n - It would be helpful if you provided glosses in Figure 2.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "49e181ef1e612218",
    "paper_id": "ACL_2017_433",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Nice results, nice data set. Not so much work on Creole-like languages, especially English.",
    "weaknesses": "A global feeling of \"Deja-vu\", a lot of similar techniques have been applied to other domains, other ressource-low languages. Replace word embeddings by clusters and neural models by whatever was in fashion 5 years ago and we can find more or less the same applied to Urdu or out-of-domain parsing. I liked this paper though, but I would have appreciated the authors to highlight more their contributions and position their work better within the literature.",
    "comments": "This paper presents a set of experiments designed a) to show the effectiveness of a neural parser  in a scarce resource scenario and b) to introduce a new data set of Creole English (from Singapour, called Singlish). While this data set is relatively small (1200 annotated sentences, used with 80k unlabeled sentences for word embeddings induction), the authors manage to present respectable results via interesting approach even though using features from relatively close languages are not unknown from the parsing community (see all the line of work on parsing Urdu/Hindi, on Arabic dialect using MSA based parsers, and so on). \nAssuming we can see Singlish as an extreme of Out-of-domain English and given all the set of experiments, I wonder why the authors didn’t try the classical technique on domain-adaptation, namely training with UD_EN+90% of the Singlish within a 10 cross fold experiment ? just so we can have another interesting baseline (with and without word embeddings, with bi-lingual embeddings if enough parallel data is available). \nI think that paper is interesting but I really would have appreciated more positioning regarding all previous work in parsing low-ressources languages and extreme domain adaptation. A table presenting some results for Irish and other very small treebanks would be nice. \nAlso how come the IAA is so low regarding the labeled relations?\n***************************************** Note after reading the authors' answer ***************************************** Thanks for your clarifications (especially for redoing the IAA evaluation). I raised my recommendation to 4, I hope it'll get accepted.",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "1b9a1c09f3ce3a96",
    "paper_id": "ACL_2017_433",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Singlish is a low-resource language. The NLP community needs more data for low resource languages, and the dataset accompanying this paper is a useful contribution. There is also relatively little NLP research on creoles, and the potential of using transfer-learning to analyze creoles, and this paper makes a nice contribution in that area.\nThe experimental setup used by the authors is clear. They provide convincing evidence that incorporating knowledge from an English-trained parser into a Singlish parser outperforms both an English-only parser and a Singlish-only parser on the Singlish data. They also provide a good overview of the relevant differences between English and Singlish for the purposes of syntactic parser and a useful analysis of how different parsing models handle these Singlish-specific constructions.",
    "weaknesses": "There are three main issues I see with this paper: -  There is insufficient comparison to the UD annotation of non-English languages. Many of the constructions they bring up as specific to Singlish are also present in other UD languages, and the annotations should ideally be consistent between Singlish and these languages.\n-  I'd like to see an analysis on the impact of training data size. A central claim of this paper is that using English data can improve performance on a low-resource language like Singlish. How much more Singlish data would be needed before the English data became unnecessary?\n-  What happens if you train a single POS/dep parsing model on the concatenated UD Web and Singlish datasets? This is much simpler than incorporating neural stacking. The case for neural stacking is stronger if it can outperform this baseline.",
    "comments": "Line 073: “POS taggers and dependency parsers perform poorly on such Singlish texts based on our observations” - be more clear that you will quantify this later. As such, it seems a bit hand-wavy.\nLine 169: Comparison to neural network models for multi-lingual parsing. As far as I can tell, you don't directly try the approach of mapping Singlish and English word embeddings into the same embedding space.\nLine 212: Introduction of UD Eng. At this point, it is appropriate to point out that the Singlish data is also web data, so the domain matches UD Eng.\nLine 245: “All borrowed words are annotated according to their original meanings”. Does this mean they have the same POS as in  the language from which they were borrowed? Or the POS of their usage in Singlish?\nFigure 2: Standard English glosses would be very useful in understanding the constructions and checking the correctness of the UD relations used.\nLine 280: Topic prominence: You should compare with the “dislocated” label in UD. From the UD paper: “The dislocated relation captures preposed (topics) and postposed elements”. The syntax you are describing sounds similar to a topic-comment-style syntax; if it is different, then you should make it clear how.\nLine 294: “Second, noun phrases used to modify the predicate with the presence of a preposition is regarded as a “nsubj” (nominal subject).” \nHere, I need a gloss to determine if this analysis makes sense. If the phrase is really being used to modify the predicate, then this should not be nsubj. UD makes a distinction between core arguments (nsubj, dobj, etc) and modifiers. If this is a case of modification, then you should use one of the modification relations, not a core argument relation. Should clarify the language here.\nLine 308: “In UD-Eng standards, predicative “be” is the only verb used as a copula, which often depends on its complement to avoid copular head.” This is an explicit decision made in UD, to increase parallelism with non-copular languages (e.g., Singlish). You should call this out. I think the rest of the discussion of copula handling is not necessary.\nLine 322: “NP deletion: Noun-phrase (NP) deletion often results in null subjects or objects.” This is common in other languages (zero-anaphora in e.g. Spanish, Italian, Russian, Japanese… )Would be good to point this out, and also point to how this is dealt with in UD in those languages (I believe the same way you handle it).\nLing 330: Subj/verb inversion is common in interrogatives in other languages (“Fue Marta al supermercado/Did Marta go to the supermarket?”). Tag questions are present in English (though perhaps are not as frequent). You should make sure that your analysis is consistent with these languages.\nSec 3.3 Data Selection and Annotation: The way you chose the Singlish sentences, of course an English parser will do poorly (they are chosen to be disimilar to sentences an English parser has seen before). But do you have a sense of how a standard English parser does overall on Singlish, if it is not filtered this way? How common are sentences with out-of-vocabulary terms or the constructions you discussed in 3.2?\nA language will not necessarily capture unusual sentence structure, particularly around long-distance dependencies. Did you investigate whether this method did a good job of capturing sentences with the grammatical differences to English you discussed in Section 3.2?\nLine 415: “the inter-annotator agreement has an unlabeled attachment score (UAS) of 85.30% and a labeled attachment score (LAS) of 75.72%.”\n-  What’s the agreement on POS tags? Is this integrated with LAS?\n-  Note that in Silveira et al 2014, which produced UD-Eng, they measured 94% inter-annotator agreement on a per-token basis. Why the discrepancy?\nPOS tagging and dep parsing sections: For both POS-tagging and dep parsing, I’d like to see some analysis on the effect of training set size. E.g., how much more Singlish data would be needed to train a POS tagger/dep parser entirely on Singlish and get the same accuracy as the stacked model?\nWhat happens if you just concatenate the datasets? E.g., train a model on a hybrid dataset of EN and Singlish, and see what the result is?\nLine 681: typo: “pre-rained” should be “pre-trained” 742 “The neural stacking model leads to the biggest improvement over nearly all categories except for a slightly lower yet competitive performance on “NP Deletion” cases” --- seems that the English data strongly biases the parser to expect an explicit subj/obj. you could try deleting subj/obj from some English sentences to improve performance on this construction.",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "98e83cb1f60eb6c3",
    "paper_id": "ACL_2017_435",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper develops an LSTM-based model for classifying connective uses for whether they indicate that a causal relation was intended. The guiding idea is that the expression of causal relations is extremely diverse and thus not amenable to syntactic treatment, and that the more abstract representations delivered by neural models are therefore more suitable as the basis for making these decisions.\nThe experiments are on the AltLex corpus developed by Hidley and McKeown. The results offer modest but consistent support for the general idea, and they provide some initial insights into how best to translate this idea into a model. The paper distribution includes the TensorFlow-based models used for the experiments.\nSome critical comments and questions: - The introduction is unusual in that it is more like a literature review than a full overview of what the paper contains. This leads to some redundancy with the related work section that follows it. I guess I am open to a non-standard sort of intro, but this one really doesn't work: despite reviewing a lot of ideas, it doesn't take a stand on what causation is or how it is expressed, but rather only makes a negative point (it's not reducible to syntax). We aren't really told what the positive contribution will be except for the very general final paragraph of the section.\n- Extending the above, I found it disappointing that the paper isn't really clear about the theory of causation being assumed. The authors seem to default to a counterfactual view that is broadly like that of David Lewis, where causation is a modal sufficiency claim with some other counterfactual conditions added to it. See line 238 and following; that arrow needs to be a very special kind of implication for this to work at all, and there are well-known problems with Lewis's theory (see http://bcopley.com/wp-content/uploads/CopleyWolff2014.pdf). There are comments elsewhere in the paper that the authors don't endorse the counterfactual view, but then what is the theory being assumed? It can't just be the temporal constraint mentioned on page 3!\n- I don't understand the comments regarding the example on line 256. The authors seem to be saying that they regard the sentence as false. If it's true, then there should be some causal link between the argument and the breakage. \nThere are remaining issues about how to divide events into sub-events, and these impact causal theories, but those are not being discussed here, leaving me confused.\n- The caption for Figure 1 is misleading, since the diagram is supposed to depict only the \"Pair_LSTM\" variant of the model. My bigger complaint is that this diagram is needlessly imprecise. I suppose it's okay to leave parts of the standard model definition out of the prose, but then these diagrams should have a clear and consistent semantics. What are all the empty circles between input and the \"LSTM\" boxes? The prose seems to say that the model has a look-up layer, a Glove layer, and then ... what? How many layers of representation are there? The diagram is precise about the pooling tanh layers pre-softmax, but not about this. I'm also not clear on what the \"LSTM\" boxes represent. It seems like it's just the leftmost/final representation that is directly connected to the layers above. I suggest depicting that connection clearly.\n- I don't understand the sentence beginning on line 480. The models under discussion do not intrinsically require any padding. I'm guessing this is a requirement of TensorFlow and/or efficient training. That's fine. If that's correct, please say that. I don't understand the final clause, though. How is this issue even related to the question of what is \"the most convenient way to encode the causal meaning\"? I don't see how convenience is an issue or how this relates directly to causal meaning.\n- The authors find that having two independent LSTMs (\"Stated_LSTM\") is somewhat better than one where the first feeds into the second. This issue is reminiscent of discussions in the literature on natural language entailment, where the question is whether to represent premise and hypothesis independently or have the first feed into the second. I regard this as an open question for entailment, and I bet it needs further investigation for causal relations too. \nSo I can't really endorse the sentence beginning on line 587: \"This behaviour means that our assumption about the relation between the meanings of the two input events does not hold, so it is better to encode each argument independently and then to measure the relation between the arguments by using dense layers.\" This is very surprising since we are talking about subparts of a sentence that might share a lot of information.\n- It's hard to make sense of the hyperparameters that led to the best performance across tasks. Compare line 578 with line 636, for example. Should we interpret this or just attribute it to the unpredictability of how these models interact with data?\n- Section 4.3 concludes by saying, of the connective 'which then', that the system can \"correctly disambiguate its causal meaning\", whereas that of Hidey and McKeown does not. That might be correct, but one example doesn't suffice to show it. To substantiate this point, I suggest making up a wide range of examples that manifest the ambiguity and seeing how often the system delivers the right verdict. This will help address the question of whether it got lucky with the example from table 8.",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "df343956608fa4f7",
    "paper_id": "ACL_2017_435",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper proposes a method for detecting causal relations between clauses, using neural networks (\"deep learning\", although, as in many studies, the networks are not particularly deep).  Indeed, while certain discourse connectives are unambiguous regarding the relation they signal (e.g. 'because' is causal) the paper takes advantage of a recent dataset (called AltLex, by Hidey and McKeown, 2016) to solve the task of identifying causal vs. non-causal relations when the relation is not explicitly marked.  Arguing that convolutional networks are not as adept as representing the relevant features of clauses as LSTMs, the authors propose a classification architecture which uses a Glove-based representation of clauses, input in an LSTM layer, followed by three densely connected layers (tanh) and a final decision layer with a softmax.\nThe best configuration of the system improves by 0.5-1.5% F1 over Hidey and MCkeown's 2016 one (SVM classifier).  Several examples of generalizations where the system performs well are shown (indicator words that are always causal in the training data, but are found correctly to be non causal in the test data). \nTherefore, I appreciate that the system is analyzed qualitatively and  quantitatively.\nThe paper is well written, and the description of the problem is particularly clear. However a clarification of the differences between this task and the  task of implicit connective recognition would be welcome.  This could possibly  include a discussion of why previous methods for implicit connective  recognition cannot be used in this case.\nIt is very appreciable that the authors uploaded their code to the submission site (I inspected it briefly but did not execute it).  Uploading the (older) data (with the code) is also useful as it provides many examples.  It was not clear to me what is the meaning of the 0-1-2 coding in the TSV files, given that the paper mentions binary classification. I wonder also, given that this is the data from Hidey and McKeown, if the authors have the right to repost it as they do.  -- One point to clarify in the paper would be the meaning of \"bootstrapping\", which apparently extends the corpus by about 15%: while the construction of the corpus is briefly but clearly explained in the paper, the additional bootstrapping is not.  While it is certainly interesting to experiment with neural networks on this task, the merits of the proposed system are not entirely convincing.  It seems indeed that the best configuration (among 4-7 options) is found on the test data, and it is this best configuration that is announced as improving over Hidey by \"2.13% F1\".  However, a fair comparison would involve selecting the best configuration on the devset.\nMoreover, it is not entirely clear how significant the improvement is. On the one hand, it should be possible, given the size of the dataset, to compute some statistical significance indicators.  On the other hand, one should consider also the reliability of the gold-standard annotation itself (possibly from the creators of the dataset).  Upon inspection, the annotation obtained from the English/SimpleEnglish Wikipedia is not perfect, and therefore the scores might need to be considered with a grain of salt.\nFinally, neural methods have been previously shown to outperform human engineered features for binary classification tasks, so in a sense the results  are rather a confirmation of a known property. It would be interesting to see experiments with simpler networks used as baselines, e.g. a 1-layer LSTM.  The analysis of results could try to explain why the neural method seems to favor  precision over recall.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "59e2d8e2ee4f41d6",
    "paper_id": "ACL_2017_440",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper presents an extension to A* CCG parsing to include dependency information.  Achieving this while maintaining speed and tractability is a very impressive feature of this approach.  The ability to precompute attachments is a nice trick.                  I also really appreciated the evaluation of the effect of the head-rules on normal-form violations and would love to see more details on the remaining cases.",
    "weaknesses": "I'd like to see more analysis of certain dependency structures.  I'm particularly interested in how coordination and relative clauses are handled when the predicate argument structure of CCG is at odds with the dependency structures normally used by other dependency parsers.",
    "comments": "I'm very happy with this work and feel it's a very nice contribution to the literature.  The only thing missing for me is a more in-depth analysis of the types of constructions which saw the most improvement (English and Japanese) and a discussion (mentioned above) reconciling Pred-Arg dependencies with those of other parsers.",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "28ac7893da81a381",
    "paper_id": "ACL_2017_440",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper describes a state-of-the-art CCG parsing model that decomposes into tagging and dependency scores, and has an efficient A* decoding algorithm. \nInterestingly, the paper slightly outperforms Lee et al. (2016)'s more expressive global parsing model, presumably because this factorization makes learning easier. It's great that they also report results on another language, showing large improvements over existing work on Japanese CCG parsing. One surprising original result is that modeling the first word of a constituent as the head substantially outperforms linguistically motivated head rules.  Overall this is a good paper that makes a nice contribution. I only have a few suggestions: - I liked the way that the dependency and supertagging models interact, but it would be good to include baseline results for simpler variations (e.g. not conditioning the tag on the head dependency).\n- The paper achieves new state-of-the-art results on Japanese by a large margin. However, there has been a lot less work on this data - would it also be possible to train the Lee et al. parser on this data for comparison?\n- Lewis, He and Zettlemoyer (2015) explore combined dependency and supertagging models for CCG and SRL, and may be worth citing.",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "f401e1e591a7839f",
    "paper_id": "ACL_2017_444",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper presents evaluation metrics for lyrics generation exploring the need for the lyrics to be original,but in a similar style to an artist whilst being fluent and co-herent. The paper is well written and the motivation for the metrics are well explained.   The authors describe both hand annotated metrics (fluency, co-herence and match) and an automatic metric for ‘Similarity'. Whilst the metric for Similarity is unique and interesting the paper does not give any evidence of this as an effective automatic metric as correlations between this metric and the others are low, (which they say that they should be used separately). The authors claim it can be used to meaningfully analyse system performance but we have to take their word for it as again there is no correlation with any hand-annotated performance metric.  Getting worse scores than a baseline system isn’t evidence that the metric captures quality (e.g. you could have a very strong baseline).\nSome missing references, e.g. recent work looking at automating co-herence, e.g. using mutual information density (e.g. Li et al. 2015). In addition, some reference to style matching from the NLG community are missing (e.g. Dethlefs et al. 2014 and the style matching work by Pennebaker).",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "1dfbb2be289dcdd0",
    "paper_id": "ACL_2017_444",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper studies how to properly evaluate systems that produce ghostwriting of rap lyrics. \nThe authors present manual evaluation along three key aspects: fluency, coherence, and style matching. \nThey also introduce automatic metrics that consider uniqueness via maximum training similarity, and stylistic similarity via rhyme density.\nI can find some interesting analysis and discussion in the paper. \nThe way for manually evaluating style matching especially makes sense to me.\nThere also exist a few important concerns for me.\nI am not convinced about the appropriateness of only doing fluency/coherence ratings at line level. \nThe authors mention that they are following Wu (2014), but I find that work actually studying a different setting of hip hop lyrical challenges and responses, which should be treated at line level in nature. \nWhile in this work, a full verse consists of multiple lines that normally should be topically and structurally coherent. \nCurrently I cannot see any reason why not to evaluate fluency/coherence for a verse as a whole.\nAlso, I do not reckon that one should count so much on automatic metrics, if the main goal is to ``generate similar yet unique lyrics''. \nFor uniqueness evaluation, the calculations are performed on verse level. \nHowever, many rappers may only produce lyrics within only a few specific topics or themes. \nIf a system can only extract lines from different verses, presumably we might also get a fluent, coherent verse with low verse level similarity score, but we can hardly claim that the system ``generalizes'' well. \nFor stylistic similarity with the specified artist, I do not think rhyme density can say it all, as it is position independent and therefore may not be enough to reflect the full information of style of an artist.\nIt does not seem that the automatic metrics have been verified to be well correlated with corresponding real manual ratings on uniqueness or stylistic matching. \nI also wonder if one needs to evaluate semantic information commonly expressed by a specified rapper as well, other than only caring about rhythm.\nMeanwhile, I understand the motivation for this study is the lack of *sound* evaluation methodology. \nHowever, I still find one statement particularly weird: ``our methodology produces a continuous numeric score for the whole verse, enabling better comparison.'' \nIs enabling comparisons really more important than making slightly vague but more reliable, more convincing judgements?\nMinor issue: Incorrect quotation marks in Line 389",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "2e34963d90fb15be",
    "paper_id": "ACL_2017_444",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper proposes to present a more comprehensive evaluation methodology for the assessment of automatically generated rap lyrics (as being similar to a target artist).  While the assessment of the generation of creative work is very challenging and of great interest to the community, this effort falls short of its claims of a comprehensive solution to this problem.\nAll assessment of this nature ultimately falls to a subjective measure -- can the generated sample convince an expert that the generated sample was produced by the true artist rather than an automated preocess?  This is essentially a more specific version of a Turing Test.   The effort to automate some parts of the evaluation to aid in optimization and to understand how humans assess artistic similarity is valuable.  However, the specific findings reported in this work do not encourage a belief that these have been reliably identified.\nSpecifically -- Consider the central question: Was a sample generated by a target artist?        The human annotators who were asked this were not able to consistently respond to this question.        This means either 1) the annotators did not have sufficient expertise to perform the task, or 2) the task was too challenging, or some combination of the two.   The proposed automatic measures also failed to show a reliable agreement to human raters performing the same task.        This dramatically limits their efficacy in providing a proxy for human assessment.   The low interannotator agreement may be \"expected\" because the task is subjective, but the idea of decomposing the evaluation into fluency and coherence components is meant to make it more tractable, and thereby improve the consistency of rater scores.  A low IAA for an evaluation metric is a cause for concern and limits its viability as a general purpose tool.   Specific questions/comments: - Why is a line-by-line level evaluation prefered to a verse level analysis. \nSpecifically for \"coherence\", a line by line analysis limits the scope of coherence to consequtive lines.\n- Style matching -- This term assumes that these 13 artists each have a distinct style, and always operate in that style. I would argue that some of these artists (kanye west, eminem, jay z, drake, tupac and notorious big) have produced work in multiple styles.  A more accurate term for this might be \"artist matching\".\n- In Section 4.2 The central automated component of the evaluation is low tf*idf with existing verses, and similar rhyme density.  Given the limitations of rhyme density -- how well does this work.  Even with the manual intervention described?\n- In Section 6.2 -- This description should include how many judges were used in this study? In how many cases did the judges already know the verse they were judging?  In this case the test will not assess how easy it is to match style, but rather, the judges recall and rap knowledge.",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "635651fe677010e7",
    "paper_id": "ACL_2017_447",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper proposed to explore discourse structure, as defined by Rhetorical Structure Theory (RST) to improve text categorization. A RNN with attention mechanism is employed to compute a representation of text. The experiments on various of dataset shows the effectiveness of the proposed method. Below are my comments: (1) From Table 2, it shows that “UNLABELED” model performs better on four out of five datasets than the “FULL” model. The authors should explain more about this, because intuitively, incorporating additional relation labels should bring some benefits. Is the performance of relation labelling so bad and it hurts the performance instead?\n(2) The paper also transforms the RST tree into a dependency structure as a pre-process step. Instead of transforming, how about keep the original tree structure and train a hierarchical model on that?\n(3) For the experimental datasets, instead of comparing with only one dataset with each of the previous work, the authors may want to run experiments on more common datasets used by previous work.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "ada18fdc8c346799",
    "paper_id": "ACL_2017_447",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The main strength of this paper is the incorporation of discourse structure in the DNN's attention model, which allows the model to learn the weights given to different EDUs.\nAlso the paper is very clear, and provides a good explanation of both RST and how it is used in the model. \nFinally, the evaluation experiments are conducted thoroughly with strong, state-of-the-art baselines.",
    "weaknesses": "The main weakness of the paper is that the results do not strongly support the main claim that discourse structure can help text classification. Even the UNLABELED variant, which performs best and does outperform the state of the art, only provides minimal gains (and hurts in the legal/bills domain). The approach (particularly the FULL variant) seems to be too data greedy but no real solution is provided to address this beyond the simpler UNLABELED and ROOT variants.",
    "comments": "In general, this paper feels like a good first shot at incorporating discourse structure into DNN-based classification, but does not fully convince that RST-style structure will significantly boost performance on most tasks (given that it is also very costly to build a RST parser for a new domain, as would be needed in the legal/bill domains described in this paper). I wish the authors had explored or at least mentioned next steps in making this approach work, in particular in the face of data sparsity. For example, how about defining (task-independent) discourse embeddings? Would it be possible to use a DNN for discourse parsing that could be incorporated in the main task DNN and optimized jointly  end-to-end? Again, this is good work, I just wish the authors had pushed it a little further given the mixed results.",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "b2a7763fc97383d7",
    "paper_id": "ACL_2017_462",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The approach described in the manuscript outperformed the previous approaches and achieved the state-of-the-art result.\nRegarding data, the method used the combination of market and text data.\nThe approach used word embeddings to define the weight of each lexicon term by extending it to the similar terms in the document.",
    "weaknesses": "Deep-learning based methods were known to be able to achieve relatively good performances without much feature engineering in sentimental analysis. More literature search is needed to compare with the related works would be better.\nThe approach generally improved performance by feature-based methods without much novelty in model or proposal of new features.",
    "comments": "The manuscript described an approach in sentimental analysis. The method used a relatively new method of using word embeddings to define the weight of each lexicon term. However, the novelty is not significant enough.",
    "overall_score": "2",
    "confidence": "3"
  },
  {
    "review_id": "cb027365142a972f",
    "paper_id": "ACL_2017_462",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- Weaknesses: - General Discussion: This paper investigates sentiment signals in  companies’ annual 10-K filing reports to forecast volatility.  The authors evaluate information retrieval term weighting models which are seeded with a finance-oriented sentiment lexicon and expanded with word embeddings. PCA is used to reduce dimensionality before Support Vector Regression is applied for similarity estimation.\nIn addition to text-based features, the authors also use non-text-based market features (e.g. sector information and volatility estimates).\nMultiple fusion methods to combine text features with market features are evaluated.\nCOMMENTS It would be interesting to include two more experimental conditions, namely 1) a simple trigram SVM which does not use any prior sentiment lexica, and 2) features that reflect delta-IDFs scores for individual features. \nAs an additional baseline, it would be good to see binary features.\nThis paper could corroborate your references: https://pdfs.semanticscholar.org/57d6/29615c19caa7ae6e0ef2163eebe3b272e65a.pdf",
    "weaknesses": "- General Discussion: This paper investigates sentiment signals in  companies’ annual 10-K filing reports to forecast volatility.  The authors evaluate information retrieval term weighting models which are seeded with a finance-oriented sentiment lexicon and expanded with word embeddings. PCA is used to reduce dimensionality before Support Vector Regression is applied for similarity estimation.\nIn addition to text-based features, the authors also use non-text-based market features (e.g. sector information and volatility estimates).\nMultiple fusion methods to combine text features with market features are evaluated.\nCOMMENTS It would be interesting to include two more experimental conditions, namely 1) a simple trigram SVM which does not use any prior sentiment lexica, and 2) features that reflect delta-IDFs scores for individual features. \nAs an additional baseline, it would be good to see binary features.\nThis paper could corroborate your references: https://pdfs.semanticscholar.org/57d6/29615c19caa7ae6e0ef2163eebe3b272e65a.pdf",
    "comments": "This paper investigates sentiment signals in  companies’ annual 10-K filing reports to forecast volatility.  The authors evaluate information retrieval term weighting models which are seeded with a finance-oriented sentiment lexicon and expanded with word embeddings. PCA is used to reduce dimensionality before Support Vector Regression is applied for similarity estimation.\nIn addition to text-based features, the authors also use non-text-based market features (e.g. sector information and volatility estimates).\nMultiple fusion methods to combine text features with market features are evaluated.\nCOMMENTS It would be interesting to include two more experimental conditions, namely 1) a simple trigram SVM which does not use any prior sentiment lexica, and 2) features that reflect delta-IDFs scores for individual features. \nAs an additional baseline, it would be good to see binary features.\nThis paper could corroborate your references: https://pdfs.semanticscholar.org/57d6/29615c19caa7ae6e0ef2163eebe3b272e65a.pdf",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "a10be5d16bf47149",
    "paper_id": "ACL_2017_467",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper presents an iterative method to induce bilingual word embeddings using large monolingual corpora starting with very few (or automatically obtainable numeral) mappings between two languages. Compared to state-of-the-art using larger bilingual dictionaries or parallel/comparable corpora, the results obtained with the presented method that relies on very little or no manually prepared input are exciting and impressive.",
    "weaknesses": "I would have liked to see a discussion on the errors of the method, and possibly a discussion on how the method could be adjusted to deal with them.",
    "comments": "Does the frequency of the seeds in the monolingual corpora matter?\nIt would be interesting to see the partial (in the sense of after n number of iterations) evolution of the mapping between words in the two languages for a few words.  What happens with different translations of the same word (like different senses)?\nOne big difference between German and English is the prevalence of compounds in German. What happens to these compounds? What are they mapped onto? Would a preprocessing step of splitting the compounds help? ( using maybe only corpus-internal unigram information) What would be the upper bound for such an approach? An analysis of errors -- e.g. words very far from their counterpart in the other language -- would be very interesting. It would also be interesting to see a discussion of where these errors come from, and if they could be addressed with the presented approach.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "b780710271b604e1",
    "paper_id": "ACL_2017_467",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This work proposes a self-learning bootstrapping approach to learning bilingual word embeddings, which achieves competitive results in tasks of bilingual lexicon induction and cross-lingual word similarity although it requires a minimal amount of bilingual supervision: the method leads to competitive performance even when the seed dictionary is extremely small (25 dictionary items!) or is constructed without any language pair specific information (e.g., relying on numerals shared between languages).  The paper is very well-written, admirably even so. I find this work 'eclectic' in a sense that its original contribution is not a breakthrough finding (it is more a 'short paper idea' in my opinion), but it connects the dots from prior work drawing inspiration and modelling components from a variety of previous papers on the subject, including the pre-embedding work on self-learning/bootstrapping (which is not fully recognized in the current version of the paper). I liked the paper in general, but there are few other research questions that could/should have been pursued in this work. These, along with only a partial recognition of related work and a lack of comparisons with several other relevant baselines, are my main concern regarding this paper, and they should be fixed in the updated version(s).\n*Self-learning/bootstrapping of bilingual vector spaces: While this work is one of the first to tackle this very limited setup for learning cross-lingual embeddings (although not the first one, see Miceli Barone and more works below), this is the first truly bootstrapping/self-learning approach to learning cross-lingual embeddings. However, this idea of bootstrapping bilingual vector spaces is not new at all (it is just reapplied to learning embeddings), and there is a body of work which used exactly the same idea with traditional 'count-based' bilingual vector spaces. I suggest the authors to check the work of Peirsman and Pado (NAACL 2010) or Vulic and Moens (EMNLP 2013), and recognize the fact that their proposed bootstrapping approach is not so novel in this domain. There is also related work of Ellen Riloff's group on bootstrapping semantic lexicons in monolingual settings.\n*Relation to Artetxe et al.: I might be missing something here, but it seems that the proposed bootstrapping algorithm is in fact only an iterative approach which repeatedly utilises the previously proposed model/formulation of Artetxe et al. The only difference is the reparametrization (line 296-305). It is not clear to me whether the bootstrapping approach draws its performance from this reparametrization (and whether it would work with the previous parametrization), or the performance is a product of both the algorithm and this new parametrization. Perhaps a more explicit statement in the text is needed to fully understand what is going on here.\n*Comparison with prior work: Several very relevant papers have not been mentioned nor discussed in the current version of the paper. For instance, the recent work of Duong et al. (EMNLP 2016) on 'learning crosslingual word embeddings without bilingual corpora' seems very related to this work (as the basic word overlap between the two titles reveals!), and should be at least discussed if not compared to. Another work which also relies on mappings with seed lexicons and also partially analyzes the setting with only a few hundred seed lexicon pairs is the work of Vulic and Korhonen (ACL 2016) 'on the role of seed lexicons in learning bilingual word embeddings': these two papers might also help the authors to provide more details for the future work section (e.g., the selection of reliable translation pairs might boost the performance further during the iterative process). Another very relevant work has appeared only recently: Smith et al. (ICLR 2017) discuss 'offline bilingual word vectors, orthogonal transformations and the inverted softmax'. This paper also discusses learning bilingual embeddings in very limited settings (e.g., by relying only on shared words and cognates between two languages in a pair). As a side note, it would be interesting to report results obtained using only shared words between the languages (such words definitely exist for all three language pairs used in the experiments). This would also enable a direct comparison with the work of Smith et al. (ICLR 2017) which rely on this setup.\n*Seed dictionary size and bilingual lexicon induction: It seems that the proposed algorithm (as discussed in Section 5) is almost invariant to the starting seed lexicon, yielding very similar final BLI scores regardless of the starting point. While a very intriguing finding per se, this also seems to suggest an utter limitation of the current 'offline' approaches: they seem to have hit the ceiling with the setup discussed in the paper; Vulic and Korhonen (ACL 2016) showed that we cannot really improve the results by simply collecting more seed lexicon pairs, and this work suggests that any number of starting pairs (from 25 to 5k) is good enough to reach this near-optimal performance, which is also very similar to the numbers reported by Dinu et al. (arXiv 2015) or Lazaridou et al. (ACL 2015). I would like to see more discussion on how to break this ceiling and further improve BLI results with such 'offline' methods. Smith et al. (ICLR 2017) seem to report higher numbers on the same dataset, so again it would be very interesting to link this work to the work of Smith et al. In other words, the authors state that in future work they plan to fine-tune the method so that it can learn without any bilingual evidence. This is an admirable 'philosophically-driven' feat, but from a more pragmatic point of view, it seems more pragmatic to detect how we can go over the plateau/ceiling which seems to be hit with these linear mapping approaches regardless of the number of used seed lexicon pairs (Figure 2).\n*Convergence criterion/training efficiency: The convergence criterion, although crucial for the entire algorithm, both in terms of efficiency and efficacy, is mentioned only as a side note, and it is not entirely clear how the whole procedure terminates. I suspect that the authors use the vanishing variation in crosslingual word similarity performance as the criterion to stop the procedure, but that makes the method applicable only to languages which have a cross-lingual word similarity dataset. I might be missing here given the current description in the paper, but I do not fully understand how the procedure stops for Finnish, given that there is no crosslingual word similarity dataset for English-Finnish.\n*Minor: - There is a Finnish 'Web as a Corpus' (WaC) corpus (lines 414-416): https://www.clarin.si/repository/xmlui/handle/11356/1074 - Since the authors claim that the method could work with a seed dictionary containing only shared numerals, it would be very interesting to include an additional language pair which does not share the alphabet (e.g., English-Russian, English-Bulgarian or even something more distant such as Arabic and/or Hindi).\n*After the response: I would like to thank the authors for investing their time into their response which helped me clarify some doubts and points raised in my initial review. I hope that they would indeed clarify these points in the final version, if given the opportunity.",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "0ec31064d1116c5a",
    "paper_id": "ACL_2017_467",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "is that the seed lexicon is directly encoded in the learning process as a binary matrix. Then the self-learning framework solves a global optimization problem in which the seed lexicon is not explicitly involved. Its role is to establish the initial mapping between the two embeddings. This guarantees the convergence. The initial seed lexicon could be quite small (25 correspondences).\nThe small size of the seed lexicon is appealing for mappings between languages for which there are not large bilingual lexicons.\nIt will be good to evaluate the framework with respect to the quality of the two word embeddings. If we have languages (or at least one of the languages) with scarce language resources then the word embeddings for both languages could differ in their structure and coverage. I think it could be simulated on the basis of the available data via training the corresponding word embeddings on different subcorpora for each language.",
    "weaknesses": "",
    "comments": "",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "2e5966860f616b8e",
    "paper_id": "ACL_2017_477",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "i. Motivation is well described. \nii. Provides detailed comparisons with various models across diverse languages",
    "weaknesses": "i.          The conclusion is biased by the selected languages. \nii.           The experiments do not cover the claim of this paper completely.",
    "comments": "This paper issues a simple but fundamental question about word representation: what subunit of a word is suitable to represent morphologies and how to compose the units. To answer this question, this paper applied word representations with various subunits (characters, character-trigram, and morphs) and composition functions (LSTM, CNN, and a simple addition) to the language modeling task to find the best combination. In addition, this paper evaluated the task for more than 10 languages. This is because languages are typologically diverse and the results can be different according to the word representation and composition function. From their experimental results, this paper concluded that character-level representations are more effective, but they are still imperfective in comparing them with a model with explicit knowledge of morphology. Another conclusion is that character-trigrams show reliable perplexity in the majority of the languages.  However, this paper leaves some issues behind.\n-         First of all, there could be some selection bias of the experimental languages. This paper chose ten languages in four categories (up to three languages per a category). But, one basic question with the languages is “how can it be claimed that the languages are representatives of each category?” \nAll the languages in the same category have the same tendency of word representation and composition function? How can it be proved? For instance, even in this paper, two languages belonging to the same typology (agglutinative) show different results. Therefore, at least to me, it seems to be better to focus on the languages tested in this paper instead of drawing a general conclusions about all languages.  -         There is some gap between the claim and the experiments. Is the language modeling the best task to prove the claim of this paper? Isn’t there any chance that the claim of this paper breaks in other tasks? Further explanation on this issue is needed.\n-         In Section 5.2, this paper evaluated the proposed method only for Arabic. Is there any reason why the experiment is performed only for Arabic? \nThere are plenty of languages with automatic morphological analyzers such as Japanese and Turkish.\n-         This paper considers only character-trigram among various n-grams. Is there any good reason to choose only character-trigram? Is it always better than character-bigram or character-fourgram? In general, language modeling with n-grams is affected by corpus size and some other factors.  Minor typos:  - There is a missing reference in Introduction. ( 88 line in Page 1) - root-and-patter -> root-and-pattern (524 line in Page 6)",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "58ee94f7f99940e3",
    "paper_id": "ACL_2017_477",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "1) A comprehensive overview of different approaches and architectures towards sub-word level modelling, with numerous experiments designed to support the core claim that the best results come from modelling morphemes.\n2) The authors introduce a novel form of sub-word modelling based on character tri-grams and show it outperforms traditional approaches on a wide variety of languages.\n3) Splitting the languages examined by typology and examining the effects of the models on various typologies is a welcome introduction of linguistics into the world of language modelling.\n4) The analysis of perplexity reduction after various classes of words in Russian and Czech is particularly illuminating, showing how character-level and morpheme-level models handle rare words much more gracefully. In light of these results, could the authors say something about how much language modelling requires understanding of semantics, and how much it requires just knowing various morphosyntactic effects?",
    "weaknesses": "1) The character tri-gram LSTM seems a little unmotivated. Did the authors try other character n-grams as well? As a reviewer, I can guess that character tri-grams roughly correspond to morphemes, especially in Semitic languages, but what made the authors report results for 3-grams as opposed to 2- or 4-? In addition, there are roughly 26^3=17576 possible distinct trigrams in the Latin lower-case alphabet, which is enough to almost constitute a word embedding table. Did the authors only consider observed trigrams? How many distinct observed trigrams were there?\n2) I don't think you can meaningfully claim to be examining the effectiveness of character-level models on root-and-pattern morphology if your dataset is unvocalised and thus doesn't have the 'pattern' bit of 'root-and-pattern'. I appreciate that finding transcribed Arabic and Hebrew with vowels may be challenging, but it's half of the typology.\n3) Reduplication seems to be a different kind of phenomenon to the other three, which are more strictly morphological typologies. Indonesian and Malay also exhibit various word affixes, which can be used on top of reduplication, which is a more lexical process. I'm not sure splitting it out from the other linguistic typologies is justified.",
    "comments": "1) The paper was structured very clearly and was very easy to read.\n2) I'm a bit puzzled about why the authors chose to use 200 dimensional character embeddings. Once the dimensionality of the embedding is greater than the size of the vocabulary (here the number of characters in the alphabet), surely you're not getting anything extra?\n------------------------------- Having read the author response, my opinions have altered little. I still think the same strengths and weakness that I have already discussed hold.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "a957cd98165d1bbb",
    "paper_id": "ACL_2017_481",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "In this work, the authors extend MS-COCO by adding an incorrect caption to each existing caption, with only one word of difference. \nThe authors demonstrate that two state-of-the-art methods (one for VQA and one for captioning) perform extremely poorly at a) determining if a caption is fake, b) determining which word in a fake caption is wrong, and c) selecting a replacement word for a given fake word.\nThis work builds upon a wealth of literature regarding the underperformance of vision/language models relative to their apparent capacities. I think this work makes concrete some of the big, fundamental questions in this area: are vision/language models doing \"interesting\" things, or not? The authors consider a nice mix of tasks and models to shed light on the \"broken-ness\" of these settings, and perform some insightful analyses of factors associated with model failure (e.g., Figure 3).\nMy biggest concerns with the paper are similarity to Ding et al. That being said, I do think the authors make some really good points; Ding et al. generate similar captions, but the ones here differ by only one word and *still* break the models -- I think that's a justifiably fundamental difference. That observation demonstrates that Ding et al.'s engineering is not a requirement, as this simple approach still breaks things catastrophically.\nAnother concern is the use of NeuralTalk to select the \"hardest\" foils.              While a clever idea, I am worried that the use of this model creates a risk of self-reinforcement bias, i.e., NeuralTalk's biases are now fundamentally \"baked-in\" to FOIL-COCO.  I think the results section could be a bit longer, relative to the rest of the paper (e.g. I would've liked more than one paragraph -- I liked this part!)\nOverall, I do like this paper, as it nicely builds upon some results that highlight defficiencies in vision/language integration. In the end, the Ding et al. similarity is not a \"game-breaker,\" I think -- if anything, this work shows that vision/language models are so easy to fool, Ding et al.'s method is not even required.\nSmall things: I would've liked to have seen another baseline that simply concatenates BoW + extracted CNN features and trains a softmax classifier over them. The \"blind\" model is a nice touch, but what about a \"dumb\" vision+langauge baseline? I bet that would do close to as well as the LSTM/Co-attention. That could've made the point of the paper even stronger.\n330: What is a supercategory? Is this from WordNet? Is this from COCO? \nI understand the idea, but not the specifics.\n397: has been -> were 494: that -> than 693: artefact -> undesirable artifacts (?)\n701: I would have included a chance model in T1's table -- is 19.53% [Line 592] a constant-prediction baseline? Is it 50% (if so, can't we flip all of the \"blind\" predictions to get a better baseline?) I am not entirely clear, and I think a \"chance\" line here would fix a lot of this confusion.\n719: ariplane ~~ After reading the author response... I think this author response is spot-on. Both my concerns of NeuralTalk biases and additional baselines were addressed, and I am confident that these can be addressed in the final version, so I will keep my score as-is.",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "047958c21b32c605",
    "paper_id": "ACL_2017_483",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This is the first neural network-based approach to argumentation mining. The proposed method used a Pointer Network (PN) model with multi-task learning and outperformed previous methods in the experiments on two datasets.",
    "weaknesses": "This is basically an application of PN to argumentation mining. Although the combination of PN and multi-task learning for this task is novel, its novelty is not enough for ACL long publication. The lack of qualitative analysis and error analysis is also a major concern.",
    "comments": "Besides the weaknesses mentioned above, the use of PN is not well-motivated. Although three characteristics of PN were described in l.138-143, these are not a strong motivation against the use of bi-directional LSTMs and the attention mechanism. The authors should describe what problems are solved by PN and discuss in the experiments how much these problems are solved.\nFigures 2 and 3 are difficult to understand. What are the self link to D1 and the links from D2 to E1 and D3/D4 to E2? These are just the outputs from the decoder and not links. The decoder LSTM does not have an input from e_j in these figures, but it does in Equation (3). Also, in Figure 3, the abbreviation \"FC\" is not defined.\nEquation (8) is strange. To calculate the probability of each component type, the probability of E_i is calculated.\nIn the experiments, I did not understand why only \"PN\", which is not a joint model, was performed for the microtext corpus.\nIt is not clear whether the BLSTM model is trained with the joint-task objective.\nThere are some studies on discourse parsing using the attention mechanism. The authors should describe the differences from these studies.\nMinor issues: l.128: should related -> should be related l.215: (2015) is floating l.706: it able -> it is able I raised my recommendation score after reading the convincing author responses. \nI strongly recommend that the authors should discuss improved examples by PN as well as the details of feature ablation.",
    "overall_score": "3",
    "confidence": "5"
  },
  {
    "review_id": "c83379c74ba6e328",
    "paper_id": "ACL_2017_483",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- Thorough review of prior art in the specific formulation of argument mining handled in this paper.\n- Simple and effective modification of an existing model to make it suitable for the task. The model is mostly explained clearly.\n- Strong results as compared to prior art in this task.",
    "weaknesses": "- 071: This formulation of argumentation mining is just one of several proposed subtask divisions, and this should be mentioned. For example, in [1], claims are detected and classified before any supporting evidence is detected. \nFurthermore, [2] applied neural networks to this task, so it is inaccurate to say (as is claimed in the abstract of this paper) that this work is the first NN-based approach to argumentation mining.\n- Two things must be improved in the presentation of the model: (1) What is the pooling method used for embedding features (line 397)? and (2) Equation (7) in line 472 is not clear enough: is E_i the random variable representing the *type* of AC i, or its *identity*? Both are supposedly modeled (the latter by feature representation), and need to be defined. Furthermore, it seems like the LHS of equation (7) should be a conditional probability.\n- There are several unclear things about Table 2: first, why are the three first baselines evaluated only by macro f1 and the individual f1 scores are missing? \nThis is not explained in the text. Second, why is only the \"PN\" model presented? Is this the same PN as in Table 1, or actually the Joint Model? What about the other three?\n- It is not mentioned which dataset the experiment described in Table 4 was performed on.",
    "comments": "",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "9277a0e4932f7f75",
    "paper_id": "ACL_2017_484",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "It provides a solid work of hybrid CTC-attention framework in training and decoding, and the experimental results showed that the proposed method could provide an improvement in Japanese CSJ and Mandarin Chinese telephone speech recognition task.",
    "weaknesses": "The only problem is that the paper sounds too similar with Ref [Kim et al., 2016] which will be officially published in the coming IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), March 2017. \nKim at al., 2016, proposes joint CTC-attention using MTL for English ASR task, and this paper proposes joint CTC-attention using MTL+joint decoding for Japanese and Chinese ASR tasks. I guess the difference is on joint decoding and the application to Japanese/Chinese ASR tasks. However, the difference is not clearly explained by the authors. So it took sometimes to figure out the original contribution of this paper.\n(a) Title:  The title in Ref [Kim et al., 2016] is “Joint CTC- Attention Based End-to-End Speech Recognition Using Multi-task Learning”, while the title of this paper is “Joint CTC-attention End-to-end Speech Recognition”. I think the title is too general. If this is the first paper about \"Joint CTC-attention\" than it is absolutely OK. Or if Ref [Kim et al., 2016] will remain only as pre-published arXiv, then it might be still acceptable. But since [Kim et al., 2016] will officially publish in IEEE conference, much earlier than this paper, then a more specified title that represents the main contribution of this paper in contrast with the existing publication would be necessary.  (b) Introduction: The author claims that “We propose to take advantage of the constrained CTC alignment in a hybrid CTC-attention based system. During training, we attach a CTC objective to an attention-based encoder network as a regularization, as proposed by [Kim at al., 2016].“ Taking advantage of the constrained CTC alignment in a hybrid CTC-attention is the original idea from [Kim at al., 2016]. So the whole argument about attention-based end-to-end ASR versus CTC-based ASR, and the necessary of CTC-attention combination is not novel. \nFurthermore, the statement “we propose … as proposed by [Kim et al, 2016]” is somewhat weird. We can build upon someone proposal with additional extensions, but not just re-propose other people's proposal. Therefore, what would be important here is to state clearly the original contribution of this paper and the position of the proposed method with respect to existing literature (c) Experimental Results: Kim at al., 2016 applied the proposed method on English task, while this paper applied the proposed method on Japanese and Mandarin Chinese tasks. I think it would be interesting if the paper could explain in more details about the specific problems in Japanese and Mandarin Chinese tasks that may not appear in English task. For example, how the system could address multiple possible outputs. i.e., Kanji, Hiragana, and Katakana given Japanese speech input without using any linguistic resources. This could be one of the important contributions from this paper.",
    "comments": "I think it would be better to cite Ref [Kim et al., 2016] from the official IEEE ICASSP conference, rather than pre-published arXiv: Kim, S., Hori, T., Watanabe, S., \"Joint CTC- Attention Based End-to-End Speech Recognition Using Multi-task Learning\", IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), March 2017, pp. to appear.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "c6e7c76d478343a8",
    "paper_id": "ACL_2017_484",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper identifies several problems stemming from the flexibility offered by the attention mechanism and shows that by combining the seq2seq network with CTC the problems are mitigated.",
    "weaknesses": "The paper is an incremental improvement over Kim et al. 2016 (since two models are trained, their outputs can just as well be ensembled). However, it is nice to see that such a simple change offers important performance improvements of ASR systems.",
    "comments": "A lot of the paper is spent on explaining the well-known, classical ASR systems. A description of the core improvement of the paper (better decoding algorithm) starts to appear only on p. 5.  The description of CTC is nonstandard and maybe should either be presented in a more standard way, or the explanation should be expanded. Typically, the relation p(C|Z) (eq. 5) is deterministic - there is one and only one character sequence that corresponds to the blank-expanded form Z. I am also unsure about the last transformation of the eq. 5.",
    "overall_score": "3",
    "confidence": "5"
  },
  {
    "review_id": "d2a7808b050ba5bc",
    "paper_id": "ACL_2017_489",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "1) The problem they tackle I find extremely interesting; as they argue, REG is a problem that had previously been addressed mainly using symbolic methods, that did not easily allow for an exploration of how speakers choose the names of the objects. The scope of the research goes beyond REG as such, as it addresses the link between semantic representations and reference more broadly.\n2) I also like how they use current techniques and datasets (cross-modal mapping and word classifiers, the ReferIt dataset containing large amounts of images with human-generated referring expressions) to address the problem at hand.  3) There are a substantial number of experiments as well as analysis into the results.",
    "weaknesses": "1) The main weakness for me is the statement of the specific hypothesis, within the general research line, that the paper is probing: I found it very confusing.  As a result, it is also hard to make sense of the kind of feedback that the results give to the initial hypothesis, especially because there are a lot of them and they don't all point in the same direction.\nThe paper says: \"This paper pursues the hypothesis that an accurate model of referential word meaning does not need to fully integrate visual and lexical knowledge (e.g. as expressed in a distributional vector space), but at the same time, has to go beyond treating words as independent labels.\"\nThe first part of the hypothesis I don't understand: What is it to fully integrate (or not to fully integrate) visual and lexical knowledge? Is the goal simply to show that using generic distributional representation yields worse results than using specific, word-adapted classifiers trained on the dataset? \nIf so, then the authors should explicitly discuss the bounds of what they are showing: Specifically, word classifiers must be trained on the dataset itself and only word classifiers with a sufficient amount of items in the dataset can be obtained, whereas word vectors are available for many other words and are obtained from an independent source (even if the cross-modal mapping itself is trained on the dataset); moreover, they use the simplest Ridge Regression, instead of the best method from Lazaridou et al. 2014, so any conclusion as to which method is better should be taken with a grain of salt. However, I'm hoping that the research goal is both more constructive and broader. Please clarify.  2) The paper uses three previously developed methods on a previously available dataset. The problem itself has been defined before (in Schlangen et al.). In this sense, the originality of the paper is not high.  3) As the paper itself also points out, the authors select a very limited subset of the ReferIt dataset, with quite a small vocabulary (159 words). I'm not even sure why they limited it this way (see detailed comments below).\n4) Some aspects could have been clearer (see detailed comments).\n5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?",
    "comments": "[Added after author response] Despite the weaknesses, I find the topic of the paper very relevant and also novel enough, with an interesting use of current techniques to address an \"old\" problem, REG and reference more generally, in a way that allows aspects to be explored that have not received enough attention. The experiments and analyses are a substantial contribution, even though, as mentioned above, I'd like the paper to present a more coherent overall picture of how the many experiments and analyses fit together and address the question pursued.\n- Detailed comments: Section 2 is missing the following work in computational semantic approaches to reference: Abhijeet  Gupta,  Gemma  Boleda,  Marco  Baroni,  and Sebastian  Pado. 2015.   Distributional                                            vectors  encode  referential         attributes. \nProceedings of EMNLP, 12-21 Aurelie Herbelot and Eva Maria Vecchi.                                            2015. \nBuilding a shared world: mapping distributional to model-theoretic semantic spaces. Proceedings of EMNLP, 22–32.\n142 how does Roy's work go beyond early REG work?\n155 focusses links 184 flat \"hit @k metric\": \"flat\"?\nSection 3: please put the numbers related to the dataset in a table, specifying the image regions, number of REs, overall number of words, and number of object names in the original ReferIt dataset and in the version you use. By the way, will you release your data? I put a \"3\" for data because in the reviewing form you marked \"Yes\" for data, but I can't find the information in the paper.\n229 \"cannot be considered to be names\" ==> \"image object names\" 230 what is \"the semantically annotated portion\" of ReferIt?\n247 why don't you just keep \"girl\" in this example, and more generally the head nouns of non-relational REs? More generally, could you motivate your choices a bit more so we understand why you ended up with such a restricted subset of ReferIt?\n258 which 7 features? ( list) How did you extract them?\n383 \"suggest that lexical or at least distributional knowledge is detrimental when learning what a word refers to in the world\": How does this follow from the results of Frome et al. 2013 and Norouzi et al. 2013? Why should cross-modal projection give better results? It's a very different type of task/setup than object labeling.\n394-395 these numbers belong in the data section Table 1: Are the differences between the methods statistically significant? \nThey are really numerically so small that any other conclusion to \"the methods perform similarly\" seems unwarranted to me. Especially the \"This suggests...\" part (407).  Table 1: Also, the sim-wap method has the highest accuracy for hit @5 (almost identical to wac); this is counter-intuitive given the @1 and @2 results. Any idea of what's going on?\nSection 5.2: Why did you define your ensemble classifier by hand instead of learning it? Also, your method amounts to majority voting, right?  Table 2: the order of the models is not the same as in the other tables + text.\nTable 3: you report cosine distances but discuss the results in terms of similarity. It would be clearer (and more in accordance with standard practice in CL imo) if you reported cosine similarities.\nTable 3: you don't comment on the results reported in the right columns. I found it very curious that the gold-top k data similarities are higher for transfer+sim-wap, whereas the results on the task are the same. I think that you could squeeze more information wrt the phenomenon and the models out of these results.\n496 format of \"wac\" Section 6 I like the idea of the task a lot, but I was very confused as to how you did and why: I don't understand lines 550-553. What is the task exactly? An example would help.  558 \"Testsets\" 574ff Why not mix in the train set examples with hypernyms and non-hypernyms?\n697 \"more even\": more wrt what?\n774ff \"Previous cross-modal mapping models ... force...\": I don't understand this claim.\n792 \"larger test sets\": I think that you could even exploit ReferIt more (using more of its data) before moving on to other datasets.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "7c686b6802d39032",
    "paper_id": "ACL_2017_489",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "Unclear novelty and significance of contributions. The work seems like an experimental extension of the cited Anonymous paper where the main method was introduced.     Another weakness is the limited size of the vocabulary in the zero-shot experiments that seem to be the most contributive part.  Additionally, the authors never presented significance scores for their accuracy results. This would have solidified the empirical contribution of the work which its main value.    My",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "afa2e45662249ffa",
    "paper_id": "ACL_2017_494",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- nice, clear application of linguistics ideas to distributional semantics - demonstrate very clear improvements on both intrinsic and extrinsic eval",
    "weaknesses": "- fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings)",
    "comments": "The paper describes \"morph-fitting\", a type of retrofitting for vector spaces that focuses specifically on incorporating morphological constraints into the vector space. The framework is based on the idea of \"attract\" and \"repel\" constraints, where attract constraints are used to pull morphological variations close together (e.g. look/looking) and repel constraints are used to push derivational antonyms apart (e.g. responsible/irresponsible). They test their algorithm on multiple different vector spaces and several language, and show consistent improvements on intrinsic evaluation (SimLex-999, and SimVerb-3500). They also test on the extrinsic task of dialogue state tracking, and again demonstrate measurable improvements over using morphologically-unaware word embeddings.\nI think this is a very nice paper. It is a simple and clean way to incorporate linguistic knowledge into distributional models of semantics, and the empirical results are very convincing. I have some questions/comments below, but nothing that I feel should prevent it from being published.\n- Comments for Authors 1) I don't really understand the need for the morph-simlex evaluation set. It seems a bit suspect to create a dataset using the same algorithm that you ultimately aim to evaluate. It seems to me a no-brainer that your model will do well on a dataset that was constructed by making the same assumptions the model makes. I don't think you need to include this dataset at all, since it is a potentially erroneous evaluation that can cause confusion, and your results are convincing enough on the standard datasets.\n2) I really liked the morph-fix baseline, thank you for including that. I would have liked to see a baseline based on character embeddings, since this seems to be the most fashionable way, currently, to side-step dealing with morphological variation. You mentioned it in the related work, but it would be better to actually compare against it empirically.\n3) Ideally, we would have a vector space where morphological variants are just close together, but where we can assign specific semantics to the different inflections. Do you have any evidence that the geometry of the space you end with is meaningful. E.g. does \"looking\" - \"look\" + \"walk\" = \"walking\"? It would be nice to have some analysis that suggests the morphfitting results in a more meaningful space, not just better embeddings.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "6fb29f3539cb8cb5",
    "paper_id": "ACL_2017_494",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- The proposed method is simple and shows nice performance improvements across a number of evaluations and in several languages. Compared to previous knowledge-based retrofitting approaches (Faruqui et al., 2015), it relies on a few manually-constructed rules, instead of a large-scale knowledge base, such as an ontology.\n- Like previous retrofitting approaches, this method is easy to apply to existing sets of embeddings and therefore it seems like the software that the authors intend to release could be useful to the community.\n- The method and experiments are clearly described.",
    "weaknesses": "- I was hoping to see some analysis of why the morph-fitted embeddings worked better in the evaluation, and how well that corresponds with the intuitive motivation of the authors.  - The authors introduce a synthetic word similarity evaluation dataset, Morph-SimLex. They create it by applying their presumably semantic-meaning-preserving morphological rules to SimLex999 to generate many more pairs with morphological variability. They do not manually annotate these new pairs, but rather use the original similarity judgements from SimLex999. \nThe obvious caveat with this dataset is that the similarity scores are presumed and therefore less reliable. Furthermore, the fact that this dataset was generated by the very same rules that are used in this work to morph-fit word embeddings, means that the results reported on this dataset in this work should be taken with a grain of salt. The authors should clearly state this in their paper.\n- (Soricut and Och, 2015) is mentioned as a future source for morphological knowledge, but in fact it is also an alternative approach to the one proposed in this paper for generating morphologically-aware word representations. The authors should present it as such and differentiate their work.\n- The evaluation does not include strong morphologically-informed embedding baselines.",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "4fb12403381dc32d",
    "paper_id": "ACL_2017_496",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The authors have nice coverage of a different range of language settings to isolate the way that relatedness and amount of morphology interact (i.e., translating between closely related morphologically rich languages vs distant ones) in affecting what the system learns about morphology. They include an illuminating analysis of what parts of the architecture end up being responsible for learning morphology, particularly in examining how the attention mechanism leads to more impoverished target side representations. \nTheir findings are of high interest and practical usefulness for other users of NMT.",
    "weaknesses": "They gloss over the details of their character-based encoder. \nThere are many different ways to learn character-based representations, and omitting a discussion of how they do this leaves open questions about the generality of their findings. Also, their analysis could've been made more interesting had they chosen languages with richer and more challenging morphology such as Turkish or Finnish, accompanied by finer-grained morphology prediction and analysis.",
    "comments": "This paper brings insight into what NMT models learn about morphology by training NMT systems and using the encoder or decoder representations, respectively, as input feature representations to a POS- or morphology-tagging classification task. This paper is a straightforward extension of \"Does String-Based Neural MT Learn Source Syntax?,\" using the same methodology but this time applied to morphology. Their findings offer useful insights into what NMT systems learn.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "5a40b606e3191326",
    "paper_id": "ACL_2017_496",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- This paper describes experiments that aim to address a crucial problem for NMT: understanding what does the model learn about morphology and syntax, etc.. - Very clear objectives and experiments effectively laid down.              Good state of the art review and comparison. In general, this paper is a pleasure to read.\n- Sound experimentation framework. Encoder/Decoder Recurrent layer outputs are used to train POS/morphological classifiers. They show the effect of certain changes in the framework on the classifier accuracy (e.g. use characters instead of words).\n- Experimentation is carried out on many language pairs.\n- Interesting conclusions derived from this work, and not all agree with intuition.",
    "weaknesses": "-  The contrast of character-based vs word-based representations  is slightly lacking: NMT with byte-pair encoding is showing v. strong performance in the literature. It would have been more relevant to have BPE in the mix, or replace word-based representations if three is too many. \n - Section 1: \"… while higher layers are more focused on word meaning\"; similar sentence in Section 7. I am ready to agree with this intuition, but I think the experiments in this paper do not support this particular sentence. \nTherefore it should not be included, or it should be clearly stressed that this is a reasonable hypothesis based on indirect evidence (translation performance improves but morphology on higher layers does not).\nDiscussion: This is a  fine paper that presents a thorough and systematic analysis of the NMT model, and derives several interesting conclusions based on many data points across several language pairs. I find particularly interesting that (a) the target language affects the quality of the encoding on the source side; in particular, when the target side is a morphologically-poor language (English) the pos tagger accuracy for the encoder improves. ( b) increasing the depth of the encoder does not improve pos accuracy (more experiments needed to determine what does it improve); (c) the attention layer hurts the quality of the decoder representations.  I wonder if (a) and (c) are actually related? The attention hurts the decoder representation, which is more difficult to learn for a morphologically rich language; in turn, the encoders learn based on the global objective, and this backpropagates through the decoder. Would this not be a strong indication that we need separate objectives to govern the encoder/decoder modules of the NMT model?",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "fe76e61d885ab4a4",
    "paper_id": "ACL_2017_49",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper presents an interesting extension to attention-based neural MT approaches, which leverages source-sentence chunking as additional piece of information from the source sentence. The model is modified such that this chunking information is used differently by two recurrent layers: while one focuses in generating a chunk at a time, the other focuses on generating the words within the chunk. This is interesting. I believe readers will enjoy getting to know this approach and how it performs. \nThe paper is very clearly written, and alternative approaches are clearly contrasted. The evaluation is well conducted, has a direct contrast with other papers (and evaluation tables), and even though it could be strengthened (see my comments below), it is convincing.",
    "weaknesses": "As always, more could be done in the experiments section to strengthen the case for chunk-based models. For example, Table 3 indicates good results for Model 2 and Model 3 compared to previous papers, but a careful reader will wonder whether these improvements come from switching from LSTMs to GRUs. In other words, it would be good to see the GRU tree-to-sequence result to verify that the chunk-based approach is still best.\nAnother important aspect is the lack of ensembling results. The authors put a lot of emphasis is claiming that this is the best single NMT model ever published. While this is probably true, in the end the best WAT system for Eng-Jap is at 38.20 (if I'm reading the table correctly) - it's an ensemble of 3. If the authors were able to report that their 3-way chunk-based ensemble comes top of the table, then this paper could have a much stronger impact.\nFinally, Table 3 would be more interesting if it included decoding times. The authors mention briefly that the character-based model is less time-consuming (presumably based on Eriguchi et al.'16), but no cite is provided, and no numbers from chunk-based decoding are reported either. Is the chunk-based model faster or slower than word-based? Similar? Who know... Adding a column to Table 3 with decoding times would give more value to the paper.",
    "comments": "Overall I think the paper is interesting and worth publishing. I have minor comments and suggestions to the authors about how to improve their presentation (in my opinion, of course).  - I think they should clearly state early on that the chunks are supplied externally - in other words, that the model does not learn how to chunk. This only became apparent to me when reading about CaboCha on page 6 - I don't think it's mentioned earlier, and it is important.\n- I don't see why the authors contrast against the char-based baseline so often in the text (at least a couple of times they boast a +4.68 BLEU gain). I don't think readers are bothered... Readers are interested in gains over the best baseline.\n- It would be good to add a bit more detail about the way UNKs are being handled by the neural decoder, or at least add a citation to the dictionary-based replacement strategy being used here.\n- The sentence in line 212 (\"We train a GRU that encodes a source sentence into a single vector\") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.\n- The motivating example of lines 69-87 is a bit weird. Does \"you\" depend on \"bite\"? Or does it depend on the source side? Because if it doesn't depend on \"bite\", then the argument that this is a long-dependency problem doesn't really apply.",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "df6fad4433c625fb",
    "paper_id": "ACL_2017_49",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper is well-written and clear about the proposed models and its contributions.  The proposed models to incorporating chunk information into NMT models are novel and well-motivated. I think such models can be generally applicable for many other language pairs.",
    "weaknesses": "There are some minor points, listed as follows: 1) Figure 1: I am a bit surprised that the function words dominate the content ones in a Japanese sentence. Sorry I may not understand Japanese.  2) In all equations, sequences/vectors (like matrices) should be represented as bold texts to distinguish from scalars, e.g., hi, xi, c, s, ... 3) Equation 12: s_j-1 instead of s_j.\n4) Line 244: all encoder states should be referred to bidirectional RNN states.\n5) Line 285: a bit confused about the phrase \"non-sequential information such as chunks\". Is chunk still sequential information???\n6) Equation 21: a bit confused, e.g, perhaps insert k into s1(w) like s1(w)(k) to indicate the word in a chunk.   7) Some questions for the experiments: Table 1: source language statistics?  For the baselines, why not running a baseline (without using any chunk information) instead of using (Li et al., 2016) baseline (|V_src| is different)? It would be easy to see the effect of chunk-based models. Did (Li et al., 2016) and other baselines use the same pre-processing and post-processing steps? Other baselines are not very comparable. After authors's response, I still think that (Li et al., 2016) baseline can be a reference but the baseline from the existing model should be shown.  Figure 5: baseline result will be useful for comparison? chunks in the translated examples are generated *automatically* by the model or manually by the authors? Is it possible to compare the no. of chunks generated by the model and by the bunsetsu-chunking toolkit? In that case, the chunk information for Dev and Test in Table 1 will be required. BTW, the authors's response did not address my point here.  8) I am bit surprised about the beam size 20 used in the decoding process. I suppose large beam size is likely to make the model prefer shorter generated sentences.  9) Past tenses should be used in the experiments, e.g., Line 558: We *use* (used) ... Line 579-584: we *perform* (performed) ... *use* (used) ... ...",
    "comments": "Overall, this is a solid work - the first one tackling the chunk-based NMT; and it well deserves a slot at ACL.",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "f991fd1e2356adec",
    "paper_id": "ACL_2017_501",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper is well-written and well-structured. \nIt is clear with its contributions and well supports them by empirical evidence. So the paper is very easy to read.  The paper is well motivated. A method of selecting the most appropriate caption given a list of misleading candidates will benefit other image-caption/understanding models, by acting as a post-generation re-ranking method.",
    "weaknesses": "I am not sure if the proposed algorithm for decoys generation is effective, which as a consequence puts the paper on questions.\nFor each target caption, the algorithm basically picks out those with similar representation and surface form but do not belong to the same image. But a fundamentally issue with this approach is: not belonging to the image-A does not mean not appropriate to describe image-A, especially when the representation and surface form are close. So the ground-truth labels might not be valid. As we can see in Figure-1, the generated decoys are either too far from the target to be a *good* decoy (*giraffe* vs *elephant*), or fair substitutes for the target (*small boy playing kites* vs *boy flies a kite*).\nThus, I am afraid that the dataset generated with this algorithm can not train a model to really *go beyond key word recognition*, which was claimed as contribution in this paper. As shown in Figure-1, most decoys can be filtered by key word mismatch---*giraffe vs elephant*, *pan vs bread*, *frisbee vs kite*, etc. And when they can not be separated by *key word match*, they look very tempting to be a correct option.\nFurthermore, it is interesting that humans only do correctly on 82.8% on a sampled test set. Does it mean that those examples are really too hard even for human to correctly classify? Or are some of the *decoys* in fact good enough to be the target's substitute (or even better) so that human choose them over ground-truth targets?",
    "comments": "I think this is a well-written paper with clear motivation and substantial experiments. \nThe major issue is that the data-generating algorithm and the generated dataset do not seem helpful for the motivation. This in turn makes the experimental conclusions less convincing. So I tend to reject this paper unless my concerns can be fully addressed in rebuttal.",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "df35da3db6b7fe12",
    "paper_id": "ACL_2017_501",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Authors generate a dataset of “rephrased” captions and are planning to make this dataset publicly available.\nThe way authors approached DMC task has an advantage over VQA or caption generation in terms of metrics. It is easier and more straightforward to evaluate problem of choosing the best caption. Authors use accuracy metric. \nWhile for instance caption generation requires metrics like BLUE or Meteor which are limited in handling semantic similarity.\nAuthors propose an interesting approach to “rephrasing”, e.g. selecting decoys. They draw decoys form image-caption dataset. E.g. decoys for a single image come from captions for other images. These decoys however are similar to each other both in terms of surface (bleu score) and semantics (PV similarity). \nAuthors use lambda factor to decide on the balance between these two components of the similarity score. I think it would be interesting to employ these for paraphrasing.\nAuthors support their motivation for the task with evaluation results. They show that a system trained with the focus on differentiating between similar captions performs better than a system that is trained to generate captions only. These are, however, showing that system that is tuned for a particular task performs better on this task.",
    "weaknesses": "It is not clear why image caption task is not suitable for comprehension task and why author’s system is better for this. In order to argue that system can comprehend image and sentence semantics better one should apply learned representation, e.g. embeddings. E.g. apply representations learned by different systems on the same task for comparison.\nMy main worry about the paper is that essentially authors converge to using existing caption generation techniques, e.g. Bahdanau et al., Chen et al. They way formula (4) is presented is a bit confusing. From formula it seems that both decoy and true captions are employed for both loss terms. However, as it makes sense, authors mention that they do not use decoy for the second term. \nThat would hurt mode performance as model would learn to generate decoys as well. The way it is written in the text is ambiguous, so I would make it more clear either in the formula itself or in the text. Otherwise it makes sense for the model to learn to generate only true captions while learning to distinguish between true caption and a decoy.",
    "comments": "Authors formulate a task of Dual Machine Comprehension. They aim to accomplish the task by challenging computer system to solve a problem of choosing between two very similar captions for a given image. Authors argue that a system that is able to solve this problem has to “understand” the image and captions beyond just keywords but also capture semantics of captions and their alignment with image semantics.\nI think paper need to make more focus on why chosen approach is better than just caption generation and why in their opinion caption generation is less challenging for learning image and text representation and their alignment.\nFor formula (4). I wonder if in the future it is possible to make model to learn “not to generate” decoys by adjusting second loss term to include decoys but with a negative sign. Did authors try something similar?",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "ddef72a4212ae749",
    "paper_id": "ACL_2017_501",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The DMC task seems like a good test of understanding language and vision. I like that the task has a clear evaluation metric.\nThe failure of the caption generation model on the DMC task is quite interesting. This result further demonstrates that these models are good language models, but not as good at capturing the semantics of the image.",
    "weaknesses": "The experiments are missing a key baseline: a state-of-the-art VQA model trained with only a yes/no label vocabulary.  I would have liked more details on the human performance experiments. How many of the ~20% of incorrectly-predicted images are because the captions are genuinely ambiguous? Could the data be further cleaned up to yield an even higher human accuracy?",
    "comments": "My concern with this paper is that the data set may prove to be easy or gameable in some way. The authors can address this concern by running a suite of strong baselines on their data set and demonstrating their accuracies. I'm not convinced by the current set of experiments because the chosen neural network architectures appear quite different from the state-of-the-art architectures in similar tasks, which typically rely on attention mechanisms over the image.\nAnother nice addition to this paper would be an analysis of the data set. How many tokens does the correct caption share with distractors on average? What kind of understanding is necessary to distinguish between the correct and incorrect captions? I think this kind of analysis really helps the reader understand why this task is worthwhile relative to the many other similar tasks.  The data generation technique is quite simple and wouldn't really qualify as a significant contribution, unless it worked surprisingly well.\n- Notes I couldn't find a description of the FFNN architecture in either the paper or the supplementary material. It looks like some kind of convolutional network over the tokens, but the details are very unclear. I'm also confused about how the Veq2Seq+FFNN model is applied to both classification and caption generation. Is the loglikelihood of the caption combined with the FFNN prediction during classification? Is the FFNN score incorporated during caption generation?\nThe fact that the caption generation model performs (statistically significantly) *worse* than random chance needs some explanation. How is this possible?\n528 - this description of the neural network is hard to understand. The final paragraph of the section makes it clear, however. Consider starting the section with it.",
    "overall_score": "2",
    "confidence": "5"
  },
  {
    "review_id": "9ca1de3750fa0e3c",
    "paper_id": "ACL_2017_503",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This one is a tough call, because I do think that there are some important, salvageable technial results in here (notably the parsing algorithm), but the paper as a whole has very little cohesion.        It is united around an overarching view of formal languages in which a language being \"probabilistic\" or not is treated as a formal property of the same  variety as being closed under intersection or not.  In my opinion, what it  means for a formal language to be probabilistic in this view has not been  considered with sufficient rigor for this viewpoint to be compelling.\nI should note, by the way, that the value of the formal results provided mostly does not depend on the flimsiness of the overarching story.  So what we have here is not bad research, but a badly written paper.  This needs  more work.\nI find it particulary puzzling that the organization of the paper leaves so little space for elucidating the parsing result that soundness and completeness are relegated to a continuation of the paper in the form of supplementary notes.  I also find the mention of probabilistic languages in the title of the paper to be very disingenuous --- there is in fact no probabilistic reasoning in this submission.\nThe sigificance of the intersection-closure result of section 3 is also being somewhat overstated, I think.  Unless there is something I'm not understanding about the restrictions on the right-hand sides of rules (in which case, please elaborate), this is merely a matter of folding a finite intersection into the set of non-terminal labels.",
    "overall_score": "2",
    "confidence": "5"
  },
  {
    "review_id": "76771b20941fa90c",
    "paper_id": "ACL_2017_503",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The introduction shows relevance, the overall aim, high level context and is nice to read. \nThe motivation is clear and interesting.\nThe paper  is extremely clear but requires close reading and much formal background. \nIt nicely takes into account certain differences in terminology.\nIt was interesting to see how the hyper-edge grammars generalize familiar grammars  and Earley's algorithm.  For example, Predict applies to nonterminal edges, and Scan applies to terminal edges.   If the parsing vs. validation in NLP context is clarified, the paper is useful because it is formally correct, nice contribution, instructive and can give new ideas to other researchers.   The described algorithm can be used in semantic parsing to rerank hypergraphs that are produced by another parser.   In this restricted way, the method can be part of the machinery what we in NLP use in natural language parsing and thus relevant to the ACL.",
    "weaknesses": "Reranking use is not mentioned in the introduction.\nIt would be a great news in NLP context if an Earley parser would run in linear time for NLP grammars (unlike special kinds of formal language grammars). \nUnfortunately, this result involves deep assumptions about the grammar and the kind of input.  Linear complexity of parsing of an input graph seem right for a top-down deterministic grammars but the paper does not recognise the fact that an input string in NLP usually gives rise to an exponential number of graphs.  In other words, the parsing complexity result must be interpreted in the context of graph validation or where one wants to find out a derivation of the graph, for example, for the purposes of graph transduction via synchronous derivations.\nTo me, the paper should be more clear in this as a random reader may miss the difference between semantic parsing (from strings) and parsing of semantic parses  (the current work).\nThere does not seem to be any control of the linear order of 0-arity edges.  It might be useful to mention that if the parser is extended to string inputs with the aim to find the (best?) hypergraph for a given external nodes, then the item representations of the subgraphs must also keep track of the covered 0-arity edges.                          This makes the string-parser variant exponential.   - Easily correctable typos or textual problems: 1)  Lines 102-106 is misleading.   While intersection and probs are true, \"such distribution\" cannot refer to the discussion in the above.\n2) line 173:  I think you should rather talk about validation or recognition algorithms than parsing algorithms as \"parsing\" in NLP means usually completely different thing that is much more challenging due to the lexical and structural ambiguity.\n3) lines 195-196 are unclear:  what are the elements of att_G; in what sense they are pairwise distinct.  Compare Example 1 where ext_G and att_G(e_1) are not disjoint sets.\n4) l.206.  Move *rank* definition earlier and remove redundancy.\n5) l. 267:  rather \"immediately derives\", perhaps.\n6) 279: add \"be\" 7) l. 352:  give an example of a nontrivial internal path.\n8) l. 472:   define a subgraph of a hypergraph 9) l. 417, l.418:  since there are two propositions, you may want to tell how they contribute to what is quoted.\n10) l. 458:  add \"for\" Table:                          Axiom:              this is only place where this is introduced as an axiom.                    Link to the text that says it is a trigger.",
    "comments": "It might be useful to tell about MSOL graph languages and their yields, which are context-free string languages.                           What happens if the grammar is ambiguous and not top-down deterministic? \nWhat if there are exponential number of parses even for the input graph due to lexical ambiguity or some other reasons.  How would the parser behave then? \nWouldn't the given Earley recogniser actually be strictly polynomial to m or k ?\nEven a synchronous derivation of semantic graphs can miss some linguistic phenomena where a semantic distinction is expressed by different linguistic means.                    E.g. one language may add an affix to a verb when another language may express the same distinction by changing the object.  I am suggesting that although AMR increases language independence in parses it may have such cross-lingual challenges.\nI did not fully understand the role of the marker in subgraphs.  It was elided later and not really used.\nl. 509-510:                 I already started to miss the remark of lines 644-647 at this point.\nIt seems that the normal order is not unique.  Can you confirm this?\nIt is nice that def 7, cond 1 introduces lexical anchors to predictions. \nCompare the anchors in lexicalized grammars.\nl. 760.  Are you sure that non-crossing links do not occur when parsing linearized sentences to semantic graphs?\n- Significant questions to the Authors: Linear complexity of parsing of an input graph seem right for a top-down deterministic grammars but the paper does not recognise the fact that an input string in NLP usually gives rise to an exponential number of graphs.  In other words, the parsing complexity result must be interpreted in the context of graph validation or where one wants to find out a derivation of the graph, for example, for the purposes of graph transduction via synchronous derivations.\nWhat would you say about parsing complexity in the case the RGG is a non-deterministic, possibly ambiguous regular tree grammar, but one is interested to use it to assign trees to frontier strings like a context-free grammar?  Can one adapt the given Earley algorithm to this purpose (by guessing internal nodes and their edges)? \nAlthough this question might seem like a confusion, it is relevant in the NLP context.\nWhat prevents the RGGs to generate hypergraphs whose 0-arity edges (~words) are then linearised?   What principle determines how they are linearised?                  Is the linear order determined by the Earley paths (and normal order used in productions) or can one consider an actual word order in strings of a natural language?  There is no clear connection to (non)context-free string languages or sets of (non)projective dependency graphs used in semantic parsing.  What is written on lines 757-758 is just misleading:  Lines 757-758 mention that HRGs can be used to generate non-context-free languages.  Are these graph languages or string languages?    How an NLP expert should interpret the (implicit) fact that RGGs generate only context-free languages?  Does this mean that the graphs are noncrossing graphs in the sense of Kuhlmann & Jonsson (2015)?",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "0374b26f7ce388a6",
    "paper_id": "ACL_2017_516",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper offers a natural and useful extension to recent efforts in interactive topic modeling, namely by allowing human annotators to provide multiple \"anchor words\" to machine-induced topics. The paper is well-organized and the combination of synthetic and user experiments make for a strong paper.",
    "weaknesses": "The paper is fairly limited in scope in terms of the interactive topic model approaches it compares against. I am willing to accept this, since they do make reference to most of them and explain that these other approaches are not necessarily fast enough for interactive experimentation or not conducive to the types of interaction being considered with an \"anchoring\" interface. Some level of empirical support for these claims would have been nice, though.\nIt would also have been nice to see experiments on more than one data set (20 newsgroups, which is now sort of beaten-to-death).",
    "comments": "In general, this is a strong paper that appears to offer an incremental but novel and practical contribution to interactive topic modeling. The authors made the effort to vet several variants of the approach in simulated experiments, and to conduct fairly exhaustive quantitative analyses of both simulated and user experiments using a variety of metrics that measure different facets of topic quality.",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "5fb400caafb85625",
    "paper_id": "ACL_2017_516",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Clear description of methods and evaluation Successfully employs and interprets a variety of evaluations Solid demonstration of practicality of technique in real-world interactive topic modeling",
    "weaknesses": "Missing related work on anchor words Evaluation on 20 Newsgroups is not ideal Theoretical contribution itself is small",
    "comments": "The authors propose a new method of interactive user specification of topics called Tandem Anchors. The approach leverages the anchor words algorithm, a matrix-factorization approach to learning topic models, by replacing the individual anchors inferred from the Gram-Schmidt algorithm with constructed anchor pseudowords created by combining the sparse vector representations of multiple words that for a topic facet. The authors determine that the use of a harmonic mean function to construct pseudowords is optimal by demonstrating that classification accuracy of document-topic distribution vectors using these anchors produces the most improvement over Gram-Schmidt. They also demonstrate that their work is faster than existing interactive methods, allowing interactive iteration, and show in a user study that the multiword anchors are easier and more effective for users.\nGenerally, I like this contribution a lot: it is a straightforward modification of an existing algorithm that actually produces a sizable benefit in an interactive setting. I appreciated the authors’ efforts to evaluate their method on a variety of scales. While I think the technical contribution in itself is relatively small (a strategy to assemble pseudowords based on topic facets) the thoroughness of the evaluation merited having it be a full paper instead of a short paper. It would have been nice to see more ideas as to how to build these facets in the absence of convenient sources like category titles in 20 Newsgroups or when initializing a topic model for interactive learning.\nOne frustration I had with this paper is that I find evaluation on 20 Newsgroups to not be great for topic modeling: the documents are widely different lengths, preprocessing matters a lot, users have trouble making sense of many of the messages, and naive bag-of-words models beat topic models by a substantial margin. Classification tasks are useful shorthand for how well a topic model corresponds to meaningful distinctions in the text by topic; a task like classifying news articles by section or reviews by the class of the subject of the review might be more appropriate. It would also have been nice to see a use case that better appealed to a common expressed application of topic models, which is the exploration of a corpus.\nThere were a number of comparisons I think were missing, as the paper contains little reference to work since the original proposal of the anchor word model. \nIn addition to comparing against standard Gram-Schmidt, it would have been good to see the method from Lee et. al. (2014), “Low-dimensional Embeddings for Interpretable Anchor-based Topic Inference”. I also would have liked to have seen references to Nguyen et. al. (2013), “Evaluating Regularized Anchor Words” and Nguyen et. al. (2015) “Is Your Anchor Going Up or Down? Fast and Accurate Supervised Topic Models”, both of which provide useful insights into the anchor selection process.\nI had some smaller notes: - 164: …entire dataset - 164-166: I’m not quite sure what you mean here. I think you are claiming that it takes too long to do one pass? My assumption would have been you would use only a subset of the data to retrain the model instead of a full sweep, so it would be good to clarify what you mean.\n- 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn’t clear to me why the ones you chose were better options.\n- 337: Usenet should be capitalized - 338-340: Why fewer than 100 (as that is a pretty aggressive boundary)? Also, did you remove headers, footers, and/or quotes from the messages?\n- 436-440: I would have liked to see a bit more explanation of what this tells us about confusion.\n- 692: using tandem anchors Overall, I think this paper is a meaningful contribution to interactive topic modeling that I would like to see available for people outside the machine learning community to investigate, classify, and test hypotheses about their corpora.\nPOST-RESPONSE: I appreciate the thoughtful responses of the authors to my questions. I would maintain that for some of the complimentary related work that it's useful to compare to non-interactive work, even if it does something different.",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "13ee009b4c865120",
    "paper_id": "ACL_2017_520",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper proposes a method for generating datasets of pictures from simple building blocks, as well as corresponding logical forms and language descriptions. \nThe goal seems to be to have a method where the complexity of pictures and corresponding desciptions can be controlled and parametrized.   - The biggest downside seems to be that the maximally achievable complexity is very limited, and way below the complexity typically faced with image-captioning and other multimodal tasks. \n - The relative simplicity is also a big difference to the referenced bAbI tasks (which cover the whole qualitative spectrum of easy-to-hard reasoning tasks), whereas in the proposed method a (qualitatively) easy image reconition task can only be quantitatively made harder, by increasing the number of objects, noise etc in unnatural ways. \n - This is also reflected in the experimental section. Whenever the experimental performance results are not satisfying, these cases seem like basic over/underfitting issues that may easily be tackled by restricting/extending the capacity of the networks or using more data. It is hard for me to spot any other qualitative insight. \n - In the introduction it is stated that the \"goal is not too achieve optimal performance\" but to find out whether \"architectures are able to successfully demonstrate the desired understanding\" - there is a fundamental contradiction here, in that the proposed task on the one side is meant to provide a measure as to whether architectures demontrate \"understanding\", on the other hand the score is not supposed to be taken as meaningful/seriously.\nGeneral comments: The general approach should be made more tangible earlier (i.e. in the introction rather than in section 3)",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "df347c25522ec4d4",
    "paper_id": "ACL_2017_520",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The authors introduce a new software package called ShapeWorld for automatically generating data for image captioning problems. The microworld used to generate the image captions is simple enough to make the data being generated and errors by a model readily interpretable. However, the authors demonstrate that configurations of the packages produce data that is challenging enough to serve as a good benchmark for ongoing research.",
    "weaknesses": "The primary weakness of this paper is that it does look a bit like a demo paper. The authors do provides experiments that evaluate a reasonable baseline image captioning system on the data generated by ShapeWorld. However, similar experiments are included in demo papers.\nThe paper includes a hyperlink to the software package on github that presumably unmasks the authors of the paper.",
    "comments": "Scientific progress often involves some something analogous to vygotsky's zone of proximal development, whereby progress can be made more quickly if research focuses on problems with just the right level of difficulty (e.g., the use of tidigits for speech recognition research in the early 90s). This paper is exciting since it offers a simple microworld that is easy for researchers to completely comprehend but that also is just difficult enough for existing models.\nThe strengths of the work are multiplied by the fact that the software is opensource, is readily available on github and generates the data in a format that can be easily used with models built using modern deep learning libraries (e.g., TensorFlow).\nThe methods used by the software package to generate the artificial data are clearly explained. It is also great that the authors did experiments with different configurations of their software and a baseline image caption model in order to demonstrate the strengths and weakness of existing techniques.  My only real concern with this paper is whether the community would be better served by placing it in the demo section. Publishing it in the non-demo long paper track might cause confusion as well as be unfair to authors who correctly submitted similar papers to the ACL demo track.",
    "overall_score": "2",
    "confidence": "5"
  },
  {
    "review_id": "b90afa260c877625",
    "paper_id": "ACL_2017_524",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "* Elaborate evaluation data creation and evaluation scheme. \n * Range of compared techniques: baseline/simple/complex",
    "weaknesses": "* No in-depth analysis beyond overall evaluation results.",
    "comments": "This paper compares several techniques for robust HPSG parsing.\nSince the main contribution of the paper is not a novel parsing technique but the empirical evaluation, I would like to see a more in-depth analysis of the results summarized in Table 1 and 2. \nIt would be nice to show some representative example sentences and sketches of its analyses, on which the compared methods behaved differently.\nPlease add EDM precision and recall figures to Table 2. \nThe EDM F1 score is a result of a mixed effects of (overall and partial) coverage, parse ranking, efficiency of search, etc. \nThe overall coverage figures in Table 1 are helpful but addition of EDM recall to Table 2 would make the situations clearer.\nMinor comment: - Is 'pacnv+ut' in Table 1 and 2 the same as 'pacnv' described in 3.4.3?",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "d28c29a732b404f7",
    "paper_id": "ACL_2017_524",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Well-written.",
    "weaknesses": "Although the title and abstract of the paper suggest that robust parsing methods for HPSG are being compared, the actual comparison is limited to only a few techniques applied to a single grammar, the ERG (where in the past the  choice has been made to create a treebank for only those sentences that are in the coverage of the grammar). Since the ERG is quite idiosyncratic in this respect, I fear that the paper is not interesting for researchers working in other precision grammar frameworks.\nThe paper lacks comparison with robustness techniques that are routinely applied for systems based on other precision grammars such as various systems based on CCG, LFG, the Alpage system for French, Alpino for Dutch and there is probably more. In the same spirit, there is a reference for supertagging to Dridan 2013 which is about supertagging for ERG whereas supertagging for other precision grammar systems has been proposed at least a decade earlier.\nThe paper lacks enough detail to make the results replicable. Not only are various details not spelled out (e.g. what are those limits on resource allocation), but perhaps more importantly, for some of the techniques that are being compared (eg the robust unification), and for the actual evaluation metric, the paper refers to another paper that is still in preparation.\nThe actual results of the various techniques are somewhat disappointing. With the exception of the csaw-tb method, the resulting parsing speed is extreme - sometimes much slower than the baseline method - where the baseline method is a method in which the standard resource limitations do not apply. The csaw-tb method is faster but not very accurate, and in any case it is not a method introduced in this paper but an existing PCFG approximation technique.\nIt would be (more) interesting to have an idea of the results on a representative dataset (consisting of both sentences that are in the coverage of the grammar and those that are not). In that case, a comparison with the \"real\" baseline system (ERG with standard settings) could be obtained.\nMethodological issue: the datasets semcor and wsj00ab consist of sentences which an older version of ERG could not parse, but a newer version could. For this reason, the problems in these two datasets are clearly very much biased. \nIt is no suprise therefore that the various techniques obtain much better results on those datasets. But to this reviewer, those results are somewhat meaningless.  minor: EDM is used before explained \"reverseability\"",
    "comments": "",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "0c8942581dd7abed",
    "paper_id": "ACL_2017_524",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- technique for creating dataset for evaluation of out-of-coverage items, that could possibly be used to evaluation other grammars as well.  - the writing in this paper is engaging, and clear (a pleasant surprise, as compared to the typical ACL publication.)",
    "weaknesses": "- The evaluation datasets used are small and hence results are not very convincing (particularly wrt to the alchemy45 dataset on which the best results have been obtained) - It is disappointing to see only F1 scores and coverage scores, but virtually no deeper analysis of the results. For instance, a breakdown by type of error/type of grammatical construction would be interesting.  - it is still not clear to this reviewer what is the proportion of out of coverage items due to various factors (running out of resources,  lack of coverage for \"genuine\" grammatical constructions in the long tail, lack of coverage due to extra-grammatical factors like interjections, disfluencies, lack of lexical coverage, etc.",
    "comments": "This paper address the problem of \"robustness\" or lack of coverage for a hand-written HPSG grammar (English Resource Grammar). The paper compares several approaches for increasing coverage, and also presents two creative ways of obtaining evaluation datasets (a non-trivial issue due to the fact that gold standard evaluation data is by definition available only for in-coverage inputs).  Although hand-written precision grammars have been very much out of fashion for a long time now and have been superseded by statistical treebank-based grammars, it is important to continue research on these in my opinion. The advantages of high precision and deep semantic analysis provided by these grammars has not been reproduced by non-handwritten grammars as yet. For this reason, I am giving this paper a score of 4, despite the shortcomings mentioned above.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "851d9a0e9a5f5e01",
    "paper_id": "ACL_2017_543",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- Thinking about Chinese/Japanese/Korean characters visually is a great idea!",
    "weaknesses": "- Experimental results show only incremental improvement over baseline, and the choice of evaluation makes it hard to verify one of the central arguments: that visual features improve performance when processing rare/unseen words.\n- Some details about the baseline are missing, which makes it difficult to interpret the results, and would make it hard to reproduce the work.",
    "comments": "The paper proposes the use of computer vision techniques (CNNs applied to images of text) to improve language processing for Chinese, Japanese, and Korean, languages in which characters themselves might be compositional. The authors evaluate their model on a simple text-classification task (assigning Wikipedia page titles to categories). They show that a simple one-hot representation of the characters outperforms the CNN-based representations, but that the combination of the visual representations with standard one-hot encodings performs better than the visual or the one-hot alone. They also present some evidence that the visual features outperform the one-hot encoding on rare words, and present some intuitive qualitative results suggesting the CNN learns good semantic embeddings of the characters.\nI think the idea of processing languages like Chinese and Japanese visually is a great one, and the motivation for this paper makes a lot of sense. However, I am not entirely convinced by the experimental results. The evaluations are quite weak, and it is hard to say whether these results are robust or simply coincidental. I would prefer to see some more rigorous evaluation to make the paper publication-ready. If the results are statistically significant (if the authors can indicate this in the author response), I would support accepting the paper, but ideally, I would prefer to see a different evaluation entirely.\nMore specific comments below: - In Section 3, paragraph \"lookup model\", you never explicitly say which embeddings you use, or whether they are tuned via backprop the way the visual embeddings are. You should be more clear about how the baseline was implemented. If the baseline was not tuned in a task-specific way, but the visual embeddings were, this is even more concerning since it makes the performances substantially less comparable.\n- I don't entirely understand why you chose to evaluate on classifying wikipedia page titles. It seems that the only real argument for using the visual model is its ability to generalize to rare/unseen characters. Why not focus on this task directly? E.g. what about evaluating on machine translation of OOV words? I agree with you that some languages should be conceptualized visually, and sub-character composition is important, but the evaluation you use does not highlight weaknesses of the standard approach, and so it does not make a good case for why we need the visual features.  - In Table 5, are these improvements statistically significant?\n- It might be my fault, but I found Figure 4 very difficult to understand. \nSince this is one of your main results, you probably want to present it more clearly, so that the contribution of your model is very obvious. As I understand it, \"rank\" on the x axis is a measure of how rare the word is (I think log frequency?), with the rarest word furthest to the left? And since the visual model intersects the x axis to the left of the lookup model, this means the visual model was \"better\" at ranking rare words? Why don't both models intersect at the same point on the x axis, aren't they being evaluated on the same set of titles and trained with the same data? In the author response, it would be helpful if you could summarize the information this figure is supposed to show, in a more concise way.  - On the fallback fusion, why not show performance for for different thresholds? 0 seems to be an edge-case threshold that might not be representative of the technique more generally.\n- The simple/traditional experiment for unseen characters is a nice idea, but is presented as an afterthought. I would have liked to see more eval in this direction, i.e. on classifying unseen words - Maybe add translations to Figure 6, for people who do not speak Chinese?",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "c8f09cfc4c511cd9",
    "paper_id": "ACL_2017_553",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "A nice, solid piece of work that builds on previous studies in a productive way. Well-written and clear.",
    "weaknesses": "Very few--possibly avoid some relatively \"empty\" statements: 191 : For example, if our task is to identify words used similarly across contexts, our scoring function can be specified to give high scores to terms whose usage is similar across the contexts.\n537 : It is educational to study how annotations drawn from the same data are similar or different.",
    "comments": "In the first sections I was not sure that much was being done that was new or interesting, as the methods seemed very reminiscent of previous methods used over the past 25 years to measure similarity, albeit with a few new statistical twists, but conceptually in the same vein. Section 5, however, describes an interesting and valuable piece of work that will be useful for future studies on the topic. In retrospect, the background provided in sections 2-4 is useful, if not necessary, to support the experiments in section 5.  In short, the work and results described will be useful to others working in this area, and the paper is worthy of presentation at ACL.\nMinor comments: Word, punctuation missing? \n264 : For word annotations, we used PPMI, SVD, and SGNS (skipgram with negative sampling from Mikolov et al. (2013b)) word vectors released by Hamilton et al. (2016).\nUnclear what \"multiple methods\" refers to : 278 : some words were detected by multiple methods with CCLA",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "7e2d6de40f465b6b",
    "paper_id": "ACL_2017_553",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- The framework proposed in this paper is generalizable and can be applied to different applications, and accommodate difference notation of context, different similarity functions, different type of word annotations.  - The paper is well written. Very easy to follow.",
    "weaknesses": "- I have concerns in terms of experiment evaluation. The paper uses qualitative evaluation metrics, which makes it harder to evaluate the effectiveness, or even the validity of proposed method. For example, table 1 compares the result with Hamilton et, al using different embedding vector by listing top 10 words that changed from 1900 to 1990. It's hard to tell, quantitatively, the performances of CCLA. The same issue also applies to experiment 2 (comparative lexical analysis over context). The top 10 words may be meaningful, but what about top 20, 100? what about the words that practitioner actually cares? \nWithout addressing the evaluation issue, I find it difficult to claim that CCLA will benefit downstream applications.",
    "comments": "",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "e0c6a82f92379c10",
    "paper_id": "ACL_2017_554",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "a) The paper presents a Bayesian learning approach for recurrent neural network language model. The method outperforms standard SGD with dropout on three tasks. \nb) The idea of using Bayesian learning with RNNs appears to be novel. \nc) The computationally efficient Bayesian algorithm for RNN would be of interest to the NLP community for various applications.",
    "weaknesses": "Primary concern is about evaluation: Sec 5.1: The paper reports the performance of difference types of architectures (LSTM/GRU/vanilla RNN) on character LM task while comparing the learning algorithms on the Penn Treebank task. Furthermore, RMSprop and pSGLD are compared for the character LM while SGD +/- dropout is compared with SGLD +/- dropout on word language model task. This is inconsistent!  I would suggest reporting both these dimensions (i.e. architectures and the exact same learning algorithms) on both character and word LM tasks. It would be useful to know if the results from the proposed Bayesian learning approaches are portable across both these tasks and data sets.\nL529: The paper states that 'the performance gain mainly comes from adding gradient noise and model averaging'. This statement is not justified empirically. To arrive at this conclusion, an A/B experiment with/without adding gradient noise and/or model averaging needs to be done.  L724: Gal's dropout is run on the sentence classification task but not on language model/captions task. Since Gal's dropout is not specific to sentence classification,  I would suggest reporting the performance of this method on all three tasks. This would allow the readers to fully assess the utility of the proposed algorithms relative to all existing dropout approaches.\nL544: Is there any sort order for the samples? ( \\theta_1, ..., \\theta_K)? e.g. are samples with higher posterior probabilities likely to be at higher indices? \nWhy not report the result of randomly selecting K out of S samples, as an additional alternative?\nRegular RNN LMs are known to be expensive to train and evaluate. It would be very useful to compare the training/evaluation times for the proposed Bayesian learning algorithms with SGD+ dropout. That would allow the readers to trade-off improvements versus increase in training/run times.\nClarifications: L346: What does \\theta_s refer to? Is this a MAP estimate of parameters based on only the sample s? \nL453-454: Clarify what \\theta means in the context of dropout/dropconnect.  Typos: L211: output L738: RMSProp",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "eef02718862506a1",
    "paper_id": "ACL_2017_554",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "1) The paper is trying to bridge the gap between Stochastic Gradient MCMC and Stochastic Optimization in deep learning context. Given dropout/dropConnect and variational inference are commonly used to reduce the overfit, the more systematic way to introduce/analyse such bayesian learning based algorithms would benefit deep learning community. \n2) For language modeling tasks, the proposed SG-MCMC optimizer + dropout outperforms RMSProp + dropout, which clearly shows that uncertainty modeling would help reducing the over-fitting, hence improving accuracy. \n3) The paper has provided the details about the model/experiment setups so the results should be easily reproduced.",
    "weaknesses": "1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm. \n2) The paper only shows the comparison between SG-MCMC vs RMSProp and did not conduct other comparison. It should explain more about the relation between pSGLD vs RMSProp other than just mentioning they are conterparts in two families. \n2) The paper does not talk about the training speed impact with more details.",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "18adac131122b730",
    "paper_id": "ACL_2017_554",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper explores a relatively under-explored area of practical application of ideas behind Bayesian neural nets in NLP tasks. With a Bayesian treatment of the parameters of RNNs, it is possible to incorporate benefits of model averaging during inference. Further, their gradient based sampling approximation to the posterior estimation leads to a procedure which is easy to implement and is potentially much cheaper than other well-known techniques for model averaging like ensembling.   The effectiveness of this approach is shown on three different tasks -- language modeling, image captioning and sentence classification; and performance gains are observed over the baseline of single model optimization.",
    "weaknesses": "Exact experimental setup is unclear. The supplementary material contains important details about burn-in, number of epochs and samples collected that should be in the main paper itself. Moreover, details on how the inference is performed would be helpful. Were the samples that were taken following HMC for a certain number of epochs after burn in on the training data fixed for inference (for every \\tilda{Y} during test time, same samples were used according to eqn 5) ? Also, an explicit clarification regarding an independence assumption that p(D|\\theta) = p(Y,X| \\theta) = p(Y| \\theta,X)p(X), which lets one use the conditional RNN model (if I understand correctly) for the potential U(\\theta) would be nice for completeness.\nIn terms of comparison, this paper would also greatly benefit from a discussion/ experimental comparison with ensembling and distillation methods (\"Sequence level knowledge distillation\"; Kim and Rush, \"Distilling an Ensemble of Greedy Dependency Parsers into One MST Parser\"; Kuncoro et al.) which  are intimately related by a similar goal of incorporating effects of model averaging.\nFurther discussion related to preference of HMC related sampling methods over other sampling methods or variational approximation would be helpful.\nFinally, equation 8 hints at the potential equivalence between dropout and the proposed approach and the theoretical justification behind combining SGLD and dropout (by making the equivalence more concrete) would lead to a better insight into the effectiveness of the proposed approach.",
    "comments": "Points addressed above.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "b15d119443464ad6",
    "paper_id": "ACL_2017_557",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- The paper is clearly written and well-structured.   - The system newly applied several techniques including global optimization to end-to-end neural relation extraction, and the direct incorporation of the parser representation is interesting.\n - The proposed system has achieved the state-of-the-art performance on both ACE05 and CONLL04 data sets.\n - The authors include several analyses.",
    "weaknesses": "- The approach is incremental and seems like just a combination of existing methods.    - The improvements on the performance (1.2 percent points on dev) are relatively small, and no significance test results are provided.",
    "comments": "- Major comments:  - The model employed a recent parser and glove word embeddings. How did they affect the relation extraction performance?\n - In prediction, how did the authors deal with illegal predictions?\n- Minor comments:  - Local optimization is not completely \"local\". It \"considers structural correspondences between incremental decisions,\" so this explanation in the introduction is misleading.\n - Points in Figures 6 and 7 should be connected with straight lines, not curves.\n - How are entities represented in \"-segment\"?\n - Some citations are incomplete. Kingma et al. (2014) is accepted to ICLR, and Li et al. (2014) misses pages.",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "81d1f205cdd867e0",
    "paper_id": "ACL_2017_561",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper introduces a general method for improving NLP tasks using embeddings from language models. Context independent word representations have been very useful, and this paper proposes a nice extension by using context-dependent word representations obtained from the hidden states of neural language models. \nThey show significant improvements in tagging and chunking tasks from including embeddings from large language models. There is also interesting analysis which answers several natural questions.\nOverall this is a very good paper, but I have several suggestions: - Too many experiments are carried out on the test set. Please change Tables 5 and 6 to use development data - It would be really nice to see results on some more tasks - NER tagging and chunking don't have many interesting long range dependencies, and the language model might really help in those cases. I'd love to see results on SRL or CCG supertagging.\n- The paper claims that using a task specific RNN is necessary because a CRF on top of language model embeddings performs poorly. It wasn't clear to me if they were backpropagating into the language model in this experiment - but if not, it certainly seems like there is potential for that to make a task specific RNN unnecessary.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "78a02916f933d1e1",
    "paper_id": "ACL_2017_561",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "For the most part, the paper is well-written and easy to follow. The method is extensively documented. The discussion is broad and thorough.",
    "weaknesses": "Sequence tagging does not equal chunking and NER. I am surprised not to see POS tagging included in the experiment, while more sequence tagging tasks would be welcome: grammatical error detection, supersense tagging, CCG supertagging, etc. This way, the paper is on chunking and NER for English, not for sequence tagging in general, as it lacks both the multilingual component and the breadth of tasks.\nWhile I welcomed the extensive description of the method, I do think that figures 1 and 2 overlap and that only one would have sufficed.\nRelated to that, the method itself is rather straightforward and simple. While this is by all means not a bad thing, it seems that this contribution could have been better suited for a short paper. Since I do enjoy the more extensive discussion section, I do not necessarily see it as a flaw, but the core of the method itself does not strike me as particularly exciting. It's more of a \"focused contribution\" (short paper description from the call) than \"substantial\" work (long paper).",
    "comments": "Bottomline, the paper concatenates two embeddings, and sees improvements in English chunking and NER.\nAs such, does it warrant publication as an ACL long paper? I am ambivalent, so I will let my score reflect that, even if I slightly lean towards a negative answer. Why? Mainly because I would have preferred to see more breadth: a) more sequence tagging tasks and b) more languages.\nAlso, we do not know how well this method scales to low(er)-resource scenarios. \nWhat if the pre-trained embeddings are not available? What if they were not as sizeable as they are? The experiments do include a notion of that, but still far above the low-resource range. Could they not have been learned in a multi-task learning setup in your model? That would have been more substantial in my view.\nFor these reasons, I vote borderline, but with a low originality score. The idea of introducing context via the embeddings is nice in itself, but this particular instantiation of it leaves a lot to ask for.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "7d4ffdb403cb5962",
    "paper_id": "ACL_2017_562",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Zero-shot relation extraction is an interesting problem. The authors have created a large dataset for relation extraction as question answering which would likely be useful to the community.",
    "weaknesses": "Comparison and credit to existing work is severely lacking. Contributions of the paper don't seen particularly novel.",
    "comments": "The authors perform relation extraction as reading comprehension. In order to train reading comprehension models to perform relation extraction, they create a large dataset of 30m “querified” (converted to natural language) relations by asking mechanical turk annotators to write natural language queries for relations from a schema. They use the reading comprehension model of Seo et al. 2016, adding the ability to return “no relation,” as the original model must always return an answer. The main motivation/result of the paper appears to be that the authors can perform zero-shot relation extraction, extracting relations only seen at test time.\nThis paper is well-written and the idea is interesting. However, there are insufficient experiments and comparison to previous work to convince me that the paper’s contributions are novel and impactful.\nFirst, the authors are missing a great deal of related work: Neelakantan at al. 2015 (https://arxiv.org/abs/1504.06662) perform zero-shot relation extraction using RNNs over KB paths. Verga et al. 2017 (https://arxiv.org/abs/1606.05804) perform relation extraction on unseen entities. The authors cite Bordes et al. (https://arxiv.org/pdf/1506.02075.pdf), who collect a similar dataset and perform relation extraction using memory networks (which are commonly used for reading comprehension). However, they merely note that their data was annotated at the “relation” level rather than at the triple (relation, entity pair) level… but couldn’t Bordes et al. have done the same in their annotation? \nIf there is some significant difference here, it is not made clear in the paper. There is also a NAACL 2016 paper (https://www.aclweb.org/anthology/N/N16/N16-2016.pdf) which performs relation extraction using a new model based on memory networks… and I’m sure there are more. Your work is so similar to much of this work that you should really cite and establish novelty wrt at least some of them as early as the introduction -- that's how early I was wondering how your work differed, and it was not made clear.\nSecond, the authors neither 1) evaluate their model on another dataset or 2) evaluate any previously published models on their dataset. This makes their empirical results extremely weak. Given that there is a wealth of existing work that performs the same task and the lack of novelty of this work, the authors need to include experiments that demonstrate that their technique outperforms others on this task, or otherwise show that their dataset is superior to others (e.g. since it is much larger than previous, does it allow for better generalization?)",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "100ecf2113decb58",
    "paper_id": "ACL_2017_562",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The technique seems to be adept at identifying relations (a bit under 90 F-measure). It works well both on unseen questions (for seen relations) and relatively well on unseen relations. The authors describe a method for obtaining a large training dataset",
    "weaknesses": "I wish performance was also shown on standard relation extraction datasets - it is impossible to determine what types of biases the data itself has here (relations are generated from Wikidata via WikiReading - extracted from Wikipedia, not regular newswire/newsgroups/etc). It seems to me that the NIST TAC-KBP slot filling dataset is good and appropriate to run a comparison.\nOne comparison that the authors did not do here (but should) is to train a relation detection model on the generated data, and see how well it compares with the QA approach.",
    "comments": "I found the paper to be well written and argued, and the idea is interesting, and it seems to work decently. I also found it interesting that the zero-shot NL method behaved indistinguishably from the single question baseline, and not very far from the multiple questions system.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "9aa7febcf168f224",
    "paper_id": "ACL_2017_562",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper models the relation extraction problem as reading comprehension and extends a previously proposed reading comprehension (RC) model to extract unseen relations. The approach has two main components: 1. Queryfication: Converting a relation into natural question. Authors use crowdsourcing for this part.\n2. Applying RC model on the generated questions and sentences to get the answer spans. Authors extend a previously proposed approach to accommodate situations where there is no correct answer in the sentence.\nMy comments: 1. The paper reads very well and the approach is clearly explained.\n2. In my opinion, though the idea of using RC for relation extraction is interesting and novel, the approach is not novel. A part of the approach is crowdsourced and the other part is taken directly from a previous work, as I mention above.\n3. Relation extraction is a well studied problem and there are plenty of recently published works on the problem. However, authors do not compare their methods against any of the previous works. This raises suspicion on the effectiveness of the approach. As seen from Table 2, the performance numbers of the proposed method on the core task are not very convincing. However, this maybe because of the dataset used in the paper. Hence, a comparison with previous methods would actually help assess how the current method stands with the state-of-the-art.\n4. Slot-filling data preparation: You say \"we took the first sentence s in D to contain both e and a\". How can you get the answer sentence for (all) the relations of an entity from the first sentence of the entity's Wikipedia article? Please clarify this. See the following paper. They have a set of rules to locate (answer) sentences corresponding to an entity property in its Wikipedia page: Wu, Fei, and Daniel S. Weld. \" Open information extraction using Wikipedia.\" \nProceedings of the 48th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2010.\nOverall, I think the paper presents an interesting approach. However, unless the effectiveness of the approach is demonstrated by comparing it against recent works on relation extraction, the paper is not ready for publication.",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "12c901328eb2dcd7",
    "paper_id": "ACL_2017_563",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The idea to investigate the types of relations between lexical items is very interesting and challenging. The authors make a good argument why going beyond analogy testing makes sense.",
    "weaknesses": "The paper does not justify or otherwise contextualize the choice of clustering for evaluation, rather than using a classification task, despite the fact that classification tasks are more straightforward to evaluate. No attempt is being made to explain the overall level of the results. How well would humans do on this task (given only the words, no context)?",
    "comments": "I have read the authors' response.",
    "overall_score": "4",
    "confidence": "2"
  },
  {
    "review_id": "b443272b0ae96324",
    "paper_id": "ACL_2017_563",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper investigates the application of distributional vectors of meaning in tasks that involve the identification of semantic relations, similar to the analogical reasoning task of Mikolov et al. (2013): Given an expression of the form “X is for France what London is for the UK”, X can be approximated by the simple vector arithmetic operation London-UK+France. The authors argue that this simple method can only capture very specific forms of analogies, and they present a measure that aims at identifying a wider range of relations in a more effective way.\nI admit I find the idea of a single vector space model being able to capture a number of semantic relationships and analogies rather radical and infeasible. \nAs the authors mention in the paper, a number of studies already suggest for the opposite. The reason is quite simple: behind all these models lies (some form of) the distributional hypothesis (words in similar contexts have similar meanings), and this poses certain limitations in their expressive abilities; for example, words like “big” and “small” will always be considered as semantically similar from a vector perspective (although they express opposite meanings), since they occur in similar contexts. So I cannot see how the example given in Figure 1 is relevant to the very nature of vector spaces (or to any other semantic model for that matter!): there is a certain analogy between “man-king”, and “woman-queen”, but asking from a word space to capture “has-a” relationships of the form “owl-has-claws”, hence “hospital-has-walls”, doesn’t make much sense to me.\nThe motivation behind the main proposal of the paper (a similarity measure that involves a form of cross-comparison between vectors of words and vectors representing the contexts of the words) is not clearly explained. Further, the measure is tested on the relation categories of the SemEval 2010 task with rather unsatisfactory results; in almost all cases, a simple baseline that takes into account only partial similarities between the tested word pairs present very high performance, with a difference from the best-performing model which seems to me statistically insignificant. So from both a methodological and an experimental perspective, the paper has weaknesses, and in its current form seems to describe work in progress;  as such I am inclined against its presentation in ACL.\n(Formatting issue: The authors use the LaTeX styles for ACL 2016 — this should be fixed in case the paper is accepted).\nAUTHORS RESPONSE ================ Thank you for the clarifications. I am still not comfortable with the idea of a metric or a vector space that tries to capture both semantic and relational similarity, and I think you don't present enough experimental evidence that your method works. I have to agree with one of the other reviewers that a more appropriate format for this work would be a short paper.",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "647164c40737e905",
    "paper_id": "ACL_2017_563",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper is clearly written and easy to understand.",
    "weaknesses": "My main complaint about the paper is the significance of its contributions. I believe it might be suitable as a short paper, but certainly not a full-length paper.\nUnfortunately, there is little original thought and no significantly strong experimental results to back it up. The only contribution of this paper is an 'in-out' similarity metric, which is itself adapted from previous work. The results seem to be sensitive to the choice of clusters and only majorly outperforms a very naive baseline when the number of clusters is set to the exact value in the data beforehand.\nI think that relation classification or clustering from semantic vector space models is a very interesting and challenging problem. This work might be useful as an experimental nugget for future reference on vector combination and comparison techniques, as a short paper. Unfortunately, it does not have the substance to merit a full-length paper.",
    "comments": "",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "df5a4b5403274a14",
    "paper_id": "ACL_2017_564",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "of the paper is that the experiments are somewhat limited. \nThe interactive MT simulation shows that the method basically works, but it is difficult to get a sense of how well - for instance, in how many cases the constraint was incorporated in an acceptable manner (the large BLEU score increases are only indirect evidence). Similarly, adaptation should have been  compared to the standard “fine-tuning” baseline, which would be relatively inexpensive to run on the 100K Autodesk corpus.\nDespite this weakness, I think this is a decent contribution that deserves to be published.\nFurther details: 422 Given its common usage in PBMT, “coverage vector” is a potentially misleading term. The appropriate data structure seems more likely to be a coverage set.\nTable 2 should also give some indication of the number of constraints per source sentence in the test corpora, to allow for calibration of the BLEU gains.",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "56af5fd9fc3fa140",
    "paper_id": "ACL_2017_56",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper presents an extension of many popular methods for learning vector representations of text.  The original methods, such as skip-gram with negative sampling, Glove, or other PMI based approaches currently use word cooccurrence statistics, but all of those approaches could be extended to n-gram based statistics.  N-gram based statistics would increase the complexity of every algorithm because both the vocabulary of the embeddings and the context space would be many times larger.  This paper presents a method to learn embeddings for ngrams with ngram context, and efficiently computes these embeddings.  On similarity and analogy tasks, they present strong results.",
    "weaknesses": "I would have loved to see some experiments on real tasks where these embeddings are used as input beyond the experiments presented in the paper.  That would have made the paper far stronger.",
    "comments": "Even with the aforementioned weakness, I think this is a nice paper to have at ACL.\nI have read the author response.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "c34e46dcbd0903dd",
    "paper_id": "ACL_2017_56",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The idea to train word2vec-type models with ngrams (here specifically: bigrams) instead of words is excellent. The range of experimental settings (four word2vec-type algorithms, several word/bigram conditions) covers quite a bit of ground. The qualitative inspection of the bigram embeddings is interesting and shows the potential of this type of model for multi-word expressions.",
    "weaknesses": "This paper would benefit from a check by a native speaker of English, especially regarding the use of articles. The description of the similarity and analogy tasks comes at a strange place in the paper (4.1 Datasets).",
    "comments": "As is done at some point well into the paper, it could be clarified from the start that this is simply a generalization of the original word2vec idea, redefining the word as an ngram (unigram) and then also using bigrams. It would be good to give a rationale why larger ngrams have not been used.\n(I have read the author response.)",
    "overall_score": "4",
    "confidence": "2"
  },
  {
    "review_id": "455e2876d213d5a6",
    "paper_id": "ACL_2017_56",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- The proposed work seems like a natural extension of existing work on learning word embeddings. By integrating bigram information, one can expect to capture richer syntactic and semantic information.",
    "weaknesses": "- While the authors propose learning embeddings for bigrams (bi_bi case), they actually do not evaluate the embeddings for the learned bigrams except for the qualitative evaluation in Table 7. A more quantitative evaluation on paraphrasing or other related tasks that can include bigram representations could have been a good contribution.\n- The evaluation and the results are not very convincing - the results do not show consistent trends, and some of the improvements are not necessarily statistically significant.\n- The paper reads clunkily due to significant grammar and spelling errors, and needs a major editing pass.",
    "comments": "This paper is an extension of standard embedding learning techniques to include information from bigram-bigram coocurance. While the work is interesting and a natural extension of existing work, the evaluation and methods leaves some open questions. Apart from the ones mentioned in the weaknesses, some minor questions for the authors : - Why is there significant difference between the overlap and non-overlap cases? I would be more interested in finding out more than the quantitative difference shown on the tasks.\nI have read the author response. I look forward to seeing the revised version of the paper.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "81e0eb66c0994628",
    "paper_id": "ACL_2017_578",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper proposes an end-to-end neural model for semantic graph parsing, based on a well-designed transition system. \nThe work is interesting, learning semantic representations of DMRS, which is capable of resolving semantics such as scope underspecification. This work shows a new scheme for computational semantics, benefiting from an end-to-end transition-based incremental framework, which resolves the parsing with low cost.",
    "weaknesses": "My major concern is that the paper only gives a very common introduction for the definition of DMRS and EP, and the example even makes me a little confused because I cannot see anything special for DMRS. The description can be a little more detailed, I think. However, upon the space limitation, it is understandable. The same problem exists for the transition system of the parsing model. If I do not have any background of MRS and EP, I can hardly learn something from the paper, just seeing that this paper is very good.",
    "comments": "Overall, this paper is very interesting to me. I like the DMRS for semantic parsing very much and like the paper very much. Hope that the open-source codes and datasets can make this line of research being a hot topic.",
    "overall_score": "5",
    "confidence": "4"
  },
  {
    "review_id": "b7b0eac646b5c158",
    "paper_id": "ACL_2017_579",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "[+] Well motivated, tackles an interesting problem; [+] Clearly written and structured, accompanied by documented code and dataset; [+] Encouraging results.",
    "weaknesses": "[-] Limited to completely deterministic, hand-engineered minimization rules; [-] Some relevant literature on OIE neglected; [-] Sound but not thorough experimental evaluation.",
    "comments": "This paper tackles a practical issue of most OIE systems, i.e. redundant, uninformative and inaccurate extractions. The proposed approach, dubbed MinOIE, is designed to actually \"minimize\" extractions by removing overly specific portions and turning them into structured annotations of various types (similarly to OLLIE). The authors put MinIE on top of a state-of-the-art OIE system (ClausIE) and test it on two publicly available datasets, showing that it effectively leads to more concise extractions compared to standard OIE approaches, while at the same time retaining accuracy.\nOverall, this work focuses on an interesting (and perhaps underinvestigated) aspect of OIE in a sound and principled way. The paper is clearly written, sufficiently detailed, and accompanied by supplementary material and a neat Java implementation. \nMy main concern is, however, with the entirely static, deterministic and rule-based structure of MinIE. Even though I understand that a handful of manually engineered rules is technically the best strategy when precision is key, these approaches are typically very hard to scale, e.g. in terms of languages (a recent trend of OIE, see Faruqui and Kumar, 2015; Falke et al., 2016). In other words, I think that this contribution somehow falls short of novelty and substance in proposing a pipeline of engineered rules that are mostly inspired by other OIE systems (such as ClausIE or ReVerb); for instance, I would have really appreciated an attempt to learn these minimization rules instead of hard-coding them.\nFurthermore, the authors completely ignore a recent research thread on “semantically-informed” OIE (Nakashole et al., 2012; Moro and Navigli, 2012; 2013; Delli Bovi et al., 2015) where traditional extractions are augmented with links to underlying knowledge bases and sense inventories (Wikipedia, Wikidata, Yago, BabelNet). These contributions are not only relevant in terms of related literature: in fact, having text fragments (or constituents) explicitly linked to a knowledge base would reduce the need for ad-hoc minimization rules such as those in Sections 6.1 and 6.2. In the example with \"Bill of Rights\" provided by the authors (line 554), an OIE pipeline with a proper Entity Linking module would recognize automatically the phrase as mention of a registered entity, regardless of the shape of its subconstituents. \nAlso, an underlying sense inventory would seamlessly incorporate the external information about collocations and multi-word expressions used in Section 6.2: not by chance, the authors rely on WordNet and Wiktionary to compile their dictionary of collocations.\nFinally, some remarks on the experimental evaluation: - Despite the claim of generality of MinIE, the authors choose to experiment only with ClausIE as underlying OIE system (most likely the optimal match). It would have been very interesting to see if the improvement brought by MinIE is consistent also with other OIE systems, in order to actually assess its flexibility as a post-processing tool.\n- Among the test datasets used in Section 7, I would have included the recent OIE benchmark of Stanovsky and Dagan (2016), where results are reported also for comparison systems not included in this paper (TextRunner, WOIE, KrakeN).\nReferences: - Manaal Faruqui and Shankar Kumar. Multilingual Open Relation Extraction using Cross-lingual Projection. NAACL-HLT, 2015.\n- Tobias Falke, Gabriel Stanovsky, Iryna Gurevych and Ido Dagan. Porting an Open Information Extraction System from English to German. EMNLP 2016.\n- Ndapandula Nakashole, Gerhard Weikum and Fabian Suchanek. PATTY: A Taxonomy of Relational Patterns with Semantic Types. EMNLP 2012.\n- Andrea Moro, Roberto Navigli. WiSeNet: Building a Wikipedia-based Semantic Network with Ontologized Relations. CIKM 2012.\n- Andrea Moro, Roberto Navigli. Integrating Syntactic and Semantic Analysis into the Open Information Extraction Paradigm. IJCAI 2013.\n- Claudio Delli Bovi, Luca Telesca and Roberto Navigli. Large-Scale Information Extraction from Textual Definitions through Deep Syntactic and Semantic Analysis. TACL vol. 3, 2015.\n- Gabriel Stanovsky and Ido Dagan. Creating a Large Benchmark for Open Information Extraction. EMNLP 2016.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "e5dbc29f26fc174d",
    "paper_id": "ACL_2017_588",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The collection of desiderata for the task is well-chosen to advance the field: predicting blanked-out named entities, a task that has already shown to be interesting in the CNN/Daily Mail dataset, but in a way that makes the task hard for language models; and the focus on rare entities should drive the field towards more interesting models.  The collection of baselines is well chosen to show that neither a NN model without external knowledge nor a simple cosine similarity based model with external knowledge can do the task well.\nThe two main models are chosen well.\nThe text is clear and well argued.",
    "weaknesses": "I was a bit puzzled by the fact that using larger contexts, beyond the sentences with blanks in them, did not help the models. After all, you were in a way using additional context in the HierEnc model, which accumulates knowledge from other contexts. There are two possible explanations: Either the sentences with blanks in them are across the board more informative for the task than the sentences without. This is the explanation suggested in the paper, but it seems a bit unintuitive that this should be the case. Another possible explanation is that the way that you were using additional context in HierEnc, using the temporal network, is much more useful than by enlarging individual contexts C and feeding that larger C into the recurrent network.  Do you think that that could be what is going on?",
    "comments": "I particularly like the task and the data that this paper proposes. This setup can really drive the field forward, I think. This in my mind is the main contribution.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "3e5ebf0c13c38ca3",
    "paper_id": "ACL_2017_588",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The proposed task requires encoding external knowledge, and the associated dataset may serve as a good benchmark for evaluating hybrid NLU systems.",
    "weaknesses": "1) All the models evaluated, except the best performing model (HIERENC), do not have access to contextual information beyond a sentence. This does not seem sufficient to predict a missing entity. It is unclear whether any attempts at coreference and anaphora resolution have been made. It would generally help to see how well humans perform at the same task.\n2) The choice of predictors used in all models is unusual. It is unclear why similarity between context embedding and the definition of the entity is a good indicator of the goodness of the entity as a filler.\n3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. \nThis does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\n4) The results are not very informative. Given that this is a rare entity prediction problem, it would help to look at type-level accuracies, and  analyze how the accuracies of the proposed models vary with frequencies of entities.\n- Questions to the authors: 1) An important assumption being made is that d_e are good replacements for entity embeddings. Was this assumption tested?\n2) Have you tried building a classifier that just takes h_i^e as inputs?\nI have read the authors' responses. I still think the task+dataset could benefit from human evaluation. This task can potentially be a good benchmark for NLU systems, if we know how difficult the task is. The results presented in the paper are not indicative of this due to the reasons stated above. Hence, I am not changing my scores.",
    "comments": "The paper deals with the task of predicting missing entities in a given context using the Freebase definitions of those entities. The authors highlight the importance of the problem, given that the entities come from a long-tailed distribution. They use popular sequence encoders to encode the context and the definitions of candidate entities, and score them based on their similarity with the context. While it is clear that the task is indeed important, and the dataset may be useful as a benchmark, the approach has some serious weaknesses and the evaluation leaves some questions unanswered.  - Strengths: The proposed task requires encoding external knowledge, and the associated dataset may serve as a good benchmark for evaluating hybrid NLU systems.\n- Weaknesses: 1) All the models evaluated, except the best performing model (HIERENC), do not have access to contextual information beyond a sentence. This does not seem sufficient to predict a missing entity. It is unclear whether any attempts at coreference and anaphora resolution have been made. It would generally help to see how well humans perform at the same task.\n2) The choice of predictors used in all models is unusual. It is unclear why similarity between context embedding and the definition of the entity is a good indicator of the goodness of the entity as a filler.\n3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. \nThis does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.\n4) The results are not very informative. Given that this is a rare entity prediction problem, it would help to look at type-level accuracies, and  analyze how the accuracies of the proposed models vary with frequencies of entities.\n- Questions to the authors: 1) An important assumption being made is that d_e are good replacements for entity embeddings. Was this assumption tested?\n2) Have you tried building a classifier that just takes h_i^e as inputs?\nI have read the authors' responses. I still think the task+dataset could benefit from human evaluation. This task can potentially be a good benchmark for NLU systems, if we know how difficult the task is. The results presented in the paper are not indicative of this due to the reasons stated above. Hence, I am not changing my scores.",
    "overall_score": "2",
    "confidence": "5"
  },
  {
    "review_id": "3fdc4d484e44c2fe",
    "paper_id": "ACL_2017_588",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper empirically verifies that using external knowledge is a benefit.",
    "weaknesses": "Real world NLP applications should utilize external knowledge for making better predictions. The authors propose Rare Entity prediction task to demonstrate this is the case. However, the motivation of the task is not fully justified. \nWhy is this task important? How would real world NLP applications benefit from this task? The paper lacks a convincing argument for proposing a new task. For current reading comprehension task, the evidence for a correct answer can be found in a given text, thus we are interested in learning a model of the world (i.e causality for example), or a basic reasoning model. Comparing to reading comprehension, rare entity prediction is rather unrealistic as humans are terrible with remembering name. The authors mentioned that the task is difficult due to the large number of rare entities, however challenging tasks with the same or even more difficult level exist, such as predicting correct morphological form of a word in morphologically rich languages. Such tasks have obvious applications in machine translation for example.",
    "comments": "It would be helpful if the authors characterize the dataset in more details. \nFrom figure 1 and table 4, it seems to me that overlapping entities is an important feature. There is noway i can predict the **blank** in figure 1 if I don't see the word London in Peter Ackoyd description. That's being said, before brutalizing neural networks, it is essential to understand the characteristic of the data and the cognitive process that searches for the right answer.\nGiven the lack of characteristic of the dataset, I find that the baselines are inappropriate. First of all, the CONTENC is a natural choice at the first sigh. \nHowever as the authors mentioned that candidate entities are rare, the embeddings of those entities are unrealizable. As a consequence, it is expected that CONTENC doesn't work well. Would it is fairer if the embeddings are initialized from pre-trained vectors on massive dataset? One would expect some sort of similarity between Larnaca and Cyprus in the embedding space and CONTENC would make a correct prediction in Table 4. What would be the performance of TF-IDF+COS and AVGEMB+COS if only entities are used to compute those vectors?\nFrom modeling perspective, I appreciate that the authors chose a sigmoid predictor that output a numerical score between (0,1). This would help avoiding normalization over the list of candidates, which are rare and is difficult to learn reliable weights for those. However, a sidestep technique does exist, such as Pointer Network. A representation h_i for C_i (*blank* included) can be computed by an LSTM or BiLSTM, then Pointer Network would give a probabilistic interpretation p(e_k|C_i) \\propto exp(dot(d_{e_k}, h_i)). In my opinion, Pointer Network would be an appropriate baseline. Another related note: Does the unbalanced set of negative/positive labels affect the training? During training, the models make 1 positive prediction while number of negative predictions is at least 4 times higher?\nWhile I find the task of Rare Entity prediction is unrealistic, having the dataset, it would be more interesting to learn about the reasoning process that leads to the right answer such as which set of words the model attends to when making prediction.",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "f84fff52886f2fd5",
    "paper_id": "ACL_2017_606",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper introduces a new approach to semantic parsing in which the model is equipped with a neural sequence to sequence (seq2seq) model (referred to as the “programmer”) which encodes a natural language question and produces a program. The programmer is also equipped with a ‘key variable’ memory component which stores (a) entities in the questions (b) values of intermediate variables formed during execution of intermediate programs. These variables are referred to further build the program.                    The model is also equipped with certain discrete operations (such as argmax or 'hop to next edges in a KB'). A separate component (\"interpreter/computer\") executes these operations and stores intermediate values (as explained before). Since the ‘programmer' is inherently a seq2seq model, the \"interpreter/computer” also acts as a syntax/type checker only allowing the decoder to generate valid tokens. For example, the second argument to the “hop” operation has to be a KB predicate. Finally the model is trained with weak supervision and directly optimizes the metric which is used to evaluate the performance (F score). \nBecause of the discrete operations and the non differentiable reward functions, the model is trained with policy gradients (REINFORCE). Since gradients obtained through REINFORCE have high variance, it is common to first pretrain the model with a max-likelihood objective or find some good sequences of actions trained through some auxiliary objective. This paper takes a latter approach in which it finds good sequences via an iterative maximum likelihood approach. The results and discussion sections are presented in a very nice way and the model achieves SOTA results on the WebQuestions dataset when compared to other weakly supervised model.\nThe paper is written clearly and is very easy to follow.\nThis paper presents a new and exciting direction and there is scope for a lot of future research in this direction. I would definitely love to see this presented in the conference.\nQuestions for the authors (important ones first) 1. Another alternative way of training the model would be to bootstrap the parameters (\\theta) from the iterative ML method instead of adding pseudo gold programs in the beam (Line 510 would be deleted). Did you try that and if so why do you think it didn’t work? \n2. What was the baseline model in REINFORCE. Did you have a separate network which predicts the value function. This must be discussed in the paper in detail. \n3. Were there programs which required multiple hop operations? Or were they limited to single hops. If there were, can you provide an example? ( I will understand if you are bound by word limit of the response) 4. Can you give an example where the filter operation would be used? \n5. I did not follow the motivation behind replacing the entities in the question with special ENT symbol Minor comments: Line 161 describe -> describing Line 318 decoder reads ‘)’ -> decoder generates ‘)'",
    "overall_score": "5",
    "confidence": "4"
  },
  {
    "review_id": "f8a354dcf67bc885",
    "paper_id": "ACL_2017_606",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- The idea of using discrete, symbolic memories for neural execution models is novel.                    Although in implementation it may simply reduce to copying previously executed variable tokens from an extra buffer, this approach is still impressive since it works well for a large-scale semantic parsing task.\n- The proposed revised REINFORCE training schema using imperfect hypotheses derived from maximum likelihood training is interesting and effective, and could inspire future exploration in mixing ML/RL training for neural sequence-to-sequence models.\n- The scale of experiments is larger than any previous works in modeling neural execution and program induction. The results are impressive.\n- The paper is generally clear and well-written, although there are some points which might require further clarification (e.g., how do the keys ($v_i$'s in Fig. 2) of variable tokens involved in computing action probabilities? \nConflicting notations: $v$ is used to refer to variables in Tab. 1 and memory keys in Fig 1.).\nOverall, I like this paper and would like to see it in the conference.",
    "weaknesses": "- [Choice of Dataset] The authors use WebQuestionsSP as the testbed. Why not using the most popular WebQuestions (Berant et al., 2013) benchmark set? Since NSM only requires weak supervision, using WebQuestions would be more intuitive and straightforward, plus it could facilitate direct comparison with main-stream QA research.\n- [Analysis of Compositionality] One of the contribution of this work is the usage of symbolic intermediate execution results to facilitate modeling language compositionality. One interesting question is how well questions with various compositional depth are handled. Simple one-hop questions are the easiest to solve, while complex multi-hop ones that require filtering and superlative operations (argmax/min) would be highly non-trivial. The authors should present detailed analysis regarding the performance on question sets with different compositional depth.\n- [Missing References] I find some relevant papers in this field missing. For example, the authors should cite previous RL-based methods for knowledge-based semantic parsing (e.g., Berant and Liang., 2015), the sequence level REINFORCE training method of (Ranzato et al., 2016) which is closely related to augmented REINFORCE, and the neural enquirer work (Yin et al., 2016) which uses continuous differentiable memories for modeling neural execution.\n- Misc.\n- Why is the REINFORCE algorithm randomly initialized (Algo. 1) instead of using parameters pre-trained with iterative ML?\n- What is KG server in Figure 5?",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "bae43fd3908bef66",
    "paper_id": "ACL_2017_614",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- The key idea of marrying vector space model based approaches and sense inventories for the lexsub task is useful since these two techniques seem to have complementary information, especially since the vector space models are typically unaware of sense and polysemy.\n- The oracle evaluation is interesting as it gives a clear indication of how much gain can one expect in the best case, and while there is still a large gap between the oracle and actual scores, we can still argue for the usefulness of the proposed approach due to the large difference between the unfiltered GAP and the oracle GAP.",
    "weaknesses": "- I don't understand effectiveness of the multi-view clustering approach. \nAlmost all across the board, the paraphrase similarity view does significantly better than other views and their combination. What, then, do we learn about the usefulness of the other views? There is one empirical example of how the different views help in clustering paraphrases of the word 'slip', but there is no further analysis about how the different clustering techniques differ, except on the task directly. Without a more detailed analysis of differences and similarities between these views, it is hard to draw solid conclusions about the different views.                                   - The paper is not fully clear on a first read. Specifically, it is not immediately clear how the sections connect to each other, reading more like disjoint pieces of work. For instance, I did not understand the connections between section 2.1 and section 4.3, so adding forward/backward pointer references to sections should be useful in clearing up things. Relatedly, the multi-view clustering section (3.1) needs editing, since the subsections seem to be out of order, and citations seem to be missing (lines 392 and 393).\n- The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.",
    "comments": "The paper is mostly straightforward in terms of techniques used and experiments. Even then, the authors show clear gains on the lexsub task by their two-pronged approach, with potentially more to be gained by using stronger WSD algorithms.\nSome additional questions for the authors : - Lines 221-222 : Why do you add hypernyms/hyponyms?\n- Lines 367-368 : Why does X^{P} need to be symmetric?\n- Lines 387-389 : The weighting scheme seems kind of arbitrary. Was this indeed arbitrary or is this a principled choice?\n- Is the high performance of SubstClus^{P} ascribable to the fact that the number of clusters was tuned based on this view? Would tuning the number of clusters based on other matrices affect the results and the conclusions?\n- What other related tasks could this approach possibly generalize to? Or is it only specific to lexsub?",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "d9fc1b3a84e1fa59",
    "paper_id": "ACL_2017_614",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper presents a new method that exploits word senses to improve the task of lexical substitutability.  Results show improvements over prior methods.",
    "weaknesses": "As a reader of a ACL paper, I usually ask myself what important insight can I take away from the paper, and from a big picture point of view, what does the paper add to the fields of natural language processing and computational linguistics.  How does the task of lexical substitutability in",
    "comments": "",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "24a63306789eb223",
    "paper_id": "ACL_2017_619",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "of your work, but I am not sure about how helpful this resource is for the NLP community as a whole. Perhaps such a resource would be better presented in a specialised workshop such as BEA or a specialised conference on language resources like LREC instead of a general NLP conference like ACL.\nYou mentioned in the last paragraph that you would like to augment the corpus with more annotation. Are you also willing to include more essays?\nComments/Minor: - As you have essays by native and non-native speakers, one further potential application of this corpus is native language identification (NLI).\n- p. 7: \"where the unigram feature was used as the baseline\" - \"word unigram\". \nBe more specific.\n- p. 7: \"and the SVM classifier was used as the classifier.\" - redundant.",
    "weaknesses": "",
    "comments": "",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "72d0ba2fe6ac915b",
    "paper_id": "ACL_2017_627",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper presents a framework where a differentiable access to the KB is integrated in the joint optimisation. This is the biggest contribution of the paper.",
    "weaknesses": "Firstly, this is not a truly end-to-end system considering the response generation was handcrafted rather than learnt. Also, their E2E model actually overfits to the simulator and performs poorly in human evaluation. \nThis begs the question whether the authors are actually selling the idea of E2E learning or the soft-KB access. The soft-KB access actually brings consistent improvement, however the idea of end-to-end learning not so much. The authors tried to explain the merits of E2E in Figure 5 but I also fail to see the difference. In addition, the authors didn't motivate the reason for using the reinforce algorithm which is known to suffer from high variance problem. They didn't attempt to improve it by using a baseline or perhaps considering the natural actor-critic algorithm which is known to perform better.",
    "comments": "Apart from the mentioned weaknesses, I think the experiments are solid and this is generally an acceptable paper. However, if they crystallised the paper around the idea which actually improves the performance (the soft KB access) but not the idea of E2E learning the paper would be better.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "2efd08656e6ad1bd",
    "paper_id": "ACL_2017_636",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- Extensive experiments against various architectures (LSTM, LSTM + CRF)        - Novel architectural/training ideas (sharing blocks)",
    "weaknesses": "- Only applied to English NER--this is a big concern since the title of the paper seems to reference sequence-tagging directly.   - Section 4.1 could be clearer. For example, I presume there is padding to make sure the output resolution after each block is the same as the input resolution.  Might be good to mention this.   - I think an ablation study of number of layers vs perf might be interesting.\nRESPONSE TO AUTHOR REBUTTAL: Thank you very much for a thoughtful response. Given that the authors have agreed to make the content be more specific to NER as opposed to sequence-tagging, I have revised my score upward.",
    "comments": "",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "4977a9f37fcffb67",
    "paper_id": "ACL_2017_636",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The main strength promised by the paper is the speed advantage at the same accuracy level.",
    "weaknesses": "Presentation of the approach leaves a lot to be desired. Sections 3 and 4 need to be much clearer, from concept definition to explaining the architecture and parameterization. In particular Section 4.1 and the parameter tieing used need to be crystal clear, since that is one of the main contributions of the paper.\nMore experiments supporting the vast speed improvements promised need to be presented. The results in Table 2 are good but not great. A speed-up of 4-6X is nothing all that transformative.",
    "comments": "What exactly is \"Viterbi prediction\"? The term/concept is far from established; the reader could guess but there must be a better way to phrase it.\nReference Weiss et al., 2015 has a typo.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "7ca43dd503af70db",
    "paper_id": "ACL_2017_649",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper proposes an evaluation metric for automatically evaluating the quality of dialogue responses in non-task-oriented dialogue. The metric operates on continuous vector space representations obtained by using RNNs and it comprises two components: one that compares the context and the given response and the other that compares a reference response and the given response. The comparisons are conducted by means of dot product after projecting the response into corresponding context and reference response spaces. These projection matrices are learned by minimizing the squared error between the model predictions and human annotations.\nI think this work gives a remarkable step forward towards the evaluation of non-task-oriented dialogue systems. Different from previous works in this area, where pure semantic similarity was pursued, the authors are going beyond pure semantic similarity in a very elegant manner by learning projection matrices that transform the response vector into both context and reference space representations. I am very curious on how your projection matrices M and N differ from the original identity initialization after training the models. I think the paper will be more valuable if further discussion on this is introduced, rather than focusing so much on resulting correlations.",
    "weaknesses": "The paper also leaves lots questions related to the implementation. For instance, it is not clear whether the human scores used to train and evaluate the system were single AMT annotations or the resulting average of few annotations. Also, it is not clear how the dataset was split into train/dev/test and whether n-fold cross validation was conducted or not. Also, it would be nice to better explain why in table 2 correlation for ADEM related scores are presented for the validation and test sets, while for the other scores they are presented for the full dataset and test set. The section on pre-training with VHRED is also very clumsy and confusing, probably it is better to give less technical details but a better high level explanation of the pre-training strategy and its advantages.",
    "comments": "“There are many obvious cases where these metrics fail, as they are often incapable of considering the semantic similarity between responses (see Figure 1).” Be careful with statements like this one. This is not a problem of semantic similarity! Opposite to it, the problem is that completely different semantic cues might constitute pragmatically valid responses. Then, semantic similarity itself is not enough to evaluate a dialogue system response. \nDialogue system response evaluation must go beyond semantics (This is actually what your M and N matrices are helping to do!!!)  “an accurate model that can evaluate dialogue response quality automatically — what could be considered an automatic Turing test —“ The original intention of Turing test was to be a proxy to identify/define intelligent behaviour. It actually proposes a test on intelligence based on an “intelligent” machine capability to imitate human behaviour in such a way that it would be difficult for a common human to distinguish between such a machine responses and actual human responses. It is of course related to dialogue system performance, but I think it is not correct to say that automatically evaluating dialogue response quality is an automatic Turing test. \nActually, the title itself “Towards an Automatic Turing Test” is somehow misleading!\n“the simplifying assumption that a ‘good’ chatbot is one whose responses are scored highly on appropriateness by human evaluators.” This is certainly the correct angle to introduce the problem of non-task-oriented dialogue systems, rather than “Turing Test”. Regarding this, there has been related work you might like to take a look at, as well as to make reference to, in the WOCHAT workshop series (see the shared task description and corresponding annotation guidelines).\nIn the discussion session: “and has has been used” -> “and it has been used”",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "4e55febe630dcd17",
    "paper_id": "ACL_2017_654",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Strong model, insightful discussion/error analysis.",
    "weaknesses": "Little to no insights regarding the SRL task itself.",
    "comments": "This paper extends Zhou and Xu's ACL 2015 approach to semantic role labeling based on deep BiLSTMs. In addition to applying recent best practice techniques, leading to further quantitative improvements, the authors provide an insightful qualitative analysis of their results. The paper is well written and has a clear structure. The authors provide a comprehensive overview of related work and compare results to a representative set of other SRL models that hace been applied on the same data sets.\nI found the paper to be interesting and convincing. It is a welcome research contribution that not only shows that NNs work well, but also analyzes merits and shortcomings of an end-to-end learning approach.\n- Strengths: Strong model, insightful discussion/error analysis.\n- Weaknesses: Little to no insights regarding the SRL task itself.",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "2299733d312cd134",
    "paper_id": "ACL_2017_654",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "and the remaining issues, that give a quite valuable information to the researchers in this field.  Even though I understand that the improvement of 3 point in F1 measure is a quite meaningful result from the engineering point of view, I think the main contribution of the paper is on the extensive analysis in the experiment section and a further in-depth investigation on analysis section. The detailed analyses shown in Section 4 are performed in a quite reasonable way and give both comparable results in SRL literature and novel information such as relation between accuracies in syntactic parsing and SRL. This type of analysis had often been omitted in recent papers. However, it is definitely important for further improvement.\nThe paper is well-written and well-structured. \nI really enjoyed the paper and would like to see it accepted.",
    "weaknesses": "",
    "comments": "",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "bacedc9f3d586bf9",
    "paper_id": "ACL_2017_657",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "this paper addresses (in part) the problem of interpreting Long Short-Term Memory (LSTM) neural network models trained to categorize written justifications in values-affirmation essays. This is definitely an interesting research question. To do so, the authors want to rely on approaches that have are standard in experimental psychology. Furthermore, the authors also aim at validating sociological assumptions via this study.",
    "weaknesses": "one of the main weaknesses of the paper lies in the fact that the goals are not clear enough. One overall, ambitious goal put forward by the authors is to use approaches from experimental psychology to interpret LSTMs. \nHowever, no clear methodology to do so is presented in the paper. On the other hand, if the goal is to validate sociological assumptions, then one should do so by studying the relationships between gender markers and the written justifications, independently on any model. The claim that \"expected gender differences (are) a function of theories of gendered self-construal\" is not proven in the study.",
    "comments": "if the study is interesting, it suffers from several weak arguments. First of all, the fact that the probability shift of a token in the LSTM network are correlated with the corresponding SVM coefficients is no proof that \"these probabilities are valid ways to interpret the model\". Indeed, (a) SVM coefficients only reveal part of what is happening in the decision function of an SVM classifie and (b) it is not because one coefficient provides an interpretation in one model that a correlated coefficient provides an explanation in another model. Furthermore, the correlation coefficients are not that high, so that the point put forward is not really backed up.\nAs mentioned before, another problem lies in the fact that the authors seem to hesitate between two goals. It would be better to clearly state one goal and develop it. Concerning the relation to experimental psychology, which is a priori an important part of the paper, it would be interesting to develop and better explain the multilevel bayesian models used to quantify the gender-based self-construal assumptions. It is very difficult to assess whether the methodology used here is really appropriate without more details. As this is an important aspect of the method, it should be further detailed.",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "2c64460ef917c136",
    "paper_id": "ACL_2017_657",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper is thoroughly written and discusses its approach compared to other approaches. The authors are aware that their findings are somewhat limited regarding the mean F values.",
    "weaknesses": "Some minor orthographical mistakes and some repetive clauses. In general the paper would benefit if the sections 1 and 2 would be shortened to allow the extension of sections 3 and 4. \nThe main goal is not laid out clearly enough, which may be a result of the ambivalence of the paper's goals.",
    "comments": "Table 1 should only be one column wide, while the figures, especially 3, 5, and 6 would greatly benefit from a two column width. \nThe paper was not very easy to understand during first read. Major improvements could be achieved by straightening up the content.",
    "overall_score": "3",
    "confidence": "1"
  },
  {
    "review_id": "227978630fda36d0",
    "paper_id": "ACL_2017_660",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper presents two approaches for generating English poetry. The first approach combine a neural phonetic encoder predicting the next phoneme with a phonetic-orthographic HMM decoder computing the most likely word corresponding to a sequence of phonemes. The second approach combines a character language model with a weigthed FST to impose rythm constraints on the output of the language model. For the second approach, the authors also present a heuristic approach which permit constraining the generated poem according to theme (e.g;, love) or poetic devices (e.g., alliteration). The generated poems are evaluated both instrinsically by comparing the rythm of the generated lines with a gold standard and extrinsically by asking 70 human evaluators to (i) determine whether the poem was written by a human or a machine and (ii) rate poems wrt to readability, form and evocation.  The results indicate that the second model performs best and that human evaluators find it difficult to distinguish between human written and machine generated poems.\nThis is an interesting, clearly written article with novel ideas (two different models for poetry generation, one based on a phonetic language model the other on a character LM) and convincing results.\n For the evaluation, more precision about the evaluators and the protocol would be good. Did all evaluators evaluate all poems and if not how many judgments were collected for each poem for each task ? You mention 9 non English native speakers. Poems are notoriously hard to read. How fluent were these ?  In the second model (character based), perhaps I missed it, but do you have a mechanism to avoid generating non words ? If not, how frequent are non words in the generated poems ?\nIn the first model, why use an HMM to transliterate from phonetic to an orhographic representation rather than a CRF?  Since overall, you rule out the first model as a good generic model for generating poetry, it might have been more interesting to spend less space on that model and more on the evaluation of the second model. In particular, I would have been interested in a more detailed discussion of the impact of the heuristic you use to constrain theme or poetic devices. How do these impact evaluation results ? Could they be combined to jointly constrain theme and poetic devices ?  The combination of a neural mode with a WFST is reminiscent of the following paper which combine character based neural model to generate from dialog acts with an WFST to avoid generating non words. YOu should relate your work to theirs and cite them.  Natural Language Generation through Character-Based RNNs with Finite-State Prior Knowledge Goyal, Raghav and Dymetman, Marc and Gaussier, Eric and LIG, Uni COLING 2016",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "580ba8b3e3b15a40",
    "paper_id": "ACL_2017_660",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Good procedure for generating rhythmic poetry.\nProposals for adding control of theme and poetic devices (alliteration, consonance, asonance).\nStrong results in evaluation of rhythm.",
    "weaknesses": "Poor coverage of existing literature on poetry generation.\nNo comparison with existing approaches to poetry generation.\nNo evaluation of results on theme and poetic devices.",
    "comments": "The introduction describes the problem of poetry generation as divided into two subtasks: the problem of content (the poem's semantics) and the problem of form (the  aesthetic rules the poem follows). The solutions proposed in the paper address both of these subtasks in a limited fashion. They rely on neural networks trained over corpora  of poetry (represented at the phonetic or character level, depending on the solution) to encode the linguistic continuity of the outputs. This does indeed ensure that the  outputs resemble meaningful text. To say that this is equivalent to having found a way of providing the poem with appropriate semantics would be an overstatement. The  problem of form can be said to be addressed for the case of rhythm, and partial solutions are proposed for some poetic devices. Aspects of form concerned with structure at a  larger scale (stanzas and rhyme schemes) remain beyond the proposed solutions. \nNevertheless, the paper constitutes a valuable effort in the advancement of poetry generation.\nThe review of related work provided in section 2 is very poor. It does not even cover the set of previous efforts that the authors themselves consider worth mentioning in their paper (the work of Manurung et al 2000 and Misztal and Indurkhya 2014 is cited later in the paper - page 4 - but it is not placed in section 2 with respect to the other authors mentioned there).\nA related research effort of particular relevance that the authors should consider is: - Gabriele Barbieri, François Pachet, Pierre Roy, and Mirko Degli Esposti. \n2012. Markov constraints for generating lyrics with style. In Proceedings of the 20th European Conference on Artificial Intelligence (ECAI'12), Luc De Raedt, Christian Bessiere, Didier Dubois, Patrick Doherty, and Paolo Frasconi (Eds.). IOS Press, Amsterdam, The Netherlands, The Netherlands, 115-120. DOI: https://doi.org/10.3233/978-1-61499-098-7-115 This work addresses very similar problems to those discussed in the present paper (n-gram based generation and the problem of driving generation process with additional constraints). The authors should include a review of this work and discuss the similarities and differences with their own.\nAnother research effort that is related to what the authors are attempting (and has bearing on their evaluation process) is: - Stephen McGregor, Matthew Purver and Geraint Wiggins, Process Based Evaluation of Computer Generated Poetry,  in: Proceedings of the INLG 2016 Workshop on Computational Creativity and Natural Language Generation, pages 51–60,Edinburgh, September 2016.c2016 Association for Computational Linguistics This work is also similar to the current effort in that it models language initially at a phonological level, but considers a word n-gram level superimposed on that, and also features a layer representint sentiment. Some of the considerations McGregor et al make on evaluation of computer generated poetry are also relevant for the extrinsic evaluation described in the present paper.\nAnother work that I believe should be considered is: - \"Generating Topical Poetry\" (M. Ghazvininejad, X. Shi, Y. Choi, and K. Knight), Proc. EMNLP, 2016.\nThis work generates iambic pentameter by combining finite-state machinery with deep learning. It would be interesting to see how the proposal in the current paper constrasts with this particular approach.\nAlthough less relevant to the present paper, the authors should consider extending their classification of poetry generation systems (they mention rule-based expert systems and statistical approaches) to include evolutionary solutions. They already mention in their paper the work of Manurung, which is evolutionary in nature, operating over TAG grammars.\nIn any case, the paper as it stands holds little to no effort of comparison to prior approaches to poetry generation. The authors should make an effort to contextualise their work with respect to previous efforts, specially in the case were similar problems are being addressed (Barbieri et al, 2012) or similar methods are being applied (Ghazvininejad,  et al, 2016).",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "94c4c2fd5075ce43",
    "paper_id": "ACL_2017_66",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper describes several ways to encode arbitrarily long sequences of digits using something called the major system. In the major system, each digit is mapped to one or more characters representing consonantal phonemes; the possible mappings between digit and phoneme are predefined. The output of an encoding is typically a sequence of words constrained such that digits in the original sequence correspond to characters or digraphs in the output sequence of words; vowels added surrounding the consonant phonemes to form words are unconstrained. This paper describes several ways to encode your sequence of digits such that the output sequence of words is more memorable, generally by applying syntactic constraints and heuristics.\nI found this application of natural language processing concepts somewhat interesting, as I have not read an ACL paper on this topic before. However, I found the paper and ideas presented here to have a rather old-school feel. With much of the focus on n-gram models for generation, frequent POS-tag sequences, and other heuristics, this paper really could have been written 15-20 years ago. I am not sure that there is enough novelty in the ideas here to warrant publication in ACL in 2017. There is no contribution to NLP itself, e.g. in terms of modeling or search, and not a convincing contribution to the application area which is just an instance of constrained generation.  Since you start with one sequence and output another sequence with a very straightforward monotonic mapping, it seems like a character-based sequence-to-sequence encoder-decoder model (Sequence to Sequence Learning with Neural Networks; Sutskever et al. 2014) would work rather well here, very likely with very fluent output and fewer moving parts (e.g. trigram models and POS tag and scoring heuristics and postprocessing with a bigram model). You can use large amounts of training from an arbitrary genre and do not need to rely on an already-tagged corpus like in this paper, or worry about a parser. This would be a 2017 paper.",
    "overall_score": "2",
    "confidence": "5"
  },
  {
    "review_id": "7894e6269e39ec1e",
    "paper_id": "ACL_2017_66",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper presents a sentence based approach to generating memorable mnemonics for numbers. The evaluation study presented in the paper shows that the sentence based approach indeed produces memorable mnemonics for short 8-digit numbers (e.g. 86101521 --> Officiate Wasteland). \nOverall the paper presents the problem, the background literature and the solution in sufficient detail. Because memorizing numbers (e.g. phone numbers and account numbers) is sufficiently common, this is an interesting problem.",
    "weaknesses": "The proposed solution does not seem to scale-up well for longer numbers; seems to work well with 8-digit numbers though. But many numbers that people need to memorize such as phone numbers and credit card numbers are longer than 8-digits. Besides, a number may have a structure (e.g. a phone number has a country code + area code + personal number) which people exploit while memorizing numbers. As stated above, this paper addresses an important problem but the current solution needs to be improved further (several ideas have been listed by the authors in section 6).",
    "comments": "The current presented approach, in comparison to existing approaches, is promising.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "54219c39cbee3bc0",
    "paper_id": "ACL_2017_66",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Tackles a not very explored task, with obvious practical application Well written and motivated",
    "weaknesses": "The only method of validation is a user study, which has several weaknesses.\n- Discussion: The paper investigates various methods to generate memorable mnemonic encodings of numbers based on the “Major” system. As opposed to other methods that rely on this system to encode sequences, the methods proposed in this work return a single sequence (instead of a set of candidates) which is selected to improve memorability. Since “memorability” is an ambiguous criterion to optimize for, the authors explore various syntactic approaches that aim for short and likely sentences.  Their final model uses a POS template sampled form a set of “nice” structures, and a tri-gram language model to fill in the slots of the template.  The proposed approach is well motivated: the section on existing tools places this approach in the context of previous work on security and memorability. The authors point to results showing that passwords based on mnemonic phrases offer the best of both worlds in terms of security (vs random passwords) and memorability (vs naive passwords). This solid motivation will appease those readers initially skeptical about the importance/feasibility of such techniques.  In terms of the proposed methods, the baselines and n-gram models (unsurprisingly) generate bad encodings. The results in table 2 show that indeed Chunk and Sentence produce shorter sentences, but for short digits such as this one, how relevant are the additional characteristics of these methods (eg. POS replacements, templates etc)? It seems that a simple n-gram model with the number-of-digits-per-trigram reweighing could perform well here.  The evaluation is weaker than the rest of the paper. My main concern is that a one-time memorization setting seems inadequate to test this framework. Mnemonic techniques are meant to aid recall after repeated memorization exercises, not just a single “priming” event. Thus, a more informative setting would have had the users be reminded of the number and encoding daily over a period of time, and after a “buffer period”, test their recall. This would also more closely resemble the real-life conditions in which such a technique would be used (e.g. for password memorization).\nIn terms of the results, the difference between (long term) recall and recognition is interesting. Do the authors have some explanation for why in the former most methods performed similarly, but in the latter “Sentence” performs better? Could it be that the use of not very likely words (e.g. \"officiate\", in the example provided) make the encodings hard to remember but easy to spot? If this were the case, it would somewhat defeat the purpose of the approach.\nAlso, it would be useful for the reader if the paper provided  (e.g. in an appendix) some examples of the digits/encodings that the users were presented during the study, to get a better sense of the difficulty of recall and the quality of the encodings.  - Suggestions: It would be nice to provide some background on the Major system for those not familiar with it, which I suspect might be many in the ACL audience, myself included. Where does it come from? What’s the logic behind those digit-phoneme maps?",
    "comments": "",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "8070ef4aa5344614",
    "paper_id": "ACL_2017_676",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The proposed methods can save memory and improve decoding speed on CPUs without losing (or a little loss) performance.",
    "weaknesses": "Since the determination of the convolutional codes of Algorithm 2 and Algorithm 3 can affect the final performance, I think it would be better if the authors can explore a good method for it. And I think the argument of “Experiments show the proposed model achieves translation accuracies that approach the softmax, while reducing memory usage on the order of 1/10 to 1/1000, and also improving decoding speed on CPUs by x5 to x20.” in the Abstract is not rigorous. As far as I know, your experiments setting with “Binary” and “Hybrid-512” on ASPEC corpus show the improvements of decoding speed on CPUs by x20, but the BLEU scores are too low. So this is not a valid conclusion.",
    "comments": "This paper proposes an efficient prediction method for neural machine translation, which predicts a binary code for each word, to reduce the complexity of prediction. The authors also proposed to use the improved (error correction) binary codes method to improve the prediction accuracy and the hybrid softmax/binary model to balance the prediction accuracy and efficiency. \nThe proposed methods can save memory and improve decoding speed without losing (or a little loss) performance. I think this is a good paper.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "223fca7b78354f31",
    "paper_id": "ACL_2017_676",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper is well written, and with clear, well-designed   figures. The reader can easily understand the methodology even only   with those figures.\n  Predicting the binary code directly is a clever way to reduce the   parameter space, and the error-correction code just works   surprisingly well. I am really surprised by how 44 bits can achieve   26 out of 31 BLEU.     The parameter reducing technique described in this work is   orthogonal to current existing methods: weight pruning and   sequence-level knowledge distilling.\n  The method here is not restricted by Neural Machine Translation, and   can be used in other tasks as long as there is a big output   vocabulary.",
    "weaknesses": "The most annoying point to me is that in the relatively large   dataset (ASPEC), the best proposed model is still 1 BLEU point lower   than the softmax model. What about some even larger dataset, like   the French-English? There are at most 12 million sentences   there. Will the gap be even larger?\n  Similarly, what's the performance on some other language pairs ?\n  Maybe you should mention this paper,   https://arxiv.org/abs/1610.00072. It speeds up the decoding speed by   10x and the BLEU loss is less than 0.5.",
    "comments": "The paper describes a parameter reducing method for large vocabulary softmax. By applying the error-corrected code and hybrid with softmax, its BLEU approaches that of the orignal full vocab softmax model.\nOne quick question: what is the hidden dimension size of the models? \nI couldn't find this in the experiment setup.\nThe 44 bits can achieve 26 out of 31 BLEU on E2J, that was surprisingly good. However, how could you increase the number of bits to increase the classification power ? 44 is too small, there's plenty of room to use more bits and the computation time on GPU won't even change.\nAnother thing that is counter-intuitive is that by predicting the binary code, the model is actually predicting the rank of the words. So how should we interpret these bit-embeddings ? There seems no semantic relations of all the words that have odd rank. Is it because the model is so powerful that it just remembers the data ?",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "2f06ca4423605fe7",
    "paper_id": "ACL_2017_676",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper has high originality, proposing a fundamentally different way of predicting words from a vocabulary that is more efficient than a softmax layer and has comparable performance on NMT. If successful, the approach could be impactful because it speeds up prediction.\nThis paper is nice to read with great diagrams. it's very clearly presented -- I like cross-referencing the models with the diagrams in Table 2. Including loss curves is appreciated.",
    "weaknesses": "Though it may not be possible in the time remaining, it would be good to see a comparison (i.e. BLEU scores) with previous related work like hierarchical softmax and differentiated softmax.\nThe paper is lacking a linguistic perspective on the proposed method. Compared to a softmax layer and hierarchical/differentiated softmax, is binary code prediction a natural way to predict words? Is it more or less similar to how a human might retrieve words from memory? Is there a theoretical reason to believe that binary code based approaches should be more or less suited to the task than softmax layers?\nThough the paper promises faster training speeds in the introduction, Table 3 shows only modest (less than x2) speedups for training. Presumably this is because much of the training iteration time is consumed by other parts of the network. It would be useful to see the time needed for the output layer computation only.",
    "comments": "It would be nice if the survey of prior work in 2.2 explicitly related those methods to the desiderata in the introduction (i.e. specify which they satisfy).\nSome kind of analysis of the qualitative strengths and weaknesses of the binary code prediction would be welcome -- what kind of mistakes does the system make, and how does this compare to standard softmax and/or hierarchical and differentiated softmax?\nLOW LEVEL COMMENTS Equation 5: what's the difference between id(w) = id(w') and w = w' ?\n335: consider defining GPGPU Table 3: Highlight the best BLEU scores in bold Equation 15: remind the reader that q is defined in equation 6 and b is a function of w. I was confused by this at first because w and h appear on the LHS but don't appear on the right, and I didn't know what b and q were.",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "853eea1c8518b5cc",
    "paper_id": "ACL_2017_67",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- The paper tackles an important issue, that is building ontologies or thesauri - The methods make sense and seem well chosen - Methods and setups are well detailed - It looks like the authors outperform the state-of-the-art approach (but see below for my concerns)",
    "weaknesses": "The main weaknesses for me are evaluation and overall presentation/writing.\n- The list of baselines is hard to understand. Some methods are really old and it doesn't seem justified to show them here (e.g., Mpttern).\n- Memb is apparently the previous state-of-the-art, but there is no mention to any reference.\n- While it looks like the method outperforms the previous best performing approach, the paper is not convincing enough. Especially, on the first dataset, the difference between the new system and the previous state-of-the-art one is pretty small.\n- The paper seriously lacks proofreading, and could not be published until this is fixed – for instance, I noted 11 errors in the first column of page 2.\n- The CilinE hierarchy is very shallow (5 levels only). However apparently, it has been used in the past by other authors. I would expect that the deeper the more difficult it is to branch new hyponym-hypernyms. This can explain the very high results obtained (even by previous studies)...",
    "comments": "The approach itself is not really original or novel, but it is applied to a problem that has not been addressed with deep learning yet. For this reason, I think this paper is interesting, but there are two main flaws. The first and easiest to fix is the presentation. There are many errors/typos that need to be corrected. I started listing them to help, but there are just too many of them. \nThe second issue is the evaluation, in my opinion. Technically, the performances are better, but it does not feel convincing as explained above. \nWhat is Memb, is it the method from Shwartz et al 2016, maybe? If not, what performance did this recent approach have? I think the authors need to reorganize the evaluation section, in order to properly list the baseline systems, clearly show the benefit of their approach and where the others fail. \nSignificance tests  also seem necessary given the slight improvement on one dataset.",
    "overall_score": "2",
    "confidence": "5"
  },
  {
    "review_id": "836146cdffe05dbd",
    "paper_id": "ACL_2017_67",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "* Knowledge lean, language-independent approach",
    "weaknesses": "* Peculiar task/setting   * Marginal improvement over W_Emb (Fu et al, 2014)   * Waste of space   * Language not always that clear",
    "comments": "It seems to me that this paper is quite similar to (Fu et al, 2014) and only adds marginal improvements. It contains quite a lot of redundancy (e.g. related work in  sec 1 and sec 2), uninformative figures (e.g. Figure 1 vs Figure 2), not so useful descriptions of MLP and RNN, etc. A short paper might have been a better fit.\nThe task looks somewhat idiosyncratic to me. It is only useful if you already have a method that gives you all and only the hypernyms of a given word. This seems to presuppose (Fu et al., 2013).  Figure 4: why are the first two stars connected by conjunction and the last two starts by disjunction?              Why is the output \"1\" (dark star) if the the three inputs are \"0\" (white stars)?\nSec 4.2, lines 587-589 appears to suggest that thresholds were tuned on the test data (?)  W_Emb is poorly explained (lines 650-652).\nSome parts of the text are puzzling. I can't make sense of the section titled \"Combined with Manually-Built Hierarchies\". Same for sec 4.4. What do the red and dashed lines mean?",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "3c1a19407355f4a3",
    "paper_id": "ACL_2017_684",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper presents a gated attention mechanism for machine reading. \nA key idea is to extend Attention Sum Reader (Kadlec et al. 2016) to multi-hop reasoning by fine-grained gated filter. \nIt's interesting and intuitive for machine reading. \nI like the idea along with significant improvement on benchmark datasets, but also have major concerns to get it published in ACL.\n- The proposed GA mechanism looks promising, but not enough to convince the importance of this technique over other state-of-the-art systems, because engineering tricks presented 3.1.4 boost a lot on accuracy and are blended in the result.\n- Incomplete bibliography: Nearly all published work in reference section refers arxiv preprint version. \nThis makes me (and future readers) suspicious if this work thoroughly compares with prior work. Please make them complete if the published version is available.  - Result from unpublished work (GA): GA baseline in table 1 and 3 is mentioned as previous work that is unpublished preprint. \nI don't think this is necessary at all. Alternately, I would like the author to replace it with vanilla GA (or variant of the proposed model for baseline). \nIt doesn't make sense that result from the preprint which will end up being the same as this ACL submission is presented in the same manuscript. \nFor fair blind-review, I didn't search on arvix archive though.\n- Conflict on table 1 and 2: GA-- (table 1) is the same as K=1(AS) in table 2, and GA (fix L(w)) is for K=3 in table 2. \nDoes this mean that GA-- is actually AS Reader? \nIt's not clear that GA-- is re-implementation of AS. \nI assumed K=1 (AS) in table 2 uses also GloVe initialization and token-attention, but it doesn't seem in GA--.  - I wish the proposed method compared with prior work in related work section (i.e. what's differ from related work).\n- Fig 2 shows benefit of gated attention (which translates multi-hop architecture), and it's very impressive. It would be great to see any qualitative example with comparison.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "c09d78e3437c99ea",
    "paper_id": "ACL_2017_684",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper presents an interesting model for reading comprehension, by depicting the multiplicative interactions between the query and local information around a word in a document, and the authors proposed a new gated-attention strategy to characterize the relationship. The work is quite solid, with almost state of art result on the whole four cloze-style datasets achieved. Some of the further improvement can be helpful for the similar tasks.\nNevertheless, I have some concerns on the following aspect: 1. The authors have referred many papers from arXiv, but I think some really related works are not included. Such as the works from Caiming Xiong, et al. https://openreview.net/pdf?id=rJeKjwvclx and the work form Shuohang Wang, et al. https://openreview.net/pdf?id=B1-q5Pqxl . Both of them concentrated on enhancing the attention operation to modeling the interaction between documents and queries. Although these works are not evaluated on the cloze-style corpus but the SQuAD, an experimental or fundamental comparison may be necessary.\n2. There have been some studies that adopts attention mechanism or its variants specially designed for the Reading Comprehension tasks, and the work actually share the similar ideas with this paper. My suggestion is to conduct some comparisons with such work to enhance the experiments of this paper.",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "52dee26a88fb16b5",
    "paper_id": "ACL_2017_684",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- Paper is very well-written and every aspect of the model is well-motivated and clearly explained.\n- The authors have extensively covered the previous work in the area.\n- The approach achieves state-of-the-art results across several text comprehension data sets. In addition, the experimental evaluation is very thorough.",
    "weaknesses": "- Different variants of the model achieve state-of-the-art performance across various data sets. However, the authors do provide an explanation for this (i.e. size of data set and text anonymization patterns).",
    "comments": "The paper describes an approach to text comprehension which uses gated attention modules to achieve state-of-the-art performance. Compared to previous attention mechanisms, the gated attention reader uses the query embedding and makes multiple passes (multi-hop architecture) over the document and applies multiplicative updates to the document token vectors before finally producing a classification output regarding the answer. This technique somewhat mirrors how humans solve text comprehension problems. Results show that the approach performs well on large data sets such as CNN and Daily Mail. For the CBT data set, some additional feature engineering is needed to achieve state-of-the-art performance.  Overall, the paper is very well-written and model is novel and well-motivated. \nFurthermore, the approach achieves state-of-the-art performance on several data sets.  I had only minor issues with the evaluation. The experimental results section does not mention whether the improvements (e.g. in Table 3) are statistically significant and if so, which test was used and what was the p-value. Also I couldn't find an explanation for the performance on CBT-CN data set where the validation performance is superior to NSE but test performance is significantly worse.",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "b146a3dbc834409b",
    "paper_id": "ACL_2017_68",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper broadens the applicability of readability scores to an additional language, and produces a well-validated applicability score for Vietnamese.",
    "weaknesses": "The greatest weaknesses, with respect to ACL are that 1) readability scores are of limited interest within the field of computational linguistics. While they are somewhat useful in educational and public communication fields, their impact on the progress of computational linguistics is limited.  A minor weakness is in the writing: the paper has numerous minor grammatical errors. \nAlthough the discussion compares the performance of the PDS1 and PDW1 features from the previous work, it is unclear how poorly the previous readability measures perform, relevant to the one developed here, for practical purposes.",
    "comments": "This paper would be a stronger candidate for inclusion if the corpus (and importantly, labels developed) were released. It could be used more widely than the development of scalar readability metrics, and would enable (e.g.) investigation of application of more powerful feature-selection methods.",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "66d736a7d247e54d",
    "paper_id": "ACL_2017_68",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "New Dataset,   NLP on Resource poor language",
    "weaknesses": "Incomplete related work references,   No comparison with recent methods and approaches,   Lack of technical contribution,   Weak experiments,",
    "comments": "In this paper the authors present a simple formula for readability assessment of Vietnamese Text. Using a combination of features such as word count, sentence length etc they train a simple regression model to estimate the readability of the documents.  One of the major weaknesses of the paper its lack of technical contribution - while early work in readability assessment employed simple methods like the one outlined in this paper, recent work on predicting readability uses more robust methods that rely on language models for instance (Eg : http://www.cl.cam.ac.uk/~mx223/readability_bea_2016.pdf, http://www-personal.umich.edu/~kevynct/pubs/ITL-readability-invited-article-v10 -camera.pdf). A comparison with such methods could be a useful contribution and make the paper stronger especially if simple methods such as those outlined in this paper can compete with more complicated models.  Baseline experiments with SMOG, Gunning Fog index etc should also be presented as well as the other Vietnamese metrics and datasets that the authors cite.  Another problem is that while previous readability indices were more selective and classified content into granular levels corresponding to grade levels (for instance), the authors use a coarse classification scheme to label documents as easy, medium and hard which makes the metric uninteresting. ( Also, why not use a classifier?)\nThe work is probably a bit too pre-mature and suffers from significant weaknesses to be accepted at this stage. I would encourage the authors to incorporate suggested feedback to make it better.  The paper also has quite a few grammatical errors which should be addressed in any future submission.",
    "overall_score": "1",
    "confidence": "5"
  },
  {
    "review_id": "f248cb7b53d68549",
    "paper_id": "ACL_2017_68",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The authors present a new formula for assessing readability of Vietnamese texts. The formula is developed based on a multiple regression analysis with three features. Furthermore, the authors have developed and annotated a new text corpus with three readability classes (easy, middle, hard).\nResearch on languages other than English is interesting and important, especially when it comes to low-resource languages. Therefore, the corpus might be a nice additional resource for research (but it seems that the authors will not publish it - is that right?). However, I don't think the paper is convincing in its current shape or will influence future research. Here are my reasons: - The authors provide no reasons why there is a need for delevoping a new formula for readability assessments, given that there already exist two formulas for Vietnamese with almost the same features. What are the disadvantages of those formulas and why is the new formula presented in this paper better?\n- In general, the experimental section lacks comparisons with previous work and analysis of results. The authors claim that the accuracy of their formula (81% on their corpus) is \"good and can be applied in practice\". What would be the accuracy of other formulas that already exist and what are the pros and cons of those existing formulas compared to the new one?\n- As mentioned before, an analysis of results is missing, e.g. which word / sentence lengths / number of difficult words are considered as easy/middle/hard by their model?\n- A few examples how their formula could be applied in a practical application would be nice as well.\n- The related work section is rather a \"background\" section since it only presents previously published formulas. What I'm missing is a more general discussion of related work. There are some papers that might be interesting for that, e.g., DuBay 2004: \"The principles of readability\", or Rabin 1988: \"Determining difficulty levels of text written in languages other than English\" - Since Vietnamese is syllable-based and not word-based, I'm wondering how the authors get \"words\" in their study. Do they use a particular approach for merging syllables? And if yes, which approach do they use and what's the accuracy of the approach?\n- All in all, the content of the paper (experiments, comparisons, analysis, discussion, related work) is not enough for a long paper.\nAdditional remarks: - The language needs improvements - Equations: The usage of parentheses and multiplying operators is inconsistent - Related works section: The usage of capitalized first letters is inconsistent",
    "overall_score": "1",
    "confidence": "4"
  },
  {
    "review_id": "12b063a52483ebde",
    "paper_id": "ACL_2017_691",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "PP attachment results seem solid.",
    "weaknesses": "Whether the sense embeddings are meaningful remains uninvestigated.  The probabilistic model has some details that are hard to understand. Are the \\lambda_w_i hyperparameters or trained? Where does “rank” come from, is this taken from the sense ranks in WordNet?\nRelated work: the idea of expressing embeddings of words as a convex combination of sense embeddings has been proposed a number of times previously. \nFor instance, Johansson and Nieto Piña “Embedding a semantic network in a word space” (NAACL, 2015) decomposed word embeddings into ontology-grounded sense embeddings based on this idea. Also in unsupervised sense vector training this idea has been used, for instance by Arora et al “Linear Algebraic Structure of Word Senses, with Applications to Polysemy”.\nMinor comments: no need to define types and tokens, this is standard terminology why is the first \\lamba_w_i in equation 4 needed if the probability is unnormalized?",
    "comments": "",
    "overall_score": "2",
    "confidence": "5"
  },
  {
    "review_id": "34bd10dfac9a6150",
    "paper_id": "ACL_2017_699",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper is well-organized and easy to follow (the intuition of the proposed method is clear). It includes enough details to replicate experiments. Although the application of an encoder-decoder (+ copy mechanism) is straightforward, experimental results are reasonable and support the claim (generation of absent keyphrases) presented in this paper.",
    "weaknesses": "As said above, there is little surprise in the proposed approach. Also, as described in Section 5.3, the trained model does not transfer well to new domain (it goes below unsupervised models). One of the contribution of this paper is to maintain training corpora in good quantity and quality, but it is not (explicitly) stated.",
    "comments": "I like to read the paper and would be pleased to see it accepted. I would like to know how the training corpus (size and variation) affects the performance of the proposed method. Also, it would be beneficial to see the actual values of p_g and p_c (along with examples in Figure 1) in the CopyRNN model. From my experience in running the CopyNet, the copying mechanism sometimes works unexpectedly (not sure why this happens).",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "3ebb0623eb82022f",
    "paper_id": "ACL_2017_699",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "1. The formation and extraction of key phrases, which are absent in the current document is an interesting idea of significant research interests.  2. The paper is easily understandable.\n3. The use of RNN and Copy RNN in the current context is a new idea. As, deep recurrent neural networks are already used in keyphrase extraction (shows very good performance also), so, it will be interesting to have a proper motivation to justify the use of  RNN and Copy RNN over deep recurrent neural networks.",
    "weaknesses": "1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.\n2. The evaluation process shows that the current system (which extracts 1. \nPresent and 2. Absent both kinds of keyphrases) is evaluated against baselines (which contains only \"present\" type of keyphrases). Here there is no direct comparison of the performance of the current system w.r.t. other state-of-the-arts/benchmark systems on only \"present\" type of key phrases. It is important to note that local phrases (keyphrases) are also important for the document. The experiment does not discuss it explicitly. It will be interesting to see the impact of the RNN and Copy RNN based model on automatic extraction of local or \"present\" type of key phrases.\n3. The impact of document size in keyphrase extraction is also an important point. It is found that the published results of [1], (see reference below) performs better than (with a sufficiently high difference) the current system on Inspec (Hulth, 2003) abstracts dataset.  4. It is reported that current system uses 527,830 documents for training, while 40,000 publications are held out for training baselines. Why are all publications not used in training the baselines? Additionally,        The topical details of the dataset (527,830 scientific documents) used in training RNN and Copy RNN are also missing. This may affect the chances of repeating results.\n5. As the current system captures the semantics through RNN based models. So, it would be better to compare this system, which also captures semantics. Even, Ref-[2] can be a strong baseline to compare the performance of the current system.\nSuggestions to improve: 1. As, per the example, given in the Figure-1, it seems that all the \"absent\" type of key phrases are actually \"Topical phrases\". For example: \"video search\", \"video retrieval\", \"video indexing\" and \"relevance ranking\", etc. \nThese all define the domain/sub-domain/topics of the document. So, In this case, it will be interesting to see the results (or will be helpful in evaluating \"absent type\" keyphrases): if we identify all the topical phrases of the entire corpus by using tf-idf and relate the document to the high-ranked extracted topical phrases (by using Normalized Google Distance, PMI, etc.). As similar efforts are already applied in several query expansion techniques (with the aim to relate the document with the query, if matching terms are absent in document).\nReference: 1. Liu, Zhiyuan, Peng Li, Yabin Zheng, and Maosong Sun. 2009b. Clustering to find exemplar terms for keyphrase extraction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 257–266.\n2. Zhang, Q., Wang, Y., Gong, Y., & Huang, X. (2016). Keyphrase extraction using deep recurrent neural networks on Twitter. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (pp. 836-845).",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "2015f502e4785f1b",
    "paper_id": "ACL_2017_699",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Novel model.  I particularly like the ability to generate keyphrases not present in the source text.",
    "weaknesses": "Needs to be explicit whether all evaluated models are trained and tested on the same data sets.  Exposition of the copy mechanism not quite clear/convincing.",
    "comments": "This paper presents a supervised neural network approach for keyphrase generation.  The model uses an encoder-decoder architecture that first encodes input text with a RNN, then uses an attention mechanism to generate keyphrases from the hidden states.  There is also a more advanced variant of the decoder which has an attention mechanism that conditions on the keyphrase generated in the previous time step.\nThe model is interesting and novel. And I think the ability to generate keyphrases not in the source text is particularly appealing.  My main concern is with the evaluation:  Are all evaluated models trained with the same amount of data and evaluated on the same test sets?              It's not very clear.  For example, on the NUS data set, Section 4.2 line 464 says that the supervised baselines are evaluated with cross validation.\nOther comments: The paper is mostly clearly written and easy to follow.  However, some parts are unclear: - Absent keyphrases vs OOV.  I think there is a need to distinguish   between the two, and the usage meaning of OOV should be consistent.  The RNN models   use the most frequent 50000 words as the vocabulary (Section 3.4   line 372, Section 5.1 line 568), so I suppose OOV are words not in   this 50K vocabulary.              In line 568, do you mean OOV or absent   words/keyphrases?  Speaking of this, I'm wondering how many   keyphrases fall outside of this 50K?              The use of \"unknown words\"   in line 380 is also ambiguous.  I think it's probably clearer to say that  the RNN models can generate words not present in the source text as long as they appear somewhere else in the corpus (and the 50K vocabulary) - Exposition of the copy mechanism (section 3.4).  This mechanism has a   more specific locality than the attention model in basic RNN model. \n  However, I find the explanation of the intuition misleading.              If I   understand correctly, the \"copy mechanism\" is conditioned on the   source text locations that matches the keyphrase in the previous   time step y_{t-1}.  So maybe it has a higher tendency to generate n-grams seen source text (Figure 1).  I buy the argument that the more sophisticated   attention model probably makes CopyRNN better than the RNN   overall, but why is the former model particularly better for absent   keyphrases?  It is as if both models perform equally well on present keyphrases.\n- How are the word embeddings initialized?",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "28b8caf6090f0f16",
    "paper_id": "ACL_2017_706",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- The paper is well written - It provides a compelling direction/solution to the problem of dealing with a large set of possible programs while learning natural language interfaces.",
    "weaknesses": "- The authors should discuss the effect of the incentives on the final performance ? Were other alternatives considered ?  - While the paper claims that the method can be extended to more practical domains, it is not clear to me how straightforward it is going to be. How sensitive is the method to the size of the vocabulary required in a domain ? \nWould increased ambiguity in natural language create new problems ? These questions are not discussed in the current experiments.\n- A real-world application would definitely strengthen the paper even more.",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "517866d598284090",
    "paper_id": "ACL_2017_706",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper reports on an interesting project to enable people to design their own language for interacting with a computer program, in place of using a programming language. The specific construction that the authors focus on is the ability for people to make definitions. Very nicely, they can make recursive definitions to arrive at a very general way of giving a command. The example showing how the user could generate definitions to create a palm tree was motivating. The approach using learning of grammars to capture new cases seems like a good one.",
    "weaknesses": "This seems to be an extension of the ACL 2016 paper on a similar topic. It would be helpful to be more explicit about what is new in this paper over the old one.  There was not much comparison with previous work: no related work section.  The features for learning are interesting but it's not always clear how they would come into play. For example, it would be good to see an example of how the social features influenced the outcome. I did not otherwise see how people work together to create a language.",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "985af28d94641cbc",
    "paper_id": "ACL_2017_706",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The ideas and the task addressed in this paper are beautiful and original. \nCombining indirect supervision (accepting the resulting parse) with direct supervision (giving a definition) makes it a particularly powerful way of interactively building a natural language interface to a programming language. \nThe proposed has a wide range of potential applications.",
    "weaknesses": "The paper has several typos and language errors and some text seems to be missing from the end of section 6. It could benefit from careful proofreading by a native English speaker.",
    "comments": "The paper presents a method for collaborative naturalization of a 'core' programming language by a community of users through incremental expansion of the syntax of the language. This expansion is performed interactively, whereby a user just types a command in the naturalized language, and then either selects through a list of candidate parses or provides a definition also in the natural language. The users give intuitive definitions using literals instead of variables (e.g. \"select orange\"), which makes this method applicable to non-programmers. \nA grammar is induced incrementally which is used to provide the candidate parses.\nI have read the authors' response.",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "92a083342104e43c",
    "paper_id": "ACL_2017_715",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "*- Task *- Simple model, yet the best results on SQuAD (single model0 *- Evaluation and comparison",
    "weaknesses": "*- Analysis of errors/results (See detailed comments below)",
    "comments": "In this paper the authors present a method for directly querying Wikipedia to answer open domain questions. The system consist of two components - a module to query/fetch wikipedia articles and a module to answer the question given the fetched set of wikipedia articles.  The document retrieval system is a traditional IR system relying on term frequency models and ngram counts.  The answering system uses a feature representation for paragraphs that consists of word embeddings, indicator features to determine whether a paragraph word occurs in a question, token-level features including POS, NER etc and a soft feature for capturing similarity between question and paragraph tokens in embedding space. A combined feature representation is used as an input to a bi-direction LSTM RNN for encoding. For questions an RNN that works on the word embeddings is used. \nThese are then used to train an overall classifier independently for start and end spans of sentences within a paragraph to answer questions.\nThe system has been trained using different Open Domain QA datasets such as SQuAD and WebQuestions by modifying the training data to include articles fetched by the IR engine instead of just the actual correct document/passage.\nOverall, an easy to follow interesting paper but I had a few questions: 1) The IR system has a Accuracy@5 of over 75 %, and individually the document reader performs well and can beat the best single models on SquAD. What explains the significant drop in Table 6. The authors mention that instead of the fetched results, if they test using the best paragraph the accuracy reaches just 0.49 (from 0.26) but that is still significantly below the 0.78-79 in the SQuAD task.  So, presumably the error is this large because the neural network for matching isnt doing as good a job in learning the answers when using the modified training set (which includes fetched articles) instead of the case when training and testing is done for the document understanding task. Some analysis of whats going on here should be provided. What was the training accuracy in the both cases? What can be done to improve it? To be fair, the authors to allude to this in the conclusion but I think it still needs to be part of the paper to provide some meaningful insights.\n2) I understand the authors were interested in treating this as a pure machine comprehension task and therefore did not want to rely on external sources such as Freebase which could have helped with entity typing        but that would have been interesting to use. Tying back to my first question -- if the error is due to highly relevant topical sentences as the authors mention, could entity typing have helped?\nThe authors should also refer to QuASE (Sun et. al 2015 at WWW2015) and similar systems in their related work. QuASE is also an Open domain QA system that answers using fetched passages - but it relies on the web instead of just Wikipedia.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "7737fa85ce2281c5",
    "paper_id": "ACL_2017_715",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The authors focus on a very challenging task of answering open-domain question from Wikipedia. Authors have developed 1) a document retriever to retrieve relevant Wikipedia articles for a question, and 2) Document retriever to retrieve the exact answer from the retrieved paragraphs. \nAuthors used Distant Supervision to fine-tune their model. Experiments show that the document reader performs better than WikiSearch API, and Document Reader model does better than some recent models for QA.",
    "weaknesses": "The final results are inferior to some other models, as presented by the authors. Also, no error analysis is provided.",
    "comments": "The proposed systems by the authors is end-to-end and interesting. However, I have some concerns below.\nDocument Retriever: Authors have shown a better retrieval performance than Wiki Search. However, it is not described as to how exactly the API is used. \nWikiSearch may not be a good baseline for querying \"questions\" (API suits structured retrieval more). Why don't the authors use some standard IR baselines for this?\nDistant Supervision: How effective and reliable was distant supervision? \nClearly, the authors had to avoid using many training examples because of this, but whatever examples the authors could use, what fraction was actually \"close to correct\"? Some statistics would be helpful to understand if some more fine-tuning of distant supervision could have helped.\nFull Wikipedia results: This was the main aim of the authors and as authors themselves said, the full system gives a performance of 26.7 (49.6 when correct doc given, 69.5 when correct paragraph is given). Clearly, that should be a motivation to work more on the retrieval aspect? For WebQuestions, the results are much inferior to YodaQA, and that raises the question -- whether Wikipedia itself is sufficient to answer all the open-domain questions? Should authors think of an integrated model to address this?  Overall, the final results shown in Tables 4 and 5 are inferior to some other models. While authors only use Wikipedia, the results are not indicative of this being the best strategy.\nOther points: The F1 value in Table 5 (78.4) is different from that in Table 4 (Both Dev and Test). \nTable 5: Why not \"No f_emb\"? \nError analysis: Some error analysis is required in various components of the system. \nAre there some specific type of questions, where the system does not perform well? Is there any way one can choose which question is a good candidate to be answered by Wikipedia, and use this method only for those questions? \nFor WebQuestions, DS degrades the performance further.",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "3e66538eee703510",
    "paper_id": "ACL_2017_71",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- The paper states clearly the contributions from the beginning     - Authors provide system and dataset    - Figures help in illustrating the approach    - Detailed description of the approach    - The authors test their approach performance on other datasets and compare to other published work",
    "weaknesses": "-The explanation of methods in some paragraphs is too detailed and there is no mention of other work and it is repeated in the corresponding method sections, the authors committed to address this issue in the final version. \n   -README file for the dataset [Authors committed to add README file]",
    "comments": "- Section 2.2 mentions examples of DBpedia properties that were used as features. Do the authors mean that all the properties have been used or there is a subset? If the latter please list them. In the authors' response, the authors explain in more details this point and I strongly believe that it is crucial to list all the features in details in the final version for clarity and replicability of the paper. \n   - In section 2.3 the authors use Lample et al. Bi-LSTM-CRF model, it might be beneficial to add that the input is word embeddings (similarly to Lample et al.)    - Figure 3, KNs in source language or in English? ( since the mentions have been translated to English). In the authors' response, the authors stated that they will correct the figure. \n   - Based on section 2.4 it seems that topical relatedness implies that some features are domain dependent. It would be helpful to see how much domain dependent features affect the performance. In the final version, the authors will add the performance results for the above mentioned features, as mentioned in their response. \n   - In related work, the authors make a strong connection to Sil and Florian work where they emphasize the supervised vs. unsupervised difference. The proposed approach is still supervised in the sense of training, however the generation of training data doesn’t involve human interference",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "b6bfe0ec4b317891",
    "paper_id": "ACL_2017_71",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- Very impressive resource - fully automatic system - particularly suitable for cross-lingual learning across many languages - Good evaluation both within and outside wikipedia. Good comparison to works that employed manual resources.",
    "weaknesses": "- The clarity of the paper can be improved.",
    "comments": "This paper presents \"a simple yet effective framework that can extract names from 282 languages and link them to an English KB\". Importantly, the system is fully automatic, which is particularly important when aiming to learn across such a large number of languages. Although this is far from trivial, the authors are able to put their results in context and provide evaluation both within and outside of wikipedia - I particularly like the way the put their work in the context of previous work that uses manual resources, it is a good scientific practice and I am glad they do not refrain from doing that in worry that this would not look good.\nThe clarity of the paper can improve. This is not an easy paper to write due to the quite complex process and the very large scale resource it generates. \nHowever, the paper is not very well organized and at many points I felt that I am reading a long list of details. I encourage the authors to try and give the paper a better structure. As one example, I would be happy to see a better problem definition and high level motivations from the very beginning. Other examples has to do with better exposition of the motivations, decisions and contributions in each part of the paper (I admire the efforts the authors have already made, but I think this can done even better). This is an important paper and it deserves a clearer presentation.\nAll in all I like the paper and think it provides an important resource. I would like to see this paper presented in ACL 2017.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "7e63e00a789e8e0e",
    "paper_id": "ACL_2017_723",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This is a nice paper on morphological segmentation utilizing word  embeddings. The paper presents a system which uses word embeddings to  both measure local semantic similarity of word pairs with a potential  morphological relation, and global information about the semantic validity of potential morphological segment types. The paper is well written and  represents a nice extension to earlier approaches on semantically driven  morphological segmentation.\nThe authors present experiments on Morpho Challenge data for three  languages: English, Turkish and Finnish. These languages exhibit varying  degrees of morphological complexity. All systems are trained on Wikipedia  text.  The authors show that the proposed MORSE system delivers clear  improvements w.r.t. F1-score for English and Turkish compared to the well  known Morfessor system which was used as baseline. The system fails to  reach the performance of Morfessor for Finnish. As the authors note, this  is probably a result of the richness of Finnish morphology which leads to  data sparsity and, therefore, reduced quality of word embeddings. To  improve the performance for Finnish and other languages with a similar  degree of morphological complexity, the authors could consider word  embeddings which take into account sub-word information. For example, @article{DBLP:journals/corr/CaoR16,   author    = {Kris Cao and                Marek Rei},   title     = {A Joint Model for Word Embedding and Word Morphology},   journal   = {CoRR},   volume    = {abs/1606.02601},   year                  = {2016},   url                 = {http://arxiv.org/abs/1606.02601},   timestamp = {Fri, 01 Jul 2016 17:39:49 +0200},   biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/CaoR16},   bibsource = {dblp computer science bibliography, http://dblp.org} } @article{DBLP:journals/corr/BojanowskiGJM16,   author    = {Piotr Bojanowski and                Edouard Grave and                Armand Joulin and                Tomas Mikolov},   title     = {Enriching Word Vectors with Subword Information},   journal   = {CoRR},   volume    = {abs/1607.04606},   year                  = {2016},   url                 = {http://arxiv.org/abs/1607.04606},   timestamp = {Tue, 02 Aug 2016 12:59:27 +0200},   biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/BojanowskiGJM16},   bibsource = {dblp computer science bibliography, http://dblp.org} } The authors critique the existing Morpho Challenge data sets. \nFor example, there are many instances of incorrectly segmented words in  the material. Moreover, the authors note that, while some segmentations  in the the data set may be historically valid (for example the  segmentation of business into busi-ness), these segmentations are no  longer semantically motivated. The authors provide a new data set  consisting of 2000 semantically motivated segmentation of English word  forms from the English Wikipedia. They show that MORSE deliver highly  substantial improvements compared to Morfessor on this data set.\nIn conclusion, I think this is a well written paper which presents  competitive results on the interesting task of semantically driven  morphological segmentation. The authors accompany the submission with  code and a new data set which definitely add to the value of the  submission.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "3dff7c857ce250c9",
    "paper_id": "ACL_2017_723",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper presents multiple ways to evaluate segmentation hypothesis on word embeddings, and these may be useful also in other type of methods. The results on English and Turkish data sets are convincing.\nThe paper is clearly written and organized, and the biliography is extensive.\nThe submission includes software for testing the English MORSE model and three small data sets used in the expriments.",
    "weaknesses": "The ideas in the paper are quite incremental, based mostly on the work by Soricut & Och (2015). However, the main problems of the paper concern meaningful comparison to prior work and analysis of the method's limitations.\nFirst, the proposed method does not provide any sensible way for segmenting compounds. Based on Section 5.3, the method does segment some of the compounds, but using the terminology of the method, it considers either of the constituents as an affix. Unsuprisingly, the limitation shows up especially in the results of a highly-compounding language, Finnish. While the limitation is indicated in the end of the discussion section, the introduction and experiments seem to assume otherwise.\nIn particular, the limitation on modeling compounds makes the evaluation of Section 4.4/5.3 quite unfair: Morfessor is especially good at segmenting compounds (Ruokolainen et al., 2014), while MORSE seems to segment them only \"by accident\". Thus it is no wonder that Morfessor segments much larger proportion of the semantically non-compositional compounds. A fair experiment would include an equal number of compounds that _should_ be segmented to their constituents.\nAnother problem in the evaluations (in 4.2 and 4.3) concerns hyperparameter tuning. The hyperparameters of MORSE are optimized on a tuning data, but apparently the hyperparameters of Morfessor are not. The recent versions of Morfessor (Kohonen et al. 2010, Grönroos et al. 2014) have a single hyperparameter that can be used to balance precision and recall of the segmentation. Given that the MORSE outperforms Morfessor both in precision and recall in many cases, this does not affect the conclusions, but should at least be mentioned.\nSome important details of the evaluations and results are missing: The \"morpheme-level evaluation\" method in 5.2 should be described or referred to. \nMoreover, Table 7 seems to compare results from different evaluation sets: the Morfessor and Base Inference methods seem to be from official Morpho Challenge evaluations, LLSM is from Narasimhan et al. (2015), who uses aggregated data from Morpho Challenges (probably including both development and training sets), and MORSE is evaluated Morpho Challenges 2010 development set. This might not affect the conclusions, as the differences in the scores are rather large, but it should definitely be mentioned.\nThe software package does not seem to support training, only testing an included model for English.",
    "comments": "The paper puts a quite lot of focus on the issue of segmenting semantically non-compositional compounds. This is problematic in two ways: First, as mentioned above, the proposed method does not seem to provide sensible way of segmenting _any_ compound. Second, finding the level of lexicalized base forms (e.g. freshman) and the morphemes as smallest meaning-bearing units (fresh, man) are two different tasks with different use cases (for example, the former would be more sensible for phrase-based SMT and the latter for ASR). The unsupervised segmentation methods, such as Morfessor, typically target at the latter, and critizing the method for a different goal is confusing.\nFinally, there is certainly a continuum on the (semantic) compositionality of the compound, and the decision is always somewhat arbitrary. ( Unfortunately many gold standards, including the Morpho Challenge data sets, tend to be also inconsistent with their decisions.)\nSections 4.1 and 5.1 mention the computational efficiency and limitation to one million input word forms, but does not provide any details: What is the bottleneck here? Collecting the transformations, support sets, and clusters? Or the actual optimization problem? What were the computation times and how do these scale up?\nThe discussion mentions a few benefits of the MORSE approach: Adaptability as a stemmer, ability to control precision and recall, and need for only a small number of gold standard segmentations for tuning. As far as I can see, all or some of these are true also for many of the Morfessor variants (Creutz and Lagus, 2005; Kohonen et al., 2010; Grönroos et al., 2014), so this is a bit misleading. It is true that Morfessor works usually fine as a completely unsupervised method, but the extensions provide at least as much flexibility as MORSE has.\n(Ref: Mathias Creutz and Krista Lagus. 2005. Inducing the Morphological Lexicon of a Natural Language from Unannotated Text. In Proceedings of the International and Interdisciplinary Conference on Adaptive Knowledge Representation and Reasoning (AKRR'05), Espoo, Finland, June 15-17.)\n- Miscellaneous: Abstract should maybe mention that this is a minimally supervised method (unsupervised to the typical extent, i.e. excluding hyperparameter tuning).\nIn section 3, it should be mentioned somewhere that phi is an empty string.\nIn section 5, it should be mentioned what specific variant (and implementation) of Morfessor is applied in the experiments.\nIn the end of section 5.2, I doubt that increasing the size of the input vocabulary would alone improve the performance of the method for Finnish. For a language that is morphologically as complex, you never encounter even all the possible inflections of the word forms in the data, not to mention derivations and compounds.\nI would encourage improving the format of the data sets (e.g.  using something similar to the MC data sets): For example using \"aa\" as a separator for multiple analyses is confusing and makes it impossible to use the format for other languages.\nIn the references, many proper nouns and abbreviations in titles are written in lowercase letters. Narasimhan et al. (2015) is missing all the publication details.",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "860f35b3029f38a9",
    "paper_id": "ACL_2017_723",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "I find the idea of using morphological compositionality to make decisions on segmentation quite fruitful.\nMotivation is quite clear The paper is well-structured",
    "weaknesses": "Several points are still unclear:    -- how the cases of rule ambiguity are treated (see \"null->er\" examples in general discussion)   -- inference stage seems to be suboptimal   -- the approach is limited to known words only",
    "comments": "The paper presents semantic-aware method for morphological segmentation. The method considers sets of simple morphological composition rules, mostly appearing as 'stem plus suffix or prefix'. The approach seems to be quite plausible and the motivation behind is clear and well-argumented.\nThe method utilizes the idea of vector difference to evaluate semantic confidence score for a proposed transformational rule. It's been previously shown by various studies that morpho-syntactic relations are captured quite well by doing word analogies/vector differences. But, on the other hand, it has also been shown that in case of derivational morphology (which has much less regularity than inflectional) the performance substantially drops (see Gladkova, 2016; Vylomova, 2016).   The search space in the inference stage although being tractable, still seems to be far from optimized (to get a rule matching \"sky->skies\" the system first needs to searhc though the whole R_add set and, probably, quite huge set of other possible substitutions) and limited to known words only (for which we can there exist rules).   It is not clear how the rules for the transformations which are orthographically the same, but semantically completely different are treated. \nFor instance, consider \"-er\" suffix. On one hand, if used with verbs, it transforms them into agentive nouns, such as \"play->player\". On the other hand, it could also be used with adjectives for producing comparative form, for instance, \"old->older\". Or consider \"big->bigger\" versus \"dig->digger\". \nMore over, as mentioned before, there is quite a lot of irregularity in derivational morphology. The same suffix might play various roles. For instance, \"-er\" might also represent patiental meanings (like in \"looker\"). Are they merged into a single rule/cluster?   No exploration of how the similarity threshold and measure may affect the performance is presented.",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "36a051532530a868",
    "paper_id": "ACL_2017_726",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "1\\ The paper is very clearly written, properly positioned, and I enjoyed reading it.\n2\\ The proposed model is tested and shown to perform well on 3 different domains (academic, geographic queries, and flight booking) 3\\ The online feedback loop is interesting and seems promising, despite of the small scale of the experiment.\n4\\ A new semantic corpus is published as part of this work, and additionally two existing corpora are converted to SQL format, which I believe would be beneficial for future work in this area.",
    "weaknesses": "/ clarifications: 1\\ Section 4.2 (Entity anonymization) - I am not sure I understand the choice of the length of span for querying the search engine. Why and how is it progressively reduced? ( line 333).\n2\\ Section 5 (Benchmark experiments) - If I understand correctly, the feedback loop (algorithm 1) is *not* used for these experiments. If this is indeed the case, I'm not sure when does data augmentation occur. Is all the annotated training data augmented with paraphrases? When is the \"initial data\" from templates added? Is it also added to the gold training set? If so, I think it's not surprising that it doesn't help much, as the gold queries may be more diverse.  In any case, I think this should be stated more clearly. In addition, I think it's interesting to see what's the performance of the \"vanilla\" model, without any augmentation, I think that this is not reported in the paper.\n3\\ Tables 2 and 3: I find the evaluation metric used here somewhat unclear. \nDoes the accuracy measure the correctness of the execution of the query (i.e., the retrieved answer) as the text seem to indicate? ( Line 471 mentions *executing* the query). Alternatively, are the queries themselves compared? ( as seems to be the case for Dong and Lapata in Table 2). If this is done differently for different systems (I.e., Dong and Lapata), how are these numbers comparable? In addition, the text mentions the SQL model has \"slightly lower accuracy than the best non-SQL results\" (Line 515), yet in table 2 the difference is almost 9 points in accuracy.  What is the observation based upon? \nWas some significance test performed? If not, I think the results are still impressive for direct to SQL parsing, but that the wording should be changed, as the difference in performance does seem significant.\n4\\ Line 519 - Regarding the data recombination technique used in Jia and Liang (2016): Since this technique is applicable in this scenario, why not try it as well?  Currently it's an open question whether this will actually improve performance. Is this left as future work, or is there something prohibiting the use of this technique?\n5\\ Section 6.2 (Three-stage online experiment) - several details are missing / unclear: - What was the technical background of the recruited users?\n- Who were the crowd workers, how were they recruited and trained?\n- The text says \"we recruited 10 new users and asked them to issue at least 10 utterances\". Does this mean 10 queries *each* (e.g., 100 overall), or 10 in total (1 for each).\n- What was the size of the initial (synthesized) training  set?  - Report statistics of the queries - some measure of their lexical variability / length / complexity of the generated SQL? This seems especially important for the first phase, which is doing surprisingly well. Furthermore, since SCHOLAR uses SQL and NL, it would have been nice if it were attached to this submission, to allow its review during this period.\n6\\ Section 6.3 (SCHOLAR dataset) - The dataset seems pretty small in modern standards (816 utterances in total), while one of the main advantages of this process is its scalability. What hindered the creation of a much larger dataset?\n- Comparing performance - is it possible to run another baseline on this newly created dataset to compare against the reported 67% accuracy obtained in this paper (line 730).\n7\\ Evaluation of interactive learning experiments (Section 6): I find the experiments to be somewhat hard to replicate as they involve manual queries of specific annotators. For example, who's to say if the annotators in the last phase just asked simpler questions? I realise that this is always problematic for online learning scenarios, but I think that an effort should be made towards an objective comparison. For starters, the statistics of the queries (as I mentioned earlier) is a readily available means to assess whether this happens. Second, maybe there can be some objective held out test set? This is problematic as the model relies on the seen queries, but scaling up the experiment (as I suggested above) might mitigate this risk. Third, is it possible to assess a different baseline using this online technique? I'm not sure whether this is applicable given that previous methods were not devised as online learning methods.\n- Minor comments: 1\\ Line 48 - \"requires\" -> \"require\" 2\\ Footnote 1 seems too long to me. Consider moving some of its content to the body of the text.\n3\\ Algorithm 1: I'm not sure what \"new utterances\" refers to (I guess it's new queries from users?). I think that an accompanying caption to the algorithm would make the reading easier.\n4\\ Line 218 - \"Is is\" -> \"It is\" 5\\ Line 278 mentions an \"anonymized\" utterance. This confused me at the first reading, and if I understand correctly it refers to the anonymization described later in 4.2. I think it would be better to forward reference this.",
    "comments": "Overall, I like the paper, and given answers to the questions I raised above, would like to see it appear in the conference.\n- Author Response: I appreciate the detailed response made by the authors, please include these details in a final version of the paper.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "dd78405c75796356",
    "paper_id": "ACL_2017_726",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- No intermediate representations were used.  - Release of a potentially valuable dataset on Google SCHOLAR.",
    "weaknesses": "- Claims of being comparable to state of the art when the results on GeoQuery and ATIS do not support it.",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "405faa3c51f7f04f",
    "paper_id": "ACL_2017_726",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper proposes an approach to learning a semantic parser using an encoder-decoder neural architecture, with the distinguishing feature that the semantic output is full SQL queries. The method is evaluated over two standard datasets (Geo880 and ATIS), as well as a novel dataset relating to document search.\nThis is a solid, well executed paper, which takes a relatively well established technique in the form of an encoder-decoder with some trimmings (e.g. data augmentation through paraphrasing), and uses it to generate SQL queries, with the purported advantage that SQL queries are more expressive than other semantic formalisms commonly used in the literature, and can be edited by untrained crowd workers (familiar with SQL but not semantic parsing). I buy that SQL is more expressive than the standard semantic formalisms, but then again, were there really any queries in any of your three datasets where the standard formalisms are unable to capture the full semantics of the query? I.e. are they really the best datasets to showcase the expressivity of SQL? Also, in terms of what your model learns, what fraction of SQL does it actually use? I.e. how much of the extra expressivity in SQL is your model able to capture? Also, does it have biases in terms of the style of queries that it tends to generate? That is, I wanted to get a better sense of not just the *potential* of SQL, but the actuality of what your model is able to capture, and the need for extra expressivity relative to the datasets you experiment over. Somewhat related to this, at the start of Section 5, you assert that it's harder to directly produce SQL. You never actually show this, and this seems to be more a statement of the expressivity of SQL than anything else (which returns me to the question of how much of SQL is the model actually generating).\nNext, I would really have liked to have seen more discussion of the types of SQL queries your model generates, esp. for the second part of the evaluation, over the SCHOLAR dataset. Specifically, when the query is ill-formed, in what ways is it ill-formed? When a crowd worker is required to post-edit the query, how much effort does that take them? Equally, how correct are the crowd workers at constructing SQL queries? Are they always able to construct perfect queries (experience would suggest that this is a big ask)? In a similar vein to having more error analysis in the paper, I would have liked to have seen agreement numbers between annotators, esp. for Incomplete Result queries, which seems to rely heavily on pre-existing knowledge on the part of the annotator and therefore be highly subjective.\nOverall, what the paper achieves is impressive, and the paper is well executed; I just wanted to get more insights into the true ability of the model to generate SQL, and a better sense of what subset of the language it generates.\nA couple of other minor things: l107: \"non-linguists can write SQL\" -- why refer to \"non-linguists\" here? Most linguists wouldn't be able to write SQL queries either way; I think the point you are trying to make is simply that \"annotators without specific training in the semantic translation of queries\" are able to perform the task l218: \"Is is\" -> \"It is\" l278: it's not clear what an \"anonymized utterance\" is at this point of the paper l403: am I right in saying that you paraphrase only single words at a time? \nPresumably you exclude \"entities\" from paraphrasing?\nl700: introduce a visual variable in terms of line type to differentiate the three lines, for those viewing in grayscale There are various inconsistencies in the references, casing issues (e.g. \"freebase\", \"ccg\"), Wang et al. (2016) is missing critical publication details, and there is an \"In In\" for Wong and Mooney (2007)",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "272391ead3394691",
    "paper_id": "ACL_2017_727",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "1) an interesting task, 2) the paper is very clearly written, easy to follow, 3) the created data set may be useful for other researchers, 4) a detailed analysis of the performance of the model.",
    "weaknesses": "1) no method adapted from related work for a result comparison 2) some explanations about the uniqueness of the task and discussion on limitations of previous research for solving this problem can be added to emphasize the research contributions further.",
    "comments": "The paper presents supervised and weakly supervised models for frame classification in tweets. Predicate rules are generated exploiting language-based and Twitter behavior-based signals, which are then supplied to the probabilistic soft logic framework to build classification models. 17 political frames are classified in tweets in a multi-label classification task. The experimental results demonstrate the benefit of the predicates created using the behavior-based signals. Please find my more specific comments below: The paper should have a discussion on how frame classification differs from stance classification. Are they both under the same umbrella but with different levels of granularity?\nThe paper will benefit from adding a brief discussion on how exactly the transition from long congressional speech to short tweets adds to the challenges of the task. For example, does past research rely on any specific cross-sentential features that do not apply to tweets? Consider adapting the method of a frame classification work on congressional speech (or a stance classification work on any text) to the extent possible due to its limitations on Twitter data, to compare with the results of this work.\nIt seems “weakly supervised” and “unsupervised” – these two terms have been interchangeably used in the paper (if this is not the case, please clarify in author response). I believe \"weakly supervised\" is the more technically correct terminology under the setup of this work that should be used consistently throughout. The initial unlabeled data may not have been labeled by human annotators, but the classification does use weak or noisy labels of some sort, and the keywords do come from experts. The presented method does not use completely unsupervised data as traditional unsupervised methods such as clustering, topic models or word embeddings would.   The calculated Kappa may not be a straightforward reflection of the difficulty of frame classification for tweets (lines: 252-253), viewing it as a proof is a rather strong claim. The Kappa here merely represents the annotation difficulty/disagreement. Many factors can contribute to a low value  such as poorly written annotation guidelines, selection of a biased annotator, lack of annotator training etc. \n(on top of any difficulty of frame classification for tweets by human annotators, which the authors actually intend to relate to). \n73.4% Cohen’s Kappa is strong enough for this task, in my opinion, to rely on the annotated labels.  Eq (1) (lines: 375-377) will ignore any contextual information (such as negation or conditional/hypothetical statements impacting the contributing word) when calculating similarity of a frame and a tweet. Will this have any effect on the frame prediction model? Did the authors consider using models that can determine similarity with larger text units such as perhaps using skip thought vectors or vector compositionality methods?   An ideal set up would exclude the annotated data from calculating statistics used to select the top N bi/tri-grams (line: 397 mentions entire tweets data set has been used), otherwise statistics from any test fold (or labeled data in the weakly supervised setup) still leaks into the selection process. I do not think this would have made any difference in the current selection of the bi/tri-grams or results as the size of the unlabeled data is much larger, but would have constituted a cleaner experimental setup.   Please add precision and recall results in Table 4.  Minor: please double check any rules for footnote placements concerning placement before or after the punctuation.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "ecba7de10d59ef0b",
    "paper_id": "ACL_2017_727",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The authors address a very challenging, nuanced problem in political discourse reporting a relatively high degree of success.\nThe task of political framing detection may be of interest to the ACL community.\nThe paper is very well written.",
    "weaknesses": "Quantitative results are given only for the author's PSL model and not compared against any traditional baseline classification algorithms, making it unclear to what degree their model is necessary. Poor comparison with alternative approaches makes it difficult to know what to take away from the paper.\nThe qualitative investigation is interesting, but the chosen visualizations are difficult to make sense of and add little to the discussion. Perhaps it would make sense to collapse across individual politicians to create a clearer visual.",
    "comments": "The submission is well written and covers a topic which may be of interest to the ACL community. At the same time, it lacks proper quantitative baselines for comparison.  Minor comments: - line 82: A year should be provided for the Boydstun et al. citation - It’s unclear to me why similar behavior (time of tweeting) should necessarily be indicative of similar framing and no citation was given to support this assumption in the model.\n- The related work goes over quite a number of areas, but glosses over the work most clearly related (e.g. PSL models and political discourse work) while spending too much time mentioning work that is only tangential (e.g. unsupervised models using Twitter data).\n- Section 4.2 it is unclear whether Word2Vec was trained on their dataset or if they used pre-trained embeddings.\n- The authors give no intuition behind why unigrams are used to predict frames, while bigrams/trigrams are used to predict party.\n- The authors note that temporal similarity worked best with one hour chunks, but make no mention of how important this assumption is to their results. If the authors are unable to provide full results for this work, it would still be worthwhile to give the reader a sense of what performance would look like if the time window were widened.\n- Table 4: Caption should make it clear these are F1 scores as well as clarifying how the F1 score is weighted (e.g. micro/macro). This should also be made clear in the “evaluation metrics” section on page 6.",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "069200c464a9aa27",
    "paper_id": "ACL_2017_729",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The authors propose a kernel-based method that captures high-order patterns differentiting different types of rumors by evaluating the similarities between their propagation tree structures.",
    "weaknesses": "maybe the maths is not always clear in Sect. 4.",
    "comments": "The authors propose a propagation tree kernel, a kernel-based method that captures high-order patterns differentiating types of rumors by evaluating the similarities between their propagation tree structures. The proposed approach detects rumors more quickly and with a higher accuracy compared to the one obtained by the state of the art methods.\nThe data set should be made public for research purposes.\nTypos need to be fixed (e.g. 326/3277: any subgraph which have->has; 472: TPK->PTK; 644: Table 2 show+s), missing information needs to be added (875: where was it published?), information needs to be in the same format (e.g. 822 vs 897). Figure 5 is a bit small.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "a23345f693e0ae82",
    "paper_id": "ACL_2017_741",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper presents a graph-based approach for producing sense-disambiguated synonym sets from a collection of undisambiguated synonym sets.  The authors evaluate their approach by inducing these synonym sets from Wiktionary and from a collection of Russian dictionaries, and then comparing pairwise synonymy relations (using precision, recall, and F1) against WordNet and BabelNet (for the English synonym sets) or RuThes and Yet Another RussNet (for the Russian synonym sets).\nThe paper is very well written and structured.              The experiments and evaluations (or at least the prose parts) are very easy to follow.              The methodology is sensible and the analysis of the results cogent.  I was happy to observe that the objections I had when reading the paper (such as the mismatch in vocabulary between the synonym dictionaries and gold standards) ended up being resolved, or at least addressed, in the final pages.\nThe one thing about the paper that concerns me is that the authors do not seem to have properly understood the previous work, which undercuts the stated motivation for this paper.\nThe first instance of this misunderstanding is in the paragraph beginning on line 064, where OmegaWiki is lumped in with Wiktionary and Wikipedia in a discussion of resources that are \"not formally structured\" and that contain \"undisambiguated synonyms\".  In reality, OmegaWiki is distinguished from the other two resources by using a formal structure (a relational database) based on word senses rather than orthographic forms.              Translations, synonyms, and other semantic annotations in OmegaWiki are therefore unambiguous.\nThe second, and more serious, misunderstanding comes in the three paragraphs beginning on lines 092, 108, and 120.  Here the paper claims that both BabelNet and UBY \"rely on English WordNet as a pivot for mapping of existing resources\" and criticizes this mapping as being \"error-prone\".  Though it is true that BabelNet uses WordNet as a pivot, UBY does not.  UBY is basically a general-purpose specification for the representation of lexical-semantic resources and of links between them.  It exists independently of any given lexical-semantic resource (including WordNet) and of any given alignment between resources (including ones based on \"similarity of dictionary definitions\" or \"cross-lingual links\").  Its maintainers have made available various databases adhering to the UBY spec; these contain a variety of lexical-semantic resources which have been aligned with a variety of different methods.  A given UBY database can be *queried* for synsets, but UBY itself does not *generate* those synsets.  Users are free to produce their own databases by importing whatever lexical-semantic resources and alignments thereof are best suited to their purposes.  The three criticisms of UBY on lines 120 to 125 are therefore entirely misplaced.\nIn fact, I think at least one of the criticisms is not appropriate even with respect to BabelNet.  The authors claim that Watset may be superior to BabelNet because BabelNet's mapping and use of machine translation are error-prone.  The implication here is that Watset's method is error-free, or at least significantly less error-prone.  This is a very grandiose claim that I do not believe is supported by what the authors ought to have known in advance about their similarity-based sense linking algorithms and graph clustering algorithms, let alone by the results of their study.  I think this criticism ought to be moderated.              Also, I think the third criticism (BabelNet's reliance on WordNet as a pivot) somewhat misses the point -- surely the most important issue to highlight isn't the fact that the pivot is English, but rather that its synsets are already manually sense-annotated.\nI think the last paragraph of §1 and the first two paragraphs of §2 should be extensively revised. They should focus on the *general* problem of generating synsets by sense-level alignment/translation of LSRs (see Gurevych et al., 2016 for a survey), rather than particularly on BabelNet (which uses certain particular methods) and UBY (which doesn't use any particular methods, but can aggregate the results of existing ones).  It may be helpful to point out somewhere that although alignment/translation methods *can* be used to produce synsets or to enrich existing ones, that's not always an explicit goal of the process.  Sometimes it's just a serendipitous (if noisy) side-effect of aligning/translating resources with differing granularities.\nFinally, at several points in the paper (lines 153, 433), the \"synsets\" of TWSI of JoBimText are criticized for including too many words that are hypernyms, co-hypnomyms, etc. instead of synonyms.  But is this problem really unique to TWSI and JoBimText?  That is, how often do hypernyms, co-hypernyms, etc. appear in the output of Watset?  (We can get only a very vague idea of this from comparing Tables 3 and 5, which analyze only synonym relations.)  If Watset really is better at filtering out words with other semantic relations, then it would be nice to see some quantitative evidence of this.\nSome further relatively minor points that should nonetheless be fixed: - Lines 047 to 049: The sentence about Kiselev et al. (2015) seems rather useless.  Why bother mentioning their analysis if you're not going to tell us what they found?\n- Line 091: It took me a long time to figure out how \"wat\" has any relation to \"discover the correct word sense\".  I suppose this is supposed to be a pun on \"what\".  Maybe it would have been better to call the approach \"Whatset\"?  Or at least consider rewording the sentence to better explain the pun.\n- Figure 2 is practically illegible owing to the microscopic font.  Please increase the text size!\n- Similarly, Tables 3, 4, and 5 are too small to read comfortably.  Please use a larger font.              To save space, consider abbreviating the headers (\"P, \"R\", \"F1\") and maybe reporting scores in the range 0–100 instead of 0–1, which will eliminate a leading 0 from each column.\n- Lines 517–522: Wiktionary is a moving target.  To help others replicate or compare against your work, please indicate the date of the Wiktionary database dump you used.\n- Throughout: The constant switching between Times and Computer Modern is distracting.  The root of this problem is a longstanding design flaw in the ACL 2017 LaTeX style file, but it's exacerbated by the authors' decision to occasionally set numbers in math mode, even in running text.  Please fix this by removing \\usepackage{times} from the preamble and replacing it with either \\usepackage{newtxtext} \\usepackage{newtxmath} or \\usepackage{mathptmx} References: I Gurevych, J. Eckle-Kohler, and M. Matuschek, 2016. Linked Lexical Knowledge Bases: Foundations and Applications, volume 34 of Synthesis Lectures on Human Language Technologies, chapter 3: Linking Algorithms, pages 29-44. Morgan & Claypool.\n---- I have read the author response.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "0ff44d494829a2e8",
    "paper_id": "ACL_2017_741",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper proposes a new method for word sense induction from synonymy dictionaries. The method presents a conceptual improvement over existing ones and demonstrates robust performance in empirical evaluation. The evaluation was done thoroughly, using a number of benchmarks and strong baseline methods.",
    "weaknesses": "Just a couple of small points. I would like to see more discussion of the nature of the evaluation. First, one observes that all models' scores are relatively low, under 50% F1. Is there room for much improvement or is there a natural ceiling of performance due to the nature of the task? The authors discuss lexical sparsity of the input data but I wonder how much of the performance gap this sparsity accounts for. \nSecond, I would also like to see some discussion of the evaluation metric chosen. It is known that word senses can be analyzed at different levels of granularity, which can naturally affect the scores of any system. \nAnother point is that it is not clear how the authors obtained vectors for word senses that they used in 3.4, if the senses are only determined after this step, and anyway senses are not marked in the input corpora.",
    "comments": "I recommend the paper for presentation at the ACL Meeting. Solid work.",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "25a1dbea45f06961",
    "paper_id": "ACL_2017_752",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper demonstrates that seq2seq models can be comparatively effectively applied to the tasks of AMR parsing and AMR realization by linearization of an engineered pre-processed version of the AMR graph and associated sentence, combined with 'Paired Training' (iterative back-translation of monolingual data combined with fine-tuning). While parsing performance is worse than other reported papers (e.g., Pust et al., 2015), those papers used additional semantic information.  On the task of AMR realization, the paper demonstrates that utilizing additional monolingual data (via back-translation) is effective relative to a seq2seq model that does not use such information. ( See note below about comparing realization results to previous non-seq2seq work for the realization task.)",
    "weaknesses": "At a high-level, the main weakness is that the paper aims for empirical comparisons, but in comparing to other work, multiple aspects/dimensions are changing at the same time (in some cases, not comparable due to access to different information), complicating comparisons.  For example, with the realization results (Table 2), PBMT (Pourdamghani et al., 2016) is apparently trained on LDC2014T12, which consists of 13,051 sentences, compared to the model of the paper, which is trained on LDC2015E86, which consists of 19,572 sentences, according to http://amr.isi.edu/download.html. \nThis is used in making the claim of over 5 points improvement over the state-of-the-art (PBMT) in line 28/29, 120/121, and line 595, and is only qualified in the caption of Table 2. To make a valid comparison, the approach of the paper or PBMT needs to be re-evaluated after using the same training data.",
    "comments": "Is there any overlap between the sentences in your Gigaword sample and the test sentences of LDC2015E86? Apparently LDC2015E86 contains data from the ''proxy report data in LDC's DEFT Narrative Text Source Data R1 corpus (LDC2013E19)'' (Accessible with LDC account: https://catalog.ldc.upenn.edu/LDC2015E86). It seems LDC2013E19 contains data from Gigaword (https://catalog.ldc.upenn.edu/LDC2013E19). Apparently AMR corpus LDC2014T12 also contained ''data from newswire articles selected from the English Gigaword Corpus, Fifth Edition'' (publicly accessible link: https://catalog.ldc.upenn.edu/docs/LDC2014T12/README.txt). Please check that there is no test set contamination.\nLine 244-249: Did these two modifications to the encoder make a significant difference in effectiveness? What was the motivation behind these changes?\nPlease make it clear (in an appendix is fine) for replication purposes whether the implementation is based on an existing seq2seq framework.\nLine 321: What was the final sequence length used? ( Consider adding such details in an appendix.)\nPlease label the columns of Table 1 (presumably dev and test). Also, there is a mismatch between Table 1 and the text: ''Table 1 summarizes our development results for different rounds of self-training.'' It appears that only the results of the second round of self-training are shown.\nAgain, the columns for Table 1 are not labeled, but should the results for column 1 for CAMR instead be 71.2, 63.9, 67.3--the last line of Table 2 in http://www.aclweb.org/anthology/S16-1181 which is the configuration for +VERB+RNE+SRL+WIKI? It looks like the second from last row of Table 2 in CAMR (Wang et al., 2016) is currently being used. On this note, how does your approach handle the wikification information introduced in LDC2015E86?  7.1.Stochastic is missing a reference to the example.\nLine 713-715: This seems like a hypothesis to be tested empirically rather than a forgone conclusion, as implied here.\nGiven an extra page, please add a concluding section.\nHow are you performing decoding? Are you using beam search?\nAs a follow-up to line 161-163, it doesn't appear that the actual vocabulary size used in the experiments is mentioned. After preprocessing, are there any remaining unseen tokens in dev/test? In other words, is the unknown word replacement mechanism (using the attention weights), as described in Section 3.2, ever used?  For the realization case study, it would be of interest to see performance on phenomena that are known limitations of AMR, such as quantification and tense (https://github.com/amrisi/amr-guidelines/blob/master/amr.md).\nThe paper would benefit from a brief discussion (perhaps a couple sentences) motivating the use of AMR as opposed to other semantic formalisms, as well as why the human-annotated AMR information/signal might be useful as opposed to learning a model (e.g., seq2seq itself) directly for a task (e.g., machine translation).\nFor future work (not taken directly into account in the scores given here for the review, since the applicable paper is not yet formally published in the EACL proceedings): For parsing, what accounts for the difference from previous seq2seq approaches? Namely, between Peng and Xue, 2017 and AMR-only (as in Table 1) is the difference in effectiveness being driven by the architecture, the preprocessing, linearization, data, or some combination thereof? Consider isolating this difference. ( Incidentally, the citation for Peng and Xue, 2017 [''Addressing the Data Sparsity Issue in Neural AMR Parsing''] should apparently be Peng et al. 2017 (http://eacl2017.org/index.php/program/accepted-papers; https://arxiv.org/pdf/1702.05053.pdf). The authors are flipped in the References section.\nProofreading (not necessarily in the order of occurrence; note that these are provided for reference and did not influence my scoring of the paper): outperform state of the art->outperform the state of the art Zhou et al. (2016), extend->Zhou et al. (2016) extend (2016),Puzikov et al.->(2016), Puzikov et al. POS-based features, that->POS-based features that language pairs, by creating->language pairs by creating using a back-translation MT system and mix it with the human translations.->using a back-translation MT system, and mix it with the human translations.\nProbBank-style (Palmer et al., 2005)->PropBank-style (Palmer et al., 2005) independent parameters ,->independent parameters, for the 9.6% of tokens->for 9.6% of tokens maintaining same embedding sizes->maintaining the same embedding sizes Table 4.Similar->Table 4. Similar realizer. The->realizer. The Notation: Line 215, 216: The sets C and W are defined, but never subsequently referenced. ( However, W could/should be used in place of ''NL'' in line 346 if they are referring to the same vocabulary.)",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "728d926e718a4e79",
    "paper_id": "ACL_2017_752",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The authors use self-training to train a seq2seq-based AMR parser using a small annotated corpus and large amounts of unlabeled data. They then train a similar, seq2seq-based AMR-to-text generator using the annotated corpus and automatic AMRs produced by their parser from the unlabeled data. They use careful delexicalization for named entities in both tasks to avoid data sparsity. This is the first sucessful application of seq2seq models to AMR parsing and generation, and for generation, it most probably improves upon state-of-the art.\nIn general, I really liked the approach as well as the experiments and the final performance analysis. \nThe methods used are not revolutionary, but they are cleverly combined to achieve practial results. \nThe description of the approach is quite detailed, and I believe that it is possible to reproduce the experiments without significant problems. \nThe approach still requires some handcrafting, but I believe that this can be overcome in the future and that the authors are taking a good direction.\n(RESOLVED BY AUTHORS' RESPONSE) However, I have been made aware by another reviewer of a data overlap in the Gigaword and the Semeval 2016 dataset. This is potentially a very serious problem -- if there is a significant overlap in the test set, this would invalidate the results for generation (which are the main achievemnt of the paper). Unless the authors made sure that no test set sentences made their way to training through Gigaword, I cannot accept their results.\n(RESOLVED BY AUTHORS' RESPONSE)  Another question raised by another reviewer, which I fully agree with, is the  5.4 point claim when comparing to a system tested on an earlier version of the AMR dataset. The paper could probably still claim improvement over state-of-the art, but I am not sure I can accept the 5.4 points claim in a direct comparison to Pourdamghani et al. -- why haven't the authors also tested their system on the older dataset version (or obtained Pourdamghani et al.'s scores for the newer version)?\nOtherwise I just have two minor comments to experiments:  - Statistical significance tests would be advisable (even if the performance difference is very big for generation).\n- The linearization order experiment should be repeated with several times with different random seeds to overcome the bias of the particular random order chosen.\nThe form of the paper definitely could be improved. \nThe paper is very dense at some points and proofreading by an independent person (preferably an English native speaker) would be advisable. \nThe model (especially the improvements over Luong et al., 2015) could be explained in more detail; consider adding a figure. The experiment description is missing the vocabulary size used. \nMost importantly, I missed a formal conclusion very much -- the paper ends abruptly after qualitative results are described, and it doesn't give a final overview of the work or future work notes.\nMinor factual notes: - Make it clear that you use the JAMR aligner, not the whole parser (at 361-364). Also, do you not use the recorded mappings also when testing the parser (366-367)?\n- Your non-Gigaword model only improves on other seq2seq models by 3.5 F1 points, not 5.4 (at 578).\n- \"voters\" in Figure 1 should be \"person :ARG0-of vote-01\" in AMR.\nMinor writing notes: - Try rewording and simplifying text near 131-133, 188-190, 280-289, 382-385, 650-659, 683, 694-695.\n- Inter-sentitial punctuation is sometimes confusing and does not correspond to my experience with English syntax. There are lots of excessive as well as missing commas.\n- There are a few typos (e.g., 375, 615), some footnotes are missing full stops.\n- The linearization description is redundant at 429-433 and could just refer to Sect. 3.3.\n- When refering to the algorithm or figures (e.g., near 529, 538, 621-623), enclose the references in brackets rather than commas.\n- I think it would be nice to provide a reference for AMR itself and for the multi-BLEU script.\n- Also mention that you remove AMR variables in Footnote 3.\n- Consider renaming Sect. 7 to \"Linearization Evaluation\".\n- The order in Tables 1 and 2 seems a bit confusing to me, especially when your systems are not explicitly marked (I would expect your systems at the bottom). \nAlso, Table 1 apparently lists development set scores even though its description says otherwise.\n- The labels in Table 3 are a bit confusing (when you read the table before reading the text).\n- In Figure 2, it's not entirely visible that you distinguish month names from month numbers, as you state at 376.\n- Bibliography lacks proper capitalization in paper titles, abbreviations and proper names should be capitalized (use curly braces to prevent BibTeX from lowercasing everything).\n- The \"Peng and Xue, 2017\" citation is listed improperly, there are actually four authors.\n*** Summary: The paper presents first competitive results for neural AMR parsing and probably new state-of-the-art for AMR generation, using seq2seq models with clever preprocessing and exploiting large a unlabelled corpus. Even though revisions to the text are advisable, I liked the paper and would like to see it at the conference.  (RESOLVED BY AUTHORS' RESPONSE) However, I am not sure if the comparison with previous state-of-the-art on generation is entirely sound, and most importantly, whether the good results are not actually caused by data overlap of Gigaword (additional training set) with the test set.\n*** Comments after the authors' response: I thank the authors for addressing both of the major problems I had with the paper. I am happy with their explanation, and I raised my scores assuming that the authors will reflect our discussion in the final paper.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "097875d0f7bc4738",
    "paper_id": "ACL_2017_759",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper is written to be easy to read. Technical details are described fully, and high performance is also shown in experimental evaluation. It also shows useful comparisons with related research in the field of discourse structure analysis and key phrase identification. It is interesting to note that not only the performance evaluation of phrase selection from discourse, discourse relation labeling, and summary generation as their applications, but also application to the prediction of the consistency of  understanding by team members is also verified .",
    "weaknesses": "Jointly Modeling salient phrase extraction and discourse relationship labeling between speaker turns has been proposed. If intuitive explanation about their interactivity and the usefulness of considering it is fully presented.",
    "comments": "SVM-based classifier is set as a comparative method in the experiment. It would be useful to mention the validity of the setting.",
    "overall_score": "3",
    "confidence": "2"
  },
  {
    "review_id": "efd030308e5d2ad9",
    "paper_id": "ACL_2017_760",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The work simulates an intuitive \"skimming\" behavior of a reader, mirroring Shen et al. who simulate (self-terminated) repeated reading. A major attribute of this work is its simplicity. Despite the simplicity, the approach yields favorable results. In particular, the authors show through a well-designed synthetic experiment that the model is indeed able to learn to skip when given oracle jump signals. In text classification using real-world datasets, the model is able to perform competitively with the non-skimming model while being clearly faster.  The proposed model can potentially have meaningful practical implications: for tasks in which skimming suffices (e.g., sentiment classification), it suggests that we can obtain equivalent results without consuming all data in a completely automated fashion. To my knowledge this is a novel finding.",
    "weaknesses": "It's a bit mysterious on what basis the model determines its jumping behavior so effectively (other than the synthetic dataset). I'm thinking of a case where the last part of the given sentence is a crucial evidence, for instance:  \"The movie was so so and boring to the last minute but then its ending blew me away.\"  In this example, the model may decide to skip the rest of the sentence after reading \"so so and boring\". But by doing so it'll miss the turning point \"ending blew me away\" and mislabel the instance as negative. For such cases a solution can be running the skimming model in both directions as the authors suggest as future work. But in general the model may require more sophisticated architecture for controlling skimming.\nIt seems one can achieve improved skimming by combining it with multi-pass reading (presumably in reverse directions). That's how humans read to understand text that can't be digested in one skim; indeed, that's how I read this draft.  Overall, the work raises an interesting problem and provides an effective but intuitive solution.",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "e90c05edf33fffce",
    "paper_id": "ACL_2017_768",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "A well written paper, examining the use of context in lexical entailment task is a great idea, a well defined approach and experimental set-up and good analysis of the results",
    "weaknesses": "Some information is missing or insufficient, e.g., the table captions should be more descriptive, a clear description for each of the word type features should be given.\nGeneral Discussion:  The paper presents a proposal of consideration of context in lexical entailment task. The results from the experiments demonstrate that context-informed models do better than context-agnostic models on the entailment task.  I liked the idea of creating negative examples to get negative annotations automatically in the two ways described in the paper based on WordNet positive examples. ( new dataset; an interesting method to develop dataset) I also liked the idea of transforming already-used context-agnostic representations into contextualized representations, experimenting with different ways to get contextualized representations (i.e., mask vs contetx2vec), and testing the model on 3 different datasets (generalizability not just across different datasets but also cross-linguistically).\nMotivations for various decisions in the experimental design were good to see, e.g., why authors used the split they used for CONTEXT-PPDB (it showed that they thought out clearly what exactly they were doing and why).\nLines 431-434: authors might want to state briefly how the class weights were determined and added to account for the unbalanced data in the CONTEXT-WN experiments. Would it affect direct comparisons with previous work, in what ways?  Change in Line 589: directionality 4 --> directionality, as in Table 4 Suggested change in Line 696-697: is-a hierarchy of WordNet --> \"is-a\" hierarchy of WordNet  For the sake of completeness, represent \"mask\" also in Figure 1.\nI have read the author response.",
    "comments": "",
    "overall_score": "4",
    "confidence": "2"
  },
  {
    "review_id": "2616ae04b486ca48",
    "paper_id": "ACL_2017_768",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper proposes a method for recognizing lexical entailment (specifically, hypernymy) in context. The proposed method represents each context by averaging, min-pooling, and max-pooling its word embeddings. These representations are combined with the target word's embedding via element-wise multiplication. The in-context representation of the left-hand-side argument is concatenated to that of the right-hand-side argument's, creating a single vectorial representation of the input. This input is then fed into a logistic regression classifier.\nIn my view, the paper has two major weaknesses. First, the classification model used in this paper (concat + linear classifier) was shown to be inherently unable to learn relations in \"Do Supervised Distributional Methods Really Learn Lexical Inference Relations?\" ( Levy et al., 2015). Second, the paper makes superiority claims in the text that are simply not substantiated in the quantitative results. In addition, there are several clarity and experiment setup issues that give an overall feeling that the paper is still half-baked.\n= Classification Model = Concatenating two word vectors as input for a linear classifier was mathematically proven to be incapable of learning a relation between words (Levy et al., 2015). What is the motivation behind using this model in the contextual setting?\nWhile this handicap might be somewhat mitigated by adding similarity features, all these features are symmetric (including the Euclidean distance, since |L-R| = |R-L|). Why do we expect these features to detect entailment?\nI am not convinced that this is a reasonable classification model for the task.\n= Superiority Claims = The authors claim that their contextual representation is superior to context2vec. This is not evident from the paper, because: 1) The best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.\n2) This experiment uses ready-made embeddings (GloVe) and parameters (context2vec) that were tuned on completely different datasets with very different sizes. Comparing the two is empirically flawed, and probably biased towards the method using GloVe (which was a trained on a much larger corpus).\nIn addition, it seems that the biggest boost in performance comes from adding similarity features and not from the proposed context representation. This is not discussed.\n= Miscellaneous Comments = - I liked the WordNet dataset - using the example sentences is a nice trick.\n- I don’t quite understand why the task of cross-lingual lexical entailment is interesting or even reasonable.\n- Some basic baselines are really missing. Instead of the \"random\" baseline, how well does the \"all true\" baseline perform? What about the context-agnostic symmetric cosine similarity of the two target words?\n- In general, the tables are very difficult to read. The caption should make the tables self-explanatory. Also, it is unclear what each variant means; perhaps a more precise description (in text) of each variant could help the reader understand?\n- What are the PPDB-specific features? This is really unclear.\n- I could not understand 8.1.\n- Table 4 is overfull.\n- In table 4, the F1 of \"random\" should be 0.25.\n- Typo in line 462: should be \"Table 3\" = Author Response = Thank you for addressing my comments. Unfortunately, there are still some standing issues that prevent me from accepting this paper: - The problem I see with the base model is not that it is learning prototypical hypernyms, but that it's mathematically not able to learn a relation.\n- It appears that we have a different reading of tables 3 and 4. Maybe this is a clarity issue, but it prevents me from understanding how the claim that contextual representations substantially improve performance is supported. \nFurthermore, it seems like other factors (e.g. similarity features) have a greater effect.",
    "overall_score": "2",
    "confidence": "5"
  },
  {
    "review_id": "2905c0d9d0791865",
    "paper_id": "ACL_2017_768",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper addresses the task of lexical entailment detection in context, e.g. is \"chess\" a kind of \"game\" given a sentence containing each of the words -- relevant for QA. The major contributions are: (1) a new dataset derived from WordNet using synset exemplar sentences, and  (2) a \"context relevance mask\" for a word vector, accomplished by elementwise multiplication with feature vectors derived from the context sentence. Fed to a logistic regression classifier, the masked word vectors just beat state of the art on entailment prediction on a PPDB-derived dataset from previous literature. Combined with other existing features, they beat state of the art by a few points. They also beats the baseline on the new WN-derived dataset, although the best-scoring method on that dataset doesn't use the masked representations.\nThe paper also introduces some simple word similarity features (cosine, euclidean distance) which accompany other cross-context similarity features from previous literature. All of the similarity features, together, improve the classification results by a large amount, but the features in the present paper are a relatively small contribution.\nThe task is interesting, and the work seems to be correct as far as it goes, but incremental. The method of producing the mask vectors is taken from existing literature on encoding variable-length sequences into min/max/mean vectors, but I don't think they've been used as masks before, so this is novel. \nHowever, excluding the PPDB features it looks like the best result does not use the representation introduced in the paper.\nA few more specific points: In the creation of the new Context-WN dataset, are there a lot of false negatives resulting from similar synsets in the \"permuted\" examples? If you take word w, with synsets i and j, is it guaranteed that the exemplar context for a hypernym synset of j is a bad entailment context for i? What if i and j are semantically close?\nWhy does the masked representation hurt classification with the context-agnostic word vectors (rows 3, 5 in Table 3) when row 1 does so well? \nWouldn't the classifier learn to ignore the context-agnostic features?\nThe paper should make clearer which similarity measures are new and which are from previous literature. It currently says that previous lit used the \"most salient\" similarity features, but that's not informative to the reader.\nThe paper should be clearer about the contribution of the masked vectors vs the similarity features. It seems like similarity is doing most of the work.\nI don't understand the intuition behind the Macro-F1 measure, or how it relates to \"how sensitive are our models to changes in context\" -- what changes? How do we expect Macro-F1 to compare with F1?\nThe cross-language task is not well motivated.\nMissing a relevant citation: Learning to Distinguish Hypernyms and Co-Hyponyms. \nJulie Weeds, Daoud Clarke, Jeremy Reffin, David Weir and Bill Keller. COLING 2014.\n== I have read the author response. As noted in the original reviews, a quick examination of the tables shows that the similarity features make the largest contribution to the improvement in F-score on the two datasets (aside from PPDB features). The author response makes the point that similarities include contextualized representations. However, the similarity features are a mixed bag, including both contextualized and non-contextualized representations. This would need to be teased out more (as acknowledged in the response).\nNeither Table 3 nor 4 gives results using only the masked representations without the similarity features. This makes the contribution of the masked representations difficult to isolate.",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "8f455412d539faf5",
    "paper_id": "ACL_2017_769",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper proposes a method for building dialogue agents involved in a symmetric collaborative task, in which the agents need to strategically communicate to achieve a common goal.   I do like this paper.  I am very interested in how much data-driven techniques can be used for dialogue management.  However, I am concerned that the approach that this paper proposes, is actually not specific to symmetric collaborative tasks, but to tasks that can be represented as graph operations, such as finding an intersection between objects that the two people know about.\nIn Section 2.1, the authors introduce symmetric collaborative dialogue setting. \n However, such dialogs have been studied before, such as Clark and Wilkes-Gibbs explored (Cognition '86), and Walker's furniture layout task (Journal of Artificial Research '00).\nOn line 229, the authors say that this domain is too rich for slot-value semantics.  However, their domain is based on attribute value pairs, so their domain could use a semantics represenation based on attribute value-pairs, such as first order logic.\nSection 3.2 is hard to follow.        The authors often refer to Figure 2, but I didn't find this example that helpful.        For example, for section 3.1, at what point of the dialogue does this represent?  Is this the same after `anyone went to columbia?'",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "03ba79a4b230fda1",
    "paper_id": "ACL_2017_769",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This model is very novel for both goal-directed and open ended dialogue. The presented evaluation metrics show clear advantage for the presented model.",
    "weaknesses": "In terms of the presentation, mathematical details of how the embeddings are computed are not sufficiently clear. While the authors have done an extensive evaluation, they haven't actually compared the system with an RL-based dialogue manager which is current state-of-the-art in goal-oriented systems. Finally, it is not clear how this approach scales to more complex problems. The authors say that the KB is 3K, but actually what the agent operates is about 10 (judging from Table 6).",
    "comments": "Overall, I think this is a good paper. Had the theoretical aspects of the paper been better presented I would give this paper an accept.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "73eaad14ae789c97",
    "paper_id": "ACL_2017_775",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper proposes an approach for classifying literal and metaphoric adjective-noun pairs. The authors create a word-context matrix for adjectives and nouns where each element of the matrix is the PMI score. They then use different methods for selecting dimensions of this matrix to represent each noun/adjective as a vector. The geometric properties of average, nouns, and adjective vectors and their normalized versions are used as features in training a regression model for classifying the pairs to literal or metaphor expressions. Their approach performs similarly to previous work that learns a vector representation for each adjective.\nSupervision and zero-shot learning. The authors argue that their approach requires less supervision (compared to previous work)  and can do zero-shot learning. I don’t think this is quite right and given that it seems to be one of the main points of the paper, I think it is worth clarifying. The approach proposed in the paper is a supervised classification task: The authors form vector representations from co-occurrence statistics, and then use the properties of these representations and the gold-standard labels of each pair to train a classifier. The model (similarly to any other supervised classifier) can be tested on words that did not occur in the training data; but, the model does not learn from such examples. Moreover, those words are not really “unseen” because the model needs to have a vector representation of those words.\nInterpretation of the results. The authors provide a good overview of the previous related work on metaphors. However, I am not sure what the intuition about their approach is (that is, using the geometric properties such as vector length in identifying metaphors). For example, why are the normalized vectors considered? It seems that they don’t contribute to a better performance. \nMoreover, the most predictive feature is the noun vector; the authors explain that this is a side effect of the data which is collected such that each adjective occurs in both metaphoric and literal expressions. ( As a result, the adjective vector is less predictive.) It seems that the proposed approach might be only suitable for the given data. This shortcoming is two-fold: (a) From the theoretical perspective (and especially since the paper is submitted to the cognitive track), it is not clear what we learn about theories of metaphor processing. ( b) From the NLP applications standpoint, I am not sure how generalizable this approach is compared to the compositional models.\nNovelty. The proposed approach for representing noun/adjective vectors is very similar to that of Agres et al. It seems that the main contribution of the paper is that they use the geometric properties to classify the vectors.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "946486d418daa3a9",
    "paper_id": "ACL_2017_775",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- A geometric approach to metaphor interpretation is a new research strand altogether.  - The paper is well written.\n- Author's claim is the beauty of their model lies in its simplicity, I do agree with their claim. But the implication of the simplicity is not been addressed in simple ways. Please refer the weakness section.",
    "weaknesses": "Regarding writing =============== No doubt the paper is well-written. But the major issue with the paper is its lucidness. Indeed, poetic language, elegance is applaud-able, but clarity in scientific writing is very much needed. \nI hope you will agree with most of the stuff being articulated here: https://chairs-blog.acl2017.org/2017/02/02/last-minute-writing-advice/ Let me put my objections on writing here: - \"while providing a method which is effectively zero-shot\"..left readers in the blank. The notion of zero-shot has not been introduced yet!\n- Figure 2: most, neutral, least - metaphoric. How did you arrive at such differentiations?\n- Talk more about data. Otherwise, the method is less intuitive.\n- I enjoyed reading the analysis section. But it is not clear why the proposed simple (as claimed) method can over-perform than other existing techniques? \nPutting some examples would be better, I believe.\nTechnicality ============  \"A strength of this model is its simplicity\" - indeed, but the implication is not vivid from the writing. Mathematical and technical definition of a problem is one aspect, but the implication from the definition is quite hard to be understood. When that's the note-able contribution of the paper. Comparing to previous research this paper shows only marginal accuracy gain.\n- Comparison only with one previous work and then claiming that the method is capable of zero-shot, is slightly overstated. Is the method extendable to Twitter, let's say.",
    "comments": "",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "ed33307d8762cd89",
    "paper_id": "ACL_2017_777",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The deviation between \"vocal\" users and \"average users\" is an interesting discovery that could be applied as a way to identify different types of users.",
    "weaknesses": "I see it as an initial work on a new topic that should be expanded in the future. A possible comparison between matrix factorization and similar topics  in distributional semantics (e.g. latent semantic analysis) would be useful.",
    "comments": "In this paper, the authors describe an approach for modeling the stance/sentiment of Twitter users about topics. In particular, they address the task of inter-topic preferences modeling. This task consists of measuring the degree to which the stances about different topics are mutually related. This work is claimed to advance state of the art in this task, since previous works were case studies, while the proposed one is about unlimited topics on real-world data. The adopted approach consists of the following steps: A set of linguistic patterns was manually created and, through them, a large number of tweets expressing stance towards various topics was collected. Next, the texts were expressed as triples containing user, topic, and evaluation. The relationships represented by the tuples were arranged as a sparse matrix. After matrix factorization, a low-rank approximation was performed. The optimal rank was identified as 100. The definition of cosine similarity is used to measure the similarity between topics and, thus, detect latent preferences not represented in the original sparse matrix. Finally, cosine similarity is also used to detect inter-topic preferences. A preliminary empirical evaluation shows that the model predicts missing topics preferences. Moreover, predicted inter-topic preferences moderately correlate with the corresponding values from a crowdsourced gold-standard collection of preferences. \nAccording to the overview discussed in the related work section, there are no previous systems to be compared in the latter task (i.e. prediction of inter-topic preferences) and, for this reason, it is promising.\nI listed some specific comments below.\n- Rows 23 and 744, \"high-quality\": What makes them high-quality? If not properly defined, I would remove all the occurrences of \"high-quality\" in the paper.\n- Row 181 and caption of Figure 1: I would remove the term \"generic.\"\n- Row 217, \"This section collect\": -> \"We collected\" or \"This section explains how we collected\"- Row 246: \"ironies\" -> \"irony\" - Row 269, \"I support TPP\": Since the procedure can detect various patterns such as \"to A\" or \"this is A,\" maybe the author should explain that all possible patterns containing the topic are collected, and next manually filtered?\n- Rows 275 and 280, \"unuseful\": -> useless - Row 306, \"including\": -> are including - Row 309:  \"of\" or \"it\" are not topics but, I guess, terms retrieved by mistakes as topics.  - Rows 317-319: I would remove the first sentence and start with \"Twitter user...\" - Rows 419-439: \"I like the procedure used to find the optimal k. In previous works, this number is often assumed, while it is useful to find it empirically.\"\n- Row 446, \"let\": Is it \"call\"?",
    "overall_score": "3",
    "confidence": "2"
  },
  {
    "review_id": "8a6d8859abb303ea",
    "paper_id": "ACL_2017_779",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper proposes a novel strategy for zero-resource translation where (source, pivot) and (pivot, target) parallel corpora are available. A teacher model for p(target|pivot) is first trained on the (pivot, target) corpus, then a student model for p(target|source) is trained to minimize relative entropy with respect to the teacher on the (source, pivot) corpus. When using word-level relative entropy over samples from the teacher, this approach is shown to outperform previous variants on standard pivoting, as well as other zero-resource strategies.\nThis is a good contribution: a novel idea, clearly explained, and with convincing empirical support. Unlike some previous work, it makes fairly minimal assumptions about the nature of the NMT systems involved, and hence should be widely applicable.\nI have only a few suggestions for further experiments. First, it would be interesting to see how robust this approach is to more dissimilar source and pivot languages, where intuitively the true p(target|source) and p(target|pivot) will be further apart. Second, given the success of introducing word-based diversity, it was surprising not to see a sentence n-best or sentence-sampling experiment. This would be more costly, but not much more so since you’re already doing beam search with the teacher. Finally, related to the previous, it might be interesting to explore transition from word-based diversity to sentence-based as the student converges and no longer needs the signal from low-probability words.\nSome further comments: line 241: Despite its simplicity -> Due to its simplicity 277: target sentence y -> target word y 442: I assume that K=1 and K=5 mean that you compare probabilities of the most probable and 5 most probable words in the current context. If so, how is the current context determined - greedily or with a beam?\nSection 4.2. The comparison with an essentially uniform distribution doesn’t seem very informative here: it would be extremely surprising if p(y|z) were not significantly closer to p(y|x) than to uniform. It would be more interesting to know to what extent p(y|z) still provides a useful signal as p(y|x) gets better. This would be easy to measure by comparing p(y|z) to models for p(y|x) trained on different amounts of data or for different numbers of iterations. \nAnother useful thing to explore in this section would be the effect of the mode approximation compared to n-best for sentence-level scores.\n555: It’s odd that word beam does worse than word greedy, since word beam should be closer to word sampling. Do you have an explanation for this?\n582: The claimed advantage of sent-beam here looks like it may just be noise, given the high variance of these curves.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "82bad4135361e405",
    "paper_id": "ACL_2017_779",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The results the authors present, show that their idea is promising. Also, the authors present several sets of results that validate their assumptions.",
    "weaknesses": "However, there are many points that need to be address before this paper is ready for publication.\n1)            Crucial information is missing Can you flesh out more clearly how training and decoding happen in your training framework? I found out that the equations do not completely describe the approach. It might be useful to use a couple of examples to make your approach clearer.\nAlso, how is the montecarlo sampling done?  2)            Organization The paper is not very well organized. For example, results are broken into several subsections, while they’d better be presented together.  The organization of the tables is very confusing. Table 7 is referred before table 6. This made it difficult to read the results.\n3)            Inconclusive results: After reading the results section, it’s difficult to draw conclusions when, as the authors point out in their comparisons, this can be explained by the total size of the corpus involved in their methods (621  ).  4)            Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.",
    "comments": "Other: 578:  We observe that word-level models tend to have lower valid loss compared with sentence- level methods…. \nIs it valid to compare the loss from two different loss functions?\nSec 3.2, the notations are not clear. What does script(Y) means? \nHow do we get p(y|x)? this is never explained Eq 7 deserves some explanation, or better removed. \n320: What approach did you use? You should talk about that here 392 : Do you mean 2016?\nNitty-gritty: 742  : import => important 772  : inline citation style 778: can significantly outperform  275: Assumption 2 needs to be rewritten … a target sentence y from x should be close to that from its counterpart z.",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "04120d2b56d12de1",
    "paper_id": "ACL_2017_779",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This is  a well written paper. \nThe paper is very clear for the most part. \nThe experimental comparisons are very well done. \nThe experiments are well designed and executed. \nThe idea of using KD for zero-resource NMT is impressive.",
    "weaknesses": "There were many sentences in the abstract and in other places in the paper where the authors stuff too much information into a single sentence. This could be avoided. One can always use an extra sentence to be more clear. \nThere could have been a section where the actual method used could be explained in a more detailed. This explanation is glossed over in the paper. It's non-trivial to guess the idea from reading the sections alone. \nDuring test time, you need the source-pivot corpus as well. This is a major disadvantage of this approach. This is played down - in fact it's not mentioned at all. I could strongly encourage the authors to mention this and comment on it.",
    "comments": "This paper uses knowledge distillation to improve zero-resource translation. \nThe techniques used in this paper are very similar to the one proposed in Yoon Kim et. al. The innovative part is that they use it for doing zero-resource translation. They compare against other prominent works in the field. Their approach also eliminates the need to do double decoding.\nDetailed comments: - Line 21-27 - the authors could have avoided this complicated structure for two simple sentences. \nLine 41 - Johnson et. al has SOTA on English-French and German-English. \nLine 77-79 there is no evidence provided as to why combination of multiple languages increases complexity. Please retract this statement or provide more evidence. Evidence in literature seems to suggest the opposite.\nLine 416-420 - The two lines here are repeated again. They were first mentioned in the previous paragraph. \nLine 577 - Figure 2 not 3!",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "e8835d5bcb9f0da8",
    "paper_id": "ACL_2017_792",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "1. The presentation of the paper, up until the final few sections, is excellent and the paper reads very well at the start. The paper has a clear structure and the argumentation is, for the most part, good. \n2. The paper addresses an important problem by attempting to incorporate word order information into word (and sense) embeddings and the proposed solution is interesting.",
    "weaknesses": "1. Unfortunately, the results are rather inconsistent and one is not left entirely convinced that the proposed models are better than the alternatives, especially given the added complexity. Negative results are fine, but there is insufficient analysis to learn from them. Moreover, no results are reported on the word analogy task, besides being told that the proposed models were not competitive - this could have been interesting and analyzed further. \n2. Some aspects of the experimental setup were unclear or poorly motivated, for instance w.r.t. to corpora and datasets (see details below). \n3. Unfortunately, the quality of the paper deteriorates towards the end and the reader is left a little disappointed, not only w.r.t. to the results but with the quality of the presentation and the argumentation.",
    "comments": "1. The authors aim \"to learn representations for both words and senses in a shared emerging space\". This is only done in the LSTMEmbed_SW version, which rather consisently performs worse than the alternatives. In any case, what is the motivation for learning representations for words and senses in a shared semantic space? This is not entirely clear and never really discussed in the paper. \n2. The motivation for, or intuition behind, predicting pre-trained embeddings is not explicitly stated. Also, are the pre-trained embeddings in the LSTMEmbed_SW model representations for words or senses, or is a sum of these used again? If different alternatives are possible, which setup is used in the experiments? \n3. The importance of learning sense embeddings is well recognized and also stressed by the authors. Unfortunately, however, it seems that these are never really evaluated; if they are, this remains unclear. Most or all of the word similarity datasets considers words independent of context. \n4. What is the size of the training corpora? For instance, using different proportions of BabelWiki and SEW is shown in Figure 4; however, the comparison is somewhat problematic if the sizes are substantially different. The size of SemCor is moreover really small and one would typically not use such a small corpus for learning embeddings with, e.g., word2vec. If the proposed models favor small corpora, this should be stated and evaluated. \n5. Some of the test sets are not independent, i.e. WS353, WSSim and WSRel, which makes comparisons problematic, in this case giving three \"wins\" as opposed to one. \n6. The proposed models are said to be faster to train by using pre-trained embeddings in the output layer. However, no evidence to support this claim is provided. This would strengthen the paper. \n7. Table 4: why not use the same dimensionality for a fair(er) comparison? \n8. A section on synonym identification is missing under similarity measurement that would describe how the multiple-choice task is approached. \n9. A reference to Table 2 is missing. \n10. There is no description of any training for the word analogy task, which is mentioned when describing the corresponding dataset.",
    "overall_score": "2",
    "confidence": "3"
  },
  {
    "review_id": "63192b4e628fbee8",
    "paper_id": "ACL_2017_79",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper considers the problem of KB completion and proposes ITransF for this purpose. Unlike STransE that assigns each relation an independent matrix, this paper proposes to share the parameters between different relations. A model is proposed where a tensor D is constructed that contains various relational matrices as its slices and a selectional vector \\alpha is used to select a subset of relevant relational matrix for composing a particular semantic relation. The paper then discuss a technique to make \\alpha sparse. \nExperimental results on two standard benchmark datasets shows the superiority of ITransF over prior proposals.\nThe paper is overall well written and the experimental results are good. \nHowever, I have several concerns regarding this work that I hope the authors will answer in their response.\n1. Just by arranging relational matrices in a tensor and selecting (or more appropriately considering a linearly weighted sum of the relational matrices) does not ensure any information sharing between different relational matrices. \nThis would have been the case if you had performed some of a tensor decomposition and projected the different slices (relational matrices) into some common lower-dimensional core tensor. It is not clear why this approach was not taken despite the motivation to share information between different relational matrices. \n2. The two requirements (a) to share information across different relational matrices and (b) make the attention vectors sparse are some what contradictory. \nIf the attention vector is truly sparse and has many zeros then information will not flow into those slices during optimisation. \n3. The authors spend a lot of space discussing techniques for computing sparse attention vectors. The authors mention in page 3 that \\ell_1 regularisation did not work in their preliminary experiments. However, no experimental results are shown for \\ell_1 regularisation nor they explain why \\ell_1 is not suitable for this task. To this reviewer, it appears as an obvious baseline to try, especially given the ease of optimisation. You use \\ell_0 instead and get into NP hard optimisations because of it. Then you propose a technique and a rather crude approximation in the end to solve it. All that trouble could be spared if \\ell_1 was used. \n4. The vector \\alpha is performing a selection or a weighing over the slices of D. It is slightly misleading to call this as “attention” as it is a term used in NLP for a more different type of models (see attention model used in machine translation). \n5. It is not clear why you need to initialise the optimisation by pre-trained embeddings from TransE. Why cannot you simply randomly initialise the embeddings as done in TransE and then update them? It is not fair to compare against TransE if you use TransE as your initial point.\nLearning the association between semantic relations is an idea that has been used in related problems in NLP such as relational similarity measurement [Turney JAIR 2012] and relation adaptation [Bollegala et al. IJCAI 2011]. It would be good to put the current work with respect to such prior proposals in NLP for modelling inter-relational correlation/similarity.\nThanks for providing feedback. I have read it.",
    "overall_score": "3",
    "confidence": "5"
  },
  {
    "review_id": "ae894eb3149d0ea7",
    "paper_id": "ACL_2017_805",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "originality of the CORE evaluation measure, good accuracy of proposed similarity measure and large number and diversity of datasets for evaluation.",
    "weaknesses": "# some typos    - line 116-117, 'to design of a new' -> 'to design a new'    - line 176-177, figure 2 -> figure 1    - line 265, 'among the the top' -> 'among the top'    - line 320, 'figure 4' should be introduced within the article body. \n   - line 434, 'the dataset was contains' -> 'the dataset contains'    - line 486-487, table 3 -> table 1    - a 'Tensorflow' should be replaced by 'TextFlow'  # imprecisions    - features computation accuracy of lemma, pos or wordnet synset should be detailed in the paper and it should be discussed if it impacts the general similarity accuracy evaluation or not   - the neural networks are said to be implemented in Python but the code is not said to be available - to be able to repeat the experiment   - the training and evaluation sets are said to be shared, but it is not said how (on demand?, under license?) - to be able to repeat the experiment",
    "comments": "",
    "overall_score": "4",
    "confidence": "3"
  },
  {
    "review_id": "b185240b455c33a0",
    "paper_id": "ACL_2017_818",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The topic of the paper is very creative and the purpose of the research really worthwhile: the paper aims at extracting common knowledge from text, overcoming the well-known problem of reporting bias (the fact that people will not state the obvious, such as the fact that a person is usually bigger than a ball), by doing joint inference on information that is possible to extract from text.",
    "weaknesses": "1) Many aspects of the approach need to be clarified (see detailed comments below). What worries me the most is that I did not understand how the approach makes knowledge about objects interact with knowledge about verbs such that it allows us to overcome reporting bias. The paper gets very quickly into highly technical details, without clearly explaining the overall approach and why it is a good idea.\n2) The experiments and the discussion need to be finished. In particular, there is no discussion of the results of one of the two tasks tackled (lower half of Table 2), and there is one obvious experiment missing: Variant B of the authors' model gives much better results on the first task than Variant A, but for the second task only Variant A is tested -- and indeed it doesn't improve over the baseline.",
    "comments": "The paper needs quite a bit of work before it is ready for publication.  - Detailed comments: 026 five dimensions, not six Figure 1, caption: \"implies physical relations\": how do you know which physical relations it implies?\nFigure 1 and 113-114: what you are trying to do, it looks to me, is essentially to extract lexical entailments (as defined in formal semantics; see e.g. Dowty 1991) for verbs. Could you please explicit link to that literature?\nDowty, David. \" Thematic proto-roles and argument selection.\" Language (1991): 547-619.\n135 around here you should explain the key insight of your approach: why and how does doing joint inference over these two pieces of information help overcome reporting bias?\n141 \"values\" ==> \"value\"?\n143 please also consider work on multimodal distributional semantics, here and/or in the related work section. The following two papers are particularly related to your goals: Bruni, Elia, et al. \"Distributional semantics in technicolor.\" Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012.\nSilberer, Carina, Vittorio Ferrari, and Mirella Lapata. \" Models of Semantic Representation with Visual Attributes.\" ACL (1). 2013.\n146 please clarify that your contribution is the specific task and approach -- commonsense knowledge extraction from language is long-standing task.\n152 it is not clear what \"grounded\" means at this point Section 2.1: why these dimensions, and how did you choose them?\n177 explain terms \"pre-condition\" and \"post-condition\", and how they are relevant here 197-198 an example of the full distribution for an item (obtained by the model, or crowd-sourced, or \"ideal\") would help.\nFigure 2. I don't really see the \"x is slower than y\" part: it seems to me like this is related to the distinction, in formal semantics, between stage-level vs. individual-level predicates: when a person throws a ball, the ball is faster than the person (stage-level) but it's not true in general that balls are faster than people (individual-level). \nI guess this is related to the pre-condition vs. post-condition issue. Please spell out the type of information that you want to extract.\n248 \"Above definition\": determiner missing Section 3 \"Action verbs\": Which 50 classes do you pick, and you do you choose them? Are the verbs that you pick all explicitly tagged as action verbs by Levin?  306ff What are \"action frames\"? How do you pick them?\n326 How do you know whether the frame is under- or over-generating?\nTable 1: are the partitions made by frame, by verb, or how? That is, do you reuse verbs or frames across partitions? Also, proportions are given for 2 cases (2/3 and 3/3 agreement), whereas counts are only given for one case; which?\n336 \"with... PMI\": something missing (threshold?)\n371 did you do this partitions randomly?\n376 \"rate *the* general relationship\" 378 \"knowledge dimension we choose\": ? ( how do you choose which dimensions you will annotate for each frame?)\nSection 4 What is a factor graph? Please give enough background on factor graphs for a CL audience to be able to follow your approach. What are substrates, and what is the role of factors? How is the factor graph different from a standard graph? \nMore generally, at the beginning of section 4 you should give a higher level description of how your model works and why it is a good idea.\n420 \"both classes of knowledge\": antecedent missing.\n421 \"object first type\" 445 so far you have been only talking about object pairs and verbs, and suddenly selectional preference factors pop in. They seem to be a crucial part of your model -- introduce earlier? In any case, I didn't understand their role.\n461 \"also\"?\n471 where do you get verb-level similarities from?\nFigure 3: I find the figure totally unintelligible. Maybe if the text was clearer it would be interpretable, but maybe you can think whether you can find a way to convey your model a bit more intuitively. Also, make sure that it is readable in black-and-white, as per ACL submission instructions.\n598 define term \"message\" and its role in the factor graph.\n621 why do you need a \"soft 1\" instead of a hard 1?\n647ff you need to provide more details about the EMB-MAXENT classifier (how did you train it, what was the input data, how was it encoded), and also explain why it is an appropriate baseline.\n654 \"more skimp seed knowledge\": ?\n659 here and in 681, problem with table reference (should be Table 2).  664ff I like the thought but I'm not sure the example is the right one: in what sense is the entity larger than the revolution? Also, \"larger\" is not the same as \"stronger\".\n681 as mentioned above, you should discuss the results for the task of inferring knowledge on objects, and also include results for model (B) (incidentally, it would be better if you used the same terminology for the model in Tables 1 and 2) 778 \"latent in verbs\": why don't you mention objects here?\n781 \"both tasks\": antecedent missing The references should be checked for format, e.g. Grice, Sorower et al for capitalization, the verbnet reference for bibliographic details.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "65378a9d03c84394",
    "paper_id": "ACL_2017_818",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- The paper proposes a novel method to address an important problem of mining common sense attribute relations from text.",
    "weaknesses": "- I would have liked to see more examples of objects pairs, action verbs, and predicted attribute relations.                          What are some interesting action verbs and corresponding attributes relations?  The paper also lacks analysis/discussion  on what kind of mistakes their model makes.\n- The number of object pairs (3656) in the dataset is very small.  How many distinct object categories are there?  How scalable is this approach to larger number of object pairs?\n- It's a bit unclear how the frame similarity factors and attributes similarity factors are selected.",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "cb48b968caf0efab",
    "paper_id": "ACL_2017_818",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper tries to solve an interesting and challenging problem. The problem is hard due to reporting bias, and the key insight/approach in the paper is inspiring.\nThe model is innovative and clearly described. And the idea of handling text sparsity with semantic similarity factors is also appropriate.  The empirical evidence well supports the effectiveness of the model (compared to other baselines).  The paper is well-written, with informative visualization, except for some minor errors like *six dimensions* in abstract but *five* everywhere else.",
    "weaknesses": "The benefits and drawbacks of model components are still somehow under-discussed, and hard to tell with the limited quantitative results in the paper.  For example, is there any inherent discrepancy between *cross-verb frame similarity*, *within-verb frame similarity* and *action-object compatibility*? \nFrames of *A throw B* and *C thrown by D* share a verb primitive *throw*, so should it infer C>D (by *within-verb*) if A>B is given? \nOn the other side, frames of *C thrown by D* and *E kicked by F* share the frame *XXX by*, so if F>E is known, is D>C inferred? How does the current model deal with such discrepancy?\nThe paper might be better if it has more qualitative analysis. And more evidence also needs to be provided to gauge how difficult the task/dataset is.\nFor example, are the incorrectly-classified actions/objects also ambiguous for human? On what types of actions/objects does the model tend to make mistakes? \nIs the verb with more frame types usually harder than others for the model?\nMore interestingly, how are the mistakes made? Are they incorrectly enforced by any proposed *semantic similarity*?\nI think more analysis on the model components and qualitative results may inspire more general framework for this task.",
    "comments": "/* After author response */ After reading the response, I tend to keep my current rating and accept this paper. The response well addresses my concerns. And I tend to believe that necessary background and experimental analysis can be added given some re-organization of the paper and one extra page, as it is not hard.  /* Before author response */ I think this paper is in general solid and interesting. \nI tend to accept it, but it would be better if the questions above can be answered.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "82b2578c9914cffb",
    "paper_id": "ACL_2017_87",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Nicely written and understandable. \nClearly organized. Targeted answering of research questions, based on  different experiments.",
    "weaknesses": "Minimal novelty. The \"first sentence\" heuristic has been in the summarization literature for many years. This work essentially applies this heuristic (evolved) in the keyword extraction setting. This is NOT to say that the work is trivial: it is just not really novel.\nLack of state-of-the-art/very recent methods. The experiment on the system evaluation vs state-of-the-art systems simply uses strong baselines. Even though the experiment answers the question \"does it perform better than baselines?\", I am not confident it illustrates that the system performs better than the current state-of-the-art. This somewhat reduces the value of the paper.",
    "comments": "Overall the paper is good and I propose that it be published and presented.  On the other hand, I would propose that the authors position themselves (and the system performance) with respect to: Martinez‐Romo, Juan, Lourdes Araujo, and Andres Duque Fernandez. \" SemGraph: Extracting keyphrases following a novel semantic graph‐based approach.\" \nJournal of the Association for Information Science and Technology 67.1 (2016): 71-82. \n(with which the work holds remarkable resemblance in some points) Le, Tho Thi Ngoc, Minh Le Nguyen, and Akira Shimazu. \" Unsupervised Keyphrase Extraction: Introducing New Kinds of Words to Keyphrases.\" Australasian Joint Conference on Artificial Intelligence. Springer International Publishing, 2016.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "62ec44c3d32d8bbf",
    "paper_id": "ACL_2017_94",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper makes several novel contributions to (transition-based) dependency parsing by extending the notion of non-monotonic transition systems and dynamic oracles to unrestricted non-projective dependency parsing. The theoretical and algorithmic analysis is clear and insightful, and the paper is admirably clear.",
    "weaknesses": "Given that the main motivation for using Covington's algorithm is to be able to recover non-projective arcs, an empirical error analysis focusing on non-projective structures would have further strengthened the paper. And even though the main contributions of the paper are on the theoretical side, it would have been relevant to include a comparison to the state of the art on the CoNLL data sets and not only to the monotonic baseline version of the same parser.",
    "comments": "The paper extends the transition-based formulation of Covington's dependency parsing algorithm (for unrestricted non-projective structures) by allowing non-monotonicity in the sense that later transitions can change structure built by earlier transitions. In addition, it shows how approximate dynamic oracles can be formulated for the new system. Finally, it shows experimentally that the oracles provide a tight approximation and that the non-monotonic system leads to improved parsing accuracy over its monotonic counterpart for the majority of the languages included in the study.\nThe theoretical contributions are in my view significant enough to merit publication, but I also think the paper could be strengthened on the empirical side. In particular, it would be relevant to investigate, in an error analysis, whether the non-monotonic system improves accuracy specifically on non-projective structures. Such an analysis can be motivated on two grounds: (i) the ability to recover non-projective structures is the main motivation for using Covington's algorithm in the first place; (ii) non-projective structures often involved long-distance dependencies that are hard to predict for a greedy transition-based parser, so it is plausible that the new system would improve the situation.  Another point worth discussion is how the empirical results relate to the state of the art in light of recent improvements thanks to word embeddings and neural network techniques. For example, the non-monotonicity is claimed to mitigate the error propagation typical of classical greedy transition-based parsers. But another way of mitigating this problem is to use recurrent neural networks as preprocessors to the parser in order to capture more of the global sentence context in word representations. Are these two techniques competing or complementary? A full investigation of these issues is clearly outside the scope of the paper, but some discussion would be highly relevant.\nSpecific questions: Why were only 9 out of the 13 data sets from the CoNLL-X shared task used? I am sure there is a legitimate reason and stating it explicitly may prevent readers from becoming suspicious.  Do you have any hypothesis about why accuracy decreases for Basque with the non-monotonic system? Similar (but weaker) trends can be seen also for Turkish, Catalan, Hungarian and (perhaps) German.\nHow do your results compare to the state of the art on these data sets? This is relevant for contextualising your results and allowing readers to estimate the significance of your improvements.\nAuthor response: I am satisfied with the author's response and see no reason to change my previous review.",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "f430886fb86f12f9",
    "paper_id": "ACL_2017_96",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "the paper is well written the dataset is collected in a proper manner the experiments are carefully done and the analysis is sound.",
    "weaknesses": "lack statistics of the datsets (e.g. average length, vocabulary size) the baseline (Moses) is not proper because of the small size of the dataset the assumption \"sarcastic tweets often differ from their non sarcastic interpretations in as little as one sentiment word\" is not supported by the data.",
    "comments": "This discussion gives more details about the weaknesses of the paper.  Half of the paper is about the new dataset for sarcasm interpretation. \nHowever, the paper doesn't show important information about the dataset such as average length, vocabulary size. More importantly, the paper doesn't show any statistical evidence to support their method of focusing on sentimental words.  Because the dataset is small (only 3000 tweets), I guess that many words are rare. Therefore, Moses alone is not a proper baseline. A proper baseline should be a MT system that can handle rare words very well. In fact, using clustering and declustering (as in Sarcasm SIGN) is a way to handle rare words.\nSarcasm SIGN is built based on the assumption that \"sarcastic tweets often differ from their non sarcastic interpretations in as little as one sentiment word\". Table 1 however strongly disagrees with this assumption: the human interpretations are often different from the tweets at not only sentimental words. I thus strongly suggest the authors to give statistical evidence from the dataset that supports their assumption. Otherwise, the whole idea of Sarcasm SIGN is just a hack.\n-------------------------------------------------------------- I have read the authors' response. I don't change my decision because of the following reasons:  - the authors wrote that \"the Fiverr workers might not take this strategy\": to me it is not the spirit of corpus-based NLP. A model must be built to fit given data, not that the data must follow some assumption that the model is built on.\n- the authors wrote that \"the BLEU scores of Moses and SIGN are above 60, which is generally considered decent in the MT literature\": to me the number 60 doesn't  show anything at all because the sentences in the dataset are very short. And that, if we look at table 6, %changed of Moses is only 42%, meaning that even more than half of the time translation is simply copying, the BLUE score is more than 60.\n- \"While higher scores might be achieved with MT systems that explicitly address rare words, these systems don't focus on sentiment words\": it's true, but I was wondering whether sentiment words are rare in the corpus. If they are, those MT systems should obviously handle them (in addition to other rare words).",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "233541afae4f553e",
    "paper_id": "ACL_2017_96",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "(1) A new dataset would be useful for other researchers in this area (2) An algorithm with sentiment words based machine translation is proposed to interpret sarcasm tweets.",
    "weaknesses": "(1) Do not provide detailed statistics of constructed dataset.\n(2) Integrating sentiment word clustering with machine translation techniques only is simple and straightforward, novelty may be a challenging issue.",
    "comments": "Overall, this paper is well written. The experiments are conducted carefully and the analysis is reasonable.  I offer some comments as follows. \n(1) According to data collection process, each tweet should be annotated five times. How to determine which one is regarded as gold standard for measure performance?\n(2) The MT technique (Moses) is well known, but it may not be a good baseline. Another MT technique (RNN) should be put together for comparison.    (3) Differ from most work focuses on sarcasm detection. The research topic is interesting. It attempts to interpret sarcasm for reflecting semantics.",
    "overall_score": "3",
    "confidence": "3"
  },
  {
    "review_id": "5ccd865a821d1c59",
    "paper_id": "ACL_2017_96",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Among the positive aspects of your work, I would like to mention the parallel corpus you presented. I think it will be very useful for other researchers in the area for identifying and interpreting sarcasm in social media. An important contribution is also the attempt to evaluate the parallel corpora using existing measures such as the ones used in MT tasks. But also because you used human judgements to evaluate the corpora in 3 aspects: fluency, adequacy and equivalent sentiment.\n- Room for improvement: Tackling the problem of interpretation as a monolingual machine translations task is interesting, while I do appreciate the intent to compare the MT with two architectures, I think that due the relatively small dataset (needed for RNN) used it was predictable that the “Neural interpretation” is performing worse than “moses interpretation”. You came to the same conclusion after seeing the results in Table3. In addition to comparing with this architecture, I would've liked to see other configuration of the MT used with moses. Or at least, you should provide some explanation of why you use the configuration described in lines 433 through 442; to me this choice is not justified. \n  - thank you for your response, I understand it is difficult to write down all the details but I hope you include a line with some of your answer in the paper, I believe this could add valuable information.\nWhen you presented SING, it is clear that you evaluate some of its components beforehand, i.e. the MT. But other important components are not evaluated, particularly, the clustering you used of positive and negative words. While you did said you used k-means as a clustering algorithm it is not clear to me why you wanted to create clusters with 10 words. Why not test with other number of k, instead of 7 and 16, for positive and negative words respectively. Also you could try another algorithm beside kmeans, for instance, the star clustering algorithm (Aslam et al. 2004), that do not require a k parameter. \n   - thanks for clarifying.\nYou say that SIGN searches the tweet for sentiment words if it found one it changes it for the cluster ID that contain that word. I am assuming that there is not a limit for the number of sentiment words found, and the MT decides by itself how many sentiment words to change. For example, for the tweet provided in Section 9: “Constantly being irritated, anxious and depressed is a great feeling” the clustering stage of SIGN should do something like “Constantly being cluster-i, cluster-j and cluster-k is a cluster-h feeling”, Is that correct? If not, please explain what SIGN do. \n    - Thanks for clarifying - Minor comments: In line 704, section 7, you said: “SIGN-context’s interpretations differ from the original sarcastic tweet in 68.5% of the cases, which come closer to the 73.8% in the gold standard human interpretations.” This means that 25% of the human interpretations are the same as the original tweet? Do you have any idea why is that?\nIn section 6, line 539 you could eliminate the footnote 7 by adding “its cluster ID” or “its cluster number”.\nReferences: Aslam, Javed A., Pelekhov, Ekaterina, and Rus, Daniela. \" The star clustering algorithm for static and dynamic information organization..\" Journal of Graph Algorithms and Applications 8.1 (2004): 95-129. <http://eudml.org/doc/51529>.",
    "weaknesses": "",
    "comments": "",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "2453e03ca63c0b60",
    "paper_id": "ACL_2017_97",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper represents an application of an interesting NLP problem -- recognizing textual entailment -- to an important task -- written test scoring.",
    "weaknesses": "There isn't anything novel in the paper. It consist of an application of an existing technology to a known problem.\nThe approach described in the paper is not autonomous -- it still needs a human to do the actual scoring. The paper lacks any quantitative or qualitative evaluation of how useful such system is. That is, is it making the job of the scorer easier? Is the scorer more effective as compared to not having automatic score?\nThe system contains multiple components and it is unclear how the quality of each one of them contributes to the overall experience.\nThe paper needs more work with the writing. Language and style is rough in several places.\nThe paper also contains several detailed examples, which don't necessarily add a lot of value to the discussion.\n For the evaluation of classification, what is the baseline of predicting the most frequent class?",
    "comments": "I find this paper not very inspiring. I don't see the message in the paper apart from announcing having build such a system",
    "overall_score": "1",
    "confidence": "4"
  },
  {
    "review_id": "0b1a40254fd46bda",
    "paper_id": "ACL_2017_97",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper tries to tackle a very practical problem: automated short answer scoring (SAS), in particular for Japanese which hasn't gotten as much attention as, say, English-language SAS.",
    "weaknesses": "The paper simply reads like a system description, and is light on experiments or insights. The authors show a lack of familiarity with more recent related work (aimed at English SAS), both in terms of methodology and evaluation. Here are a couple: https://www.aclweb.org/anthology/W/W15/W15-06.pdf#page=97 https://www.aclweb.org/anthology/N/N15/N15-1111.pdf There was also a recent Kaggle competition that generated several methodologies: https://www.kaggle.com/c/asap-sas",
    "comments": "To meet ACL standards, I would have preferred to see more experiments (feature ablation studies, algorithm comparisons) that motivated the final system design, as well as some sort of qualitative evaluation with a user study of how the mixed-initiative user interface features led to improved scores. As it is, it feels like a work in progress without any actionable new methods or insights.\nAlso, Pearson/Spearman correlation and kappa scores are considered more appropriate than accuracy for these sorts of ordinal human scores.",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "fbe60004b3657d5b",
    "paper_id": "ACL_2017_97",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper presents a text classification method based on pre-training technique using both labeled and unlabeled data. The authors reported experimental results with several benchmark data sets including TREC data, and showed that the method improved overall performance compared to other comparative methods.\nI think the approach using pre-training and fine-tuning itself is not a novel one, but the originality is the use of both labeled and unlabeled data in the pre-training step. \nThe authors compare their results against three baselines, i.e. without pre-training and a deep learning with unsupervised pre-training using deep autoencoders, but I think that I would be interesting to compare the method against other methods presented in the introduction section.",
    "overall_score": "3",
    "confidence": "3"
  }
]