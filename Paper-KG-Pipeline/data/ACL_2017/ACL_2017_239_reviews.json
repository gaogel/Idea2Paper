[
  {
    "review_id": "c9fc8a3f351f6858",
    "paper_id": "ACL_2017_239",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- The transfer learning / data efficiency motivation is an interesting one, as it directly relates to the idea of using embeddings as a simple \"semi-supervised\" approach.",
    "weaknesses": "- A good evaluation approach would be one that propagates to end tasks. \nSpecifically, if the approach gives some rank R for a set of embeddings, I would like it to follow the same rank for an end task like text classification, parsing or machine translation. However, the approach is not assessed in this way so it is difficult to trust the technique is actually more useful than what is traditionally done.\n- The discussion about injective embeddings seems completely out-of-topic and does not seem to add to the paper's understanding.\n- The experimental section is very confusing. Section 3.7 points out that the analysis results in answers to questions as \"is it worth fitting syntax specific embeddings even when supervised datset is large?\" but I fail to understand where in the evaluation the conclusion was made.\n- Still in Section 3.7, the manuscript says \"This hints, that purely unsupervised large scale pretraining might not be suitable for NLP applications\". This is a very bold assumption and I again fail to understand how this can be concluded from the proposed evaluation approach.\n- All embeddings were obtained as off-the-shelf pretrained ones so there is no control over which corpora they were trained on. This limits the validity of the evaluation shown in the paper.\n- The manuscript needs proofreading, especially in terms of citing figures in the right places (why Figure 1, which is on page 3, is only cited in page 6?).",
    "comments": "",
    "overall_score": "2",
    "confidence": "3"
  },
  {
    "review_id": "77957bfdfd8c136f",
    "paper_id": "ACL_2017_239",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper proposed an interesting and important metric for evaluating the quality of word embeddings, which is the \"data efficiency\" when it is used in other supervised tasks.\nAnother interesting point in the paper is that the authors separated out three questions: 1) whether supervised task offers more insights to evaluate embedding quality; 2) How stable is the ranking vs labeled data set size; 3) The benefit to linear vs non-linear models.\nOverall, the authors presented comprehensive experiments to answer those questions, and the results see quite interesting to know for the research community.",
    "weaknesses": "The overall result is not very useful for ML practioners in this field, because it merely confirms what has been known or suspected, i.e. it depends on the task at hand, the labeled data set size, the type of the model, etc. So, the result in this paper is not very actionable. The reviewer noted that this comprehensive analysis deepens the understanding of this topic.",
    "comments": "The paper's presentation can be improved. Specifically:  1) The order of the figures/tables in the paper should match the order they are mentioned in the papers. Right now their order seems quite random.\n2) Several typos (L250, 579, etc). Please use a spell checker.\n3) Equation 1 is not very useful, and its exposition looks strange. It can be removed, and leave just the text explanations.\n4) L164 mentions the \"Appendix\", but it is not available in the paper.\n5) Missing citation for the public skip-gram data set in L425.\n6) The claim in L591-593 is too strong. It must be explained more clearly, i.e. when it is useful and when it is not.\n7) The observation in L642-645 is very interesting and important. It will be good to follow up on this and provide concrete evidence or example from some embedding. Some visualization may help too.\n8) In L672 should provide examples of such \"specialized word embeddings\" and how they are different than the general purpose embedding.\n9) Figuer 3 is too small to read.",
    "overall_score": "3",
    "confidence": "4"
  }
]