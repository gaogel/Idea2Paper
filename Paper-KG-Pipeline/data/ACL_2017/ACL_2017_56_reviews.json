[
  {
    "review_id": "56af5fd9fc3fa140",
    "paper_id": "ACL_2017_56",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper presents an extension of many popular methods for learning vector representations of text.  The original methods, such as skip-gram with negative sampling, Glove, or other PMI based approaches currently use word cooccurrence statistics, but all of those approaches could be extended to n-gram based statistics.  N-gram based statistics would increase the complexity of every algorithm because both the vocabulary of the embeddings and the context space would be many times larger.  This paper presents a method to learn embeddings for ngrams with ngram context, and efficiently computes these embeddings.  On similarity and analogy tasks, they present strong results.",
    "weaknesses": "I would have loved to see some experiments on real tasks where these embeddings are used as input beyond the experiments presented in the paper.  That would have made the paper far stronger.",
    "comments": "Even with the aforementioned weakness, I think this is a nice paper to have at ACL.\nI have read the author response.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "c34e46dcbd0903dd",
    "paper_id": "ACL_2017_56",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The idea to train word2vec-type models with ngrams (here specifically: bigrams) instead of words is excellent. The range of experimental settings (four word2vec-type algorithms, several word/bigram conditions) covers quite a bit of ground. The qualitative inspection of the bigram embeddings is interesting and shows the potential of this type of model for multi-word expressions.",
    "weaknesses": "This paper would benefit from a check by a native speaker of English, especially regarding the use of articles. The description of the similarity and analogy tasks comes at a strange place in the paper (4.1 Datasets).",
    "comments": "As is done at some point well into the paper, it could be clarified from the start that this is simply a generalization of the original word2vec idea, redefining the word as an ngram (unigram) and then also using bigrams. It would be good to give a rationale why larger ngrams have not been used.\n(I have read the author response.)",
    "overall_score": "4",
    "confidence": "2"
  },
  {
    "review_id": "455e2876d213d5a6",
    "paper_id": "ACL_2017_56",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- The proposed work seems like a natural extension of existing work on learning word embeddings. By integrating bigram information, one can expect to capture richer syntactic and semantic information.",
    "weaknesses": "- While the authors propose learning embeddings for bigrams (bi_bi case), they actually do not evaluate the embeddings for the learned bigrams except for the qualitative evaluation in Table 7. A more quantitative evaluation on paraphrasing or other related tasks that can include bigram representations could have been a good contribution.\n- The evaluation and the results are not very convincing - the results do not show consistent trends, and some of the improvements are not necessarily statistically significant.\n- The paper reads clunkily due to significant grammar and spelling errors, and needs a major editing pass.",
    "comments": "This paper is an extension of standard embedding learning techniques to include information from bigram-bigram coocurance. While the work is interesting and a natural extension of existing work, the evaluation and methods leaves some open questions. Apart from the ones mentioned in the weaknesses, some minor questions for the authors : - Why is there significant difference between the overlap and non-overlap cases? I would be more interested in finding out more than the quantitative difference shown on the tasks.\nI have read the author response. I look forward to seeing the revised version of the paper.",
    "overall_score": "3",
    "confidence": "4"
  }
]