[
  {
    "review_id": "bae43fd3908bef66",
    "paper_id": "ACL_2017_614",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- The key idea of marrying vector space model based approaches and sense inventories for the lexsub task is useful since these two techniques seem to have complementary information, especially since the vector space models are typically unaware of sense and polysemy.\n- The oracle evaluation is interesting as it gives a clear indication of how much gain can one expect in the best case, and while there is still a large gap between the oracle and actual scores, we can still argue for the usefulness of the proposed approach due to the large difference between the unfiltered GAP and the oracle GAP.",
    "weaknesses": "- I don't understand effectiveness of the multi-view clustering approach. \nAlmost all across the board, the paraphrase similarity view does significantly better than other views and their combination. What, then, do we learn about the usefulness of the other views? There is one empirical example of how the different views help in clustering paraphrases of the word 'slip', but there is no further analysis about how the different clustering techniques differ, except on the task directly. Without a more detailed analysis of differences and similarities between these views, it is hard to draw solid conclusions about the different views.                                   - The paper is not fully clear on a first read. Specifically, it is not immediately clear how the sections connect to each other, reading more like disjoint pieces of work. For instance, I did not understand the connections between section 2.1 and section 4.3, so adding forward/backward pointer references to sections should be useful in clearing up things. Relatedly, the multi-view clustering section (3.1) needs editing, since the subsections seem to be out of order, and citations seem to be missing (lines 392 and 393).\n- The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.",
    "comments": "The paper is mostly straightforward in terms of techniques used and experiments. Even then, the authors show clear gains on the lexsub task by their two-pronged approach, with potentially more to be gained by using stronger WSD algorithms.\nSome additional questions for the authors : - Lines 221-222 : Why do you add hypernyms/hyponyms?\n- Lines 367-368 : Why does X^{P} need to be symmetric?\n- Lines 387-389 : The weighting scheme seems kind of arbitrary. Was this indeed arbitrary or is this a principled choice?\n- Is the high performance of SubstClus^{P} ascribable to the fact that the number of clusters was tuned based on this view? Would tuning the number of clusters based on other matrices affect the results and the conclusions?\n- What other related tasks could this approach possibly generalize to? Or is it only specific to lexsub?",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "d9fc1b3a84e1fa59",
    "paper_id": "ACL_2017_614",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The paper presents a new method that exploits word senses to improve the task of lexical substitutability.  Results show improvements over prior methods.",
    "weaknesses": "As a reader of a ACL paper, I usually ask myself what important insight can I take away from the paper, and from a big picture point of view, what does the paper add to the fields of natural language processing and computational linguistics.  How does the task of lexical substitutability in",
    "comments": "",
    "overall_score": "2",
    "confidence": "4"
  }
]