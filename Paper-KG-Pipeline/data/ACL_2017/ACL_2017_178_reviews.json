[
  {
    "review_id": "0949bf76c9e2cb32",
    "paper_id": "ACL_2017_178",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "The paper describes an extension of word embedding methods to also provide representations for phrases and concepts that correspond to words.  The method works by fixing an identifier for groups of phrases, words and the concept that all denote this concept, replace the occurrences of the phrases and words by this identifier in the training corpus, creating a \"tagged\" corpus, and then appending the tagged corpus to the original corpus for training.  The concept/phrase/word sets are taken from an ontology.  Since the domain of application is biomedical, the related corpora and ontologies are used.  The researchers also report on the generation of a new test dataset for word similarity and relatedness for real-world entities, which is novel.\nIn general, the paper is nicely written.  The technique is pretty natural, though not a very substantial contribution. The scope of the contribution is limited, because of focused evaluation within the biomedical domain.\nMore discussion of the generated test resource could be useful.  The resource could be the true interesting contribution of the paper.\nThere is one small technical problem, but that is probably just a matter of mathematical expression than implementation.\nTechnical problem: Eq. 8: The authors want to define the MAP calculation.                          This is a good idea, thought I think that a natural cut-off could be defined, rather than ranking the entire vocabulary.                          Equation 8 does not define a probability; it is quite easy to show this, even if the size of the vocabulary is infinite.  So you need to change the explanation (take out talk of a probability).\nSmall corrections: line: 556: most concepts has --> most concepts have",
    "overall_score": "2",
    "confidence": "3"
  },
  {
    "review_id": "47e9a46f2e60d0db",
    "paper_id": "ACL_2017_178",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The key question addressed by the paper is that phrases that are not lexically similar can be semantically close and, furthermore, not all phrases are compositional in nature. To this end, the paper proposes a plausible model to train phrase embeddings. The trained embeddings are shown to be competitive or better at identifying similarity between concepts.\nThe software released with the paper could be useful for biomedical NLP researchers.\n-",
    "weaknesses": "The primary weakness of the paper is that the model is not too novel. It is essentially a tweak to skip-gram.  Furthermore, the full model presented by the paper doesn't seem to be the best one in the results (in Table 4). On the two Mayo datasets, the Choi baseline is substantially better. A similar trend seems to dominate Table 6 too. On the larger UMNSRS data, the proposed model is at best competitive with previous simpler models (Chiu).",
    "comments": "The paper says that it is uses known phrases as distant supervision to train embeddings. However, it is not clear what the \"supervision\" here is. If I understand the paper correctly, every occurrence of a phrase associated with a concept provides the context to train word embeddings. But this is not supervision in the traditional sense (say for identifying the concept in the text or other such predictive tasks). So the terminology is a bit confusing.\n The notation introduced in Section 3.2 (E_W, etc) is never used in the rest of the paper.\nThe use of \\beta to control for compositionality of phrases by words is quite surprising. Essentially, this is equivalent to saying that there is a single global constant that decides \"how compositional\" any phrase should be. The surprising part here is that the actual values of \\beta chosen by cross validation from Table 3 are odd. For PM+CL and WikiNYT, it is zero, which basically argues against compositionality.  The experimental setup for table 4 needs some explanation. The paper says that the data labels similarity/relatedness of concepts (or entities). However, if the concepts-phrases mapping is really many-to-many, then how are the phrase/word vectors used to compute the similarities? It seems that we can only use the concept vectors.\nIn table 5, the approximate phr method (which approximate concepts with the average of the phrases in them) is best performing. So it is not clear why we need the concept ontology. Instead, we could have just started with a seed set of phrases to get the same results.",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "24123c381443cb99",
    "paper_id": "ACL_2017_178",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- The proposed joint embedding model is straightforward and makes reasonable sense to me. Its main value in my mind is in reaching a (configurable) middle ground between treating phrases as atomic units on one hand to considering their compositionallity on the other. The same approach is applied to concepts being ‘composed’ of several representative phrases.\n-  The paper describes a decent volume of work, including model development, an additional contribution in the form of a new evaluation dataset, and several evaluations and analyses performed.",
    "weaknesses": "- The evaluation reported in this paper includes only intrinsic tasks, mainly on similarity/relatedness datasets. As the authors note, such evaluations are known to have very limited power in predicting the utility of embeddings in extrinsic tasks. Accordingly, it has become recently much more common to include at least one or two extrinsic tasks as part of the evaluation of embedding models.\n- The similarity/relatedness evaluation datasets used in the paper are presented as datasets recording human judgements of similarity between concepts. However, if I understand correctly, the actual judgements were made based on presenting phrases to the human annotators, and therefore they should be considered as phrase similarity datasets, and analyzed as such.\n- The medical concept evaluation dataset, ‘mini MayoSRS’ is extremely small (29 pairs), and its larger superset ‘MayoSRS’ is only a little larger (101 pairs) and was reported to have a relatively low human annotator agreement. The other medical concept evaluation dataset, ‘UMNSRS’, is more reasonable in size, but is based only on concepts that can be represented as single words, and were represented as such to the human annotators. This should be mentioned in the paper and makes the relevance of this dataset questionable with respect to representations of phrases and",
    "comments": "",
    "overall_score": "2",
    "confidence": "4"
  }
]