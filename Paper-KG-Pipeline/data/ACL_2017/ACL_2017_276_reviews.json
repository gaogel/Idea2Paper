[
  {
    "review_id": "29f2e106080d11ca",
    "paper_id": "ACL_2017_276",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The contribution is quite well-written and easy to follow for the most part. \nThe model is exposed in sufficient detail, and the experiments are thorough within the defined framework. The benefits of introducing an auxiliary objective are nicely exposed.",
    "weaknesses": "The paper shows very limited awareness of the related work, which is extensive across the tasks that the experiments highlight. Tables 1-3 only show the three systems proposed by the contribution (Baseline, +dropout, and +LMcost), while some very limited comparisons are sketched textually.\nA contribution claiming novelty and advancements over the previous state of the art should document these improvements properly: at least by reporting the relevant scores together with the novel ones, and ideally through replication. \nThe datasets used in the experiments are all freely available, the previous results well-documented, and the previous systems are for the most part publicly available.\nIn my view, for a long paper, it is a big flaw not to treat the previous work more carefully.\nIn that sense, I find this sentence particularly troublesome: \"The baseline results are comparable to the previous best results on each of these benchmarks.\" The reader is here led to believe that the baseline system somehow subsumes all the previous contributions, which is shady on first read, and factually incorrect after a quick lookup in related work.\nThe paper states \"new state-of-the-art results for error detection on both FCE and CoNLL-14 datasets\". Looking into the CoNLL 2014 shared task report, it is not straightforward to discern whether the latter part of the claim does holds true, also as per Rei and Yannakoudakis' (2016) paper. The paper should support the claim by inclusion/replication of the related work.",
    "comments": "The POS tagging is left as more of an afterthought. The comparison to Plank et al. (2016) is at least partly unfair as they test across multiple languages in the Universal Dependencies realm, showing top-level performance across language families, which I for one believe to be far more relevant than WSJ benchmarking. How does the proposed system scale up/down to multiple languages, low-resource languages with limited training data, etc.? The paper leaves a lot to ask for in that dimension to further substantiate its claims.\nI like the idea of including language modeling as an auxiliary task. I like the architecture, and sections 1-4 in general. In my view, there is a big gap between those sections and the ones describing the experiments (5-8).\nI suggest that this nice idea should be further fleshed out before publication. \nThe rework should include at least a more fair treatment of related work, if not replication, and at least a reflection on multilinguality. The data and the systems are all there, as signs of the field's growing maturity. The paper should in my view partake in reflecting this maturity, and not step away from it. In faith that these improvements can be implemented before the publication deadline, I vote borderline.",
    "overall_score": "3",
    "confidence": "4"
  },
  {
    "review_id": "f99ca7aae6d6160a",
    "paper_id": "ACL_2017_276",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The article is well written; what was done is clear and straightforward. Given how simple the contribution is, the gains are substantial, at least in the error correction task.",
    "weaknesses": "The novelty is fairly limited (essentially, another permutation of tasks in multitask learning), and only one way of combining the tasks is explored. E.g., it would have been interesting to see if pre-training is significantly worse than joint training; one could initialize the weights from an existing RNN LM trained on unlabeled data; etc.",
    "comments": "I was hesitating between a 3 and a 4. While the experiments are quite reasonable and the combinations of tasks sometimes new, there's quite a bit of work on multitask learning in RNNs (much of it already cited), so it's hard to get excited about this work. I nevertheless recommend acceptance because the experimental results may be useful to others.\n- Post-rebuttal: I've read the rebuttal and it didn't change my opinion of the paper.",
    "overall_score": "4",
    "confidence": "4"
  }
]