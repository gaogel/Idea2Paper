[
  {
    "review_id": "fc280bbea3dcf036",
    "paper_id": "ARR_2022_363",
    "reviewer": null,
    "paper_summary": "This paper suggests there is a vanishing-gradient issue when using a global learning rate to train additive late-fusion multimodal models and presents a solution named Modality-Specific Learning Rate (MSLR). The authors explored 3 different variants of MSLR, showing that when optimal unimodal learning rates are close, “smooth” variant performs the best, and when optimal unimodal LRs are far apart, “keep” and “dynamic” variants are better. The authors also performed layer conductance analysis to validate the hypothesis about vanishing gradients, showing that MSLR leads to models with larger layer conductance values. ",
    "strengths": "1) This paper introduces a conceptually simple and intuitive idea of Modality-Specific Learning Rates, which empirically works well for late-fusion additive models; 2) This paper presents layer conductance analysis and shows that layer conductance disparity between modalities are mitigated when MSLR is applied; Overall, this paper proposes a simple yet empirically beneficial method to apply modality-specific learning rates and this improves late-fusion performance. ",
    "weaknesses": "1) It is unclear how one should do the smoothing of LR in the “Smooth” variant. The authors proposed some values in the reasonable range but without any discussion on how they are decided. It would be great if there is a methodical approach or a description of how this is done. \n2) Layer conductance is not a good way to measure vanishing gradients. It is a measure of neuron importance for a given input. While the result of conductance analysis is in the form of “gradients”, it is a different concept from the gradients the model receives during training. Hence, I would argue that the layer conductance analysis is actually not measuring what the authors aim to measure and the conclusions should be revised. \n3) MSLR requires knowing the unimodal optimal learning rates in advance - which means we have to spend resources to tune unimodal models and discard them in the end. It would be interesting to see how this compares to directly tuning a late-fusion model with separate LR for each modality, especially with a similar or less compute budget. ",
    "comments": "1) Relating to weakness 2), the authors need to properly measure gradients during training and validate their hypothesis about vanishing gradients, which would be easy to measure with norm of gradients. If the authors do not have the resources to conduct this analysis in time, I suggest the authors to revise their claims on vanishing gradients as layer conductance does not measure that. \n2) I suggest the authors to add a brief discussion about how to pick learning rates in the “Smooth” variant; 3) Typo - line 440, “...from the all the joint…” -> “...from all the joint…” 4) Typo - line 561, “while MSLR while achieves…” -> “while MSLR achieves…” ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]