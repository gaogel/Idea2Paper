[
  {
    "review_id": "217a8d9fbdc29525",
    "paper_id": "ARR_2022_306",
    "reviewer": "Di Jin",
    "paper_summary": "This work has created a benchmark dataset for multi-task learning for biomedical datasets. Based on this new benchmark dataset, this work has proposed instruction learning based multi-task learning, which has shown to outperform single-task learning as well as vallina multi-task learning. ",
    "strengths": "1. This work has newly aggregated more than 20 biomedical datasets in 9 categories into a new multi-task paradigm and formalize them into a text to text format so that we can build one unified model for all different tasks. \n2. This work has proposed using manually created instructions for multi-task learning so that the model can be instructed to perform each task without confusion. And this method has been shown to outperform a lot the vanilla multi-task learning and also outperform single-task learning in some cases. ",
    "weaknesses": "1. In the proposed method, the BI would be concatenated with instances as the input to the BART model, and in the BI, examples are provided. Actually these examples are extracted from those instances, then why should we still have examples in BI? How about just having those instructions in the BI? \n2. One important baseline is missing: in those methods proposed for DecaNLP and UnifiedQA, etc., other types of tokens or phrases are used to indicate which task/dataset each input instance belongs to, which is very important to let the model know what the input instance it is. However, in the baseline of vanilla multi-task learning (V-BB), no such kinds of special tokens are used at all, which forms a very unfair baseline to be compared with. The model are fed by so many instances from various kinds of tasks without any differentiation, which for sure would lead to deteriorate performance. For this reason, the effectiveness or the necessity of BI is questionable. \n3. More deep analysis over the impacts of different kinds of designs of the BI is needed, since such designs can vary a lot among different designers or writers. If so, the performance would be very unstable due to the variance of BI, which makes this type of method not applicable to real-world problems. \n4. Only Rouge-L is used for evaluation, which makes the evaluation not that reliable. Especially for some classification tasks, Rouge-L is not sensitive enough. ",
    "comments": "1. In lines 382-384, it is mentioned that \"We have discarded long samples (>1024 token length) from validation and testing data as well.\". I think it is not appropriate to throw any examples from the test set. ",
    "overall_score": "2.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]