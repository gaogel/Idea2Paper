[
  {
    "review_id": "747249d6df576d91",
    "paper_id": "ARR_2022_74",
    "reviewer": "Wenjuan Han",
    "paper_summary": "Some prior art focuses on detecting adversarial examples instead of adversarial training to defense the adversarial examples. This work pay attention to this detection topic and propose a new approach by fitting a parametric density estimation model to the features obtained from a classification model. They also release a benchmark for adversarial example detection on 4  attack methods across 4 models and 3 datasets. My concern is mainly on the model and task scales and the  comparison of the baseline model. ",
    "strengths": "Since most current work study the defense approach by adversarial training, detection approach in advance of the victim model is worth study. Moreover, this work provide a benchmark to take the first step. ",
    "weaknesses": "My concern is mainly on the model and task scales and the comparison of the baseline model.  They release a benchmark for adversarial example detection on 4  attack methods across 4 models and 3 datasets. Covering more tasks may be better.\nIn my view, detection is also a dense. Comparison with the adversarial training is necessary. ",
    "comments": "Comparison with the adversarial training is necessary. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "58ef2606ffcc1a9d",
    "paper_id": "ARR_2022_74",
    "reviewer": null,
    "paper_summary": "The paper explores the task of detecting of adversarial examples in the space of NLP problems, as opposed to the more common types of defence against them (e.g. adversarial training).  Models for this task require large sets of adversarial examples, and the paper notes as a contribution that it provides a dataset of such examples from “four popular attack methods on three datasets and four NLP models to encourage further research in this field”.  The paper also proposes a model for the task, which it compares against a range of baseline methods. ",
    "strengths": "- This is an important task that has been underexplored in the NLP space.  It has been investigated more in the image processing space, where a distinct set of techniques has been proposed, argued for, and contrasted with “pro-active” methods like adversarial training.\n- It’s a good, effective proposed detection method.  It’s described as a “competitive baseline”; that feels a bit like underselling it, as there’s already an existing baseline (FGWS) in addition to the common-sense PPL, and the proposed method is well motivated, well investigated (e.g. in the analysis of error bounds in App A.2) and performs well with respect to these existing baselines.\n- There are a lot of detailed method definitions and experimental results provided in the main paper and in the supplementary material, including ablation studies etc, helpfully pre-empting a lot of potential questions.  (But not all: see below.) ",
    "weaknesses": "- This isn’t a major weakness, but the paper could do a better job at the start of characterizing what the innovation of the approach to the task is.  I’d suggest the authorsnote up front in Sec 1 the FGWS method used as a comparator as an existing detection method: the current intro conveys the idea that existing methods are more limited than they are, and this is only rectified in Sec 5.  In particular, I’d characterise the differences with respect to FGWS – at a high level they have the similar idea of detecting low frequency samples (Fig 1 caption of present paper), but the approach is quite different.  Also, DISP as a key baseline for the FGWS work.\n- The pre-generation of adversarial examples doesn’t feel like a major contribution, although it’s true it will be help for others developing adversarial example detection techniques.\n- Again, not a major weakness, but the MLE baseline is completely unclear to me.  Line 363 says that MLE denotes the technique of Lee et al (2018) from image processing, but I don’t know which technique from that paper, and I don’t know how it’s adapted for the tasks in this paper.  In Lee et al (2018), their core technique is based on a Mahalanobis distance-based score, which doesn't sound like the brief description of MLE here; neither of their baselines sound like MLE either.  Shortly after that, a list of “two variants of our method” contains three items, including MLE.  I don’t know whether there was just some bad editing around this section, but I really don’t know what the MLE baseline method is supposed to be. ",
    "comments": "- line 69: adversrial - line 121: Should “a” begin a new sentence?  Is there something missing?\n- line 456-458: Maybe the reason FGWS works best on PWWS is that they’re closely aligned: both use word-based notions of weighted frequency-related characteristics.  It would be interesting to know if FGWS still works best against a version of PWWS that doesn’t align quite so closely.\n- Sec 5 on related work is reasonable, although could include the most recent work from the image processing space, e.g. [CSG20].\n[CSG20] Gilad Cohen, Guillermo Sapiro, Raja Giryes (2020). Detecting Adversarial Samples Using Influence Functions and Nearest Neighbors. Proc. CVPR. ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "543343cfc7a6dae0",
    "paper_id": "ARR_2022_74",
    "reviewer": null,
    "paper_summary": "The authors released a dataset of adversarial examples generated with four NLP models for three text classification datasets under four attack algorithms. The dataset was created to evaluate how well a model can discriminate adversarial examples from clean ones. A baseline for detecting adversarial examples also has been proposed in which the authors assume that the distributions of the clean example’s features follow a multivariate Gaussian distribution and apply kPCA to reduce the feature dimensionality and MCD to remove the outliers in the training set. However, it is not very useful to statically detect the adversarial examples generated by a particular attack algorithm because, in the real situation, an adversary attempts to find the weakness of a victim model by continually querying the victim through a trial-and-error strategy, and such dynamically generated adversarial examples are hard to be detected especially when the attack algorithm used by the adversary is unknown in advance. To me, it seems very hard to model natural texts via a multivariate Gaussian distribution due to the discrete nature of texts since the number of examples in a dataset is not sufficient for such modeling. ",
    "strengths": "This paper is generally well written and easy to follow. \nThe proposed baseline based on density estimation achieved the highest AUC on 21 out of dataset-attack-model combinations. ",
    "weaknesses": "(1) It is not very useful to detect the adversarial examples generated in advance by a particular attack algorithm because an adversary attempts to find the weakness of a victim model by continually querying the victim through a trial-and-error strategy, and such dynamically generated adversarial examples are hard to be detected especially when the attack algorithm used by the adversary is unknown. \n(2) It seems very hard to model natural texts via a multivariate Gaussian distribution due to the discrete nature of texts since the number of examples in a dataset is normally not sufficient for such modeling. \n(3) It is unrealistic to assume that a defender knows which attack algorithm is used unless the authors show that the adversarial examples generated by different attack algorithms share some common features that can be leveraged to detect them. \n(4) The method used to create the dataset seems very simple. \n(5) In the experimentation, the authors mentioned that \"… our method is relatively susceptible to PWWS\". But the authors did not give any reasonable explanation. ",
    "comments": "The authors would better evaluate the performance of the defense method combined with the proposed detection model compared to existing defense methods.\nSome typos: (1) counter measure --> countermeasure (2) relatively little efforts --> relatively few efforts (3) be harmful for --> be harmful to (4) detecting adversrial misspellings --> detecting adversarial misspellings (5) require training nor validation --> require training or validation ",
    "overall_score": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]