[
  {
    "review_id": "454ac9163c957dcc",
    "paper_id": "ARR_2022_110",
    "reviewer": null,
    "paper_summary": "This paper is about the task of vision-and-language navigation (VLN), with the aim of solving two challenges: 1. utilizing multilingual instructions for improved instruction-path grounding 2. navigating through unseen environments.\nTo do so, an agent is trained to learn shared and visually-aligned cross lingual representations (English, Hindi, Telugu), and an environment agnostic visual representation.  Two major claims are: 1. improve SOTA performance when generalizing to unseen environments (Room-across-room dataset) 2. learned representations can be successfully transferred to other datasets. ",
    "strengths": "1. The work takes the VLN task into a very interesting and practical dimension of understanding multilingual instructions; this is especially intriguing for Indic languages (such as Hindi and Telugu) which colloquially often employ code-switching between different Indic languages or between Indic languages and English (for example the word English word \"sink\" is used in the Hindi instruction in Figure 1, and a transliteration of \"wash basin\" has been used in Telugu). An agent that understands such phenomena could be quite useful in such situations. \n2. The work is well motivated -- different languages may contain different forms of instructions and learning jointly could lead to more robust representations. \n3. Overfitting is a significant challenge in reinforcement learning methods in navigation.  The proposed solution addresses that issue by means of contrastive learning between semantically aligned paths. \n4. Experimental settings seem to be sound, and benchmarking is exhaustive. \n5. Gains w.r.t. baselines are non-trivial on multiple evaluation metrics. ",
    "weaknesses": "1. My biggest concern is that the analysis doesn't provide insights on WHICH language benefits WHICH other language in a multilingual setup. The only comparison provided is mono-vs-tri-lingual, but we should also compare vs bi-lingual (For example -- comparing En+Hi, En+Te, Hi+Te vs En+Hi+Te).  This analysis may point to interesting observations on which language combination is more effective. I understand that the authors want to show us that multi-lingual representations improve performance, but understanding the relationship between different languages in this context of VLN is an equally important aspect -- the experimental setting of this paper is perfectly poised to provide this analysis, but the results are tucked away in the Appendix -- this should be moved to the main paper. \n2. [ presentation of results] Table 2 should also include the best result from Shen et al. 2021 (one of the baselines), so that we can compare the finegrained results on each language and metric. \n3. There seems to be some discrepancy between the scores for RxR in Table 1 and the scores reported by Shen et al for RxR (see Table 4 in Shen et al.) -- could you elaborate why? \n4. Sec 6.3 results are provided inline, but not as a figure/table.  It is difficult to understand the improvements easily. I would strongly recommend making this a table, and also providing SOTA methods for R2R and CVDN in that table (not just the baselines). This will tell us how far away the proposed transferred model fares vs fully supervised models on the respective datasets. \n5. Sec 6.3: The definition of \"transfer\" is unclear.  Does it mean that only the navigation step is trained on R2R and CVDN while the representation step is used from RxR? Or are both steps retrained (in that case it isn't a transfer).  What about zero-shot transfer from RxR to R2R? How does the CLEAR approach fare on zero-shot? \n6. Sec N of the appendix includes another baseline (SimCSE, Gao et al) -- but this is missing from the tables in the main paper. Why? \n7. Are LSTMs a better choice for the navigation step? Would a transformer/attention mechanism be better suited for learning longer sequences? This architectural choice should be justified. \n8. In Related Work (Sec 2), other datasets/methods for V&L with multilinguality are mentioned (VQA, captioning, retrieval...) -- is your method inspired by/related to any of these methods? If not, how is it different? ",
    "comments": "1. Sec 6.3 is titled \"Generalization to other V&L tasks\" -- this is misleading since both eval datasets are also VLN datasets. Other tasks has the connotation of other V&L tasks such as VQA, captioning, ...  2. There seems to be a related cross-lingual VLN dataset (English-Chinese) based on R2R -- https://arxiv.org/abs/1910.11301 .  You might want to test CLEAR on this benchmark if possible -- I'm not sure how practical it is, so I've not included this under \"Weaknesses\".  At the very least, it would be prudent to include this in related work. \n3. Overall, I would encourage the authors to re-think which analysis should go into the main paper and which in the appendix. In my opinion, every study that directly contributed to the 2 claims from the abstract, should be moved to the main paper.  The main contribution of this paper seems to be the multi-lingual training -- as such ablation studies about this part of the algorithm are more important that the ablation study about visual features.  (Table 9 for example) This paper makes a step in a very interesting direction -- however, I would like to see (1) re-organization of the analysis (between main and appendix), (2) focused and precise analysis of multi-lingual training, and (3) more details and exhaustive experiments in the transfer learning setup. My current score of 3.5 reflects these weaknesses + other unclear details that I've mentioned in \"Weaknesses\" above.   In case the paper doesn't go through to your ideal *ACL venue, please address these issues before resubmitting and I'll be happy to continue as a reviewer of this paper. ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]