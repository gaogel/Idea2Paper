[
  {
    "review_id": "324a61be0ca93af3",
    "paper_id": "ARR_2022_219",
    "reviewer": null,
    "paper_summary": "The paper works on creating sentence embeddings targeting the semantic similarity (STS) task. It identifies a core issue in two previous approaches. The discrete augmentation approaches (Wu et al., 2020; Meng et al., 2021) sometimes distorts semantic meaning of sentences, creating positive pairs that do not indeed have the same meaning. For this, the paper purposes a stronger baseline based on semantic role labeling (SRL). On the other hand, the more promising continuous augmentation approaches (Carlsson et al., 2021; Gao et al., 2021) only have positive training examples that are embeddings that come from identical sentences, which introduce cues about sentence length. The paper then verifies this issue by showing a performance drop of one such method SimCSE when such cue is manually removed.  The paper then proposes a remedy to said issue by mapping the representations of both positive and negative examples to a pseudo sentence with the same length and structure. The proposed approach proves effective by achieving new state-of-the-art on several STS benchmarks. Analyses also show that the pseudo tokens play an important role. ",
    "strengths": "- The paper identifies an intuitive issue of the current SOTA method (i.e., Gao et al., 2021) that uses embeddings from the same sentence, though with different drop-out masks, as positive examples. Such issue of relying on sentence length is exposed by manually removing the cue of length from the dataset. This strongly motivates the paper's subsequent modeling endeavors.  - The paper proposes a model that is seemingly sound to address the previous issue.\n- The paper shows through evaluation on a suite of benchmarks that the proposed method is superior. There are also adequate analysis showing that the core idea of the method, pseudo tokens, does play an important role. ",
    "weaknesses": "- The paper hypothesizes that SimCSE suffers from the cue of sentence length and syntax. However, the experiments only targets sentence length but not syntax.  - The writing of this paper can benefit by some work (see more below). Specifically, I find Section 3 difficult to understand as someone who does not directly work on this task. Specifically, a good mount of terminologies are introduced without explanations. I suggest a good effort of rewriting this section to be easier understandable by general NLP researchers.\n- Though a universal issue in related papers and should not be blamed on the authors, why only consider BERT-base? It is known that other models such as BERT-large, RoBERTa, DeBERTa, etc. could produce better embeddings, and that the observations in these works do not necessarily hold in those larger and better models.  - The introduction of the SRL-based discrete augmentation approach (line 434 onwards) is unclear and cannot be possibly understood by readers without any experience in SRL. I suggest at least discussing the following:   - Intuitively why relying on semantic roles is better than work like CLEAR   - What SRL model you use   - What the sequence \"[ARG0, PRED, ARGM − NEG, ARG1]\" mean, and what these PropBank labels mean   - What is your reason of using this sequence as opposed to alternatives ",
    "comments": "- (Line 3-6, and similarly in the Intro) The claim is a finding of the paper, so best prefix the sentence with \"We find that\". Or, if it has been discussed elsewhere, provide citations.  - (7): semantics-aware or semantically-aware - (9-10): explore -> exploit - (42): works on -> works by - Figure 1 caption: embeddings o the : typo - Figure 1 caption: \"In a realistic scenario, negative examples have the same length and structure, while positive examples act in the opposite way.\" I don't think this is true. Positive or negative examples should have similar distribution of length and structure, so that they don't become a cue during inference.  - (99): the first mention of \"momentum-encoder\" in the paper body should immediately come with citation or explanation.  - (136): abbreviations like \"MoCo\" should not appear in the section header, since a reader might not know what it means.  - (153): what is a \"key\"?\n- (180): same -> the same - (186-198): I feel that this is a better paragraph describing existing issue and motivation than (63-76). I would suggest moving it to the Intro, and just briefly re-mention the issue here in Related Work.  - (245-246): could be -> should be - (248): a -> another - (252-55): Isn't it obvious that \"positive examples in SimCSE have the same length\", since SimCSE enocdes the same sentence differently as positive examples? How does this need \"Through careful observation\"?\n- (288): \"textual similarity\" should be \"sentence length and structure\"? Because the models are predicting textual similarity, after all.\n- (300-301): I don't understand \"unchangeable syntax\" - (310-314): I don't understand \"query\", \"key and value\". What do they mean here? Same for \"core\", \"pseudo syntactic\".  - (376): It might worth mentioning SimCSE is the state-of-the-art method mention in the Abstract.  - (392): Remove \"In this subsection\" ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "0d9b3952c1795a76",
    "paper_id": "ARR_2022_219",
    "reviewer": null,
    "paper_summary": "This paper proposes to learn sentence embeddings with psudo tokens, aiming at addressing the sentence length bias issue of existing approaches that learn sentence embeddings based on contrastive loss. ",
    "strengths": "The idea of mapping the sentence onto a fixed number of pseudo tokens and then maps it back is very interesting. \nExperiments on STS task shows the proposed approach achieves good improvement over existing sentence embedding models. ",
    "weaknesses": "Regarding sentence length feature:   -- The paper argues that the approach in SimCSE relies on sentence length features.  I don't agree with the argument since from the modelling point of view, there is no explicit inductive bias such that the SimCSE would prefer sentences that share the same number of tokens.     -- The authors stated that \"Through careful observation we find that all positive examples have the same length ..\".   I am not surprised since positive example pair in SimCSE is constructed by applying different dropout masks twice to the feedforward layer of the Transformer models, while the input to the Transformer model remains untouched.       -- I think sentence length side-effect is an issue of engineering/implementation, and can also be addressed from engineering side.  For example, one could implement a sampling procedure where all examples in the same batch have similar number of tokens.   The paper argues that the SimCSE model also affected by \"syntactic structures\".  It would be great to have some examples or more descriptions on what does \"syntactic structures\" referring to.\nRegarding the proposed model:   -- If I understand correctly, Y_i in eq 1 is the sentence embedding induced by BERT model.  In such case, the attention in EQ1 would always output a weight matrix of shape [m, 1], i.e., a vector of all 1s.  Here m denotes the number of pseudo tokens. This is because there is only one key in the dictionary, i.e., the sentence embedding.  Thus all attention weights are put on that key.\nRegarding the experiments.  Although the proposed approach achieves good improvement, it is unclear whether the improvement is significantly better than the baseline systems. ",
    "comments": "Is the proposed improvement also additive? E.g., if all systems are ALSO fine-tuned on supervised data such as NLI, does the improvement over the baseline system still hold.\nThis paper created a subset by filtering based on length of the positive/negative example pairs. How big is this new subset? ",
    "overall_score": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "907bb75931042242",
    "paper_id": "ARR_2022_219",
    "reviewer": "Avi Caciularu",
    "paper_summary": "This work presents PT-BERT, an unsupervised method for training sentence embeddings. The main claim is that  current contrastive learning-based methods, which are based on either discrete augmentation or continuous augmentation, do not consider the impact of superficial features such as sentence length and syntax. Thus the authors propose a method, where they incorporate pseudo embedding and momentum updating to further help the contrastive learning.\nExperiments over multiple downstream tasks, mostly over semantic similarity benchmarks, demonstrate the effectiveness of the method, mostly compared to the recent SimCSE method. ",
    "strengths": "The idea of spotting the syntax and length “overfitting” of sentence embedding models is correct and in place. The problems of existing models that the authors presented are really crucial for better text embedding. It is novel and interesting and seems to be effective and it is really highly appreciated.\nThe authors apply their method over multiple state-of-the-art sentence encoding methods and show nice gains over many downstream tasks. They include also additional beneficial analyses at the end of the paper, such as alignment and uniformity measurements. ",
    "weaknesses": "The paper is written in a way that only readers that are familiar with MoCo can follow this work. For example, the authors write about a “dictionary”, and a “memory bank” without providing further explanations about how they use them and how they are composed. The paper should be more self-contained and is pretty hard to follow in its current form.\nIt is not clear to me how the pseudo sentences are actually generated. The authors write “During training, we 397 create a pseudo sentence {0, 1, 2, ..., 127} for every 398 input and map the original sentence to this pseudo sentence by attention”, but how exactly is a different pseudo embedding is created per sentence? According to Fig 2, all the pseudo sentences are the same, and I can’t see what is the motivation for that (In Fig 1 we can correctly see different sentences per instance). If there is a different pseudo sentence per sentence, then the mapping strategy is missing from the paper.\nOne discrete data augmentation ablation missing from the paper is the synonym/paraphrase replacement method (e.g., [1]) which matches the (correct) intuition of the authors, presented in Fig 1.\nA comment about the architecture: There is neither intuition nor motivation as to why a single same attention layer should be used for both stages of obtaining the sentence embedding (Eq. 1 and 3). Also, an ablation regarding the architecture is missing, e.g., a number of layers, using different sets of parameters in Eq 1 and 3, etc., which could have been at least added to the appendix in case there is no space left.\nRegarding the evaluations, an interesting missing evaluation is single-sentence classification tasks, such as sentiment analysis, which was carried out in similar papers (SimCSE). I encourage the authors to add such important analysis to their revised version of the paper. Also, there is no code available and hyperparameter choices are not depicted at all.\nLastly, the paper lacks some interesting/cherry-picked examples for sentences that can provide some insights regarding the method. A PCA/TSNE plot of the new embeddings compared to the previous embeddings can be informative to show the changes after post-processing.  [1] Jiao, Xiaoqi, et al. \"Tinybert: Distilling BERT for natural language understanding.\" arXiv preprint arXiv:1909.10351 (2019).‏ ",
    "comments": "- L96: There is an incorrect reference for the attention mechanism.\n- L300: The presented sentence here is a series of numbers and not tokens. Also, it has a length of m+1 (please indicate whether the first token is the CLS). Moreover, in Fig. 2 the sentence starts with 1 and not with 0.\n- L304: “the parameter” -> “each parameter” - L305: “The size of this layer depends on the length of pseudo sentences”- This sentence is vague and not clear at all. How can a neural network depend on the length of the input?\n- L352: “obtained” -> “obtains” - L360: “is” -> “are” - L444: The SRL-guided data construction is not clear. Please provide an example.\n- L464-L465: These are the asymptotic formulas, which are obviously not used for creating Fig 3. Please change to the concrete formulas, or at least state that. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]