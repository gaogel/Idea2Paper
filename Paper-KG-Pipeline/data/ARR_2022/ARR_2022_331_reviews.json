[
  {
    "review_id": "6c26dceacb73b871",
    "paper_id": "ARR_2022_331",
    "reviewer": "Siyao Peng",
    "paper_summary": "This paper compiles and annotates a multi-reference Chinese Grammatical Error Correction (MuCGEC) evaluation dataset from three different Chinese Second Language sources. Multiple annotators and reviewers are involved in rewriting ungrammatical sentences and creating the most multi-referenced GEC dataset for Chinese. \nThis work ensembles SOTA Seq2Edit and Seq2Seq models with Structured BERT and beats previous SOTAs on an earlier benchmark dataset NLPCC18. Moreover, char-based evaluation metrics are suggested, and ablation experiments are implemented on the number of references, error types, text sources, etc. ",
    "strengths": "The strict and (hopefully) accessible annotation guidelines and multiple annotators and reviewers ensure annotation quality. \nTheir ensemble model compensates for the drawbacks of Seq2Edit and Seq2Seq models on Recall. \nSince the previous SOTA models do not provide code access, this paper's data and release contribute largely to GEC for Chinese. ",
    "weaknesses": "Here are a few suggestions: 1) How was the quality of reviewers' final golden decisions assessed? For example, the annotators' agreements in Figure 2 look decent, but they are compared against reviewers' final golden answers. Would it be possible to compare different reviewers' decisions on approving the same set of annotated corrections? And see how much the reviewers agree?\n2) Comparing the rewritten corrections to NLPCC18 and CGED's original error-coded correction would be an interesting analysis to show how much they overlap in reference corrections and whether MuCGEC provides further diversity.   3) Briefly discussing what kinds of data augmentation techniques  ( â™¥ )  other works use in Table 4 would be great (and maybe why not used in this paper). ",
    "comments": "Line 17: add a space \"datasets. We\" --> \"datasets. We\" ",
    "overall_score": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "3014c7d4fe2505a0",
    "paper_id": "ARR_2022_331",
    "reviewer": null,
    "paper_summary": "This paper introduces a new corpus of error-annotated sentences for grammatical error correction for Chinese. The dataset is composed of a subset of sentences from three different sources which were corrected by humans. The dataset contains multiple correction references for each erroneous sentence and is split into development and test in an attempt to standardise evaluation. The authors also make a case that character-based evaluation is better suited than word-based evaluation for Chinese GEC and carry out experiments with different systems on their new dataset using this approach.  This is the second revision of the paper that I have been asked to review. I was pleasantly surprised to see that the authors took my previous review into account and addressed as many of my points as possible in this new version. The paper has been thoroughly revised and is now much clearer and comprehensive, adding further clarifications and new experiments which include ensemble models. The paper is now more solid and worthy of publication. ",
    "strengths": "- Contribution of a human-annotated dataset for grammatical error correction for Chinese.\n- Multi-reference and multi-source (multi-domain) dataset.\n- Insights about better evaluation practices.\n- A very good set of benchmark results on the new dataset.\n- Comparison with previous work.\n- Some error analysis. ",
    "weaknesses": "- While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker. ",
    "comments": "1) Line 29: \"To support the GEC study...\". Your study or GEC in general? Maybe you mean \"To support GEC research/development/solutions\"?\n2) Line 53: \"Because, obviously, there are usually multiple acceptable references with close meanings for an incorrect sentence, as illustrated by the example in Table 1.\" This is not a well-formed sentence. Rewrite or attach to the previous one.\n3) Line 59: Choose either \"the model will be unfairly *penalised*\" or \"*performance* will be unfairly underestimated\".\n4) Line 83: \"... for detailed illustration\". ???\n5) Line 189: \"To facilitate illustration, our guidelines adopt a two-tier hierarchical error taxonomy...\" You said earlier that you adopted the \"direct re-rewriting\" approach so why does your annotation guidelines provide a taxonomy or errors? Is it just to make sure that all ocurrences of the same types of errors are handled equally by all annotators? Weren't they free to correct the sentences in any way they wanted as you stated in lines 178-180?\n6) Line 264: \"We attribute this to our strict control of the over-correction phenomenon.\" What do you mean exactly? The original annotation considered some sentences to be erroneous while your guidelines did not?\n7) Line 310: \"Since it is usually ...\" This is not a well-formed sentence. Rewrite or attach to the previous one.\n8) Line 399: \"... and only use the erroneous part for training\" Do you mean you discard correct sentences? As it stands, it sounds as if you only kept the incorrect sentences without their corrections. You might want to make this clearer.\n9) \"... which does not need error-coded annotation\". This is not exactly so. ERRANT computes P, R and F from M2 files containing span-level annotations. For English, it is able to automatically generate these annotations from parallel text using an alignment and edit extraction algorithm. In the case of Chinese, you did this yourself. So while it is not necessary to manually annotate the error spans, you do need to extract them somehow before ERRANT can compute the measures.\n10) Table 5: \"For calculating the human performance, each submitted result is considered as a sample if an annotator submits multiple results.\" I am afraid this does not clearly explain how human performance was computed. Each annotator against the rest? Averaged across all of them? How are multiple corrections from a single annotator handled?  If you compared each annotation to the rest but the systems were compared to all the annotations, then I believe human evaluation is an underestimation. This is still not clear.\n11) Line 514: \"The word-order errors can be identified by heuristic rules following Hinson et al. (2020).\" Did you classify the errors in the M2 files before feeding them into ERRANT?\n12) Line 544: \"... we remove all extra references if a sentence has more than 2 gold-standard references\". Do you remove them randomly or sequentially?\n13) TYPOS/ERRORS: \"them\" -> \"this approach\"? ( line 72), \"both formal/informal\" -> \"both formal and informal\" (line 81), \"supplement\" -> \"supply\"? ( lines 89, 867), \"from total\" -> \"from *the* total\" (line 127), \"Finally, we have obtained 7,137 sentences\" -> \"In the end, we obtained 7,137 sentences\" (line 138), \"suffers from\" -> \"poses\" (line 155), \"illustration\" -> \"annotation\"? ( line 189), \"Golden\" -> \"Gold\" (lines 212, 220, 221, 317), \"sentence numbers\" -> \"number of sentences\" (Table 3 caption), \"numbers (proportion)\" -> \"number (proportion)\" (Table 3 caption), \"averaged character numbers\" -> \"average number of characters\" (Table 3 caption), \"averaged edit numbers\" -> \"average number of edits\" (Table 3 caption), \"averaged reference numbers\" -> \"average number of references\" (Table 3 caption), \"in the parenthesis of the...\" -> \"in parentheses in the...\" (Table 3 caption), \"previous\" -> \"original\"? ( line 262), \"use\" (delete, line 270), \"in *the* re-annotated\" (line 271), \"twice of that\" -> \"twice that\" (line 273), \"edit number\" -> \"number of edits\" (line 281), \"the sentence length\" -> \"sentence length\" (line 282), \"numbers\" -> \"number\" (lines 283, 297), \"numbers\" -> \"the number\" (line 295), \"Same\" -> \"Identical\" (line 298), \"calculated\" -> \"counted\" (line 299), \"the different\" -> \"different\" (Figure 1 caption), \"reference number\" -> \"number of references\" (line 305), \"for\" -> \"to\" (line 307), \"the descending\" -> \"descending\" (line 326), \"sentence numbers\" -> \"number of sentences\" (line 327), \"It\" -> \"This\" (line 331), \"annotate\" -> \"annotated\" (Figure 2 caption), \"limitation\" -> \"limitations\" (line 343), \"SOTA\" -> \"state-of-the-art (SOTA)\" (line 353), \"these\" -> \"this\" (line 369), \"where\" -> \"on which\" (line 393), \"hugging face\" -> \"Hugging Face\" (431), \"these\" -> \"this\" (line 464), \"The\" -> \"A\" (line 466), \"reference number\" -> \"the number of references\" (Figure 3 caption), \"start\" -> \"have started\" (line 571), \"will be\" -> \"are\" (line 863), \"false\" -> \"incorrect\"? ( line 865). ",
    "overall_score": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",
    "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work."
  },
  {
    "review_id": "cabc87768a08817d",
    "paper_id": "ARR_2022_331",
    "reviewer": null,
    "paper_summary": "The paper releases a new dataset for Chinese Grammatical Error Correction which differs from the previous ones by its multi-source and multi-reference nature. The authors describe the annotation process and test several models of different type (Seq2Seq and Seq2Edits) on their data. ",
    "strengths": "-- a new dataset is released -- it is of larger size, higher quality and has more references than the previous ones -- the authors convincingly describe why the direct rewriting annotation paradigm that they follow is better than the error-coded one used for the previous Chinese GEC dataset collection. ",
    "weaknesses": "-- the contribution is probably not significant outside GEC community ",
    "comments": "l.17 -- no space after period l.66 \"it will be taxing\" -> \"it will be difficult\" l.260-265 \"most of the sentences are considered to contain grammatical errors in the previous annotation, but a considerable part of them are not corrected in our annotation\" You can make such comparison only on the same data, thus I compare the number of erroneous sentences for NLPCC18(Orig) and MuCGEC(NLPCC18), but the number of correct sentences has increased only by about 80. May be it is more convincing to provide the average number of errors per sentence?\nl.266-267 -> NLPCC18 has the shortest sentences, whereas CGED sentences are much longer.\nl.267-270 This is possibly because, since HSK is an official Chinese proficiency test, candidates tend to use long sentences to show their ability in Chinese use -> ...because of the fact that HSK is ..., so candidates l.471 -472 I am sure whether I understood the terminology: do you call redundant errors the cases when the text contains more characters than its correction. May be, it is better to call them insertion and deletion errors? ",
    "overall_score": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]