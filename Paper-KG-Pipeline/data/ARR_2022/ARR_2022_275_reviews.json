[
  {
    "review_id": "a4fd8920152e81ea",
    "paper_id": "ARR_2022_275",
    "reviewer": null,
    "paper_summary": "This paper proposes a new pinyin input method with Chinese GPT. \nTheir method tries to output Chinese characters given perfect pinyin, which consists of both initials and finals, or abbreviated pinyin, which only uses characters' initials. \nAbbreviated pinyin input is user-friendly since the number of typing characters is fewer. Still, it is a more complex problem since the model has to search for the correct output from more candidates than perfect pinyin.\nTheir method uses Chinese GPT and adapts it by concatenating both contexts of Chinese characters and pinyin (PinyinGPT-Concat) or by using the embeddings of characters and pinyin (PinyinGPT-Embed). \nThey also construct a dataset called WD Dataset for evaluating their input methods.\nTheir experiments show that the Chinese GPT given perfect pinyin can output the correct characters even if the model is not fine-tuned for the task and surpasses previous baselines. \nHowever, their analysis shows that the vanilla GPT does not work well with the abbreviated pinyin. \nTheir proposed PinyinGPT-Concat method surpasses the vanilla GPT baseline on abbreviated pinyin settings. ",
    "strengths": "1. They adapt Chinese GPT for the pinyin input method and the results show their method works well compared to the baselines.\n2. They created a new pinyin IME evaluation dataset called WD Dataset and it would be useful for further researches. ",
    "weaknesses": "1. Their experiments compared methods trained with different datasets and different model parameters. It is still difficult to distinguish this model architecture is nice, or the larger dataset/parameters are good.\n2. They only compare the previous method in the prefect pinyin settings though several works on abbreviated pinyin IMEs are mentioned in the related work section. \nIt would be better to add a comparison of these.\n3. I could not fully understand why people want higher pinyin IME accuracy with full use of V100 GPU. It might be possible to run this on the standard laptops in the future, though. ",
    "comments": "l.299, please add the explanation of the abbreviation of \"precision at top-K\"-> \"precision at top-k (P@K)\". ",
    "overall_score": "2.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "31ffabcd4514d0e9",
    "paper_id": "ARR_2022_275",
    "reviewer": null,
    "paper_summary": "This paper explores adapting the Chinese GPT for pinyin input.  Their task settings are that the input of the model includes a sequence of Chinese characters as the context and a sequence of pinyin (perfect pinyin or abbreviated pinyin), and the output is a sequence of Chinese characters. The number and the pronunciation of output characters should be consistent with the input pinyin, and the meaning of output should fit the input context.\nTo adapt the Chinese GPT for pinyin input, they propose two methods. The first one is concatenating pinyin input to the context of  Chinese characters. The second one is adding a pinyin embedding layer at the bottom of GPT. Since the model needs to select the best one from characters pronounced with the same pinyin in the inference stage, they propose pinyin-constrained training.\nFor datasets, they use the common benchmark PD dataset for training and evaluation. They also propose a new WD dataset that contains 15 domains. Their results show that their approach improves the performance of abbreviated pinyin across all domains. They also conducted a series of additional experiments to understand the importance of pinyin context and pinyin constrained training. ",
    "strengths": "This paper is well-written and well-organized; the readers could easily understand their methods and statements. The additional experiments are sufficient, which help the readers understand their method deeply. Moreover, the new evaluating dataset proposed in this paper is also very useful; it enables future work to evaluate their models across several domains. ",
    "weaknesses": "The main weakness is that the novelty of the proposed method is somewhat limited. The pinyin-enhanced pre-trained models and processing the abbreviated pinyin have existed in previous works, and this paper only adapted them on a new pre-trained model. ",
    "comments": "I am not sure why the paper used a probability of 50% to sample a target sequence with less than five words (in line 318). Is that because people tend to input short sequences in real-world situations? If so, it would be better to explain the reasons. ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "debf5134addd4de0",
    "paper_id": "ARR_2022_275",
    "reviewer": null,
    "paper_summary": "This paper addresses a Chinese pinyin input method. Pinyin represents a pronunciation of a corresponding Chinese character. The Chinese pinyin input method converts an input pinyin sequence into a corresponding Chinese characters. This paper assumes two configurations for inputs. One receives all pinyin representations (perfect pinyin) and the other receives the initial characters of pinyin (abbreviated pinyin). \nThis paper indicates that the GPT model trained on Chinese corpora achieved good performance on the perfect pinyin configuration. In addition, this paper proposes two input strategies of pinyin, and the one is useful in the abbreviation pinyin configuration when the authors fine-tuned GPT parameters with the input strategy. ",
    "strengths": "This paper reported state-of-the-art performance on the Chinese pinyin input method. \nAlthough the technical novelty is limited in my understanding, this paper might have a potential of good paper as a demo paper. ",
    "weaknesses": "In my understanding, this paper doesn't contain any novel findings essentially. \nThis paper reports that a neural language model trained on a large amount of data can achieve better performance when we fine-tune the model on a target task. Recent studies [Peters et al., 18, Devlin et al., 19, Lewis et al., 20] have demonstrated that this pre-trained and fine-tuning strategy is effective in various tasks.\nThis paper applies the method to incorporate pinyin to the standard language modeling task, i.e., next word prediction task. This paper proposed two strategies. One is attaching the pinyin sequence to the input sequence. The other is combining the embeddings of pinyin with the input embeddings of characters. The former is a standard (straightforward) way, and the latter uses the idea in previous studies. For example, in abstractive summarization tasks, previous studies proposed the method which combines additional information such as length embeddings to embeddings of input tokens [Kikuchi et al., 16, Takase et al., 19].\nIn other words, this paper applies the exiting strategies to the Chinese pinyin input method but doesn't contain technical novelty.\nMoreover, this paper addresses the abbreviation pinyin configuration but I doubt this configuration is well justified. \nThis paper should describe whether human can solve this task because the abbreviation pinyin configuration seems too difficult. In addition, this paper should describe the importance of this configuration such as \"how frequent this configuration occurs\".\nPeters et al., 18: Deep Contextualized Word Representations.\nDevlin et al., 19: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.\nLewis et al., 20: BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension.\nKikuchi et al., 16: Controlling Output Length in Neural Encoder-Decoders.\nTakase et al., 19: Positional Encoding to Control Output Sequence Length. ",
    "comments": "The equation (2) is inconsistent with the task definition described in Section 2. \nSection 2 explains that the pinyin sequence starts from n+1 but the equation (2) uses pinyin sequence from 1 to n. To solve this inconsistency, the authors should prepare another variable such as N instead of using n+k in the equation (2). ",
    "overall_score": "1.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]