[
  {
    "review_id": "1576b8dd8d9c8f76",
    "paper_id": "ARR_2022_143",
    "reviewer": "Xianchao Wu",
    "paper_summary": "This paper proposes a cluster based compact network and a cluster-based pruning strategy for k-nearest-neighbor machine translation for domain adaptation. The proposed methods are able to achieve low latency and keep the BLEU scores of translation accuracies. Experiments on a couple of datasets show the good performance of the methods described in this paper. ",
    "strengths": "Strong: 1. \tNovel cluster-based compact network and pruning strategies for non-parameter domain adaptation in machine translation; 2. \tBetter results of balancing decoding latency and accuracy. ",
    "weaknesses": "Weak: 1. \tMore examples are preferred to understand the motivations, the novel part of the proposed method and the baselines (see “detailed questions and comments”); 2. \tSome higher level comparisons, such as between parametric and non- parametric solutions are preferred. Currently, most baselines are in the same technical line of kNN-MT which is too narrow to reflect the strength of the proposed algorithms/networks. ",
    "comments": "Detailed questions and comments: 1. \tTable 1, what are the hardware used? Model sizes? For “speed comparison”. \n2. \tFigure 1, what are the labels for horizontal and vertical axis? \n3. \tLines 088 to 089, hard to understand why it is “intuitively” since the figure 1 is a 2D description of high-dimension features/distributions, do you have any detailed data/experiments to support this “intuitively”? \n4. \tCan you give real-world examples and attach them to your Figure 2? \n5. \tFigure 3, can you give example real tokens, instead of “token A”, “token B”? it is a bit difficult to understand what are the “negative, positive, pivot” arrows in this figure. \n6. \tLines 170 to 171, “unreliable neighbors” any examples of “unreliable neighbors”? \n7. \tLine 458, is “0.18 BLEU” a significant improvement? Do not understand if it is “impressive result” or not. \n8. \tTable 6 is a bit difficult to understand. Can you first give references of using SP, LTP, HTP, and RP? Also why there are quite limit number of BLEU scores achieved by your “Ours method” higher than others? Can you also give speed/decoding comparison? Since based on this table, I am not sure why we shall rank your method to be higher than the other baselines. There is a big drop of from 46.94 to 46.03 of from “CKMT*” to “CKMT*+Ours”, any detailed analysis of this or any future work plan of this direction? \n9. \tTable 11, why “adaptive kNN-MT” output so many “wrong translations”? how there examples are selected? \n10. \tSection 2 “related work and background” is hard to understand. Intuitively, can you simply give a simple example of the difficulties of cross-domain translation (such as vocabulary difference, grammar difference and technical terms) and show that cluster based methods are helpful for this cross-domain translation. In addition, besides cluster based methods, can you also briefly summarize the major directions of dealing with “domain adaption for NMT”? if there is a comparison of among the major directions (not only other cluster-based methods), this paper will be ranked even higher (e.g., non-parameter solution vs. parameter solution for “domain adaption of MT”). ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "69bb60db767b1f3b",
    "paper_id": "ARR_2022_143",
    "reviewer": null,
    "paper_summary": "This paper propose a more efficient KNN-based MT approach to non-parametric models for machine translation. Instead of learning extra no. of parameters to fine-tune / adapt a model, KNN-based MT learns a smaller meta network without unfreezing the pre-trained base model. The proposed method uses clustering to reduce the features that has to be retrieved through NCE. Further pruning on the reduced dimension of the datastore before the KNN retrieval. ",
    "strengths": "1. It's really great to see the authors put effort into component analysis, e.g. notable mentions: 1a: Table 3, comparing the NCE compression to SVD and PCA (it's an arduous effort to run SVD and PCA on features on MT-tasks size datasets). \n1b. Section 4.4.2. comparing different pruning methods 2. [ Double edge point] It's an incremental improvement to K-NN based MT approach, little novelty but large engineering and execution effort, backed by good experimental design ",
    "weaknesses": "1. [ Double edge point] It's an incremental improvement to K-NN based MT approach, little novelty but large engineering and execution effort, backed by good experimental design. This weakness is a little nitpicking esp when I personally execution (replicable) beats idea (novelty); but if there's no code release is produced after the revision process, then this weakness stands given the next.\n2. Replicability of method is not clear, there's no indication that the code will be released. ",
    "comments": "- I might have missed the specific in the paper; hopefully, I get CKMT = \"Compact-network K-nearest-neighbor MT\" correct. If it is, it'll be good to have the the abbreviation explicit in its full form somewhere in the paper. Otherwise some clarification on the abbreviation would be good. Same for PCKMT = \"Pruned CKMT\", hope I get that right too. ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]