[
  {
    "review_id": "0943e5d2efbf7573",
    "paper_id": "ARR_2022_166",
    "reviewer": null,
    "paper_summary": "The paper presents a new corpus with anaphoric relations, annotating recipes for coreference and bridging. The utility of the corpus is demonstrated by training a ML classifier. ",
    "strengths": "The new corpus can benefit others studying anaphora beyond identify. The paper is well written. ",
    "weaknesses": "- Transfer learning does not look to bring significant improvements. Looking at the variance, the results with and without transfer learning overlap. ",
    "comments": "Can you please clarify/expand the evaluation methodology, e.g., given gold anaphors, you only evaluate the antecedents? At least you should state why the standard evaluation metrics fail in this case, e.g., they cannot score plurals, i.e., split-antecedent references Typo: line 513 'pretrained' ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "0db51440a48c9e4d",
    "paper_id": "ARR_2022_166",
    "reviewer": null,
    "paper_summary": "The main point of the paper is a corpus of baking recipes annotated for coreference and bridging relations (both 1-to-1 and many-to-1 transformations of ingredients) and experiments to learn coreference resolution on this genre by transfer from either just unsupervised pretraining (GloVe, ELMo) or including supervised training on an existing larger chemical corpus and subsequent transfer using a state-of-the-art model for span linking coreference. The authors show that both joint training of coreference and bridging and transfer learning from the larger annotated corpus help the performance of the model. ",
    "strengths": "- the authors present a new corpus with coreference and bridging annotation which provides an interesting dataset for exploring this task - the authors provide a nice survey of the work especially concerning bridging corpora and resolution - the authors present some experiments showing that transfer learning can be beneficial for learning bridging resolution ",
    "weaknesses": "- while the focus of the paper is on bridging resolution and the authors claim that transfer learning allows the model to incorporate procedural knowledge, this claim is not backed up by any kind of error analysis or qualitative examples that would show what the model learns exactly - the part that is novel wrt Feng et al 2021 is pretty much just the extension towards ingredient transformation and the transfer learning study (which is however less elaborate as the one in Xia&vanDurme) - while the models are reasonably state of the art (based on ELMo and in line with Lee et al 2018 and Feng et al 2021), newer research such as the Xia&vanDurme paper use more recent language models such as XLM-R as the base model, which may lead to better performance overall ",
    "comments": "Xia and van Durme has been published at EMNLP 2021 and can be cited as such.\nSection 4 413: \"For evaluation, we use precision, recall and F1\" - since MUC and B3 measures also define P/R/F1, it's good to mention here that it's P/R/F1 on individual (coreference or bridging) links. \nSection 6 555: \"Table 4 shows ...\" - the structure of Table 3 and Table 4 isn't very intuitive, since there is partial overlap in conditions and metrics but essentially it's one big collection of results (baseline, +joint, +transfer, +joint+transfer). It's not clear to me why the \"overall\" figure isn't computed based on the separate baseline classifiers ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]