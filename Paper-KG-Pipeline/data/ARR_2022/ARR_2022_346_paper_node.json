{
  "paper_id": "ARR_2022_346",
  "title": "How do we answer complex questions: Discourse structure of long form answers",
  "conference": "ARR",
  "domain": {
    "research_object": "该论文主要研究的是文本数据，尤其关注于长文本答案的篇章结构，用于回答复杂问题。",
    "core_technique": "论文涉及自然语言处理技术，可能包括文本结构分析、篇章结构建模、语义理解等方法，常用技术可能有Transformer或其他深度学习模型用于文本处理。",
    "application": "成果可应用于问答系统、对话系统、智能客服、知识检索等需要生成或理解长文本答案的实际场景。",
    "domains": [
      "自然语言处理",
      "问答系统",
      "文本生成"
    ]
  },
  "ideal": {
    "core_idea": "提出并分析长文本问答答案的句子角色结构，用于改进自动和人工评价方法。",
    "tech_stack": [
      "RoBERTa",
      "自动角色分类模型",
      "语篇结构分析",
      "数据注释"
    ],
    "input_type": "长文本问答数据，包括问题和多句答案（人工或机器生成）",
    "output_type": "每句答案的角色标签及答案整体语篇结构分析"
  },
  "skeleton": {
    "problem_framing": "论文首先指出短文本答案虽然能满足许多信息检索需求，但这种形式极大限制了可回答的问题类型和信息表达的丰富性。接着引入长文本答案的研究进展，强调其多句结构带来的表达灵活性和复杂性。通过对比短文本答案评估的简易性与长文本答案评估的挑战，突出当前评估方法的不足，进而自然引出对长文本答案结构和评估方法深入研究的必要性。整体采用了从实际痛点出发，结合学术gap的开篇策略。",
    "gap_pattern": "论文批评现有方法主要采用了‘现有方法在Y场景下失效’的逻辑。具体指出：1）短文本答案的评估方法（如span matching）不适用于长文本答案；2）自动化指标（如ROUGE）在长文本答案场景下无效且易被‘刷分’，并引用相关研究支持这一观点；3）即便是人工评测，由于长文本答案的复杂性，也难以获得可靠结果。通过这些批评，强调了现有方法在长文本答案评估中的局限性和失效场景。",
    "method_story": "方法部分采用了‘先整体后局部、从简单到复杂’的叙述策略。首先简要说明实验设置和使用的基础模型（RoBERTa），然后依次介绍不同输入方式的变体：仅用答案句、问题+答案句、答案上下文、问题+答案上下文。每种变体都明确给出输入格式，逐步增加输入信息的复杂度，帮助读者理解各模块的作用和对比关系。",
    "experiments_story": "实验部分采用了‘主实验+人类评测可靠性分析+指标设计与报告’的策略。首先回顾相关工作对自动评测指标的质疑，随后详细描述了人工A/B测试的设计、采样方式和评测流程，并报告了不同类型答案对比下的人类一致性结果。实验还包括对模型在摘要句选择上的加权精度、召回、F1等指标的计算方法说明。整体上，实验既有对主任务的系统性评测，也有对评测方法本身可靠性的深入分析。"
  },
  "tricks": [
    {
      "name": "问题设定升级",
      "type": "writing-level",
      "purpose": "突出研究意义和创新性，将任务从传统短文本问答提升到更具挑战性的长文本问答",
      "location": "introduction",
      "description": "通过指出短文本答案的局限性，引出长文本问答的复杂性和必要性，强调本研究面向更高阶的问题。"
    },
    {
      "name": "引用前沿工作",
      "type": "writing-level",
      "purpose": "增强说服力，表明本研究建立在最新进展之上并回应了领域内的挑战",
      "location": "introduction",
      "description": "多次引用近期相关文献（如Fan et al., 2019; Krishna et al., 2021），说明当前自动评价指标的不足。"
    },
    {
      "name": "数据集多样性与规模展示",
      "type": "experiment-level",
      "purpose": "证明实验的完备性和代表性，增强结论的可靠性",
      "location": "introduction",
      "description": "详细列举所用数据集（ELI5, NQ）、标注规模和类型，强调覆盖面和数据量。"
    },
    {
      "name": "人机对比分析",
      "type": "experiment-level",
      "purpose": "突出方法的有效性和现有模型的不足，强调研究的必要性",
      "location": "experiments",
      "description": "通过对比人类和机器生成答案的结构，揭示两者之间的显著差距。"
    },
    {
      "name": "自动与人工评价对比",
      "type": "experiment-level",
      "purpose": "展示现有自动评价指标的局限性，论证研究方向的合理性",
      "location": "experiments",
      "description": "通过实验表明ROUGE等自动指标在长文本问答评价中不可靠，强调人工评价的挑战。"
    },
    {
      "name": "细粒度与粗粒度标注结合",
      "type": "experiment-level",
      "purpose": "提升分析的深度和广度，满足不同层次的研究需求",
      "location": "introduction",
      "description": "同时提供细粒度和粗粒度的句子角色标注，丰富数据分析维度。"
    },
    {
      "name": "多输入变体实验",
      "type": "method-level",
      "purpose": "验证模型的鲁棒性和适用性，提升方法的说服力",
      "location": "method",
      "description": "设计多种输入方式（仅句子、句子+问题、上下文、上下文+问题）系统比较模型表现。"
    },
    {
      "name": "人类一致性检验",
      "type": "experiment-level",
      "purpose": "揭示评价任务的难度，论证研究方法的必要性",
      "location": "experiments",
      "description": "通过A/B测试和Fleiss Kappa统计，量化人工评价的一致性，发现即使人类也难以一致评价长文本答案。"
    },
    {
      "name": "理论与实证结合",
      "type": "writing-level",
      "purpose": "提升可解释性和理论深度，帮助读者理解方法原理",
      "location": "introduction",
      "description": "提出基于语言学的句子功能角色分析框架，并结合实际标注和实验进行验证。"
    },
    {
      "name": "任务动机递进式铺垫",
      "type": "writing-level",
      "purpose": "增强叙事结构的连贯性和逻辑性，逐步引导读者进入研究核心",
      "location": "introduction",
      "description": "先介绍短文本问答的局限，再引出长文本问答的挑战，最后提出自身方法和研究目标。"
    },
    {
      "name": "定量与定性结果结合",
      "type": "experiment-level",
      "purpose": "提升实验说服力和结论的全面性",
      "location": "experiments",
      "description": "既有统计指标（如F1、Kappa），也有对结构差异的定性分析，丰富实验结果。"
    },
    {
      "name": "基线模型对比",
      "type": "experiment-level",
      "purpose": "突出自身方法的有效性和先进性",
      "location": "introduction / experiments",
      "description": "提出并评测自动角色分类基线模型，将其表现与人工一致性进行对比。"
    }
  ]
}