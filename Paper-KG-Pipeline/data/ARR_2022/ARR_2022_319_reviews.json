[
  {
    "review_id": "bfef226d24e52a45",
    "paper_id": "ARR_2022_319",
    "reviewer": null,
    "paper_summary": "This paper presents several logic failures in the way that attribution methods are evaluated. For each failure they identify, the authors discuss why it will prevent proper identification of attribution quality, and design experiments that crystallize this potential failure. The authors survey the attribution evaluation literature, and argue that such logic failures hinder the advancements of better, more reliable attribution methods. ",
    "strengths": "This is a well written paper that discusses an important and very timely subject. It highlights important failures that are often overlooked in the model interpretability literature. Each failure the authors identify is clearly explained and motivated, and the experiments that demonstrate it are neat and clear. I believe that this paper can improve the way we evaluate attribution methods, and can be beneficial for many in the community. ",
    "weaknesses": "While I have a generally favorable opinion of this paper, I have two issues with it that if addressed would improve the paper in my opinion.\nFirst, I agree with the reasoning and premise, but I think the paper is missing a quite obvious connection to causal inference. Another way of explaining the current limitation of attribution evaluation is that these methods are almost never compared against counterfactuals (the only counterexamples I can think of are the causal model explanation papers). That is, we need benchmarks that include counterfactual examples where we know what is the type of manipulation that was done, and compare model predictions between the original examples and the counterfactual examples.\nMore broadly, we can think of model explanation as a causal estimation problem, and evaluate methods as we do in causal inference (sensitivity analysis and the likes). While this paper is not the place to invent new evaluation methods, I think that it’s worthwhile to highlight the potential connections between all these failures and the lack of causal thinking in the model interpretability literature.\nSecond, I think that the discussion points are a bit too vague and are not as thought out as the described logic failures. Especially in 3.3, I agree that we should want models that are robust to perturbations, but it is not a problem of evaluation methods, it’s a problem of model robustness. We do want our attribution methods to capture such inconsistency if it exists, this is exactly why we use it. ",
    "comments": "The paper is generally very well-written, but may benefit from an additional quick round of editing.  Some typos for example:  Line 616: “make it more robustness” ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "a5dcd6bee1007207",
    "paper_id": "ARR_2022_319",
    "reviewer": "Micheal Abaho",
    "paper_summary": "The paper uncovers critical issues around interpretability and explainability of the reasoning and performance of modern deep learning models. Using several theoretical and experimental analysis, it specifically reveals the weaknesses of existing attribution methods designed to qualify and support research propositions by assessing model predictions. Because of their deceptive nature i.e. easily disregarded or even missed, the paper presents these weaknesses as logic traps. For example, if input features responsible for certain predictions are perturbed, scores provided by attribution methods should reflect the significant change or difference in the model’s predictions given the perturbations. The paper highlights the potential damage of neglecting these traps such as inaccurate evaluation, unfair comparison, unmerited pressure to re-use unreliable evaluation techniques etc. In its analysis, the paper shows factual information of how attribution methods can be misleading when they approve of a model’s prediction countering the actual norm or expectation.  Two helpful examples of logic traps include, (1) the assumption that a humans' decision making process is equivalent to that of a neural network. In a question answering task, BERT base maintained its test set performance despite replacing development set samples with empty strings. Ideally, the performance would drop because of the perturbations, but it did not, therefore clearly misleading human decision making. Moreover, the paper empirically proves this error by showing a drop in the model's confidence in its prediction on unchanged samples. ( 2) A second example of a logic trap is using arribution methods as ground truth to evaluate target attribution method. In an evaluation based on meaningful perturbation, such as Area over the perturbation curve (AOPC) which is precisely the average difference between probabilities predicted with original input feature and those predicted with perturbed input features. While the norm expects this AOPC to be significant or at least representative of the degree of perturbation, the paper illustrates that the perturbation (modification) strategy will dictate the eventual AOPC value, i.e. it varies with respect to the modification strategy. Scores of different attribution methods when inputs are perturbed by token deletion are inconsistent with their scores when inputs are perturbed by token replacement. The paper concludes with suggestions of limiting the impact of logic traps such as the ones discussed. Enhancing the target model as well as Excluding predictions with lower confidence. ",
    "strengths": "Precise and concise abstract.\nThe paper is organised and well written.\nThe subject addressed is crucial for the deep learning community. Redirecting attention to the appropriately selecting attribution methods (during research task evaluation stages) can subtly reduce the immense effort and focus researchers have in outperforming previous works which in any case can potentially be premised on unreliable evaluation methods.\nA sufficient number of examples to illustrate the logic traps is used, which is very helpful especially because they are deceptively obvious. ",
    "weaknesses": "Lines 179-182 Surprisingly, the trained MRC model maintained the original prediction on 64.0% of the test set samples (68.4% on correctly answered samples and 55.4% on wrongly answered samples).\nIt’s clear that the MRC model surprisingly maintains prediction accuracy when evaluated on 64% of the test samples, however, what follows in the brackets is unclear i.e. \"68.4% on correctly answered samples and 55.4% on wrongly answered samples\" is an unclear statement.  Lines 183-185 Moreover, we analyze the model confidence change in these unchanged samples, where the probability on the predicted label is used as the confidence score.\nWhat unchanged samples are you referring to? Did you replace all the development samples with empty strings or was it just a portion you replaced with empty strings hence retaining a few that you refer to as unchanged? If not, are the unchanged samples in the test set or?\nEvaluation 3 and Logic trap 3.\nYou use a hard to follow example to illustrate the logic trap you define as  “the change in attribution scores is brought about by the model reasoning process rather than the attribution method unreliability”. Because you indicate that deep models are vulnerable to adversarial samples, which indeed is right and therefore you would expect attribution scores to be faithful to the shift caused by the attack.\nThe argument feels more like, the change in attribution scores is with respect to the change in samples which eventually will meet a different model reasoning process?\nFor the results you discuss and summarize, Is the claim that the original evaluation method correctly obtains a low similarity in the F1 scores of the adversarial sample subset and the original sample subset, whereas the attribution method says otherwise?\nA rewrite of this section particularly the experiment and its results to add clarity or rather use of a different example would improve the work. ",
    "comments": "Lines 088-092 Last, the over-belief in existing evaluation metrics encourages efforts to propose more accurate attribution methods, notwithstanding the evaluation system is unreliable.\nThe statement above looks more like you intended to say discourages rather than encourages. Please have a look.\nLines 276 and 328 AOPC rather than APOC Lines 528 With no overlap between the two subsets, there is no way we can hypothesis the adversarial samples share similar model reasoning to the original samples.\nhypothesise rather than hypothesis in the above sentence.\nYou can probably do away with some repetitions of long sentences such as what is in the introduction as well as in the conclusion. “ Though strictly accurate evaluation metrics for attribution methods might be a “unicorn” which will likely never be found, we should not just ignore logic traps in existing evaluation methods and draw conclusions recklessly.” ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "f9be3cb8b9c23c80",
    "paper_id": "ARR_2022_319",
    "reviewer": "Piotr Mardziel",
    "paper_summary": "The work argues that input attribution method evaluation criteria (the means in which one attribution method is compared against another) posses several ignored flaws and have \"negatively affected the research field\". The paper exemplifies the flaws by highlighting how high-level goals of evaluation methods can be subverted or as the paper calls them, have \"logical traps\". These issues are shown exploitable to produce inconsistent preferences for an attribution method; i.e. the evaluation criteria are thus rendered pointless. The authors also make some points with regard to some views of what attributions should or should not be and argues otherwise. ",
    "strengths": "+ On its face the paper is right in its main point that evaluation criteria can be exploited to   produce inconsistent ranking of attribution methods, and in general the use of evaluation metrics   can be problematic to research in the area. ",
    "weaknesses": "1 The paper poses mostly subjective points which are themselves largely not novel or are flawed in   themselves. It starts with the emphasized sentence on page 1 attributed to Jacovi et al which I   will call \"Opinion A\":     \"As an explanation method, the evaluation criteria of attribution methods should be how      accurately it reflects the true reasoning process of the model (faithfulness), not how      convincing it is to humans (plausibility)\"   This point is debatable. Whether attributions should be purely faithful to model behaviour or   offer human-interpretability is a decision to be made in the definition of an attribution method   or its motivation. Neither option is wrong as long as the method is applied to its intended use   case. Further, the noted choice of faithfulness has implications on some of the \"logic traps\"   described subsequently. I elaborate on each in the comments section below.\n  Significant positioning fixes need to be implemented in this work starting with Opinion A.   Either arguments for faithfulness need to be made or the paper needs to restrict itself to   attributions whose primary concern is faithfulness. The latter option would have to be written   from the perspective that faithfulness is an apriori goal.\n  The novelty of arguments being made needs to be addressed as well. I find that the cores of the   \"logic traps\" are all known points and are often well considered in attribution, evaluation, and   robustness work. Presently the paper does not demonstrate that (logic traps) \"in existing   evaluation methods have been ignored for a long time\". If there is a gap in some segment of the   community, a survey or systemization paper where applicable would be more appropriate.\n2 Arguments and experiments with regard to \"model reasoning process\" are vague in key definitions   and thus non-convincing. Specifically, no definition of \"model reasoning process\" is given. \n  Experiment outlined in Figure 7 argues that extractor's bits are equivalent to \"model reasoning   process\" but this is arguable. The extractor can derive the same bits using different means or   different bits using very similar means. While I do not disagree with the main points regarding   model reasoning process and robustness, I do not think the experiments demonstrate them. ",
    "comments": "Detailed comments regarding Weakness 1):   * Logic Trap 1: The point being made is obvious:     \"The decision-making process of neural networks is not equal to the decision-making      process of humans.\"\n  The example Experiment 1 incorporates not a single attribution method. Is it presumed that all   attributions will be wrong as there is no human explanation possible? Can we also say that any   human explanation would be wrong for the same reason? If there is no ground truth, how can it be   wrong for an attribution method to say anything?\n  Regardless, no-one *expects* models to fully replicate the reasoning of a human. Replicating   human behaviour may be useful to gauge human-interpretability of explanations but this is   precluded by Opinion A.   Finally, it is well understood that what is correct to a human may not be correct in a model as   per faithfulness vs. explainability discussion which is had alongside attribution evaluations in   literature and in Opinion A (some examples from cited works below).\n  * Logic Trap 2: The second point made is a roundabout way of saying that ablation orders are     varied:     \"Using an attribution method as the ground truth to evaluate the target attribution method.\"\n  The authors are rightly pointing out that numerous forms of attribution evaluation based on   ablation or reconstruction inputs have been used to motivate attribution methods. Here the   opposite conclusion to Opinion A is helpful. Expecting humans to interpret an attribution one way   may lead to one ablation order whereas another form of interpretation may lead to another. There   is no single correct metric because there is no single interpretation. This point has been made   at least in [a].\n  * Logic Trap 3: The third point is known:     \"The change in attribution scores maybe because the model reasoning process is really changed     rather than the attribution method is unreliable.\"\n  Ghorbani et al. (2019) note in their concluding discussion that fragility of attribution is   reflective of fragility of the model (and that their attacks have not \"broken\" an attribution). \n  Likewise Alvarez-Melis et al. (2018) point out that the non-robust artifacts in attributions they   discover are actually reasonable if faithfulness to model is the only goal of an attribution. \n  Thus Logic Trap 3 does not seem to add anything beyond Opinion A.   * 3.1 -- \"Attacking attribution methods by replacing the target model.\"\n  This section seems to be pointing out that attribution methods have a problem of domain   faithfulness in that they often internally apply a model to inputs that they have not been   trained on or don't expect to operate reliably on.\n  * 3.2 -- \"Should We Use Attributions Methods in a Black-Box Way?\"\n  The paper argues that black-box is not a worthwhile goal. This is again highly subjective as   there are several reasons, despite presented arguments, to prefer black-box methods, like 1)   having explanations of the semantics of what a model is modeling instead of an explanation   tainted by how the model is implemented, 2) black-box means model agnostic, hence can apply to   any model structure, 3) some scenarios just do not have access to the model internals.\nOther comments: - The term \"logic traps\" is not define or explained. Dictionaries and reference materials equate   them to logical fallacies which I'm unsure is the intended meaning here.\n- The term \"erasure-based\" is used in the intro but later the term \"ablation\" is used.\n- Typo/word choice near \"with none word overlap\".\n- Typo/grammar near \"there are works (...) disprove\" - Suggestion of 3.3, point 2 is unclear. Adversarial examples can be highly confident and I suspect   means of incorporating confidence can themselves be subverted adversarially.\n- Figure 6 is not useful. I presume the paths are supposed to be indicative of model behaviour but   as mentioned in earlier comments, defining it is a crucial problem in explanation work. The   Figure is suggestive of it being a triviality.\n- Several points in the paper use phrases like \"A lot of works ...\" or \"most existing methods ...\". \n  I think it would be more appropriate to list the works or methods instead of noting the relative   abundance.\n- There are some inconsistencies in the notation for AOPC and the k parameter with potential   related typos in its definition.\n- Potential grammar issue near \"can be seen as an attribution\".\n- Grammar issue near \"results are mainly depend\" - Grammar issue near \"achieve 86.4% accuracy\" - Where is footnote 1?\n- In Figure 4, the resulting attributions for the target word \"good\" in LOO, Marg are not   presented or indicated by color.\nReferences from comments: [a] - Wang et al. \"Interpreting Interpretations: Organizing Attribution Methods by Criteria\". ",
    "overall_score": "1.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]