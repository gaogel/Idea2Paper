[
  {
    "review_id": "74af24c1125fd638",
    "paper_id": "ARR_2022_15",
    "reviewer": "Yunfang Wu",
    "paper_summary": "This paper makes a modification to the previous prefix-tuning to achieve controllable language generation, which considers contrastive prefixes (e.g. positive vs. negative) and train multiple prefixes simultaneously. It presents supervised, unsupervised and semi-supervised frameworks for training, and provides a unified perspective for both single-aspect control and multi-aspect control. The proposed method is novel and the experiments are comprehensive. This paper is well organized and well written. ",
    "strengths": "1.This paper innovatively applies the prefix-learning to the domain of controllable natural language generation, and achieves promising results on sentiment-control, topic-control language generation tasks. \n2. By using the contrastive prefixes to do the supervised and unsupervised training, it can further enhance the ability of prefix-learning and yield better results on the controllability of text generation. \n3.The author conducted sufficient experiments to verify the effect of the model, described the experimental parameters in detail, and compared with related models. ",
    "weaknesses": "I have two concerns about the experimental results. \n1. In the few-shot learning, when the samples are increased from 24 to 1K, the attribute relevance drops by 2 points for positive class as shown in Table 1, and the toxicity metric becomes worse for the detoxification task as shown in Table 2. Please give some explanations. \n2. In human evaluation, the inter-annotator agreement on the sentiment task and the AGNews 482 task is only 0.39 and 0.30 in Fleiss’ κ, which is low to guarantee a high quality of the evaluation data. ",
    "comments": "The paper is well organized and well written. ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "d9bd95358dcf0881",
    "paper_id": "ARR_2022_15",
    "reviewer": null,
    "paper_summary": "***This is from Reviewer XYZ of the previous version.*** \nI have read other reviews, author responses, and the new version. I would like to keep my original score. \n---- This paper presents a lightweight method to train a controllable text generation model by learning small prefix vectors on top of a frozen LM such as GPT-2. The parameter-efficient prefix vectors are of the same design as the prefix-tuning paper (Li and Liang 2021), while this paper proposes to focus on using prefixes to steer the LM generation process according to given attributes & aspects, such as sentiment (pos./neg.) and topics (sports/world). The author(s) argue(s) that the proposed method is also the first work to explore multi-aspect control.\nThe prefixes can be learned either without attribute labels (via label variables; similar to the VQ-VAE method) or with attribute labels (via a margin loss). The experiments are conducted over multiple datasets for both single and multiple aspect control for text generation. Perplexity and attribute relevance are used for automatic evaluation; Attribute relevance and linguistic quality are two human-evaluation metrics. The results show the effectiveness of the proposed method, although there are still some limitations (e.g., tending to shift to movie reviews regardless of prompts). ",
    "strengths": "- The proposed method is a natural extension of prefix-tuning Li and Liang (2021), which focuses on parameter-efficiently learning of conditional NLG tasks (e.g., summarization). This paper, instead, focuses on the controllable free-text generation with given attributes.\n- Benefiting from the parameter efficiency of Li and Liang (2021)'s work, the proposed method in this paper can mix multiple aspects for controlling. Also, the inference speed is maintained --- comparable to the original base model GPT-2.\n- The experiments show the effectiveness of the proposed method in most cases, outperforming several baseline methods in automatic evaluation metrics. ",
    "weaknesses": "- The modeling of the mixture of multiple aspects is not explored deeper enough, and thus the so-called \"first to explore\" (in the Introduction) sounds more like an overclaim to me.\n- The qualitative results (Table 5) and human evaluation both show that there are still limitations of the proposed method and the improvement is not so obvious.\n- Given the arguments about the downstream applications (Line 116-117), I would like to see more downstream performance of such controllable text generation models. Think about how you would connect the proposed models to real NLG applications, and show the advantage of your model. I would say style transfer could be a good start. ",
    "comments": "N/A. The authors have addressed most of my previous comments and suggestions. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work."
  },
  {
    "review_id": "1aa7fa614cb1a09a",
    "paper_id": "ARR_2022_15",
    "reviewer": null,
    "paper_summary": "This paper propose prefix-based models for controllable text generation. Similar to [1], prefixes are token embeddings of language models (e.g., GPT-2) used for learning attribute-specific information and steering the generation of the fixed language models. The authors further add a contrastive loss to enhance the models' controllability. In addition, an unsupervised learning method is introduced to handle scenarios where labels are not available. The authors evaluated the proposed models on multiple controllable text generation tasks, such as controlling sentiment and topics. The experimental results show that comparing to baselines like PPLM and GeDi, the proposed model can achieve a good balance between fluency and controllability.\n[1] Prefix-tuning: Optimizing continuous prompts for generation. ACL 2021 ",
    "strengths": "- The proposed lightweight model achieved strong performance in multiple controllable text generation tasks.\n- The idea of controlling language models in unsupervised way is interesting and new. ",
    "weaknesses": "- Missing human evaluation for the proposed unsupervised learning method. The major technical contribution (novelty) of the paper is controlling language models in unsupervised manner. Unfortunately, human evaluation is absent (in table 4) to demonstrate its effectiveness.\n- For the multi-aspect controlling experiments, CTRL[1] and PPLM[2] should be good baselines.\n[1] CTRL: A conditional transformer language model for controllable generation.  [2] Plug and play language models: A simple approach to controlled text generation. ICLR 2020 ",
    "comments": "Please consider adding new human evaluation results and baselines as mentioned in weaknesses. ",
    "overall_score": "2.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]