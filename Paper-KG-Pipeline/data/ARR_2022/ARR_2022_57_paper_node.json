{
  "paper_id": "ARR_2022_57",
  "title": "ElitePLM: An Empirical Study on General Language Ability Evaluation of Pretrained Language Models",
  "conference": "ARR",
  "domain": {
    "research_object": "该论文主要研究的是文本数据，具体关注于预训练语言模型（PLMs）的通用语言能力评估问题。",
    "core_technique": "论文采用和分析了预训练语言模型（如Transformer架构），并进行实证研究以评估其在多种语言任务上的表现。",
    "application": "论文成果可应用于自然语言处理领域的多种下游任务，如机器翻译、文本理解、问答系统、信息抽取等。",
    "domains": [
      "自然语言处理",
      "人工智能"
    ]
  },
  "ideal": {
    "core_idea": "提出ElitePLM框架，系统评估预训练语言模型的多维语言能力。",
    "tech_stack": [
      "Transformer",
      "预训练语言模型",
      "能力维度评估",
      "基准测试",
      "huggingface",
      "fairseq",
      "jiant"
    ],
    "input_type": "多种公开预训练语言模型及其在代表性NLP任务上的表现数据",
    "output_type": "各模型在记忆、理解、推理、写作四大能力维度上的定量评估结果"
  },
  "skeleton": {
    "problem_framing": "论文通过回顾近年来Transformer预训练语言模型（PLMs）在自然语言处理领域取得的巨大进展，引出对PLMs能力系统性评估的需求。开篇先强调PLMs在多个任务上取得了接近或超越人类的新SOTA成绩，提出了一个重要问题：如何从多维度系统性地评价PLMs的语言能力，并为下游任务选择合适的模型。整体上，采用了从学术gap出发和应用需求结合的策略，既指出了实际应用中模型选择的困惑，也强调了理论层面缺乏全面评估体系的痛点。",
    "gap_pattern": "论文通过梳理现有工作，批评其局限性。逻辑上，先总结已有方法要么只关注PLMs的某一单一能力（如常识、语法等），要么仅在有限的小规模任务上做简单混合测试，缺乏系统性和全面性。常用句式包括‘现有工作要么……，要么……，缺乏……’以及‘尚无详细和系统的分析……’，突出现有方法在能力维度和任务规模上的不足，强调本工作填补了这一空白。",
    "method_story": "方法部分采用了先整体后局部的叙述策略。首先总体介绍了将PLMs划分为五大类别，并列举了每类代表性模型。随后说明了实验平台和统一的训练设置，确保公平比较。最后，补充了各模型的配置和预训练设置的对比，以及影响能力的各因素分析。整体上，先给出全局框架，再细化到模型、实现细节和对比因素。",
    "experiments_story": "实验部分采用了‘设定基线-分能力测试-结果分析’的顺序。首先设置了基线，然后围绕四大能力维度（记忆、理解、推理、表达）分别设计并报告实验，涵盖了多任务和多数据集（如GLUE、SQuAD等）验证。实验类型主要为主实验（各模型在不同能力上的表现对比），并结合定量分析，突出模型能力的系统性评估。"
  },
  "tricks": [
    {
      "name": "引用权威工作建立背景",
      "type": "writing-level",
      "purpose": "增强说服力和权威性，让读者信服该领域的重要性和趋势",
      "location": "introduction",
      "description": "通过引用Transformer、BERT、GPT-3等知名工作，强调PLM在NLP中的主流地位和巨大进展"
    },
    {
      "name": "问题空白明确化",
      "type": "writing-level",
      "purpose": "突出新颖性和研究必要性，使工作动机充分",
      "location": "introduction",
      "description": "指出现有评测方法的局限性，如只关注单一能力或任务规模有限，强调缺乏系统性分析"
    },
    {
      "name": "类比人类智力测评",
      "type": "writing-level",
      "purpose": "提升可解释性和说服力，帮助读者理解方法设计的合理性",
      "location": "introduction",
      "description": "将PLM能力测评类比于WAIS人类智力测验，提出四大能力维度，增强方法的直观性和科学性"
    },
    {
      "name": "多维度能力分解",
      "type": "method-level",
      "purpose": "提升可解释性和完备性，细致刻画PLM的多方面能力",
      "location": "introduction / method",
      "description": "将能力拆解为memory、comprehension、reasoning、composition四个维度，并为每个维度设计对应任务"
    },
    {
      "name": "任务与基准多样性",
      "type": "experiment-level",
      "purpose": "增强完备性和结论可靠性，覆盖多种任务和数据集",
      "location": "introduction / method",
      "description": "为每种能力选择多个代表性任务和常用基准（如GLUE、SQuAD），确保评测全面"
    },
    {
      "name": "模型多样性与公平对比",
      "type": "experiment-level",
      "purpose": "提升对比性和说服力，保证实验结果的广泛适用性",
      "location": "method",
      "description": "选取十个公开PLM，涵盖五大类别，并统一训练设置，保证对比公平"
    },
    {
      "name": "详细实验设置说明",
      "type": "experiment-level",
      "purpose": "增强实验的可复现性和结论的可靠性",
      "location": "method",
      "description": "详细说明实验平台、训练参数、模型配置等，便于他人复现和评估"
    },
    {
      "name": "结构化叙事推进",
      "type": "writing-level",
      "purpose": "提升叙事结构的清晰度，引导读者顺畅理解研究流程",
      "location": "introduction / method / experiments",
      "description": "先提出问题和需求，再介绍方法设计，最后进入实验验证，层层递进"
    },
    {
      "name": "与现有工作对比",
      "type": "writing-level",
      "purpose": "突出自身工作的创新性和改进点",
      "location": "introduction",
      "description": "对比前人方法的不足，强调本工作在系统性和规模上的突破"
    },
    {
      "name": "实验分层分析",
      "type": "experiment-level",
      "purpose": "提升实验结论的可解释性和细致性",
      "location": "experiments",
      "description": "分能力维度设置基线和分析结果，便于读者理解各能力下PLM表现"
    }
  ]
}