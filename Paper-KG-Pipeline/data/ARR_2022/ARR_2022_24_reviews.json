[
  {
    "review_id": "c2d126038dff3afb",
    "paper_id": "ARR_2022_24",
    "reviewer": null,
    "paper_summary": "This paper is similar to \"Learning Music Helps You Read\", that is, it tests the transferability of structure from synthetic pretraining data. It uses similar synthetic data to this prior work, but adds a dependency dataset. This dependency dataset is similar to the parenthetical dataset that exists, but it uses different symbols for opening and closing, rather than the same symbol. Their results are mostly similar to the existing results, but they find that introducing different opening and closing symbols does make the nested structure important for transfer, which is different from the previous results.  They then go over results after pretraining on masked language model objectives and then fine tuning on dependency parsing. ",
    "strengths": "I was very interested in the result that a simple and well-justified modification to the parenthetical language actually reveals nested structure to be valuable.\nThey run multiple seeds for the pretraining objective, allowing them to perform statistical tests over pretrained parameters. This kind of rigor is unfortunately often lacking in many papers on transfer learning, and I was pleased to see it here. ",
    "weaknesses": "I'm not convinced that this should be an entire long paper, given that it is largely a replication of existing results.  There are a lot of stray line numbers all over the pages, which indicates to me that the authors may have modified the template in a way that perhaps could have resulted in a desk reject.\n\"We adopt sentence-level modeling [over between-sentences] because we would like to focus on the learning of sentence structures and also simplify the task setting.\" \nThis isn’t entirely clear. Do they mean that the prior work was using multiple sentences as input at a time and they are using shorter inputs? And if so, does this actually generalize to a more typical setting, which uses fixed length multiple sentence inputs? If you are making a significant change, justifying it verbally is not as good as testing the effect of it.\nThe authors describe the Transformer performance as evidence that \"Transformer encoders exhibit surprisingly good transferability to other languages\". I disagree with this characterization. It looks from the plot as though the random weights have much better performance in the Transformer model, indicating that the Transformer is not necessarily transferring the data better, but is just better at learning the fine tuned task generally. Even the authors seem to disagree with their own characterization, in lines 494-496. ",
    "comments": "A few citations on existing synthetic training: https://arxiv.org/pdf/2106.01044.pdf and https://cs.stanford.edu/~nfliu/papers/liu+levy+schwartz+tan+smith.repl4nlp2018.pdf both of which look at the inductive biases through synthetic training data.\nBe clear whether the language modeling task in section 4 is a causal model.  418: s/is/are/ There are a lot of stray line numbers all over the pages, which indicates to me that the authors may have modified the template in a way that perhaps could have resulted in a desk reject.\nFigure 2a \"unifrom\" To what degree does TILT actually differ from model stitching https://arxiv.org/abs/2106.07682 procedures? I guess it doesn’t have the stitching layer?\nWhere are the results training on the parenthetical data in section 5? Only the dependency data is there. ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "1d40cfb665e0579b",
    "paper_id": "ARR_2022_24",
    "reviewer": null,
    "paper_summary": "The paper investigates what linguistic features are beneficial for cross-lingual transfer for language models. Based on the assumption that the LM encoder is able to learn language-agnostic knowledge during a transfer, the authors construct a synthesized language with different linguistic features to probe for. Notably, the authors are interested in two features. The first is word distribution, for which a uniform distribution, a NL-like distribution based on Zipf's law, and another with word co-occurrences are considered. The second is dependency structure, for which a head-to-tail token grouping method is used.  During evaluation, the paper pre-trains LMs with causal language modeling objective on the synthesized languages and evaluate its perplexity on natural language data, and reaches a series of interesting observations about LSTM and Transformer on these languages with different linguistic features. For LMs with masked language modeling objective, downstream tasks such as POS tagging and dependency parsing are also considered.  The main takeaway is that LMs benefit the most from a nested structure from the synthesized language, with an especially strong downstream performance. ",
    "strengths": "- The paper tackles the fascinating question of how LMs are able to transfer cross-lingually. Specifically, readers like myself who are aware that LMs _can_ transfer from L1 to L2 without the same alphabets, are interested in knowing _how_.  - The proposed approach of using a synthesized language as L1 is creative and reasonable. While the idea stems from (Papadimitriou and and Jurafsky, 2020), I still applaud the novelty, especially with a suite of improvements and additions to the method.  - The experiments seem sound.  - The observations are informed and faithful to the results, and are clearly presented.  - The paper is very well written. For a reader who is no expert on such line of probing work, it is very easy to follow. ",
    "weaknesses": "- In Section 3.2.3, I struggle to understand the intuition and motivation behind the head-to-tail method. How does the fact that two tokens (i.e. \"<xyz\" and \"xyz>\") always appear in the same sentence help create a dependency relation? While the method seems to stem from Papadimitriou and Jurafsky (2020), it's desirable to explain a bit more for self-containment.  - In Section 3.2.2, I would appreciate more intuition behind the \"Random walk\" language. Mathematically (for someone who is not an expert on linear algebra), why would the dot product between a \"topic vector\" and a \"word vector\" give rise to \"non-trivial cooccurrence\"?  - I'd like to know why the authors perform different experiments on language models with CLM (causal) and MLM (masked) objectives. Specifically, why not also calculate perplexity of an MLM encoder on an NL L2? Why not also run the CLM encoder on some downstream tasks?  - For the CLM experiments, is it widely believed that perplexity is a faithful metric to evaluate \"performance\" of the encoder in question?\n- In Section 5.2, it's confusing to choose POS tagging as a downstream task, and then neglect it in the analysis because \"PoS tagging seems to require little structural knowledge on language\". I would appreciate a bit more discussion here. Does the linguistic features studied help with POS, or not? What is the purpose of reporting the POS experiment?\n- Circling back to the motivation of this work, which is the fact that \"an encoder only pretrained on L1 can be transferred to L2 without any parameter updates\". I wonder in what downstream tasks the said encoder transferred to? Are there any higher-level tasks such as NLI, QA, SRL, etc.? Would you consider other downstream tasks than dependency parsing? ",
    "comments": "None at this time. The paper is well-presented. ",
    "overall_score": "4.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "3d1b1462c3af404e",
    "paper_id": "ARR_2022_24",
    "reviewer": null,
    "paper_summary": "This paper investigates what kind of linguistic structural knowledge is transferable during language model pre-training. It explicitly probes certain knowledge/properties by designing several synthetic languages, i.e., (1) tokens are sampled independently and uniformly; (2) tokens are sampled independently but following the Zipfian distribution; (3) tokens are sampled depending on the sentence topic; (4) flat or nesting parenthesis language (Papadimitriou and Jurafsky, 2020); (5) flat or nesting dependency language. Experimental results show that for a causal LM task, conditioned token sampling and nested dependencies provide useful information, especially for LSTM LMs. Nested dependencies are most useful to pre-train a masked LM for a dependency parsing task. ",
    "strengths": "- This paper provides a novel and interesting setting to demonstrate that synthetic conditioned token sampling and synthetic nested dependencies contain knowledge that can be transferred to real language modeling or dependency parsing.\n- Many follow-up ideas could be tested along this direction. ",
    "weaknesses": "- This paper brings more questions than answers -- many results are counter-intuitive or contradictory without explanation. For example: 1) Setting the vector dimension to 10 can make the entire conditional token distribution close to the Zipfian distribution. Why is that? What if the dimension is larger or smaller than 10? \n2) In Figure 2(a), why do uniform and Zipfian token sampling even hurt the perplexity comparing with random weights? \n3) In Figure 2(b), why does L1=nesting-parenthesis is significantly worse than L1=flat-parenthesis for Transformer? \n4) In Figure 2(c), why does transferring from L1=English non-significantly worse than L1=Japanese while the task language L2=English? The flexibility of the Transformer is not a convincing explanation -- if the closeness between L1 and L2 is not a good indicator of transfer performance, then how do we conclude that a synthetic language L1 is helpful because it is closer to a real language L2? \n5) In figure 3(b), why does uniform token sampling is worse than random weights by so much?\n- There some technical mistakes. \n1) The method of sentence-dependent token sampling can not be called \"random work\". In (Arora et al. 2016), $c_t$ does a slow random walk meaning that $c_{t+1}$ is obtained from $c_t$ by adding a small random displacement vector. BTW, the correct citation should be \"Arora et al. 2016. A Latent Variable Model Approach to PMI-based Word Embeddings. In TACL\". \n2) If LSTM/Transformer models are trained with a causal (auto-regressive) LM loss, then they should be decoders, not encoders. ",
    "comments": "- Algorithm 1. How did you choose p < 0.4?\n- L395. \" the combination\" -> \"combine\" - L411 \"train the model with one iteration over the corpus\". Why only one iteration? Is the model converged?\n- After fine-tuning a LM pre-trained with conditioned token sampling (L456 \"useful inductive bias\"), you could check if embeddings of L2 have interpretable topological relations, such as analogy. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work."
  }
]