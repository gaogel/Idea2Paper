[
  {
    "review_id": "da61b78a56b46de3",
    "paper_id": "ARR_2022_48",
    "reviewer": "Manasi Patwardhan",
    "paper_summary": "To alleviate the problem of error propagation due to Automatic Speech Recognition (ASR) system for multi-modal sentiment analysis task the paper provides a refinement approach by detecting the positions of the sentiment words in the text and dynamically refine the word embeddings in the detected positions by incorporating multimodal clues such as low voice, sad face and textual context. Their approach outperforms the baselines. ",
    "strengths": "The approach does try to address the real-world problem of error propagation due to ASR. The  architecture is novel. Good benchmarking and ablation. ",
    "weaknesses": "It would be difficult to detect the positions of sentiment words if an overall sentence (more than one word) has become noisy due to ASR. The same would be true if there are inserts and deletes in the resulting noisy sentence. This approach may fail in such cases. What is the solution for the same?  Some qualitative analysis of the results and/or error analysis will be useful to understand these type of cases better. The case study section can be replaced with qualitative and /or error analysis. ",
    "comments": "The last sentence of the abstract “Furthermore, our approach can be adapted for other multimodal feature fusion models easily.” needs more explanation somewhere in the paper. This is a very vague sentence without any proper basis.  There is no mention of making the built datasets publicly available. Though the dataset can be built to reproduce the results it would be better if authors can share the same publicly. ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "fff3315bb2fd5fc7",
    "paper_id": "ARR_2022_48",
    "reviewer": "Yanghui Rao",
    "paper_summary": "This paper points out that the performance of current state-of-the-art multimodal emotion analysis (MSA) models decreases sharply when the automatic speech recognition (ASR) models predict sentiment words wrong. Therefore, this paper proposes a sentiment word aware MSA model named SWRM, which detects and alleviates the harm caused by sentiment word prediction errors of the ASR models. Specifically, SWRM consists of a sentiment word position detection module based on BERT, a multimodal feature extraction module based on LSTM and BERT, an aggregation network and a multimodal gating network for fusing different features. ",
    "strengths": "1. This paper focuses on the problem that the sentiment word prediction errors of the ASR models corrupt the effects of current state-of-the-art MSA models, which is valuable in practice. \n2. The proposed model SWRM achieved improvements over other baselines, especially when the ASR model performed poorly. ",
    "weaknesses": "1. The contribution of this paper is slightly less: the improvement on prediction effect of SWRM seems to mainly come from the capability of BERT in detecting the position of sentiment words and predicting correct sentiment words. \n2. Lacking the evaluation of sentiment word detection and correction: since the key ideas of SWRM are the detection and correction of possible sentiment word errors, I think it is necessary to conduct experiments to validate the effects of the sentiment word position detection module and the predicted sentiment word candidates. \n3. Insufficient datasets and explanation of experimental settings: this paper only used one dataset (CMU-MOSI) for evaluation, which cannot fully compare the performance and robustness between SWRM and baseline models. Particularly, CMU-MOSI is one of the datasets employed by Dumpala et al. (2018), and there is a larger dataset named CMU-MOSEI in (Dumpala et al., 2018). It is suggested to add the CMU-MOSEI dataset for evaluation. Moreover, the experimental part doesn’t mention the setting of hyperparameters. \n4. Lines 308-319 are quite confused to me. The definition of notations should be largely improved for clarity. ",
    "comments": "1. Line 220: is absent -> are absent; Line 259: in the training and testing phrases -> in the training and testing phases; Lines 310-311: k_i is defined as the number of sentiment words, but k_i is not used elsewhere; Line 352: Subsequently, We -> Subsequently, we. \n2. As mentioned above, I think that supplementary datasets are important for better comparing SWRM and baselines. Moreover, the effect of sentiment word detection and correction of SWRM should better be shown. \n3. From my opinion, instead of correcting sentiment word errors from the ASR models, improving the performance of the ASR models is a thorough solution to guarantee the effect of the MSA models in practice. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]