[
  {
    "review_id": "3fa0b7dad4a7fd34",
    "paper_id": "ARR_2022_142",
    "reviewer": null,
    "paper_summary": "The paper presents QuALITY, a new benchmark for question answering over long passages. All the questions are multiple-choice, composed by professional writers and validated by MTurk annotators to be answerable and unambiguous. A subset of especially challenging questions is also selected in a task where annotators answer the question under strict time constraints. The paper presents a detailed analysis of the dataset and a thorough evaluation of long-context and extractive QA models on the presented data, demonstrating that all the models are far behind human performance. ",
    "strengths": "- Long-passage QA datasets are harder to collect and relatively scarce, so the new dataset would be a valuable addition to the field.\n- The data collection and annotation process is very well thought out and includes multiple validation steps. The data is further validated in qualitative and quantitative analysis.\n- The experimental part is thorough: both long-context models and extractive models are evaluated, and there are additional experiments with supplementary training data and no-context baselines. The choice of the QA baselines seems reasonable to me (although my expertise in QA is limited).  - The paper is clearly written and easy to follow, and both the data collection and the experimental evaluation are documented in detail. ",
    "weaknesses": "My only (very minor) concern: the qualitative analysis is somewhat hard to understand without reading the Appendix (see comment below). That can easily be addressed given an extra page. ",
    "comments": "- Without looking at the Appendix, I found it difficult to interpret the different reasoning strategies mentioned in Section 3.6 and Table 5. This section might read more smoothly if you include an example question or a very short explanation for a few most popular types, such as \"Description\" or \"Symbolism\". It was also not clear to me how the questions were annotated for reasoning strategy without reading the passages: was it just by looking at the question, or with the Ctrl+F type keyword search in the passage?\n- This is perhaps too much to ask, but I am very curious about the 4% where the annotator-voted gold label does not match the writerâ€™s label. If the authors have done any analysis on why the annotators might disagree with the writer, I would love to see it!\n- L275: this inclusion criteria -> these inclusion criteria - L441: perhaps you meant Table 6, not Table 9? Not having to go to the Appendix for the results would make things easier for the reader. ",
    "overall_score": "4.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]