{
  "paper_id": "ARR_2022_174",
  "title": "Learning to Rank Visual Stories From Human Ranking Data",
  "conference": "ARR",
  "domain": {
    "research_object": "该论文主要研究视觉故事的数据，即由一系列图像（视觉内容）组成的故事，并结合人类排序数据来进行分析和建模，属于多模态（图像与文本）数据处理问题。",
    "core_technique": "论文采用了学习排序（Learning to Rank）的方法，利用人类提供的排序数据进行模型训练，可能涉及深度学习模型（如卷积神经网络或多模态融合网络）来理解和排序视觉故事。",
    "application": "研究成果可应用于视觉故事生成与排序、自动相册整理、社交媒体内容推荐、多媒体内容检索等实际场景。",
    "domains": [
      "计算机视觉",
      "多模态学习",
      "信息检索"
    ]
  },
  "ideal": {
    "core_idea": "提出了基于人类评价数据训练的、无参考的视觉故事生成自动评价指标Vrank。",
    "tech_stack": [
      "SIMCSE",
      "VHED数据集",
      "无参考评价",
      "视觉故事生成",
      "深度学习排序模型"
    ],
    "input_type": "视觉故事生成模型输出的故事文本对及其相关图像",
    "output_type": "对视觉故事文本的自动化质量排序或评分"
  },
  "skeleton": {
    "problem_framing": "论文从学术gap出发引出问题，指出视觉故事生成（VIST）任务虽然模型发展迅速，但评价方法研究滞后，现有自动评价指标与人工评价的相关性较差，不能有效反映故事生成的真实质量。开篇通过对比机器生成与人工故事的差距，并强调现有评价方法的局限性，突出对更好评价指标的需求。",
    "gap_pattern": "论文批评现有方法主要采用‘现有方法忽视了X’和‘现有方法在Y场景下失效’的逻辑。具体指出传统n-gram或参考文本依赖的自动评价指标（如BLEU、CIDEr、METEOR）假定人类故事总优于机器生成，且不能适应故事多样性，导致与人工评价不符。此外，最新的混合或无参考指标（如BLEURT、UNION）仍然依赖参考文本或人工结果，相关性不佳。论文用‘然而’‘但是’等转折句式，系统性地指出这些方法的不足和不适用性。",
    "method_story": "方法部分采用先整体后局部的叙述策略。首先介绍了如何重新收集和整理多篇论文中的人工评价结果，构建VHED数据集。随后说明如何利用该数据集训练新的无参考评价指标Vrank，并阐述Vrank的核心思想和技术基础（如基于SIMCSE的排序学习）。方法描述由数据构建、指标设计到训练流程，层层递进，逻辑清晰。",
    "experiments_story": "实验部分采用多数据集验证和主实验为主的策略。首先介绍主实验——故事对排序，用于衡量各自动评价指标与人工排序的一致性。实验中不仅在自建的VHED数据集上评测，还引入VIST-Edit作为未见数据集进行泛化能力测试。此外，详细说明了基线指标的选择与设置，包括传统和新型自动评价方法，以及随机基线。整体实验设计注重全面性和公正性。"
  },
  "tricks": [
    {
      "name": "问题现状批判",
      "type": "writing-level",
      "purpose": "突出当前方法的不足，为新方法的提出制造需求和合理性",
      "location": "introduction",
      "description": "作者批判现有自动评价指标（如BLEU、CIDEr、METEOR）在VIST任务中的不足，强调它们与人类判断不符，不能推动模型进步。"
    },
    {
      "name": "人类评测权威性强调",
      "type": "writing-level",
      "purpose": "增强新方法依赖人类评测数据的合理性和说服力",
      "location": "introduction",
      "description": "作者强调人类评测结果更可靠，包含更有意义的信号，应充分利用并减少对参考文本的依赖。"
    },
    {
      "name": "创新点突出",
      "type": "writing-level",
      "purpose": "清晰展示工作的创新性，吸引读者注意",
      "location": "introduction",
      "description": "明确提出本工作首次将人类评测数据（VHED）系统化，并基于此训练无参考的评价指标Vrank。"
    },
    {
      "name": "案例对比说明",
      "type": "writing-level",
      "purpose": "用具体例子帮助读者理解现有方法的问题和新方法的优势",
      "location": "introduction",
      "description": "通过对比两个模型生成的故事及其BLEU分数和人类排名，说明传统指标与人类判断的不一致。"
    },
    {
      "name": "任务定义精细化",
      "type": "method-level",
      "purpose": "让方法的评价标准更明确、可复现，增强科学性",
      "location": "experiments",
      "description": "将评价任务定义为故事对排序，详细说明如何根据自动指标与人类排名进行配对和准确率计算。"
    },
    {
      "name": "数据集多样性与独立性说明",
      "type": "experiment-level",
      "purpose": "证明实验结果的广泛性和结论的可靠性",
      "location": "experiments",
      "description": "使用VHED和VIST-Edit两个数据集，强调VIST-Edit为未见数据集，验证方法的泛化能力。"
    },
    {
      "name": "基线方法全面对比",
      "type": "experiment-level",
      "purpose": "通过与多种现有方法对比，突出新方法的优越性",
      "location": "experiments",
      "description": "实现并对比多种传统和新型自动评价指标（BLEU, ROUGE-L, METEOR, SacreBLEU, BERT-Score, BLEURT, UNION），并设置随机基线。"
    },
    {
      "name": "公平性机制设计",
      "type": "method-level",
      "purpose": "消除评测中的数据泄漏和不公平因素，增强实验说服力",
      "location": "experiments",
      "description": "提出Reference Absent Algorithm，确保在包含参考文本的故事对中，评测时不会出现无意义的满分匹配。"
    },
    {
      "name": "实验指标合理化",
      "type": "experiment-level",
      "purpose": "选择更能反映实际效果的评测指标，增强实验结论的可信度",
      "location": "experiments",
      "description": "采用pairwise accuracy作为主要评测指标，并引用相关文献说明其优于相关性指标。"
    },
    {
      "name": "逻辑递进式叙事",
      "type": "writing-level",
      "purpose": "使论文结构清晰，读者易于跟随和理解",
      "location": "introduction / method / experiments",
      "description": "从问题引入、现有方法不足、提出新数据集和新方法、到实验设计和对比，层层递进，逻辑严密。"
    }
  ]
}