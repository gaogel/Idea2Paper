[
  {
    "review_id": "4aaf67bb39a536f9",
    "paper_id": "ARR_2022_81",
    "reviewer": null,
    "paper_summary": "This paper introduces a new multimodal dialogue state tracking dataset, with simulated dialogues of moving objects that have varying shapes, colors, and sizes. They also design a novel Video Dialogue Transformer Network as a strong baseline for the task. ",
    "strengths": "The dataset and model designed in this work appear to be novel and would be broadly useful to the dialogue systems and multimodal learning communities. ",
    "weaknesses": "As the authors mention, the dataset is limited to simulated dialogues of simple objects, rather than human objects, but this is a preliminary study. In addition, it is not specified whether the data and code will be publicly released, which is the primary contribution. Some of the baselines used were weak. Other stronger baselines should be considered, e.g., SUMBT, DSDST, GCDST, SOM-DST, CSFN-DST, MinTL, and Pegasus. ",
    "comments": "- line 249: \"tuned\" -> \"tune\" - line 325: is a word missing after \"linear\"?\n- line 343: \"recovers\" -> \"recover\" - please clarify what is meant in line 375 by \"harmful\" and annotation bias in line 373 - line 494: \"Asides\" -> \"Aside\" - lines 528-529: should \"only improves the results significantly\" be \"insignificantly\"?\n- line 541: \"We observed that\" is repeated twice - line 584: please elaborate on what is meant by filtering the videos and training a new model, as well as how to interpret Table 5 ",
    "overall_score": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "d4b6a18e476d8e6b",
    "paper_id": "ARR_2022_81",
    "reviewer": null,
    "paper_summary": "This paper introduces a multimodal DST task. The paper's contributions are 3 folds:  1. defines the task which pertains to state tracking over a multi-turn conversation between the system and the user wherein the user asks questions and the system provides answers from a video scene. \n2. provides a synthetic dataset (DVD-DST) for the task. \n3. VDTN: which is a multimodal transformer-based model that combines video and dialog features to answer the user's questions. \nThey test VDTN on their synthetic dataset against baselines to showcase that VDTN does better at the multimodal DST task. ",
    "strengths": "1. Interesting task definition - traditional DST systems do not quite deal with references to multiple intra-domain objects (which is a harder task to track states over IMO). This task catches up on that distinction. \n2. The paper is well-written (extensive analysis in appendix) and tries to cover a large ground of work (defining task + providing synthetic dataset + novel model architecture) ",
    "weaknesses": "- Overall, I feel that the paper is trying to do a lot of things at one go. The authors have covered a large ground of work but maybe it's too much to put into a single paper and isn't doing justice to each of the contributions. Example, given that this is a new task - I would want to know more details about the task - how is it different/why is it beneficial etc. ( the authors have covered these aspects, but briefly). \nGetting into the details of weaknesses: - One of the major weaknesses of this paper is the use of synthetic datasets. Also, the choice of baseline (DVD) doesn't seem super clear. Based on the examples, it seems to be a very \"dumb\" baseline. Some insight into the chosen baseline would be helpful.\nHowever, despite these weaknesses it is still a comprehensive paper. ",
    "comments": "nil ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "f3f751ba8b21e054",
    "paper_id": "ARR_2022_81",
    "reviewer": "Weizhe Lin",
    "paper_summary": "This paper introduced an extended definition which extends unimodal DST to multimodal DST. To demonstrate this new schema of DST, the authors generated dialogues based on an extended CATER video split, with additional ground-truth labeling for bounding boxes and visual objects across frames.  The dialogues mainly focus on visually-grounded object reasoning, where a question is raised to query the states of objects in the given video clip.\nThe proposed DST definition comprises of dialogue states from the traditional DST setting and dialogue states that represent the states of visible objects. Two temporal-based slots are also added such that an object can be located by each turn.\nThe model architecture is based on an autoregressive approach, where the dialogue states of last turn along with turn utterances (t-1 and t) are given to generate the frame start/end and object states. The backbone is Transformer block, and Mask Vision Modeling (similar to Mask Language Modeling) is used to self-supervise the learning of visual representations.\nIn experiments, the authors presented several baseline models to evaluate different aspects of the proposed DVD-DST dataset. The model outperformed these baselines, and analyses on model components were provided. ",
    "strengths": "1. The paper is generally well-written. Most of the content does not cause confusion. \n2. The introduction of Multimodal DST (MM-DST) is interesting. Though there are some concurrent works in introducing MM-DST, the paper defines a more complete set of dialogue states: single object to multiple, and maintaining object states also becomes a DST task. This new definition is in general convincing and promising, and can be extended to more complex MM-DST problems. \n3. The creation of the dataset refers to prior work and took into consideration the distribution bias of datasets. Potential biasedness was further discussed with some simple baseline models proposed in Sec. 4. It can be seen that the dataset is generally in high quality. \n4. The video self-supervised training improved the overall performance, showing that with this technique, the Transformer blocks learn to distinguish visual cues and learn meaningful representations. \n5. The experiments and ablation studies are rich and cover most of potential research questions. ",
    "weaknesses": "1. The extension to the unimodal DST definition is good enough in the CATER dataset, but why the authors pick the CATER dataset as the basis is not obvious and lacks further discussion. This dataset contains only simple objects and questions related to their positioning and movements.  This is very “synthetic” and so that the proposed MM-DST definition is very limited to simple objects and movements. How can this definition be extended to broader and more real applications, like other real-world dialogue systems and QA machines? \n2. The clarification of different models being compared is not clear enough. Many baseline models are proposed and compared, but for most of them this paper did not explain why they were proposed, what are expected, and further discussions are still missing. The current form is a bit messy in Line 409-436, 461-509. \n3. The comparison with baseline systems is not very convincing. The baseline systems such as TRADE are already old. They have low performance in unimodal DST, to the best of my knowledge. For example, TRADE is a very basic model (48.62%), and NA-DST achieved only 50.52% on MultiWOZ 2.0. However, the current best model from the leaderboard of MultiWOZ ([budzianowski/multiwoz: Source code for end-to-end dialogue model from the MultiWOZ paper (Budzianowski et al. 2018, EMNLP) (github.com)](https://github.com/budzianowski/multiwoz)) is: [KAGE-GPT2](https://aclanthology.org/2021.emnlp-main.620.pdf) (Lin et al, 2021) with 54.86% on MultiWOZ 2.0 and [TripPy + SaCLog](https://arxiv.org/abs/2106.00291) (Dai et al. 2021) with 60.61 on MultiWOZ 2.1. Based on the approach the authors are using, [SimpleTOD](https://arxiv.org/pdf/2005.00796.pdf) (Hosseini-Asl et al. 2020) suits better since it generates dialogue states in an auto-regressive way as well. Furthermore, it is also unfair to compare with unimodal DST systems that were not trained on image understanding. Thus, it is more fair to train those baseline systems (at least one of them) with self-supervised learning. \n4. The join training of L_seg and L_obj seems to underperform either alone. This was neither tackled nor deeply discussed. This component should be crucial to the overall performance, and thus better analysis is required. ",
    "comments": "1. Though the paper presented a fair amount of good work, the current presentation should be improved. For example, the captions of tables and figures should contain necessary explanations of what are presented and what are the major findings that a reader should put focus on. Abbreviations (Tab. 2 “Dial.”, “ Vid.”) should be clearly described in captions. Reference should be put to ambiguous descriptions (e.g. refer Line 253,254 to Fig.2 such that examples of OBJ<class> can be well understood). Line 261: R^{L_obj x 4}: 4 should be x1 x2 y1 y2, and this should be stated. Table 3:  the meaning of (tracking) should be clearly stated, or use another name like “w/ oracle bounding box labels”. Line 506-509, use the formal names instead of only citations for these models. Line 543-546: not understandable. Line 193, redundant first “|”, {}|_{some conditions} is enough. \n2. In general, this paper presented a fairly good amount of work. And the shift from unimodal to multimodal is on the right track. Though there is still a lot to improve, I believe that the authors are moving towards an impactful work. \n3. Well, DVD-DST - what does DVD mean..... A better name is strongly suggested since \"DVD\" seems to distract readers from its real meaning. ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]