[
  {
    "review_id": "5553e734ee561dbc",
    "paper_id": "ARR_2022_100",
    "reviewer": null,
    "paper_summary": "The paper extrinsically studies different methods to transfer visual knowledge to language models (LMs) for commonsense reasoning tasks assumed to require physical/visual knowledge. \nThe methods differ in  (i) the domain of the data for pre-training (e.g., pre-training LMs on text of the visual domain (MSCOCO captions), and training visual-linguistic models using MSCOCO's images-caption pairs), as well as in  (ii) the pre-training objectives (voken classification, MLM, cross-modal contrastive learning (CMCL), cross-modal knowledge distillation).  Models: The authors experimentally compare text-only BERT-base and a range of existing cross-modal models, pretrained with the different objectives, as far as applicable.   Experiments are conducted on 5 downstream tasks (PIQA, Visual Paraphrasing, OBQA, Metaphora on RiddleSense). In both, the low-resource and fully supervised setting, using contrastive learning (CL) seems to be the only beneficial cross-modal pre-training method (ii) applied for the visual-linguistic models (and only with additional sampling strategies in the case of pre-training on *image*-caption pairs)---these models slightly improve over the cross-modal baseline models (MLM) as well as the text-based LM (BERT).  The authors further demonstrate that for general language understanding (GLUE benchmark), the visual-linguistic models overall are less effective than text-only BERT. ",
    "strengths": "- A comparison of a range of pretraining objectives and data for cross-modal learning towards representations/models that capture commonsense knowledge - Experiments on multiple downstream tasks, suggesting a  benefit of cross-modal contrastive learning with additional sampling strategies - Overal well written and easy to follow ",
    "weaknesses": "- What do we learn overall? That visual information, from captions or images, together with contrastive learning, is slightly beneficial for downstream tasks underlying visual commonsense? I am  missing insights into       - why the other pretraining strategies fail           - why there is, overall, basically no difference in performance between pre-training on captions or on images (shouldn't the latter even be more informative than captions?) \n         - why pre-training on visual-linguistic data is harmful for general NLU instead of just not beneficial. \n     - It is positive that such a range of different visual-linguistic models have been examined. However, from their descriptions in Section 3 I am not sure I can assess whether these models are fairly comparable.  - Text-based comparison baseline: BERT-base is a weak model compared to BERT-large or RoBERTa -- would the conclusion that visual information can be integrated and leads to better commonsense reasoning behaviour hold when these stronger purely text-based LMs were used? ",
    "comments": "- l222: *the* fully - l224f: *the* low-resource, *the* train data, *set* the size - l281: call *it* - l483: This *suggests* - Equ. ( 2): double-check i vs. j in denominator ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "c8fee84fa31cfde1",
    "paper_id": "ARR_2022_100",
    "reviewer": "Da Yin",
    "paper_summary": "This paper explores whether intermediate pre-training on visual knowledge can improve performance on commonsense reasoning tasks. The visual knowledge can be categorized into textual knowledge from captions and knowledge from image-caption pairs. Authors first study which types of knowledge can benefit which tasks. Besides, the authors also explore which knowledge transfer methods are beneficial for the downstream tasks. Experimental results show that visual knowledge helps tasks related to physical commonsense, especially in few-shot settings. Also, models can be further improved with the help of knowledge distillation and contrastive learning methods. ",
    "strengths": "1. Meaningful analysis: It is a natural idea to study how vision modality can help textual tasks. And intermediate pre-training on visual knowledge is a great choice to evaluate the role of visual knowledge in the process. \n2. Comprehensive experiments: Authors attempt to reproduce the most recent methods including VidlanKD and Vokenization, and perform analysis on various aspects which can influence the transferability. I'm impressed by the hard work of authors. ",
    "weaknesses": "1. The effect of different corpus: The sizes of different text corpus are different. But in the current experimental setup, authors didn't consider this factor. I'm also a bit confusing about why the authors choose MSCOCO as the caption dataset instead of the larger Conceptual Captions. \n2. Lack of knowledge probing tasks: Since the paper mainly studies how to evaluate the \"knowledge\" transfer, it is natural to consider adopting knowledge probing task such as LAMA (Petroni et al., 2019) to evaluate whether models already study the knowledge. ",
    "comments": "Most comments are mentioned in the weakness part. I suggest authors add the knowledge probing tasks in the updated version and conduct some qualitative analysis to show if the models indeed learn the visual knowledge and which kinds of knowledge they learn more. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]