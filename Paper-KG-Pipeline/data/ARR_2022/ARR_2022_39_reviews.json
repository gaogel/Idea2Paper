[
  {
    "review_id": "78daab8d16002833",
    "paper_id": "ARR_2022_39",
    "reviewer": null,
    "paper_summary": "This paper introduces an auto-regressive blank infilling model based upon an encoder-decoder architecture.  The model is very similar to T5 except for (i) a generic [MASK] token is used in place of unique sentinel tokens, (ii) the [MASK] tokens can be shuffled on the decoder side of the model, and (iii) 2D positional encodings are used -- the position of the corrupted text and the tokens within the corrupted span.  This architecture is intended to address the varying performance of encoder and decoder models over natural language understanding, conditional generation and unconditional generation tasks. ",
    "strengths": "- The paper is very clearly written.\n- The illustrations of easy to follow and enhance understanding of concepts.\n- Paper provides a concise and excellent review of competing architectures.\n- I believe that the model could be re-implemented from information provided in Section 2.\n- Thorough presentation of results. ",
    "weaknesses": "- Result in Table 1 are substantially lower than SOTA results, casting doubt on usefulness of this approach.\n- Approach appears to be very incremental to T5, leading to questions about novelty.\n- A clearer ablation studying comparing the seven potential combinations of the three differences to T5 is missing.\n- The array of GLM models evaluated (e.g., GLM_{Doc}, GLM_{Sent}, GLM_{410M}, etc.) were not well motivated and did not add much to the presentation. ",
    "comments": "I generally liked the paper, but I am not sure that the results (as presented) make a compelling argument to add another model to the transformer-based “pantheon of models.”  It would be great to see how larger GLM models compare to SOTA results, given that hardware resources do not appear to be a constraint.\nTypo: Should “BRET” in line 65 be “BERT”? ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]