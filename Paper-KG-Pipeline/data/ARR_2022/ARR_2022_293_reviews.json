[
  {
    "review_id": "616f1e9160debea4",
    "paper_id": "ARR_2022_293",
    "reviewer": null,
    "paper_summary": "The paper presents a novel approach to understanding math problems from their textual formulations. The approach builds on those from related work, choosing syntactic representations. The key novelties are (1) an internal graph representation of the operators and (2) a novel pretraining setting. The model achieves vast improvements over prior art. ",
    "strengths": "The new model addresses several key problems of previous work and appears to contribute a very logically motivated extension, modeling the structure of the required mathematical operations. The model description is clear and the experimental setup and results are reasonably clear and allow for an easy comparison with related work. There is also an ablation study to analyze the contribution of the individual components of the model.  The paper is easy to read. ",
    "weaknesses": "The model section seems to lack comparison with prior work. It is not entirely clear what is novel here and what is taken from prior work. It is also not entirely clear to me if pretraining is performed with data from all tasks and whether the same setup had been used previously. If this is different from prior work, that would be unfair and a major flaw. ",
    "comments": "I'd like to see my doubt about the pretraining cleared up. ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "346582f5d9e2b75a",
    "paper_id": "ARR_2022_293",
    "reviewer": "Maria Leonor Pacheco",
    "paper_summary": "This paper presents COMUS, a method to fuse the representations of the text and the graphical mathematical syntax for math understanding tasks. The method consists of constructing a math syntax graph, and leverage large language models and graph attention networks to represent the language and the symbolic structure, respectively. It proposes a syntax-aware memory network to capture the interactions between the text and formula representations, and it explores the use of pre-training tasks to 1) improve the language representation, 2) improve the graphical representation, and 3) combine them. They show that their method outperforms text-based and graph-based methods, both with and without pre-training, as well as competitive methods designed for math problems (i.e. MathBert) across four tasks in the math domain. The paper presents an ablation study to test the impact of each of the components of the proposed method, as well as experiments that show that the improvement is stable in the low supervision case. ",
    "strengths": "- Paper motivates the need for combining symbolic, structured information and text for math problem understanding. The paper identifies the work done in this space and clearly points the limitations of current methods.  - Experimental setting is clear and well-executed, using several tasks and significance testing. The set of baselines is comprehensive --text/graph methods with and without pre-training, as well methods that combine the two representations.  - Ablation study is comprehensive ",
    "weaknesses": "- I am admittedly not that familiar with the standards in dataset and tasks for this domain, but most of the tasks presented in this paper seem to be based on question type/similarity (knowledge point classification, question relation, question recommendation). What are the opportunities and limitations of this and other competitive approaches when dealing with tasks that require explicit logical/mathematical inference?  - Dataset details -- I couldnâ€™t find any reference to these datasets, are they new? If so, they are missing some important details. How were they annotated? explain and report agreement, and statistics (e.g. label distribution). If you don't have enough space, I would encourage you to add details on the appendix.  - The only other thing that this paper is missing is a discussion on limitations and directions for improvement / future work. I would encourage authors to consider this in the camera-ready version. ",
    "comments": "See above. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]