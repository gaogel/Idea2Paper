[
  {
    "review_id": "6e33f0781de36470",
    "paper_id": "ARR_2022_263",
    "reviewer": null,
    "paper_summary": "This paper addresses cross lingual summarization task. This task is generating a summary in a target language from a source document in a source language. In other words, the cross lingual summarization is the combination of machine translation and summarization.\nSome previous studies regarded the cross lingual summarization as the combination task of machine translation and summarization, and proposed multi-task learning methods. \nIn contrast, this study regards the cross lingual summarization as a super-task of machine translation and summarization, and proposes a hierarchical model to control them. The proposed method uses two kinds of latent variables: global and local. A global corresponds to cross lingual summarization, and a local corresponds to machine translation and summarization. This paper applies conditional variational auto encoder to treat these latent variables.\nExperimental results show that the proposed method achieved better performance than previous methods in automatic and human evaluation. ",
    "strengths": "The core idea, that regards cross lingual summarization as a super-task of machine translation and summarization, seems reasonable. In addition, it is a reasonable approach to apply conditional variational auto encoder to the cross lingual summarization.\nThe proposed method achieved better performance than previous methods in automatic and human evaluation. ",
    "weaknesses": "Since explanations on experiments contain unclear points, I'm not sure whether the proposed method achieved better performance than previous methods in a fair comparison. \nFor training datasets, the proposed method seems to be using only cross lingual summarization data but MS-CLS, MT-CLS, and MT-MS-CLS use machine translation, summarization, or both datasets based on descriptions in 4.2. \nI checked Appendix A but it does not contain details explicitly such as the number of sentences for each task.\nIn addition, the parameter sizes of each method might be different from each other. The proposed method share parameters of an encoder but this paper does not explain other methods explicitly. Thus, I cannot judge whether the proposed training strategy improves the performance or sharing parameters improves the performance (or other reasons for improvements). \nI recommend explaining the number of sentences and parameter sizes for each method explicitly.\nMoreover, I suppose that the proposed method requires much more training time than previous methods. If so, the authors should discuss the relation between training time and performance because a view on Green AI is important.\nFor the human evaluation, the authors used only 25 samples but the number of samples are too small. I doubt that we can conclude the effectiveness based on such small instances. ",
    "comments": "I'm not sure why the authors describe pipeline methods in Table 1. \nZhu et al., 2019 indicated that end-to-end methods achieved better performance than pipeline methods. \nI think that the results in Table 1 are copied from their paper because scores are identical to ones of them. \nThus, I cannot understand why authors re-compare pipeline methods (with reported scores) with end-to-end methods.\nIn addition, I doubt that the above pipeline methods consist of weakly methods. \nIf the authors conduct a fair comparison, the authors should prepare strong methods for machine translation and summarization. \nFor example, initializing each model with mBART and fine-tuning them on training data of each task.\nFor explanations in Section 4.3, the authors should list each method for readability such as using \\item. ",
    "overall_score": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]