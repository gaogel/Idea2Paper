[
  {
    "review_id": "71abbf4878652a7d",
    "paper_id": "ARR_2022_228",
    "reviewer": null,
    "paper_summary": "This work introduces a new dataset, the Hindi Legal Documents Corpus (HLDC), a corpus with 900 thousand legal documents in Hindi. This corpus is collected from public data, and the authors intend to release (in addition to the corpus) the scripts necessary for its creation and processing, along with models and code for the experiments in the paper. The authors examine the task of predicting the verdict of bail applications (a binary task, which is to predict whether or not the application was denied or granted). A variety of models are explored for this task; while accuracy is better than the majority baseline, there is still much room for progress. The headroom in performance even for this simple task highlights the challenges in using natural language processing and machine learning systems for legal use cases. Overall, I believe the data and experiments introduced by this work would be interesting to many, and I recommend it's acceptance. ",
    "strengths": "1. This work introduces a new, large-scale dataset containing legal documents in a low-resource language. This can be a valuable resource for many, and could help advance research in natural language processing for legal use cases.\n2. Authors thoroughly describe the process of data collection and cleaning, and intend to open-source code for reproducing these steps.\n3. Through experiments, authors demonstrate the challenges of current techniques in a simple (yet telling) task of predicting the outcome of bail applications. The authors report multiple baselines and will publicly release their code and models.\n4. The authors take many steps to anonymize the dataset, removing names, gender information, titles, locations, times, etc.   5. This paper is clear and well written. ",
    "weaknesses": "Some minor considerations: 1. It would be informative to users if authors reported sensitivity of their experiments to hyper-parameters, along with standard deviations on their numbers.\n2. The presented error analyses are anecdotal, and might not be reflective of the overall behavior of the system. It would strengthen this paper if authors further explored systematic biases in their datasets and models (e.g. how does accuracy/F1 vary by district?) ",
    "comments": "Footnote marks should come after punctuation. ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]