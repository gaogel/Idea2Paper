[
  {
    "review_id": "0ec1053d54796416",
    "paper_id": "ARR_2022_93",
    "reviewer": "Sashank Santhanam",
    "paper_summary": "The authors of this paper propose a new method that assesses dialogue models through the live evaluation process and overcomes the drawback of requiring pre-created reference dialogues. Further, the authors claim that their proposed method achieves highly reliable evaluation with a correlation value of 0.96 when experiments were replicated.  The proposed experiment design focuses on asking a crowd worker to carry out a live conversation with a randomly selected model and is asked to evaluate the model through a Likert scale and continuous scale on 7 evaluation measures. Further, as a part of the experiment, the authors also put quality checks in place to ensure that they were getting good quality data. To avoid traditional quality checks, the authors introduced a random degraded model and monitored how crowdsourced workers rated those models compared to other pretrained models obtained from the ParlAI framework. The degradation is introduced by random sampling from training dialogues and distortion of meanings.\nResults from multiple runs of the human evaluation show that the proposed data collection methodology achieves high interrater consistency across multiple runs and the 7 metrics as demonstrated in Tables 3 & 4. ",
    "strengths": "1. The authors tackle an important problem of getting good quality ratings for human evaluation of dialogue systems and results obtained in the over three different runs suggest that the high consistency in ratings from the human evaluation is achievable. \n2. The authors conduct a large-scale evaluation to provide validity to their study across multiple runs. The data collected from these experiments would be valuable to the community to analyze and build on it and the authors plan to release it. \n3. Quality criteria designed by the authors for these experiments do a  good job of eliminating ratings from low-quality crowdsourced workers. ",
    "weaknesses": "1. From an experimental design perspective, the experimental design suggested by the authors has been used widely for open-domain dialogue systems with the caveat of it not being done in live interactive settings. \n2. The authors have not referenced those works that use continuous scales in the evaluation and there is a large bunch of literature missing from the paper. Some of the references are provided in the comments section. \n3. Lack of screenshots of the experimental interface ",
    "comments": "Comments: 1. Please add screenshots of the interface that was designed. \n2. Repetition of the word Tables in Line 549 3. In Appendix A.3, the GLEU metric is reference as GLUE.\nQuestions: 1. In table 1, is there any particular reason for the reduction in pass rate % from free run 1 and free run2? \n2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response? \n3. With regards to the model section, is there any particular reason that there was an emphasis on choosing retriever-based transformer models over generative models? Even if the models are based on ConvAI2, there are other language modeling GPT2 based techniques that could have been picked. \n4. In figure 6, what are the models in the last two columns lan_model_p and lan_model? \nMissing References: 1. Howcroft, David M., Anja Belz, Miruna-Adriana Clinciu, Dimitra Gkatzia, Sadid A. Hasan, Saad Mahamood, Simon Mille, Emiel van Miltenburg, Sashank Santhanam, and Verena Rieser. \" Twenty years of confusion in human evaluation: NLG needs evaluation sheets and standardised definitions.\" In Proceedings of the 13th International Conference on Natural Language Generation, pp. 169-182. 2020. \n2. Santhanam, S. and Shaikh, S., 2019. Towards best experiment design for evaluating dialogue system output. arXiv preprint arXiv:1909.10122. \n3. Santhanam, S., Karduni, A. and Shaikh, S., 2020, April. Studying the effects of cognitive biases in evaluation of conversational agents. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (pp. 1-13). \n4. Novikova, J., Du≈°ek, O. and Rieser, V., 2018. RankME: Reliable human ratings for natural language generation. arXiv preprint arXiv:1803.05928. \n5. Li, M., Weston, J. and Roller, S., 2019. Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. arXiv preprint arXiv:1909.03087. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work."
  }
]