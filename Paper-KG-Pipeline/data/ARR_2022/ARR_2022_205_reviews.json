[
  {
    "review_id": "a1e3d5b8b31ba97f",
    "paper_id": "ARR_2022_205",
    "reviewer": null,
    "paper_summary": "The paper introduces an approach for easily incorporating both lexical and syntactic constraints to a model for sentence paraphrasing. Lexical constraints include words that are desirable to appear in the paraphrase, while syntactic constraints involve desired surface forms. Under CGPG, constraints are represented as text sequences that are concatenated to the source sentence. These concatenated inputs are then used to fine-tune pre-trained sequence-to-sequence models (e.g. ProphetNet, BART). A challenging aspect in this task is to obtain training data with constraint information, since datasets for paraphrasing only include <sentence, paraphrase> pairs. The authors experiment with different methods for constraint generation, following previous work. For syntactic constraints, in particular, the authors also propose a new method for extracting exemplars based on syntax similarity, called SSE. An exemplar is a natural sentence with a desired surface form for the output paraphrase. At training time, previous work generates these exemplars by replacing words in the target sentence. In contrast, the authors propose to use other natural sentences in the training set that have similar a syntactic structure to the target sentence. Automatic evaluation shows that GCPG + SSE achieves better scores than previous work on two paraphrasing datasets. The authors also conduct some human evaluation experiments, and analyse some of the characteristics of the output paraphrases to show the benefits of their proposed model. ",
    "strengths": "1. The proposed GCPG approach obtains significant better results than previous work, and appears to be simple to implement. This could motivate its application in other text-to-text generation tasks.\n2. The proposed SSE method for extracting syntactic constraints seems reasonable, and it could be implemented using bigger and more diverse datasets (if I understand the approach correctly), expanding its applicability.\n3. The methodology described is mostly appropriate, which makes the conclusions more reliable. I particularly appreciate that the authors conducted a human evaluation study, and that they attempted to analyse quantitatively some characteristics of the outputs that make them better than previous work. ",
    "weaknesses": "1. More emphasis could be given to explaining what the approach is actually doing better than previous ones. The model’s outputs do obtain higher scores using automatic metrics. However, I think the paper would benefit from more in-depth analysis of where this improvement comes from. Section 4.4 goes in the right direction by showing that models trained using GCPG suffer less from he copying problem and are able to generate more diverse ngrams than simply using ProphetNet or ParafraGPT. However, this does not show if the outputs actually better follow the constraints provided: how frequently and correctly do the outputs do incorporate the lexical items indicated as constraints better than previous work? How frequently and correctly do the outputs follow the syntactic structure of the exemplar (for example) better than previous work?\n2. Automatic evaluation shows (according to Table 2) that GCPG outperforms previous work in almost all metrics, but TED. Do the authors have an intuition (or better yet, evidence) of why this happens? As far as I understand, TED intends to measure how close the syntactic trees of both reference and outputs are, which is indicative of how much the model actually followed the syntactic condition. If that’s the case, then GCPG models are not better than CGEN and SGCP for this important criteria. However, this only happens in the ParaNMT dataset and not in QQP-Pos. Could the authors suggest an explanation for this? Is it related to the metric itself or maybe to some characteristics of the datasets?\n3. In NLG tasks, human evaluation is especially important. As such, I would request the authors to provide more details about the methodology they followed. For instance, it would be helpful to know: (a) how was the quality of the data collected ensured considering how noisy crowdsourced scores can be? what type of quality controls were implemented; (b) what was the interannotator agreement/reliability for the data collected?; ( c) what were the exact instructions provided to the crowdworkers? The authors provide very short explanations for loyalty (meaning preservation? adequacy?), fluency and, specially, syntax similarity. For this last one, in particular, it is important to know how the annotators were instructed (maybe trained?) to assess it.\n4. It is curious that, in the human evaluation study, the authors did not include results for GCPG-S only (Table 4). This would have helped support the statements that combining lexical and syntax constraints contribute to obtain better paraphrases, rather than applying them independently, which is somewhat suggested by the automatic evaluation. ",
    "comments": "1. I strongly recommend to explicitly state from the beginning of the paper the language that is being studied. On why that is important, I would suggest reading **Data Statements for Natural Language Processing: Toward Mitigating System Bias and Enabling Better Science** (Bender and Friedman, 2018) or https://thegradient.pub/the-benderrule-on-naming-the-languages-we-study-and-why-it-matters/ 2. I think readers unfamiliar with the area could appreciate more motivation for controllability in paraphrasing. For instance, in lines 43-44, the authors state that “lacking control might result in undesirable results”. Perhaps mentioning a couple of such “undesirable” outcomes could help with explaining why this type of controllability is important/necessary. Same with line 46, where the authors state “To obtain desirable surface forms..”. It could help to provide an example of a real-world situation/application where a user would want to indicate a desirable syntactic form for the output paraphrase. For example, in Text Simplification, one might know that a certain target audience benefits from reading sentences expressed in active voice. So, one could attempt to control the paraphrasing based on that syntactic structure.\n3. In lines 70-71, the authors state: “Since sentential exemplar are only available for testing, …”. At this point, the reader does not know that this is due to the way the available datasets for paraphrasing have been created (explained later in the paper). I think it would help to briefly explain why this happens, or to link the reader to the section of the paper where this is explained.\n4. Lines 211-214: The authors indicate that the conditions are concatenated to the source sentences via the [SEP] token. At this point, by looking at Figure 2, it would seem that more than one syntactic constraint can be concatenated at the time. However, by looking at the Experiments section, that’s not the case. Maybe it would be helpful to clarify that tin the Figure caption. In addition, how are syntactic and lexical conditions combined before being concatenated to the source sentence? Are they also separated by a [SEP] token among themselves?\n5. The Exemplar Dictionary plays a key role in generating training instances. As such, it would be beneficial for reproducibility to provide more details on how it was constructed. My guess is that for every sentence in the training set, its truncated LCT was obtained, and then a dictionary was created with these LCT being keys. If any other further post-processing or filtering was performed, then I think it should be stated. In addition, could the authors provide more information about the size of the dictionary and the average number of natural sentences for each LCT? Finally, according to my understanding, any dataset could be used to create this Exemplar Dictionary since the LCTs are obtained automatically from any sentence. Is there any specific reason why the authors decided to use the training set and not a perhaps bigger dataset that would provide more variability to the exemplars?\n6.  Lines 301-328. This section explains how the Lexical Conditions are extracted for both the training and the test sets. However, it is confusing, mainly for the test set. At first, the authors state that two strategies are used: one extracting keywords from the references, and another using a sequence-to-sequence model to predict them from the source sentence. Then the authors say that they used three methods: TF-IDF, TextRank and KeyBERT. As far as I know, neither of those three are sequence to sequence models. So, was that second strategy from (Liu et al.,  2020a) actually used? Looking at Table 3, it does not appear anywhere either.\n7. In Table 2, the authors use “B-R” (BLEU-R), shouldn’t it be BLEU-4?\n8. While the paper can be easily read and understood, here I provide a few suggestions for replacing some words or phrases for clarity: - Lines 76-77: there lacks —> there is no - Line 118: significant performances —> very good/high/strong performances - Line 139: provide —> introduce/develop - Line 229: most simple —> simplest - Line 256: use \\citep instead of \\citet for citing But et al. (2021).\n- Line 508: For the lack of space —> Due to space constraints - In several instances (167, 181, 460, etc.) the word “besides” should be replaced by “also” or “in addition”. Please makes a few more passes on the paper to double-check this. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "27c3d47c22badb94",
    "paper_id": "ARR_2022_205",
    "reviewer": null,
    "paper_summary": "Controllable paraphrase generation aims to incorporate various external conditions to generate desirable paraphrases. However, existing works only focus on lexically or syntactically CPG, lacking a unified control framework. The authors propose a general controllable paraphrase generation framework (GCPG) to achieve lexical and syntactical aware CPG. Extensive experiments demonstrate that GCPG achieves state-of-the-art performance on two popular benchmarks. ",
    "strengths": "- The proposed method is simple and effective to achieve lexical and syntactical aware CPG at the same time.\n- The approach to constructing syntax-similarity based exemplars is novel and the experimental results show the effectiveness of this feature than POS Sequence and LCT-Truncated. ",
    "weaknesses": "- The contribution of the whole paper is limited. The proposed framework just concatenate lexical and syntactical conditions as input to achieve the goal of controllable paraphrase generation. There seems to be some innovation in the approach to constructing syntax-similarity based exemplars.\n- Despite numerous experiments, many comparisons seem unfair. The improvement of the proposed method over baselines (row 6-9 in Table 2) may come from using a better backbone model, ProphetNet.\n- From row 2-4 in Table 3, the claim “KeyBERT outperforms other two keyword extraction methods.” is not valid.  TF-IDF outperforms KeyBERT in 5 metrics in all 8 metrics. ",
    "comments": "N/A ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "9cb9dbc88b782b8f",
    "paper_id": "ARR_2022_205",
    "reviewer": null,
    "paper_summary": "The paper proposed a unified framework to exert both syntactic and keyword control for paraphrase generation. The paper proposes to convert both syntactic controlled generation (target syntax can be represented by either masked sequence, linearized constituency tree, masked template, or exemplar sentences) and keyword control to a sequence-to-sequence task formulation. \nThe paper makes the following main contributions: 1. Their new seq-2se1 formulation allows use of pre-trained language models for controlled paraphrase generation. The paper reports impressive gains (>10 R-1 improvement) over prior work for two paraphrase datasets. \n2. For the exemplar-based syntax control: the paper proposes a new method of creating training exemplars. This is different from prior work that only had exemplar availability at test time. \n3. The paper shows that the proposed approach can be simultaneously used for both syntactic and lexical control. ",
    "strengths": "1. This work creates a new SOTA for syntax-controlled paraphrase generation. While my impression is that most of this gain is due to use of pre-trained transformer models (almost all prior work in this direction uses LSTM-based models), this work unifies this prior work into a seq-2-seq framework. \n2. This work is the first to explore joint syntax + lexically controlled paraphrase generation. ",
    "weaknesses": "- Missing ablations: It is unclear from the results how much performance gain is due to the task formulation, and how much is because of pre-trained language models. The paper should include results using the GCPG model without pre-trained initializations.  - Missing baselines for lexically controlled paraphrasing: The paper does not compare with any lexically constrained decoding methods (see references below). Moreover, the keyword control mechanism proposed in this method has been introduced in CTRLSum paper (He et al, 2020) for keywork-controlled summarization.\n- Related to the point above, the related work section is severely lacking (see below for missing references). Particularly, the paper completely omits lexically constrained decoding methods, both in related work and as baselines for comparison.\n- The paper is hard to follow and certain sections (particularly the Experimental Setup) needs to made clearer. It was tough to understand exactly where the exemplar/target syntax was obtained for different settings, and how these differed between training and inference for each of those settings. ",
    "comments": "- The paper should include examples of generated paraphrases using all control options studies (currently only exemplar-controlled examples are included in Figure 5). Also, including generation from baseline systems for the same examples would help illustrate the differences better.  Missing References: Syntactically controlled paraphrase generation:  Goyal et al., ACL2020, Neural Syntactic Preordering for Controlled Paraphrase Generation Sun et al. EMNLP2021, AESOP: Paraphrase Generation with Adaptive Syntactic Control  <-- this is a contemporaneous work, but would be nice to cite in next version.  Keyword-controlled decoding strategies: Hokamp et al. ACL2017, Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search Post et al, NAACL 2018, Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation Other summarization work that uses similar technique for keyword control: He et al, CTRLSum, Towards Generic Controllable Text Summarization ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]