[
  {
    "review_id": "1616a1ec4c7080f2",
    "paper_id": "ARR_2022_260",
    "reviewer": null,
    "paper_summary": "The paper looks at distilling models for abstractive summarization. The paper makes the claim that the most obvious way to do knowledge distillation with seq2seq models (just using the generated output from the teacher model as a target output for the student; also called pseudo-labeling), is problematic. The problems with pseudo-labeling relate to known but important to restate problems with current abstractive summarization models in general: they generally copy from the target document, and they generally only copy the leading line of the target document. The paper attributes the causes for both of these problems being at least in part due to the attention distribution for the teacher models being too sharp, often focusing most of its weight on the next available word. In order to counteract these effects, the paper proposes to raise the temperature for the attention soft max in order to smooth the attention distribution of the teacher model. The temperature scaling modifications are evaluated on three standard datasets (CNN/DM, XSum, and NYT), with slight improvements in distillation performance found across all three datasets. ",
    "strengths": "- The paper is easy to read and follow - A lot of detail has been provided in the paper, making it quite easy for someone to be able to reproduce.  - Evaluations are done on reasonable datasets, against reasonable baselines, and shows promising results. Human evaluations are also provided - Abalations and additional comparisons with obvious baselines (like just using sampling in the teach model for generating pseudo labels) are done. ",
    "weaknesses": "- There is unfortunately not a whole lot of new content in this paper. The proposed method really just boils down to playing with the temperature settings for the attention of the teacher model; something that is usually just considered a hyperparameter. While I have no problem with what is currently in the paper, I am just not sure that this is enough to form a long paper proposing a new method. I think if this paper couched itself as more of a 'analysis/experimental' type of paper, expanding its analysis even further (and maybe expanding its scope to other tasks besides just abstractive summarization), it could be solid contribution. Unfortunately, the paper is presented more as a 'methods' paper, with the new method being proposed simply being a change in hyperparameters. ",
    "comments": "- What hypothesis test are you using for computing the significance in table 2 and 3?\n- Are there other tasks where temperature smoothing could be beneficial? ",
    "overall_score": "2.5 ",
    "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work."
  },
  {
    "review_id": "b1fb88206c3832e9",
    "paper_id": "ARR_2022_260",
    "reviewer": "Sheng Shen",
    "paper_summary": "This paper proposes an extension of the pseudo-labeling method named PLATE for summarization distillation, extensive experiments on three datasets suggest the effectiveness of the proposed method versus vanilla sequence distillation. Ablation studies and empirical analysis reveal the performance gain may come from the diverse cross-attention distribution of the teacher model and concise and abstractive summaries generated from the teacher model. ",
    "strengths": "The paper is well-motivated and clearly presented;  The authors extensively study the effect of Pseudo-labeling with Larger Attention TEmperature on various summarization tasks. Substantial ablation studies and empirical analyses are also provided for revealing the performance gain over baseline methods, which may help future studies understand how to conduct effective summarization distillation. \nThe findings of the diverse cross attention pattern, the conciseness, and abstractiveness of produced summarization pseudo-labels as well as the attention focus of the teacher model help future studies develop better summarization models. ",
    "weaknesses": "From Table 6, the difference of attention pattern results in a great difference in novel-n-grams ratios and length in the generated teacher summarization, but this does not necessarily translate to a better student summarization system in Table 2, further explanation or better ablation studies might be needed to understand how the length, novel-n-grams ratio, the different dataset will affect the distillation effects.  Given that this paper focuses on sequence distillation, wondering how the beam size, length penalty of the teacher model will affect the sequence distillation results? ",
    "comments": "It may be good to know if a better teacher model (BART-large, PEGASUS-large) produces better pseudo-labels for summarization distillation. ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]