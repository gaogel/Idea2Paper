[
  {
    "review_id": "b9a61dd81ac6fc9f",
    "paper_id": "ARR_2022_242",
    "reviewer": null,
    "paper_summary": "The paper introduces a framework for learning a taxonomy using cognitive data, e.g., fMRI data. The authors focus on the NLP domain and compare the performance of this method with the previously used visual Taskonomy method (Saaty, 1987, Zamir et al., 2018). They find that their method performs better than random and that it provides taxonomy results that seem intuitive. While the method doesn't provide performance as high as the one in Zamir et al. (2018), it requires less compute and might therefore function as a potential alternative. ",
    "strengths": "The authors take an influential taxonomy idea from the visual domain and explore it in the domain of NLP. The learned taxonomy is based on cognitive data, providing an interesting approach to the task. ",
    "weaknesses": "In my view, the paper would benefit from refocusing on exploring its main contributions. First of all, the abstract states that the method results in \"competitive [performance] to the Analytic Hierarchy Process (Saaty, 1987) used in visual Taskonomy (Zamir et al., 2018)\". When inspecting the results, the performance is better than random but still far from the performance of the AHP method. The core contribution, therefore, seems to be that their method doesn't require expensive transfer learning between tasks. This point should be highlighted and explained further: Where does this become especially relevant and to what extent does it matter? The authors write that more advantages over AHP will be shown later (line 533ff) but it remains unclear to me when and where we will learn about those.\nSecondly although the CogTaskonomy framework is introduced as a combination of two separate methods, there is no analysis how important their combination is. CRA seems to be more important than CNM and in combination with the BERT model, their combination barely matters. Is there an intuition why one would be more important than the other? Similarly, a comparison of which cognitive data is used might be interesting as well (i.e., fMRI vs. eyetracking data, for instance).\nFinally, the taxonomy itself could be an interesting contribution but its exploration remains very limited. The authors visualize the taxonomy learned by one component of the CogTaskonomy framework (CNM) which also seems intuitive (Figure 4). However, it remains unexplored whether this is the same for all tested methods or even just what it would be for the complete CogTaskonomy framework.\nOverall, I found the paper slightly difficult to follow. Making the contribution more pronounced and how this specifically connects to the results and future opportunities might help with this. A more direct method comparison with respect to data needed, training pipeline and other important points of distinction would have also helped me. There were also a couple of conclusions that didn't follow for me from the data which I'll specify in the comments and suggestions. ",
    "comments": "*Conclusions that didn't quite follow for me:* - line 511ff: I can't follow why a higher performance of TinyBERT over BERT suggests that TinyBERT \"mak[es] sentence representations more relevant to individual tasks [through knowledge distillation] and hence result[s] in better task similarity estimation.\"\n- line 551ff: \"RSA is able to be adapted to NLP task structure detection\" seems to be already argued with a single configuration where it outperforms random.  - line 572ff: \"This is consistent with our previous finding in the main results that TinyBERT (with KD) captures more task-relevant knowledge than BERT for task relation detection.\" This is argued based on the finding that \"with a small number of cognitive signals (voxels), TinyBERT for CNM can achieve a good task ranking score\" (line 566ff). Is there a way to make this more intuitive?\n*General comments:* In the contributions (line 110), the authors state that the \"CNM is able to learn stable task relations\" (line 128f). However, this is only one of two components of the the proposed CogTaskonomy framework. What about the CRA?\nThe phrase \"sentence-level textual stimuli of cognitive data\" (e.g., line 259f) is very obscure. I recommend expanding on it once to clarify.\nI didn't understand the paragraph 4.1 Cognitive Dataset. Specifically, it's not intuitive to me why the voxels had to be randomly selected (line 400f).\nI think it would be useful to mention to the reader that high scores are undesirable in the TRS metric (e.g., Table 1 and Figure 3).\nThe authors try to make an argument across architectures but only based on BERT architectures which seems a weak generalizability claim. I would suggest either weakening the phrasing or providing more models.\n*Typos:*  - abstract: CogTaxonomy -> CogTaskonomy - line 83f: clarify that these terms are introduced by the authors and are not pre-existing parts - line 265: concepts - line 557: more robust ",
    "overall_score": "2.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]