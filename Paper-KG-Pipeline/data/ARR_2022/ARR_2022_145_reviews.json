[
  {
    "review_id": "7dbcd74204930388",
    "paper_id": "ARR_2022_145",
    "reviewer": "Siqi Sun",
    "paper_summary": "The paper proposed a promising indicator, gradient accordance, to alleviate Flooding method from hyper-parameter search. The performance of the algorithm is tested on five datasets, i.e., IMDB, AG News, SST-2, QNLI and MRPC, and achieved state-of-the-art robust accuracy. ",
    "strengths": "The paper works on an important task and the proposed metric is novel. Though Flooding method works very well if we search the flood level carefully, it is a very tedious job and could take lots of time if the size of the dataset is huge. Based on the agreement of different batches in the same epoch, the agreement between them could be used as an indicator of overfitting, and further applied to decide the flood level.  This paper tests the performance of the proposed indicator on a variety of tasks and achieved good results on some of them. Also, the analysis in the section 5 is also very helpful to the general audience as it includes detailed results and reasoning about why and how the proposed method works. ",
    "weaknesses": "I will list the detailed weakness in the next section.  In general, I think the claim in line 109 to 112 is incorrect (or overlap with the next claim). Based on my understanding, what this paper does simply is to provide an indicator to prevent tedious search on flood level. Yet the Flooding method, its ability to train a more robust model is NOT the contribution of this paper. In addition, some results in table 1 may not be reliable. ",
    "comments": "1. \" a smoother loss\" in line 184. To me the loss curves for different algorithms are all smooth, how do you define a **smoother** loss in this scenario? In addition, based on my understanding, a flooding-based algorithm will alternative between gradient descent and ascent when the loss is under some threshold, I was wondering what does the training loss look like under the same setting?  There is a text overlap between 0.00 and 0.03 around the origin.\n2. In Eq 13 around line 300: The summation is over $i$ and $j$ but they do not appear in $S_{batch accd}$, should they be $s$ and $t$? Also, it seems the computation of $S_{epoch accd}$ depends on four summation loops if we replace $S_{batch accd}$ by its definition, what is the computation cost for $S_{epoch accd}$?\n3. The reasoning between line 448 ~ 452 is confusing. Your re-implemented results are 97.0 and 91.6, which is better than Flooding-X, yet the claim in paper is \"our method is also the **best** among all the baseline methods\", which is a contradiction to me. Later the paper suggests that \"outperforming the **baselines** of our implementation.\", what is the difference between \"baseline\" and \"your re-implemented results\".  4. Some introduction to the datasets might also be helpful, such as the size of each dataset.  5. Why do you use BERT as the baseline method instead of RoBERTa, as RoBERTa outperforms BERT by a large margin in terms of clean accuracy on these datasets?\n6. I am a little confused about the test set performance on these GLUE datasets, are the labels to these sets not available to the public?\n7. The clean accuracy on SST-2, QNLI and MRPC is also confusing. Based on the results, vanilla BERT outperforms FreeLB on all three datasets, yet the FreeLB paper claims they could surpass RoBERTa baseline on all GLUE datasets, which makes me wonder if there are some implementation issues. I am also interested in the results on QQP and RTE, but they are not reported here.  8. In figure 3, the test loss increases after some epoch, will the clean accuracy drop with the increased test loss? If not why do you use test loss as the measurement of overfitting?  9. Line 22, \"Bert\" --> \"BERT\" ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "77c0de7520f2bc8b",
    "paper_id": "ARR_2022_145",
    "reviewer": null,
    "paper_summary": "This paper applies a method known as *flooding* [1] when fine-tuning BERT-base on various downstream tasks. Flooding is a regularizer that prevents the model from converging to zero training loss by choosing an appropriate *flooding level*. Moreover, according to the authors, flooding encourages the model to converge to a flat region of the loss landscape.  The authors find that fine-tuning BERT-base with flooding significantly improves its robustness, and improves over all baselines in most settings, against three adversarial attacks (TextFooler, TextBugger, and BERTAttack) from the TextAttack framework.\nBeyond the experimental contribution, the authors also propose an efficient way to find the *flooding level*, a crucial hyperparameter of the flooding method, based on *gradient accordance*. Moreover, they convincingly analyse and discuss their contributions.\n[1] https://arxiv.org/abs/2002.08709 ",
    "strengths": "- The paper shows that adversarial robustness of BERT-base against common attacks can be significantly improved by training with a regularizer instead of leveraging adversarial training, which is computationally more demanding.\n- The authors contribute an interesting method which makes the flooding method usable for fine-tuning and relate it to the notion of overfitting. This might be an interesting method for other NLP settings beyond robust training.  - The experimental results are strong and the authors provide additional analysis investigating their proposed methods. ",
    "weaknesses": "- The PGD adversarial training baseline might be a weak baseline as the authors restrict the number of gradient steps to 5. I would recommend to run PGD with more steps, at least for one of the downstream tasks, to provide a stronger baseline. ",
    "comments": "- I don't understand what Figure 1 shows. Also, It's not clear to me how the paragraph starting from line 183 connects to the previous discussion. Maybe you can clarify this?\n- Which attack was used to compute accuracy under attack in Figure 2? Could you add additional lines showing accuracy under attack against more than one method? This would illustrate if the optimal flood level generalizes across attacks.\n- Figure 3 is a bit hard to read. Maybe move MRPC to a separate figure and put it in the appendix?\nThe authors might be interesting in this concurrent work (https://arxiv.org/abs/2104.04448) from the computer vision community  that studies the relationship between adversarial robustness and flatness. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]