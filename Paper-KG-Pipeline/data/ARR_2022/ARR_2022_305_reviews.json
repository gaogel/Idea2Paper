[
  {
    "review_id": "221ad7f949cb1f5c",
    "paper_id": "ARR_2022_305",
    "reviewer": "Peifeng Li",
    "paper_summary": "The author proposed a novel detection approach that separates factual from non-factual hallucinations of entities. Empirical results suggest that this method vastly outperforms two baselines in both accuracy and F1 scores and has a strong correlation with human judgments on factuality classification tasks. Furthermore, the author used this method as a reward signal to train a summarization system using an off-line reinforcement learning (RL) algorithm that can significantly improve the factuality of generated summaries while maintaining the level of abstractiveness. ",
    "strengths": "1) One of the main contributions of this paper is to construct a dataset annotating entity- or token-level hallucination.\n2) The author proposed a method using an entityâ€™s prior and posterior probabilities to infer whether it is hallucinated and factual, and the experiment shows that the results are much better than the baselines. ",
    "weaknesses": "1) The definition of the factuality of hallucinated entity is controversial. Although the author mentioned the leverage of tools such as Wikipedia or Google search in l357-l365, it still has strong subjectivity. \n2) Lacking strong baselines both on entity-level factuality evaluation and summarization. ",
    "comments": "1) Where does the labeled entity come from? Is the complete manual annotation or extracted by the named entity recognition tool? If the latter, will cascading errors be introduced? \n2) The author mentioned that calculating the inter-annotator agreement between annotators, so whether all 800 documents are double-annotated? \n3) The BART large model is used to train CMLM and MLM, and the entities are also generated by BART on XSUM. Will there be a certain correlation between them that caused the improvement? \n4) I wonder that since the author focused on factual hallucinations, why does the author always separate factual evaluation from hallucination evaluation? ( such as Table 3 and Table 7). ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "2e42e7a204fa5d02",
    "paper_id": "ARR_2022_305",
    "reviewer": null,
    "paper_summary": "This work presents a study of factual hallucinations, wherein summarization models hallucinate information (i.e. generate information not present in the source document) which is still factually correct. An example of this would be background knowledge about the summarization topic. The main contributions are: (1) an annotation of a subset of the XSUM summarization dataset at the entity level, including both hallucination, and factuality (2) A simple, yet effective classification method for the labels of 1, using a KNN classifier on conditional and unconditional entity probabilities as well as presence in the source document (3) Use of their classifier to guide offline RL training of a summarizer to improve factuality. Authors find the classifier is effective and correlates well with humans, and the RL summarizer achieves better faithfulness/factuality than baselines while not significantly sacrificing ROUGE. ",
    "strengths": "From my perspective, the strongest aspect of the paper is the annotated dataset. The value of entity-level annotations for hallucination and factuality for XSUM will be very useful for future work, both for training new methods and allowing comparison across a common, fine-grained benchmark for factuality/hallucination classification. \nThe paper also focuses on the notion of factual hallucination, which is often treated the same as non-factual hallucination in works on factuality. Whether or not I personally agree with the current feasibility of factual hallucinations to be useful in summarization systems, this is a question that warrants more discussion such as this work.   The experiments presented are quite extensive, spanning multiple datasets and generally demonstrating that the proposed methods are effective. \nThe notion of negatively weighting non-factual entities in the offline RL setting is interesting, and a natural way to incorporate the classifier into training. ",
    "weaknesses": "Generally, I found the evaluations could have been stronger. First, there are only two classification baselines and both of them are rule-based, given a fixed set of simple features. The paper does not discuss in detail why a method using simple probability/overlap based features should in general be better than simply training a classifier on the available data (e.g. a linear layer on top of RoBERTa), nor does it compare to a trained baseline. If the point of the proposed method is that the selected features are particularly strong, then the authors should compare to other features (i.e. neural). If the point of the method is that KNN is a good choice for these features, then the authors should compare to other algorithms on the same features. I particularly focus on the evaluation of the classifier because later sections of the paper (i.e. the RL-based model) use the classifier as a sub-routine, and so demonstrating its efficacy is particularly important. \nSome minor concerns: - The work suggests that hallucinations can be described by hallucinated entities, which may be factual or not. I found this a bit unnatural, as the entities themselves are unlikely to be factual or non-factual in isolation. Rather, facts involving these entities can be classified either way. This may be something to elaborate on in future versions of the work.\n- While I found the dataset to be useful, it is generated using one summarization model (based on BART) which may not well describe the kinds of mistakes that other models make, especially as new architectures/pretrained models are released in the coming months/years. As such, the dataset may have limited usefulness in the further future. ",
    "comments": "As mentioned above, I would suggest including further baselines for the classification task. If it is financially feasible, I would also suggest a small-scale human evaluation of even a subset of the summarization systems. It is difficult to draw strong conclusions based only on automatic notions of factuality and quality, particularly when we are comparing the two. My understanding is that the goal of this work is to show that methods such as the one presented here can improve factuality and factuality in hallucination without significantly damaging the quality of the summarization model. Without at least some human judgement, it is difficult to judge whether or not this holds. ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]