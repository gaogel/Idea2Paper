[
  {
    "review_id": "02d4ef5de02582e2",
    "paper_id": "ARR_2022_183",
    "reviewer": null,
    "paper_summary": "This paper expands on previous work, that has found that MT QE is considerably biased towards the fluency of the translation, often disregarding the source sentence (adequacy). The authors analyze a known dataset for the existence of this particular bias for 7 language directions and show that there is bias for two of them (en-de and en-zh). Consequently, they proposed 4 modifications of the successful monoTransQuest model, in which the QE model is jointly trained with another task: a bilingual QE, a QE augmented with binary classification of correct vs. artificial false translations, an adversarial task of not being based on target fluency and the debiased focal loss. Finally, they perform experiments of all these modifications against the baseline, and they show that the system augmented with the binary classifier gives the better tradeoff of bias reduction and increasing QE performance. ",
    "strengths": "- authors show good analysis and understanding of the problem - the strategy of training QE with complementary tasks is novel  - intuitive choice of complementary tasks and building of the NN architecture - positive results ",
    "weaknesses": "1. the authors show that the bias appears only in two out of seven language directions, which pretty much depends on the quality of the MT systems that produced the MT outputs on which the QE systems were trained. It should be clear that for other language pairs or even datasets, other or no de-biasing may be needed. \n2. the ru-en shows biasing towards the source sentence. Although this language direction is included in the results, there are no conclusions related to it or efforts for debiasing of this case. \n3. In section 3.1 there is a lot of space used for a dataset that is already known, unless this refers to newly released data together with this paper (confused a bit). \n4. In section 3.3: what was the process of the manual annotation? How many annotators worked on that, what was their expertise, language fluency. Were there any inter- or intra-annotator statistics? ",
    "comments": "Will the authors make the software available in a repository? ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "c41a369099b82fe3",
    "paper_id": "ARR_2022_183",
    "reviewer": "Shuo Wang",
    "paper_summary": "This paper focus on mitigating the partial input bias of neural QE models, which means that some strong QE models tend to over-rely on the translation while ignoring the source sentence to some extent. To solve this problem, the authors propose to use auxiliary tasks to debias the QE model, including related tasks and adversarial tasks. Moreover, the authors also attempted to use focal loss to further reduce the partial input bias in the QE model. Experiments indicate that the involved approaches all can effectively help the QE model pay more attention to the source sentence. ",
    "strengths": "1. The research problem is clearly illustrated in this work. \n2. The proposed approaches is straightforward and easy to understand. ",
    "weaknesses": "1. Given that this work aims to investigate the partial input bias of QE models, more other representative QE models are required to demonstrate that the partial input bias problem is important and the proposed methods are effective. The authors only conduct experiments using one QE model, making the results less convincing. \n2. Figure 1 shows that the partial input bias problem is not severe in some other language pairs, such as RO-EN and ET-EN. The authors did not provide any detailed explanation of this phenomenon, which weakens the motivation to address the partial input bias for QE models. ",
    "comments": "1. I suggest to conduct more experiments using more QE models on a wider range of language pairs. \n2. Some examples may make it better to understand why partial input bias exists. ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "94db9840113bd974",
    "paper_id": "ARR_2022_183",
    "reviewer": null,
    "paper_summary": "The paper highlights an important issue of an existing bias in the SoTA methods of Quality Estimation. Due to this bias, the methods tend to only consider the features associated with the generated translations (and not the source sentences) for assessing the overall quality. The authors attribute the cause of this bias to both the model and the human annotators as they tend of provide a high score to fluent but inadequate translations.\nThe main contribution of this paper is to analyze the partial input bias problem and explore different approaches using auxiliary tasks for mitigating this bias. The proposed methods are designed by keeping in mind that they should aid in reducing the bias, be comparable or superior to the original performance and not lead to extensive computational overhead. The authors experiment with multitask and adversarial techniques and find that the former yields better results. \nThe approaches are evaluated on two high resource language pairs i.e. En-De and En-Zh. ",
    "strengths": "1. The paper is well written and structed and the motivation is clearly laid out i.e. exploration of bias reduction methods for Quality Estimation. \n2. The suggested methods would aid in reducing the problem of input bias without the need to collect additional annotated data and expensive modification to the original model. \n3. Extensive evaluation of fluent but inadequate translations for En-De revealed that the bias is not only an artifact of the modelling technique but also the annotation mechanism. \n4. For the two languages, the proposed method not only reduces the partial input bias significantly but also retains the overall quality based on SoTA baseline. ",
    "weaknesses": "1. The evaluation mechanism only takes into account the bias problem related to high resource and from-English languages. This raises a question on the generalizability of the method to medium/low resource and into-English languages. Although it is understandable that the existing MLQE dataset majorly exhibits the bias problem for En-De and En-Zh, but evaluating the method for other languages could strengthen the claim about the efficacy of the proposed approaches. ",
    "comments": "Suggestions: 1. Move the Appendix header to page 11 Questions: 1. Based on Fig 1(a), it seems like even Et-En and Si-En suffer with the problem of partial input bias when evaluated on DA scores. Were there any experiments performed to assess the efficacy of proposed methods for these languages as well ? ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]