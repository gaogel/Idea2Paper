{
  "paper_id": "ARR_2022_206",
  "title": "HiStruct+: Improving Extractive Text Summarization with Hierarchical Structure Information",
  "conference": "ARR",
  "domain": {
    "research_object": "该论文主要研究的是文本数据，具体聚焦于文本的抽取式摘要任务。",
    "core_technique": "论文采用并改进了层次结构信息的建模方法，结合了深度学习技术，可能包括Transformer等主流神经网络架构，以提升文本摘要的效果。",
    "application": "成果可应用于新闻摘要、文档自动生成摘要、信息检索、智能问答等自然语言处理相关场景。",
    "domains": [
      "自然语言处理",
      "文本摘要"
    ]
  },
  "ideal": {
    "core_idea": "显式提取、编码并注入层次结构信息以提升单文档抽取式文本摘要性能。",
    "tech_stack": [
      "Transformer Language Model (TLM)",
      "BERT",
      "RoBERTa",
      "Longformer",
      "层次结构编码",
      "句子线性位置编码",
      "句子层次位置编码",
      "自注意力机制",
      "堆叠Transformer编码器"
    ],
    "input_type": "包含内部层次结构（如章节、段落、句子）的长文本或科学论文",
    "output_type": "每个句子的摘要包含置信度（二分类标签或分数）"
  },
  "skeleton": {
    "problem_framing": "论文通过强调文本（尤其是长文档）具有内部层次结构（如章节、段落、句子、词元），而人工摘要时会利用这些结构信息，引出问题。开篇以实际应用痛点为切入点，指出人类在摘要时会关注如“方法”“讨论”“结论”等重要章节，而对“背景”等部分关注较少，强调理解层次结构对于判断重要句子的必要性。随后自然过渡到神经网络模型也应当利用这些结构信息，为后续方法提出奠定基础。",
    "gap_pattern": "论文批评现有方法时，采用了‘现有方法忽视了X’的逻辑。具体指出当前主流的Transformer类预训练语言模型（如BERT）仅通过线性位置编码建模顺序关系，未显式考虑文本的层次结构信息。通过对比已有SOTA方法（如BERTSUMEXT），强调它们在处理长文档、层次结构明显的科学论文时存在局限，未能充分利用章节标题和句子的层次位置信息。",
    "method_story": "方法部分采用‘先整体后局部’和‘分模块介绍’的叙述策略。首先整体介绍HiStruct+模型的架构，包括基础TLM编码器和两层句间Transformer。随后详细说明如何通过插入BOS token、线性位置编码、层次位置编码、章节标题编码等方式，将层次结构信息注入句子表示。最后介绍这些增强表示如何输入到后续层进行层次化建模和分类。方法流程清晰，分步骤递进，每一模块的作用和可选性都被明确指出。",
    "experiments_story": "实验部分采用‘多数据集验证+与主流方法对比’的策略。首先在多个数据集（CNN/DailyMail、PubMed、arXiv）上进行主实验，涵盖短文档和长文档，展示模型的通用性和有效性。实验内容包括与抽取式、生成式、混合式SOTA方法的对比，使用ROUGE指标系统评估。表格中突出标记改进项和SOTA超越点，强调HiStruct+模型的优势。此外，实验还通过与移除结构信息的基线模型对比，验证结构信息注入的有效性。"
  },
  "tricks": [
    {
      "name": "问题场景具象化",
      "type": "writing-level",
      "purpose": "让读者迅速理解研究背景和实际需求，提升问题的现实感和重要性",
      "location": "introduction",
      "description": "通过举例科学论文的结构，强调文本的层次结构在人工摘要中的作用，将抽象问题具体化。"
    },
    {
      "name": "现有方法局限性强调",
      "type": "writing-level",
      "purpose": "突出研究空白，增强新方法的必要性和创新性",
      "location": "introduction",
      "description": "指出主流Transformer模型只考虑线性顺序，未显式利用层次结构，为新方法铺垫合理性。"
    },
    {
      "name": "方法创新点突出",
      "type": "method-level",
      "purpose": "突出新方法的独特性和技术贡献，增强新颖性",
      "location": "introduction / method",
      "description": "反复强调HiStruct+模型显式提取、编码和注入层次结构信息，并提出新型编码方法。"
    },
    {
      "name": "模型架构可视化",
      "type": "method-level",
      "purpose": "帮助读者直观理解方法原理和流程，提升可解释性",
      "location": "method",
      "description": "通过图示（Figure 1）展示模型整体架构和各组件的关系，降低理解门槛。"
    },
    {
      "name": "模块化描述",
      "type": "method-level",
      "purpose": "让方法结构清晰，便于理解和复现，增强可解释性和完备性",
      "location": "method",
      "description": "将模型分为基础TLM、层次信息注入、两层Transformer和输出层，逐步阐述各部分功能。"
    },
    {
      "name": "可选组件与消融对照",
      "type": "method-level / experiment-level",
      "purpose": "突出方法灵活性，并为后续消融实验做铺垫，增强对比性和完备性",
      "location": "method / experiments",
      "description": "明确指出HiStruct信息注入组件可选，去除后即为主流baseline，为实验对比提供依据。"
    },
    {
      "name": "多数据集覆盖",
      "type": "experiment-level",
      "purpose": "证明方法适用性广泛，增强实验完备性和结论可靠性",
      "location": "experiments",
      "description": "在短文（CNN/DailyMail）和长文（PubMed、arXiv）三个主流数据集上系统评测方法性能。"
    },
    {
      "name": "多指标量化对比",
      "type": "experiment-level",
      "purpose": "用权威指标量化方法效果，提升说服力和对比性",
      "location": "experiments",
      "description": "采用ROUGE-1/2/L三项指标，系统展示各模型的性能差异。"
    },
    {
      "name": "与SOTA及多类方法对比",
      "type": "experiment-level",
      "purpose": "突出自身优势，增强说服力和对比性",
      "location": "experiments",
      "description": "与当前最优抽取式、生成式、混合式模型系统对比，并用符号标记性能超越。"
    },
    {
      "name": "结果细致分块展示",
      "type": "writing-level",
      "purpose": "让读者清晰区分不同模型类别和实验结果，提升叙事结构和可读性",
      "location": "experiments",
      "description": "表格分块展示抽取式、生成式、混合式及自身模型结果，突出各自表现。"
    },
    {
      "name": "结论与方法呼应",
      "type": "writing-level",
      "purpose": "强化方法有效性，形成逻辑闭环，提升叙事结构",
      "location": "experiments",
      "description": "在结果分析中反复强调HiStruct+模型带来的性能提升，呼应引言中提出的问题和方法创新。"
    },
    {
      "name": "消融实验与误差分析",
      "type": "experiment-level",
      "purpose": "证明方法细节的有效性，提升实验完备性和结论可靠性",
      "location": "experiments",
      "description": "通过消融实验和附录讨论，分析不同结构信息注入对模型性能的影响。"
    }
  ]
}