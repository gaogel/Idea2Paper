[
  {
    "review_id": "28224ff4d7b034bc",
    "paper_id": "ARR_2022_336",
    "reviewer": null,
    "paper_summary": "This paper pursues a line of research about 'prompting', a method that leverages the prediction capability of pre-trained language models to help perform few-shot classification.  The paper explores one of the research topics in this line of research: how to find good mappings from the original task labels (e.g., positive vs negative) to tokens that can be predicted by the language model (the `label words', e.g., \"great\", \"perfect\" vs \"terrible\", \"awful\").  More specifically, the authors propose a simpler method to use multiple labels words; and a selection method that makes sure that no label word is shared by multiple labels. \nExperiments are presented, with comparison to baselines and earlier methods (Shick et al., 2020; Gao et al., 2021), followed by analyses and a discussion. \nThis is a second revised version. ",
    "strengths": "- The authors propose a simple method to use multiple label words per class by summing their language model probabilities, and does not need to perform fine-tuning during label-word search.  Gao et al. (2021) instead only keep the best label word for each class, and perform fine-tuning multiple times during label-word search.\n- Tested on seven classification tasks in the BLUE benchmark, the method performs better in general than earlier prompting methods on few-shot experiments (n=16 examples in each class in train and in dev).\n- Each experiment is performed five times, mean and standard deviation are provided.\n- The Analysis and Discussion sections count three pages including tables and figures, and show example label words, examine the impact of deduplication, multiple labels, and label mapping, the impact of the number of training examples, and attempt to analyze why this methods works. ",
    "weaknesses": "- (minor) Studying the correlation between inter-class JS divergence and performance is a good point.  However, I found its results not entirely convincing because it depends on the presence of two of the six tasks: CoLA and SST-2.  CoLA is a quite different task, and the authors also report the correlation without this task, which becomes much smaller.  At that point the presence of a positive correlation entirely depends on the presence of SST-2.  Or to phrase it differently, no such correlation is observed in the subset of tasks {QQP, MRPC, QNLI, RTE}. ",
    "comments": "- The authors have taken into account all my previous comments, and plan to look into the correlation point above. ",
    "overall_score": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]