[
  {
    "review_id": "4c402a34bfd0f966",
    "paper_id": "ARR_2022_303",
    "reviewer": null,
    "paper_summary": "This paper presents a new citation-based related work dataset, CORWA, as well as a baseline model that tags against the CORWA dataset, that is built over the S2ORC dataset, specific to NLP. The authors take the now-common, end-to-end neural network perspective in training a model to tag references but specifically contributing a model for citation-driven citation fragments. The authors decompose the task as three joint tasks of discourse tagging, citation span detection and citation type recognition. The authors also discuss a human in the loop (HITL) environment for accomplishing such work, based on their incremental, correction-based annotation of a trained transformer model in their annotation loop.\n(This is a v2 submission and since the paper hasn't substantially changed, I've left the summary the same as in v1) ",
    "strengths": "-  Well written and edited. I did not find much issues with the English use and the paper reads fluently.\n- Propose a new taxonomy of related work sentences (S3.1.1).\n-  Surprisingly high interannotator agreement, albeit with just two annotators. However, it is not clear whether the annotators are part of the authoring team and this should be disclosed on first mention.   - Finding that reference vs dominant citation types differ in their citation span lengths and strategies to generate, and contributing the longformer-encoder-decoder model towards appropriate modeling of this goal. ",
    "weaknesses": "- Citation type recognition is limited to two types –– dominant and reference –– which belies the complexity of the citation function, which is a significant line of research by other scholars. However this is more of a choice of the research team in limiting the scope of research.\n- Relies on supplemental space to contain the paper.  The paper is not truly independent given this problem (esp. S3.1 reference to Sup. Fig. 6) and again later as noted with the model comparison and other details of the span vs. sentence investigation.\n- The previous report of SciBERT were removed, but this somewhat exacerbates the earlier problem in v1 where the analyses of the outcomes of the models was too cursory and unsupported by deeper analyses.  However, this isn't very fair to write as a weakness because the current paper just simply doesn't mention this.\n- Only having two annotators for the dataset is a weakness, since it's not clear how the claims might generalise, given such a small sample.\n- A summative demographics is inferrable but not mentioned in the text.  Table 1's revised caption mentions 2.9K paragraphs as the size. ",
    "comments": "This paper is a differential review given that I previously reviewed the work in the Dec 2021 version submitted to ARR. \nThere are minor changes to the introduction section, lengthening the introduction and moving the related work section to the more traditional position, right after the introduction.\nThere are no rebuttals nor notes from the authors to interpret what has been changed from the previous submission, which could have been furnished to ease reviewer burden in checking (I had to read both the new and old manuscripts side by side and align them myself) Many figures could be wider given the margins for the column.  I understand you want to preserve space to make up for the new additions into your manuscript, but the wider margins would help for legibility.\nMinor changes were made S3.3 to incorporate more connection to prior work.  S4.1 Model design was elaborated into subsections, S5.2.1 adds an introduction to LED.\n462 RoBERTa-base ",
    "overall_score": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "38eda9b605f5b516",
    "paper_id": "ARR_2022_303",
    "reviewer": "Irene Li",
    "paper_summary": "This paper introduces the Citation Oriented Related Work Annotation (CORWA) dataset for related work generation. The dataset contains three annotation tasks: discourse tagging, citation span detection and citation type recognition. Another main contribution is a baseline model which could tag the CORWA labels from unlabeled texts. Besides, this paper also proposes a novel framework for related work generation. ",
    "strengths": "The task of related work generation is very important in the academic research domain; The baseline model that does the joint-related work tagger is interesting as unlabeled data is used, compared with the previous version, the description and novelty are more clear. \nThe CORWA dataset is a large contribution in this field which fills the gap, and the analysis and statistics are comprehensive. ",
    "weaknesses": "The paper organization could be improved. I understand that there should be details about the creation, evaluation and analysis of CORWA. With the space limit, some evaluation tables are moved to supplementary material. Table 6 might be more important but it is not in the main body. It is possible to make Figure 2,6 smaller to save some space. ",
    "comments": "What is the goal for section 6? This section solves the task of full related work generation, but hard to understand how this is related to the main work.\nTable 1: the width of the table borderline seems different in each row. And this table has a different style/format with other tables. ",
    "overall_score": "3.5 ",
    "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work."
  }
]