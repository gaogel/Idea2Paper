[
  {
    "review_id": "a42c480628723aff",
    "paper_id": "ARR_2022_70",
    "reviewer": null,
    "paper_summary": "The paper proposes 6 test corpora for vision and language captioning systems that target specific competency. For each competency, examples are generated semi-automatically from existing language + vision tasks, such QA in V7W, and are created in a FOIL style, where one example correctly describes the image, while another example makes a minimal change to caption and does not describe the image. Systems are then challenged to prefer captions that correctly identify the image. The competencies tested include existence, plurality/counting, spatial reasoning (via prepositions), situational knowledge (via imSitu data), and coreference. The paper evaluates several recent pre-training based models, finding that many fail at their challenges, and that the multi-task model 12-in-1, works best. ",
    "strengths": "Proposes a fairly diverse set of challenges that could be a useful diagnostic going forward.\nThe paper evaluates currently relevant model on the diagnostic, establishing clear baselines for their dataset moving forward.  Because the paper encompasses essentially 5 independent datasets, it a very substantial body of work. It seems larger than a standard paper. ",
    "weaknesses": "(being a previous reviewer R BWRg, I will respond to previously identified weakness) I still find the argument of what is and is not included in the diagnostic unclear. In many ways, this seems like a case of a subset of competencies that we have enough visual annotations to semi-automatically create data for. In my opinion, the paper should steer away from making arguments that these examples are deeply linguistic, beyond, involving nouns, counting, verbs, and coreference. As such, I find the title and some of the introduction over-claiming, but, this is really a matter of opinion, resting on what exactly 'linguistic' means.\nThe main body of the paper still lacks examples but I appreciate their inclusion in the appendix. It's very hard to imagine the foils from the descriptions alone. This may be asking a lot, but the paper would be significantly improved if the last page were almost entirely made of examples from the appendix. This is a CVPR style of presentation, and would require significant text trimming.  The examples were good overall, but the co-ref part of the benchmark stands out. It is essentially a QA task, which isn't really compatible with just caption based training that most of the evaluated most are setup to do (with the exception of 12-1). This isn't an issue, because its not really the benchmark's problem, but I am not sure the format of the foil is that sensible. I suspect this will be the least used of the new foils, but I don't have a concrete proposal how it could be improved to really be a captioning task. ",
    "comments": "- ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]