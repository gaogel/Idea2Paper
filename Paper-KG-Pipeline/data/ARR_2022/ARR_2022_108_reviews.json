[
  {
    "review_id": "a14ce9efad302115",
    "paper_id": "ARR_2022_108",
    "reviewer": null,
    "paper_summary": "This paper studies whether and how contextual modeling in DocNMT is transferable via multilingual modeling. The authors explore the effect of 3 factors on the transfer with simple concatenation-based DocNMT: the number of teacher languages with document level data, the balance between document and sentence level data at training, and the data type of document level data. The experiments are conducted on Europarl-7 and IWSLT-10 show the feasibility of multilingual transfer for DocNMT, particularly on document specific metrics. ",
    "strengths": "A simple approach uses different proportions of sentence-level and document-level data to train and study its impact on translation performance. In addition, a large number of experiments were done to analyze the causes and human evaluation of its generation. The motivation of this paper is worthy of recognition. ",
    "weaknesses": "1. First of all, compared with other excellent papers, this paper is slightly less innovative. \n2. The baseline is is not strong enough. Expect to see experiments that compare with the baseline of the papers you cited. \n3. p indicates the proportion of documents, I would like to know how the parts of sentences and documents are extracted? Do the rules of extraction have any effect on the experiment? I hope to see a more detailed analysis. \n4. It lacks case study to show which document-level translation errors are improved by the proposed method. ",
    "comments": "1. It is suggested to add a structure diagram to illustrate your proposed method. \n2. It is suggested to add some case studies to clarify the problems you have solved, so as to clearly show your contribution. \n3. The authors are encouraged to proofread the paper more carefully and explain their methods more clearly. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "7e23ee6e56ace10f",
    "paper_id": "ARR_2022_108",
    "reviewer": null,
    "paper_summary": "The paper addresses the problem of document-aligned data scarcity by attempting to transfer contextual knowledge from language pairs with more document-aligned data to ones with less or even without any via training multilingual MT models using the concatenation approach. The reported results show that while the sentence-level metric BLEU decreases in many cases when compared to the baseline, document-level specific metrics reflect improvement, which is also backed up by human evaluation. The authors experiment with varying the number of teacher and student languages (from which to which transfer is made), adjusting the data schedule (amount of document-level data used for training), and also using back-translated documents for additional training data. They find that the best results are achieved with several teacher models and back-translated documents help just as much as genuine documents. ",
    "strengths": "The paper explores an area not yet much studied in the field of MT - using multilingual models for improving document-level MT, and presents interesting, promising results. Models are trained on a very diverse set of languages using well known data sets and easy to follow training workflows, making the whole process seemingly fully reproducible. Results are evaluated by the classic automatic MT evaluation metric BLEU, as well as document-level MT specific challenge test sets, and even support the automatic metrics by performing human evaluation. ",
    "weaknesses": "Nothing particularly wrong here. ",
    "comments": "- The authors mention using `the latest WMT evaluation sets`, which currently would be from WMT2021, but if/when the paper is published the `latest` would already be different, so it would be good to specify the exact year.\n- The paper mentions using both BPE and Sentencepiece - are they used on top of one another or different for each training data set? If so, then why? This part is unclear... - Is the motivation for using 64K token vocabulary for Europarl the larger size of the training data? The IWSLT data set has a far more diverse variety of languages/writing systems/characters, so it would seemingly make more sense use a larger vocabulary for that instead. ",
    "overall_score": "4.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "a6f83f54bd43db72",
    "paper_id": "ARR_2022_108",
    "reviewer": "Xiang Li0",
    "paper_summary": "This paper studies and analyses a novel document-level NMT framework using multilingual transfer learning to improve the document-level translation quality for some student languages without parallel document corpus by applying the teacher models trained on parallel document corpus. ",
    "strengths": "The research question about zero-shot generalization for document-level NMT is clearly stated. And the number of experiments is impressive. And the results are conniving by applying automatic and human evaluations. ",
    "weaknesses": "This key motivation of this work is just applying the idea of zero-shot multilingual sentence-level NMT to the multilingual document-level NMT task. It appears that innovation of this paper is less. ",
    "comments": "1. The reference of a related work (https://aclanthology.org/2020.wmt-1.53) is missing in this paper. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]