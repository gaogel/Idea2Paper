[
  {
    "review_id": "0f81aa2179161e30",
    "paper_id": "ARR_2022_288",
    "reviewer": null,
    "paper_summary": "This paper introduces a domain-specific model for analyzing political conflict and violence. They evaluate this model in 18 tasks and show that ConfliBERT performs better than BERT overall, particularly in settings with less annotated data. ",
    "strengths": "- The arguments for having a domain-specific BERT model for political conflict and violence is well-motivated and supported with references to benefits of domain-specific models  - Discussion of different vocabularies and tokenization in 3.1 and comparisons in the task performance results was interesting - Discussion of model details and training time was good for replicability - Thorough comparisons of ConfliBERT models pretrained from scratch and continual pretraining, as well as with BERT - The curated data and model will be useful for future NLP and political science research (cross-disciplinary impact) ",
    "weaknesses": "- Unclear how mainstream news was filtered to be relevant for political conflict. It could be useful to have some more data filtering details and evaluation task descriptions (e.g. example input, output classes, etc) in the appendix - [minor weakness] Beyond the vocabulary and model performance tables, I want even more out of the comparison between ConfliBERT and BERT because the model is the main contribution (there's no new theoretical contribution, major methodological contribution, or dataset). [ suggestion] Is it possible to do a manual analysis of cases where BERT misclassifies a text and ConfliBERT does it correctly, or both models still struggle? Why does ConfliBERT have large improvements for some tasks but not others? ",
    "comments": "Since ConfliBERT’s main gains seems to be from political conflict-relevant words in the vocabulary, how well will it generalize in a rapidly-evolving space? Would it need to be retrained as different political groups and organizations emerge? ",
    "overall_score": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "45b4eda89877a3ce",
    "paper_id": "ARR_2022_288",
    "reviewer": null,
    "paper_summary": "This paper presents a new domain-specific language model for the area of political violence and conflicts. General-purpose language models, like BERT, do not cover important domain-specific vocabularies, as a result, those models do not perform well in many downstream tasks. This paper addresses this limitation and proposes ConfliBERT, which is trained on text data from the area of social unrest, political violence, etc. ConfliBERT has two variations – (1) built from scratch, and (2) trained on top of pre-trained BERT. Experimental results show that either version performs better than BERT in several tasks related to this domain. ",
    "strengths": "+ This paper proposes a new language model for a domain. As the authors claim there are not any previous language models specific to this area. While this may boost research in this area, this new model can also help in developing new research questions. \n+ There is a lack of a large text corpus specific to this area. Addressing this issue, the authors develop this model by merging several existing datasets and creating a large dataset that is suitable to train a model like BERT. This corpus will be an important resource to conduct text data-driven research in this area. \n+ The paper evaluates its claims through a variety of experiments. The experiments are designed to include several types of problems, such as binary and multiclass classification, sequence labeling, on the other hand, each problem has been tested on several datasets. In all these experiments, the ConfliBERT has consistently outperformed the performance of the BERT model. These results demonstrate the power of this model across a diverse set of problems and corpora (across the world) and the need for a domain-specific language model in this context. \n+ The work is reproducible, where the paper furnishes details about the implementation, datasets, and hyperparameters and also the system configuration for training. ",
    "weaknesses": "-\tWhile this model is one of its kind in this area, the language model’s scope is too narrow to have a wide range of applications. Other similar BERT-based models (e.g. BioBERT, SciBERT) has a wider coverage. The authors should strengthen the claim of the importance of this model by discussing important problems in this area and how this language model will be essential in addressing a wide variety of problems in this domain. \n-\tAlthough ConfliBERT is a novel language model, the concept, methodology, implementation follows the BERT model. While it can contribute to the area of political science and computational social science in general, it is not clear how this work contributes to the area of NLP. \n-\tThe paper lacks a sound discussion to explain certain observations. Although, the experimental results unanimously show that for problems related to this domain ConfliBERT is a better choice but it is not clear which version of ConfliBERT is better, SCR or Cont. On many occasions, we see that the Cont version (built on top of BERT) performs better than SCR. This raises two questions – under what conditions Cont is more likely to perform better and given that Cont is trained on top of BERT, what are the features of BERT that are important in this case.  The paper should address the merits of BERT that are part of Cont but not of SCR. Additional experiments may help to show whether this is a data problem or a model problem. ",
    "comments": "The paper argues the need for a domain-specific language model for the area of political conflicts and violence. They validate this claim by showing how such a language model can improve downstream tasks in this area. The paper is clearly written and the claims are evaluated well using an extensive set of experiments. \nThe advantage and the utility of the proposed language model are clear but there remain a few gaps that are important to understand the full strength of this model. The authors should present a clear discussion on the advantages and disadvantages of using the domain-specific datasets used in the training. The authors discussed that their dataset has additional words related to terrorism that are not present in a more general-purpose corpus but are there any other types of words that enriched the training of ConfliBERT? We see that only 3 out of 9 problems are related to terrorism. Indeed, the SCR model performed better in all the tasks involving terrorism-related data. On the other hand for protest-related data, we see a mixed performance, where Cont doing better in 2 out of 3 cases. For the case of Cont, are there any disadvantages for not using a general-purpose model/data? What is the percentage of vocabulary that is present in the general dataset but not in the domain-specific data? If this percentage is large, will that create a roadblock in the wider applicability of this language model? On the other hand, having too many unrelated words in the general-purpose data can act as noise for more domain-dependent tasks? The paper will benefit from a more in-depth analysis and comparison of the datasets used against a more generic corpus. \nMinor point: On page 6, section 5.2, lines 514, 515, it is not clear how the p-values were computed. What tests were performed to compare the two cases? ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]