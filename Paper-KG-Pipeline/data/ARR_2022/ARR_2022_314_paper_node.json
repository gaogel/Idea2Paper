{
  "paper_id": "ARR_2022_314",
  "title": "GlobEnc: Quantifying Global Token Attribution by Incorporating the Whole Encoder Layer in Transformers",
  "conference": "ARR",
  "domain": {
    "research_object": "文本数据，主要关注Transformer模型中各个token在全局编码器层中的归因问题。",
    "core_technique": "Transformer架构，具体改进或分析了编码器层内的token归因机制。",
    "application": "自然语言处理相关任务，如机器翻译、文本分类、问答系统等需要模型可解释性的场景。",
    "domains": [
      "自然语言处理",
      "可解释性人工智能",
      "深度学习"
    ]
  },
  "ideal": {
    "core_idea": "提出了一种结合注意力机制和层归一化的全局输入token归因分析方法，提升了Transformer模型解释性。",
    "tech_stack": [
      "Transformer",
      "自注意力机制",
      "层归一化（Layer Normalization）",
      "残差连接",
      "归一化分解",
      "全局归因聚合",
      "BERT",
      "梯度归因方法"
    ],
    "input_type": "Transformer模型（如BERT）的输入token序列及模型参数",
    "output_type": "每个输入token对模型输出的全局归因分数"
  },
  "skeleton": {
    "problem_framing": "论文从学术gap出发引出问题。开篇先强调Transformer模型的卓越表现引发了对其有效性原因的关注，随后聚焦于自注意力机制的解释性分析，并指出围绕注意力权重解释的争议。接着引用最新研究表明仅用注意力权重不足以解释模型行为，提出需引入向量范数，但这些工作仍有不足。整体策略是通过梳理已有研究的不足和争议，自然引出本文要解决的问题。",
    "gap_pattern": "论文批评现有方法时采用了‘现有方法忽视了X’和‘现有方法在Y方面受限’的逻辑。具体表现为：指出以往的norm-based方法只分析了注意力块，忽略了编码器层的其他关键组件（如第二层归一化和残差连接）；现有研究只关注单层归因，缺乏全模型的归因聚合；即使采用跨层聚合，结果在微调模型上仍然很差；梯度法虽更健壮但计算开销大。句式上多用‘however’, ‘whereas’, ‘constrained to’, ‘ignore’等词汇，突出现有方法的局限。",
    "method_story": "方法部分采用‘先整体后细节、分模块递进’的叙述策略。首先整体说明方法的目标是全局归因分析，强调要覆盖编码器层的所有主要组件。然后对比并扩展已有的norm-based分析，详细介绍如何引入第二层归一化和残差连接，指出与以往工作的区别。接着介绍如何聚合多层归因，详细解释rollout技术及其适配。整体顺序是：先提出整体改进思路，再逐步细化每个关键模块的处理方式，最后介绍跨层聚合方法。",
    "experiments_story": "实验部分采用‘主实验+消融分析’的策略。首先说明实验设置和所用数据集，随后对比多种归因分析方法，逐步引入不同的编码器层组件，考察各部分对整体性能的贡献（即消融分析）。主要实验是对比不同方法与梯度法的相关性，辅以消融实验分析各组件的影响。整体上以定量实验为主，通过表格展示相关性指标，突出方法改进的有效性。"
  },
  "tricks": [
    {
      "name": "引用权威文献建立背景",
      "type": "writing-level",
      "purpose": "通过引用Transformer及相关研究，建立研究背景和可信度",
      "location": "introduction",
      "description": "作者在引言中大量引用Vaswani et al., 2017等权威文献，说明Transformer及注意力机制的重要性，为后续研究提供坚实背景。"
    },
    {
      "name": "问题递进式引入",
      "type": "writing-level",
      "purpose": "逐步引出研究空白和待解决的问题，增强叙事逻辑和说服力",
      "location": "introduction",
      "description": "作者先介绍已有关注点（注意力机制），再指出现有方法的不足（如只分析注意力块、只做单层归因），自然引出本文的研究动机。"
    },
    {
      "name": "明确列举贡献",
      "type": "writing-level",
      "purpose": "突出工作新颖性和价值，帮助读者快速把握创新点",
      "location": "introduction",
      "description": "作者在引言末尾以条列方式明确列出三项主要贡献，突出方法创新和实验表现。"
    },
    {
      "name": "对比现有方法",
      "type": "writing-level",
      "purpose": "通过与已有方法对比，突出自身方法的改进和优势",
      "location": "introduction / method / experiments",
      "description": "作者多次对比Kobayashi等人的方法和Abnar & Zuidema的聚合方法，强调自身方法在归因范围、聚合方式等方面的提升。"
    },
    {
      "name": "细致分解方法流程",
      "type": "method-level",
      "purpose": "提升可解释性，让读者清晰理解每一步的设计动机和实现细节",
      "location": "method",
      "description": "作者详细分解encoder层各组件的归因流程，逐步说明如何从注意力块扩展到全encoder，并给出公式和推导。"
    },
    {
      "name": "理论与实践结合",
      "type": "method-level",
      "purpose": "增强方法的科学性和说服力，说明设计选择的合理性",
      "location": "method",
      "description": "作者不仅给出方法公式，还解释为何忽略FFN的非线性影响、如何近似处理，体现理论分析和工程实现的结合。"
    },
    {
      "name": "分组对比实验设计",
      "type": "experiment-level",
      "purpose": "通过多组实验，验证各组件对整体性能的影响，增强结论的完备性",
      "location": "experiments",
      "description": "实验部分分别报告不同归因方法（如N, NRES, NENC等）的表现，量化各部分对最终结果的贡献。"
    },
    {
      "name": "定量指标支撑结论",
      "type": "experiment-level",
      "purpose": "用具体数据（如Spearman相关系数）量化方法优劣，增强说服力",
      "location": "experiments",
      "description": "作者用表格展示不同方法与梯度归因的相关性，直观体现自身方法的优势。"
    },
    {
      "name": "多角度归纳实验结论",
      "type": "writing-level",
      "purpose": "系统总结实验发现，呼应引言中的贡献点，提升论文整体逻辑性",
      "location": "experiments",
      "description": "作者在实验后总结归纳各部分影响，强调残差连接、归一化、跨层聚合等关键因素。"
    },
    {
      "name": "方法命名与结构化表达",
      "type": "writing-level",
      "purpose": "通过命名和结构化表达，帮助读者区分不同方法，提升可读性",
      "location": "method / experiments",
      "description": "作者为不同归因方法命名（如W, WRES, NENC），并在方法和实验中统一使用，便于对比和理解。"
    }
  ]
}