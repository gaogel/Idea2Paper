[
  {
    "review_id": "d56327f333fa416b",
    "paper_id": "ARR_2022_355",
    "reviewer": null,
    "paper_summary": "This work reports on the results of a cross-document misinformation detection system using event graph reasoning. Recognizing the need for automatic detection of real versus fake news, they train a system for event detection at two levels – document level and event level. To do this, they created their own fake data, apply cross-document coreference, group data into three sets based on event “clusters” (topically related events), and report event detection results. Their approach is as follows:  1.) \tCreation of a within-document knowledge graph based on IE (“IE” is never defined. I assume it stands for information extraction)  2.) \tCreation of a cross-document knowledge graph, connecting each cross-document event cluster to “events in the cluster”  3.) \tConnection of document nodes with all events and entities in the document.\n4.) \tEncoding of cross-document knowledge graphs via graph neural networks and use of event and document level events to detect misinformation at both levels.  Their results outperform HDSF and GROVER at the document level and random guessing and logistic regression heuristics at the event level. ",
    "strengths": "1. \tOverall, work seems valid and logical. It improves upon current misinformation detection. \n2. \tFigures are extremely helpful ",
    "weaknesses": "1. \tMissing some details to lend full credibility. This pipeline seems to be fully automated. Were humans involved at any level and were automatic annotations, such as coreference resolution, subjected to human sanity checks?  This paper states that, “We employ a cross-document event coreference resolution system (Lai et al., 2021; Wen et al., 2021) to identify clusters of events from multiple documents that refer to the same real-world events.” Was any coreference spot-checked? What is the reliability of the system employed? It next states that, “For each event cluster, we add a node to represent the overall information of the real-world complex event corresponding to the cluster.” How was this added? Automatically or by hand? If automatically, what was the reliability of the nodes themselves? If by humans, what was the agreement? These details may impact your final results. This is followed by, “Then, an edge is added between each event node and corresponding cluster node to allow reasoning among cross-document coreferential events.” How is the edge added? Finally, how were your event structures pre-defined? I assume they differ by event, how did you determine the trigger, arg1, and arg2? Were there ever more than two arguments?\n2. \tJustification of the need to create fake data is not fully motivated. You state that collection of annotation is time consuming and expensive but that these datasets are of higher quality, both of which are very true. You further state that you follow previous work that created datasets by manipulating knowledge graphs. While references are provided, no mention of validation of such manipulation is provided to satisfy a reader as to the of created fake news nor is mention made as to the heterogeneity or homogeneity of created fake news. How generalizable is this to real fake news? ",
    "comments": "1)\tMany acronyms not defined, such as “IE” and “RNN” 2)\tMissing references for some claims that are presented as if they are common knowledge. For example, it is claimed that “conspiracy theories tend to be closely related to each other and convey highly similar information because they share the same biases or aim to manipulate readers in the same way.” However, your rational for creating your own fake news is the lack of sufficient existent data. Considering prior work on fake information detection, it is not always the case that conspiracy theories and/or fake news in general is similar between authors. The claim may very well be true for specific type of data. However, work validating the claim must be included. \n3)\tYou state that you improve upon current state of the art work by 7 points. Did the state of the art work use real or fake training data? What are your results on real data? \n4)\tHeuristics used as baseline for event-level detection seem valid. However, multiple publications from 2021 report similar work and could potentially be used as a baseline (ex. COVID-19 misinformation detection via identification claims and within-document events and entities.) Many such resources are not based on coreference resolution so may not be applicable. However, comparison to a different method would seem to bolster your findings. \n5)\tGrammatical error in “the two real news compliment each other”, “in the first news”, and “the two fake news contradict the real news with different details” ",
    "overall_score": "3.5 ",
    "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work."
  },
  {
    "review_id": "f8dd2bb2c1fb28d2",
    "paper_id": "ARR_2022_355",
    "reviewer": null,
    "paper_summary": "The authors propose a cross-document misinformation detection task. The misinformation is detected based on the difference of the event information graph created using a cluster of documents based on the inconsistency of the event information provided in a document. The approach is tested on three event datasets by splitting the events into clusters smaller than the clusters in their original datasets. Misinformation is created by changing correct information in the documents. ",
    "strengths": "1- The task, approach, and dataset are worth noting. \n2- The ablation studies that investigate the contribution of event detection and cross-document event coreference shed light on the performance. \n3- Some weaknesses of the approach and the experimental setting are discussed in detail. For instance, i) fake news creatin is creative, ii) the dataset used may not be reflecting real world. ",
    "weaknesses": "1- It is not clear how this task and the approach will perform for new information (updates) about an event (even if it is contradictory to what is known about an event) 2- The approach operates on clusters. New events may not have clusters.  3- Measurement of the performance maybe highly affected by the way the evaluation dataset is created.\n4- Comparing random and logistic regression based baselines to something that is empowered by BERT is not fair. Even if it is a baseline.\n5- I appreciate the effort spent for this paper. It is a lot of work and some parts are described very well. However, many steps are either missing or could be much clear. ",
    "comments": "1- \"because events are more important to storytelling\": Do you have any investigation or reference for this? Sentiments may be playing significant role as well.\n2- The concepts \"topic\" and \"event\" are confusing as they are used in the paper. The approach is based on events. The datasets utilized are based on topics. I see the details of the datasets and I understand it. But this is an issue in the whole article.\n3- How is the entire doc used to create the knowledge graph in the subsection \"KG representation\"?\n4- resoltuion -> resolution ",
    "overall_score": "2 = Revisions Needed: This paper has some merit, but also significant flaws, and needs work before it would be of interest to the community.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]