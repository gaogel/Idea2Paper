[
  {
    "review_id": "68a64013029ef250",
    "paper_id": "ARR_2022_158",
    "reviewer": "Nora Hollenstein",
    "paper_summary": "This paper analyses whether task-specific transformer models (i.e., fine-tuned BERT models) are successful at brain encoding tasks. The authors compare the performance of encoding models across a wide range of NLP tasks and discuss the differences between encoding fMRI stimuli from reading vs. from listening to stories. They also analyse the results for various language specific brain regions. ",
    "strengths": "The paper provides initials answers to an important open questions the field of brain encoding tasks from language stimuli. \nThe methodology seems appropriate and sound. \nThe paper has a well written introduction that summarizes the current state of the field. The paper clearly distinguished between brain encoding and decoding tasks for clarification. ",
    "weaknesses": "This is a revision of a previously submitted paper. I have read the author response and the new version of the paper. The weaknesses stated in my original review have been addressed and the writing has been improved.\nMy main concern was the reasoning behind the conclusions: It is very tricky to reach clear conclusions between reading and listening from two such different datasets. The differences might be due to the stimulus presentation, but could also arise from countless other factors such as experimental conditions, the text domain of the stimuli or the number of voxels. \nThis is clearly difficult to test due to the limited availability of such datasets, but therefore requires a careful discussion. While this still is a risk to the conclusions that are drawn, it is now described in the discussion. ",
    "comments": "- Please put the equation in section 4.3 on separate lines for better readability - Capitalization of subsection titles is inconsistent. ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]