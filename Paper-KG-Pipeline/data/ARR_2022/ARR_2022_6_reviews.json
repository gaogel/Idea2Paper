[
  {
    "review_id": "da3c75eabf392377",
    "paper_id": "ARR_2022_6",
    "reviewer": "Tianxiang Sun",
    "paper_summary": "This paper gives a comprehensive analysis of the Square One Bias in NLP. Through the statistics of ACL conference papers in recent years, the authors find that the current researchers are encouraged to go beyond the prototype experiment (optimizing accuracy/F1 on an English dataset) in only a single dimension. The problems resulting from the Square One Bias, the recommendations to overcome this, the examples   (one-dimensional research) and counter-examples (multi-dimensional research) are also discussed in the paper. ",
    "strengths": "1. The paper is well-written and gives a systematic discussion on the Square One Bias in NLP. \n2. I found the examples in the paper to demonstrate the Square One Biases (including architecture biases, annotation biases, selection biases, protocol biases, and organizational biases) pretty precise and helpful. \n3. From the perspective of this paper, one can easily notice unexplored areas of the research manifold, thus I believe this paper is helpful to the community to become more diverse. Especially, this paper would be inspiring for researchers who are interested in non-standard circumstances (e.g. non-English, non-standard architectures, non-accuracy/F1 metrics, etc.). ",
    "weaknesses": "Although this paper proposed many ideal recommendations for researchers and conference organizers to mitigate the Square One Biase issue, I might doubt the value in practice. As discussed in Section 6, doing multi-dimensional research would not only increase the workload of paper authors but also requires conference reviewers to be experts in multiple areas. As a result, the development of the field may be slowed down by the cumbersome evaluation. And besides, if we develop some standard multi-dimensional evaluation protocol, it may also introduce a new Square One Bias (e.g. some particular evaluation methods of fairness and efficiency can become dominant).  But overall, I agree that research that departs from prototypes in multiple dimensions should be encouraged by the reviewers and conference organizers. ",
    "comments": "The following papers, which consider the multi-dimensional evaluation in NLP, should be closely related to this work: 1. Dynabench: Rethinking Benchmarking in NLP (https://arxiv.org/abs/2104.14337) 2. Towards Efficient NLP: A Standard Evaluation and A Strong Baseline (https://arxiv.org/abs/2110.07038) It would be better to include and discuss them. ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]