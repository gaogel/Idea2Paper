[
  {
    "review_id": "b62bb422efa89457",
    "paper_id": "ARR_2022_201",
    "reviewer": null,
    "paper_summary": "This paper introduces a method to improve both interpretations and inference by training an end-to-end model to simultaneously generate explanations and labels for the NLI and CQA tasks. Compared to previous work which either produces explanations that are used to infer the label or vice-versa, this work introduces a framework that trains both tasks jointly. This is done by iteratively updating the predicted label at each explanation decoding step. The representations of the label and the explanation tokens are combined through gating mechanisms and used as input for the next timestep, enabling the model to use the explanation generated thus far to predict the label and the predicted label to guide the generation of the next explanation token. To improve the correlation between the inferred label and the generated interpretation, an additional adversarial regularization is used. Experiments show that this model shows improvements in both inference and interpretation compared to baselines. ",
    "strengths": "1. The idea of jointly training inference and interpretation is interesting. It is intuitive that the inference and interpretation should guide each other and be dependent. \n2. Several experimental results are presented to show that the model improves over both generated explanations and predicted labels. \n3. Ablation results are presented to highlight the contributions of each component. \n4. Evaluation is conducted over out-of-domain data, and a metric is introduced to evaluate the fidelity between the inference predictions and generated interpretations. Visual analysis shows how the predicted label changes with the generated interpretation. Human evaluation is also conducted. ",
    "weaknesses": "1. The Methodology section is very hard to follow. The model architecture description is rather confusing and sometimes uses inconsistent notation. For example, Section 2.2 introduces $v^p_{t-1}$ in the description which does not appear in the equations. Some of the notation pertaining to the labels ($l_0$, $l_{t-1}$) initially gives the impression that a sequence of tokens are being generated as the label, which is not the case. \n2. It is not clear why the baseline model considered for the NLI task is based on an older work (Conneau et al., 2017) and not the work by Kumar and Talukdar (2020), which seems more relevant to this work and is also cited in the paper. In addition, all the comparisons in the result section (and the delta numbers in Table 1) are made against a baseline vanilla Transformer model. \n3. Some of the numbers presented in Table 1 are confusing. It is not clear how a BLEU score of 22.51 is obtained by evaluating the ground truth dataset against itself for the NLI task. It is also not explained how the perplexity results are obtained. Are the numbers averaged across the dataset? Would that be a valid way to present these results? \n4. Some details like whether the results are averaged across multiple runs, and what the inter-annotator agreement was in the human evaluation are missing. ",
    "comments": "- In Section 2.1, the Transformer model is declared to have been established as the dominant approach for test generation. The next sentence immediately proceeds to the model description. This paragraph would flow better if there were a sentence linking the two parts and making it explicit that the Transformer model is being used. For example, \"We therefore adopt the Transformer model as our base model\" or something to that effect.\n- The description of the model architecture needs to be rephrased for clarity. See (1) under Weaknesses for some notation inconsistencies that can be improved as well.\n- In Section 2.3, Line 266 the description states $L$, $R$ are conditioned on $X$. Presumably it is meant to be $L$, $E$ since no $R$ is introduced anywhere.\n- In Section 3.2, the description of the Transformer baseline states that an MLP layer is added for generating sentence-level interpretations. This was perhaps meant to be predicted labels and not the interpretations, unless the decoder here is generating the labels - this needs clarification or correction.\n- In the results under \"Interpretation Promotion\", it is stated that \"most of the explanations generated by our method are reasonable even though the BLEU scores are low\". It might help to add an example here to make this more convincing.\n- Line 043: comprehensibleness -> comprehensibility  - Line 044: human -> humans - Line 046: which nevertheless -> which are nevertheless - Line 059: With the annotated -> With annotated - Line 135: method achieve -> method achieves - Line 163: dataset annotated by human -> human-annotated dataset - Line 169: a MLP -> an MLP - Line 233: to better inference -> to do better inference/to infer better - Line 264: two distribution -> two distributions - Line 303: maximum the negative likelihood -> maximize the negative likelihood - Line 493--494: the opening quotes are incorrectly formatted - Line 509: exapmle -> example ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work."
  },
  {
    "review_id": "0a0e6d9a373583b7",
    "paper_id": "ARR_2022_201",
    "reviewer": null,
    "paper_summary": "Prior work has used interpretation to improve inference, while ignoring “using inference logic to enhance interpretation.” This work deals with “mutual promotion” where they “promote” in both directions. Specifically, the “mutual promotion” is done using stepwise integration mechanism (SIM; Section 2.2). Additionally, adversarial fidelity regularization is used to further “improve the fidelity of inference and interpretation” (Section 2.3).   Essentially, during training, the model generates explanation first, and at the last time-step, generates the prediction. Therefore, the explanation and prediction are using largely the same parameters, and gradient updates would influence both the explanation and prediction.\nThe authors experiment on NLI and conversational QA tasks. The explanation is scored against the gold-standard human-provided evaluation results in e-SNLI and CoS-E datasets. ",
    "strengths": "Interpretation is an important problem.  The figures are well-designed and help readers understand the algorithms.\nThe method performs well on out-of-domain datasets (MNLI and SICK-E).\nThe mutual information discussion is well-motivated. ",
    "weaknesses": "I’m not convinced that AFiRe (the adversarial regularization) brings significant improvement, especially because - BLEU improvements are small (e.g., 27.93->28.64; would humans be able to identify the differences?)\n- Hyperparameter details are missing.\n- Human evaluation protocols, payment, etc. are all missing. Who are the raters? How are they \"educated\" and how do the authors ensure the raters provide good-faith annotations? What is the agreeement?\nOther baselines are not compared against. For example, what if we just treat the explanation as a latent variable as in Zhou et al. (2021)? https://arxiv.org/pdf/2011.05268.pdf A few other points that are not fatal: - Gold-standard human explanation datasets are necessary, given the objective in line 307.  - Does it mean that inference gets slowed down drastically, and there’s no way to only do inference (i.e., predict the label)? I don’t think this is fatal though.  What’s the coefficient of the p(L, E | X) term in line 307? Why is it 1?  Hyperparamter details are missing, so it’s not clear whether baselines are well-tuned, and whether ablation studies provide confident results.  The writing is not careful, and often impedes understanding.\n- Line 229: What’s t?\n- Line 230: What’s n?\n- Line 273: having X in the equation without defining it is a bit weird; should there be an expectation over X?\n- Sometimes, the X is not bolded and not italicized (line 262). Sometimes, the X is not bolded but italicized (line 273). Sometimes, the X is bolded but not italicized (line 156).  - Line 296: L and E should be defined in the immediate vicinity. Again, sometimes L, E are italicized (line 296) and sometimes not (line 302).\n- Line 187: It’s best to treat Emb as a function. Having l’ and e’ as superscripts is confusing.\n- In Table 4, why sometimes there are punctuations and sometimes there are no punctuations? ",
    "comments": "- Perplexity does not necessarily measure fluency. For example, an overly small perplexity may correspond to repeating common n-grams. But it’s okay to use it as a coarse approximation of fluency.\n- Line 191: \\cdot should be used instead of regular dot Section 2.1: It would be best to define the dimensionalities of everything.\n- Line 182: A bit confusing what the superscript p means.\n- Line 229: What’s t?\n- Line 230: What’s n?  - Line 255: Comma should not start the line. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]