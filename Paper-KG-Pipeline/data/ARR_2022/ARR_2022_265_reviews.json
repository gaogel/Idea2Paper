[
  {
    "review_id": "0a403ec48d3198d3",
    "paper_id": "ARR_2022_265",
    "reviewer": null,
    "paper_summary": "This paper aims at generating more meaningful questions for education considering predicting question types and making an event-centric summarization. Experiments show the usefulness of these two tasks. ",
    "strengths": "The idea is intuitive and effective. The method could be useful for QG or NLG community. ",
    "weaknesses": "The claim of high cognitive demand question generation can not convince me. \nIn my view, the authors’ method helps the QG model focus on the summarized event. \nIt is similar to the sentence editing/rewriting task conditioned on the question type. \nWhy not extract keywords or event tuples? What is the difference? \nThe comparison is unfair. I can not understand why you concatenate all questions into one sentence. \nMy suggestion is to perform the QG task on each question type. \nCase study and Error analysis are expected. ",
    "comments": "N/A ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "f9d269c23f60eeda",
    "paper_id": "ARR_2022_265",
    "reviewer": null,
    "paper_summary": "This paper investigates an interesting problem of automatically generating high-cognitive-demand educational questions for children’s storybooks. To this end, this paper proposes a three-stage framework: 1) learn to predict the question type distribution, 2) extract salient question-worthy events from the text, and 3) generate educational questions based on the outputs from steps 1) and 2). Both automatic and human evaluation validates the effectiveness of the proposed method. ",
    "strengths": "- A well-motivated task worthy of study by the QG community. It is practically meaningful to generate educational questions for children's storybooks and it could be applied to many real-world educational applications.  - The proposed framework is straightforward but logically sound.  - The authors conducted comprehensive evaluation, including the automatic evaluation and human evaluation.  - The paper is fairly easy to follow. ",
    "weaknesses": "- This work is a bit incremental since it is basically an improved model of Yao et al., 2021 for FairytaleQA.  - Each component of the framework is based on existing works. For example, BERT for learning question type distribution, BART for summary generation, and question generation. Therefore, I don't see many technical contributions from this paper.  - The performance improvement over Yao et al., 2021 also seems a bit incremental in terms of BERTScore and human evaluation results. ",
    "comments": "Putting together all the strengths and weaknesses I believe the NLP and QG community will benefit from this work. However, at the same time, it does leave things to be desired. Nonetheless, I am slightly inclined to accept this work.  Some minor suggestions & questions:  - Line 516: \"We can see that our method has a better estimation of the distribution of question types, which is closer to the distribution of the ground-truth.\" Does this count as an advantage of the proposed model? I think a perfect fit of FairytaleQA's distribution of question types shows that the model can better overfit the FairytaleQA dataset, but this may hurt its generalizability to other datasets.  - In Line 136, the authors missed some related works about multi-sentence / multi-document QG, for example:    - Pan et al. Semantic Graphs for Generating Deep Questions. ACL, 2020. \n  - Xie et al. Exploring Question-Specific Rewards for Generating Deep Questions. COLING, 2020. \n  - Tuan et al. Capturing Greater Context for Question Generation. AAAI, 2020. ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]