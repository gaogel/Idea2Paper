[
  {
    "review_id": "4825eb030d8afb39",
    "paper_id": "ARR_2022_106",
    "reviewer": "Chaojun Xiao",
    "paper_summary": "The authors focus on adversarial attacks on dialog systems. The authors first present a method that generates universal adversarial triggers with language model loss or selection criteria. The proposed attack method can generate natural-looking attacks. Furthermore, the authors propose a defense model, which can improve the robustness of conversational models. ",
    "strengths": "1. The authors present a method to generate natural-looking attacks for dialog systems, with satisfactory coherency and relevancy. \n2. The proposed defender is effective for both adversarial agents and regular users. ",
    "weaknesses": "1. The attack effectiveness of the proposed models seems to be limited, with no more than 50%. \n2. The authors only compare the proposed model with UAT. It would be more convincing if the authors can compare the proposed method with other competitive baselines. ",
    "comments": "Please refer to the weaknesses. ",
    "overall_score": "3.5 ",
    "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work."
  }
]