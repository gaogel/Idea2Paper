[
  {
    "review_id": "3cb226b0d8814a65",
    "paper_id": "ARR_2022_139",
    "reviewer": "Tong Zhang",
    "paper_summary": "This work models the Knowledge Graph Completion task and Question Answering task into a sequence-to-sequence paradigm. The authors propose two easy but effective training and inference methods for the two tasks, respectively.  KGT5 outperforms the current compositional KGE models on the link prediction task. Moreover, the proposed method also achieved outstanding performance on KGQA over incomplete KGs. ",
    "strengths": "(1) The new training and inference strategies for sequence-to-sequence method are easy but effective. \n(2) The proposed method outperforms the current compositional KGE models on the link prediction task. In addition, it is demonstrated to be complementary to the SOTA KGE methods. \n(3) the proposed method has an outstanding performance on KGQA tasks over incomplete KGs. ",
    "weaknesses": "(1) The performances of the KGT5 on KGQA are only evaluated with 50% incomplete KG settings. The effectiveness of the proposed methods is not demonstrated under the complete KG situation, which is necessary for KGQA evaluation. \n(2) The performance of KGT5 on KGQA tasks could take advantage of the n-to-one mapping between an entity/relation and its textual representation. The experiments should be performed in a more fair setting. ",
    "comments": "The performance on KGQA with the complete KG setting should be provided. ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "919d93df080f1ef6",
    "paper_id": "ARR_2022_139",
    "reviewer": null,
    "paper_summary": "This paper proposes a simple but powerful approach that uses a single Transformer architecture to tackle KG link prediction and question answering treated as sequence-to-sequence tasks. This approach can reduce the model size up to 90% compared to conventional Knowledge graph embedding (KGE) models, and the performance of this approach is best among small-sized baseline models. ",
    "strengths": "1. \tThis paper uses the Transformer structure for KG link prediction and question answering tasks, and this simple approach seems powerful. \n2. \tThis paper conducts a large number of experiments on multiple datasets and analyzes the experimental results. ",
    "weaknesses": "Minor: The paper only contains a high-level description of the proposed approach that benefits the performance of KGQA. It would be better if the authors provide some explicit cases or discussions to explain how pre-training on KG link prediction can improve performance on KGQA compared with the previous representative works later. ",
    "comments": "Specific comments for improving the work: 1. The authors may provide some explicit cases or discussions to explain how pre-training on KG link prediction can improve performance on KGQA. \n2. This paper shows KG link prediction performance from the proposed model trained on Wikidata5M in section 4.4. it would be better to show the KG link prediction performance from KGT5 after finetuning for QA, and showing performance on KG link prediction and KGQA with multi-task setting is also a good choice. ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]