[
  {
    "review_id": "46d7f10cad6e3fbe",
    "paper_id": "ARR_2022_65",
    "reviewer": "Yilun Zhu",
    "paper_summary": "This paper introduces a large human-annotated Chinese predicate-argument dataset MuPAD that covers six domains. Different from the previous Chinese SRL dataset, MuPAD annotates hidden arguments (subject and object), which is a common phenomenon in Chinese and is useful for understanding semantics. The corpus construction contains a detailed annotation guideline and the annotation process includes double annotations and additional revision from the third expert annotator, which ensures the annotation quality. Additionally, this paper applies a frameless annotation strategy. To investigate the domain adaptation performance, the paper conducts a biaffine baseline and a multi-task learning model on top of the baseline model, which are the first scores on this new benchmark. ",
    "strengths": "1. This paper provides a clear, high-quality annotation process for building up a predicate-argument dataset and it firstly introduces non-canonical texts, e.g., ZX, PB, PC, into the SRL task. \n2. The paper presents the model performance on this new cross-domain benchmark and provides a reasonable analysis of the results. ",
    "weaknesses": "1. Though experts have higher annotator agreement, they are not guaranteed to perform 100% accuracy. What is the consistency score between expert annotators? \n2. The consistency and accuracy are confusing. What is the difference between arg-wise consistency and accuracy? The intuition is that given the low pred-wise consistency, the label annotation agreement should be even lower because all labels related to inconsistent predicates are different. \n3. Are the consistency and accuracy scores calculated before or after experts' revision? ",
    "comments": "1. L70-71, researches ... focus ... make -> research ... focuses ... makes 2. L389, augment -> argument ",
    "overall_score": "3.5 ",
    "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work."
  },
  {
    "review_id": "1ff96046139ba88b",
    "paper_id": "ARR_2022_65",
    "reviewer": null,
    "paper_summary": "The paper introduces a new Chinese semantic role labeling (SRL) dataset, MuPAD, that covers 6 domains (product comment, web fiction, law, medical etc.). The dataset is motivated by the fact that most Chinese SRL research has been on the news domain. A frame-free annotation scheme was adopted to avoid difficulty in determining frames of novel predicates. A total of 24 semantic role labels, including core and non-core ones, are used in the annotation. In addition to annotation procedure and quality control, the paper provides analysis on the annotations, including consistency on predicates/arguments, accuracy of annotators, and distribution of labels. Finally, the paper shows cross-domain transfer learning experiment results. The authors use a multi-task learning (MLT) approach, adding an auxiliary task of parsing a previous dataset with a different scheme but sharing the embeddings and some model parameters. On average across the domains, this approach outperforms the baseline by a large margin. They also showed that using contextualized word representation from BERT can further boost the performance, but the gap between MLT and baseline become smaller. ",
    "strengths": "1. The dataset is in a reasonably large scale. It is constructed under solid methodology and the process is documented in detail. Moreover, performance drop in domain shift is a prominent issue for learning-based approaches to NLP, so this multi-domain dataset has great potential to drive future research towards this important direction. \n2. The authors conducted systematic experiments on cross-domain transfer learning. The results illustrate the performance drop in domain shift and demonstrate how much we can get by applying several standard techniques (MTL, contextualized embeddings). These are informative for understanding the status of the task (Chinese SRL) so are worth sharing with the research community. \n3. In addition to methodology and experiments, the paper is also comprehensive in reviewing related datasets and approaches. The whole paper is organized well and is easy to read. It also does a good job explaining some Chinese-specific linguistic phenomena which may have brought challenges to the annotators. ",
    "weaknesses": "1. The paper covers little qualitative aspects of the domains, so it is hard to understand how they differ in linguistic properties. For example, I think it is vague to say that the fantasy novel is more “canonical” (line 355). Text from a novel may be similar to that from news articles in that sentences tend to be complete and contain fewer omissions, in contrast to product comments which are casually written and may have looser syntactic structures. However, novel text is also very different from news text in that it contains unusual predicates and even imaginary entities as arguments. It seems that the authors are arguing that syntactic factors are more significant in SRL performance, and the experimental results are also consistent with this. Then it would be helpful to show a few examples from each domain to illustrate how they differ structurally. \n2. The proposed dataset uses a new annotation scheme that is different from that of previous datasets, which introduces difficulties of comparison with previous results. While I think the frame-free scheme is justified in this paper, the compatibility with other benchmarks is an important issue that needs to be discussed. It may be possible to, for example, convert frame-based annotations to frame-free ones. I believe this is doable because FrameNet also has the core/non-core sets of argument for each frame. It would also be better if the authors can elaborate more on the relationship between this new scheme and previous ones. Besides eliminating the frame annotation, what are the major changes to the semantic role labels? ",
    "comments": "- In Sec. 3, it is a bit confusing why there is a division of source domain and target domain. Thus, it might be useful to mention explicitly that the dataset is designed for domain transfer experiments.\n- Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)’s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.\n- More information about the annotators would be needed. Are they all native Chinese speakers? Do they have linguistics background?\n- Were pred-wise/arg-wise consistencies used in the construction of existing datasets? I think they are not newly invented. It is useful to know where they come from.\n- In the SRL formulation (Sec. 5), I am not quite sure what is “the concerned word”. Is it the predicate? Does this formulation cover the task of identifying the predicate(s), or are the predicates given by syntactic parsing results?\n- From Figure 3 it is not clear to me how ZX is the most similar domain to Source. Grouping the bars by domain instead of role might be better (because we can compare the shapes). It may also be helpful to leverage some quantitative measure (e.g. cross entropy).\n- How was the train/dev/test split determined? This should be noted (even if it is simply done randomly). ",
    "overall_score": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "49cb81c274f23110",
    "paper_id": "ARR_2022_65",
    "reviewer": null,
    "paper_summary": "### Overview of the paper The paper presents **MuPAD**, a Chinese dataset for evaluating systems on Semantic Role Labeling (SRL), with a particular focus on out-of-domain performance. The authors argue that, over the recent years, research in SRL has predominantly focused on in-domain settings where a model is trained and evaluated on data coming from the same distribution/domain (CoNLL-2005 and CoNLL-2009 are two exceptions as they provide an out-of-domain test set, but it is extremely small only 400 sentences).\nIn order to address this issue, the authors collected sentences from various domains, more precisely, 1 source domain on which systems are usually trained and 5 target domains (out-of-distribution with respect to source domain). Then, they annotated the collected sentences with semantic role labels using a frame-free methodology (which eliminates the need to instruct the annotators about an ontology, e.g., PropBank or FrameNet), trained a system on the source domain and evaluated the resulting system on the out-of-domain test sets.\nThe main takeaway is that, indeed, even though the system achieves very good results on the test set of the source domain, there is a significant drop in performance on the target domains. These results motivate the need for MuPAD as a benchmark to evaluate current and future SRL systems on out-of-domain test sets too.\n### Overall considerations In general, the paper is quite good but there are also some important drawbacks. As a researcher in SRL, I am usually very excited to see new resources for the task but, in this case, it seems like MuPAD will not be released openly to the research community. Without an open resource, the paper is still good (findings?) as it discusses the limitations of current SRL systems on out-of-domain instances. ",
    "strengths": "### Main strengths - **Resource:** A large dataset with manual predicate-argument annotations. The dataset comprises over 30K sentences in **Chinese** that were annotated by at least two annotators (and checked by a third annotator in case of disagreement).\n- **Annotation methodology:** The authors spend a significant part of the paper describing their annotation methodology, its motivation, and the results. The methodology consisted in assigning each annotation item (sentence + predicate) to two (paid) undergraduate students who were trained beforehand. In case of disagreement, an expert annotator was involved to solve the disagreement. The results of the annotation process are described showing \"consistency ratio\" between the two annotators (which motivates the employment of an expert annotator to solve the disagreements).\n### Minor strengths - **Empirical results:** The authors show how an SRL system performs on the task (in-domain and out-of-domain results). They use a system based on that of Cai et al. (2018) which is a bit old now (surpassed by many other systems in other benchmarks), but it is still good baseline. Moreover, the authors report the results of their system when trained jointly on MuPAD and the Chinese PropBank 2.0. In this setting, the results of the system are better but there is still a very noticeable drop in performance between source and target domains.\n- **Hidden arguments:** The authors made the annotators indicate hidden arguments in the sentences, i.e., arguments that are implicit in the sentence. ",
    "weaknesses": "### Major weaknesses - **Dataset availability:** My major concern is about the availability of this dataset. Nowhere in the paper is stated that MuPAD will be openly released, which suggests that it will be released behind a paywall. In my opinion, this is the most disheartening weakness of this work in general (beyond the paper itself). The reviewers, meta-reviewers, action editors, and everybody else involved in the organization of a conference (e.g., NAACL) are being working on this paper for free, but the dataset will be a paid product. Of course, the creation of a dataset has its costs, but so does every other paper, and there are many datasets that are being released for free nowadays. A paywall is also a roadblock for research in the field. Of course, disregard this point if the dataset will be freely released.\n- **Licensing:** Part of the dataset comes from the Chinese split of CoNLL-2009 which is released with a very restricting license (CoNLL-2009 is available for purchase through the Linguistic Data Consortium and it cannot be redistributed). I imagine that MuPAD will be released with a similar license. ",
    "comments": "### Suggested additional references - **Lines 31-32:** one common reference for \"who did what to whom [...]\" is Marquez et al. (2008). \" Semantic Role Labeling: An Introduction to the Special Issue\".\n- **Lines 55-56:** references for the effectiveness of pretrained language models has been demonstrated by Shi and Lin (2019), Conia and Navigli (2020) and Paolini et al. (2021), among others.\n- In the \"SRL corpora\" paragraph (Related Work) it would be great to cite the CoNLL 2005, 2008, 2009 and 2012 tasks explicitly. For example, the CoNLL-2012 shared task (Pradhan et al., 2012) included SRL annotations for English, Chinese and Arabic, if I am not mistaken.\n- **Lines 448-451:** For completeness, using multitask learning to leverage knowledge from heterogeneous resources has been already studied in SRL (Conia et al., 2021).\n### Suggestions - Please, add hyperlinks to the papers in the bibliography.\n### Other comments Even though the authors submitted a sample (120 sentences) of their dataset for reviewing purposes, I am choosing \"No usable datasets submitted.\" so as to stress that the authors seem not to plan an open release of their resource.\n--- - Marquez et al., 2008. Semantic Role Labeling: an introduction to the special issue. Computational Linguistics.\n- Shi and Lin, 2019. Simple BERT models for relation extraction and semantic role labeling. Arxiv.\n- Conia and Navigli, 2020. Bridging the gap in multilingual semantic role labeling: a language-agnostic approach. COLING.\n- Conia et al., 2021. Unifying cross-lingual semantic role labeling with heterogeneous linguistic inventories. NAACL.\n- Paolini et al., 2021. Structured prediction as translation between augmented natural languages. ICLR. ",
    "overall_score": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]