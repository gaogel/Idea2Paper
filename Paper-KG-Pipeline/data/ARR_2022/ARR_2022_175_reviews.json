[
  {
    "review_id": "7f996fb9f31b45de",
    "paper_id": "ARR_2022_175",
    "reviewer": null,
    "paper_summary": "This paper investigates the degree of knowledge that pre-trained LM, with only access to a vocabulary of subword tokens, have about the character composition of these tokens, and if enriching these models with orthographic information about the tokens can improve them. It proposes to use a probe they name SpellingBee: a generative character-based LM that takes as input an uncontextualized word embedding from a model, and tries to predict the correct character sequence. It is trained on part of the model's vocabulary, and tested on the other: if it manages to succesfully generalize, the embedding must contain orthographic information. The probe is tested on 4 models: Roberta-base, and 3 others showing change in a particular aspect: Roberta-large for size, AraBert for language, and GPT2-Medium for an autoregressive model. The probe's capacity to predict the character sequence is evaluated by counting the exact matches, and with a finer-grained metric measuring overlap. Compared to a control experiment where the probe is not trained and only randomly initialized, the probe is able to better rebuild character sequences when fed with embeddings from the LMs (up to 30-40% from 0 for exact matches); however, it's performance is weakened when the training part of the vocabulary is filtered (removing token too similar to those in the testing part, or with the same lemma). However, using a probe trained on the full vocabulary as a way to initialize a LM does not seem to be useful, as the LM reaches the same training loss as a control one rather quickly. ",
    "strengths": "- This paper poses a research question of great interest, and answers it while carefully considering many possible factors (models, filtering the training vocabulary).\n- It investigates a potential application of this answer.\n- The paper is very clearly written, and easy to follow. ",
    "weaknesses": "- The results given by the probe are a little difficult to interpret; as, while there is a control experiment where the probe has no information, it would be very useful to have an idea of what the probe can do when fed with embeddings that we know contain orthographic information. \nIf testing the probe on static embeddings (word2vec, glove), fasttext embeddings could work; in this setting, I believe uncontextualized embeddings from CharacterBERT (El Boukkouri et al, 2020) could work. ",
    "comments": "- The samples of errors shown in Table 3 seem to often have the first, or few first characters right. Did you at some point try to filter by prefix rather than lemmas ? ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "4b5cc87c513b6a19",
    "paper_id": "ARR_2022_175",
    "reviewer": null,
    "paper_summary": "This paper addresses the question of whether the spelling of words has been retained / encoded by large language models. First, it introduces a probe to discover the spelling of a word based on the embeddings in the model input. This probe, spelling bee, is essentially a character level language model which is conditioned on the word embedding. They find that a significant amount of information about spelling is retained by the embedding, and that explicitly providing information about spelling during training by using spelling bee does not provide additional advantage to the model. The authors conclude that this indicates language models “can quickly acquire all the character level information they need without directly observing the composition of each token”. ",
    "strengths": "I liked the fact that the training and test data splits considered the possibility that words in the test set might benefit too much from training  spelling bee on a train set with similarly spelled words.\nI think that the question of whether the model has implicitly learned character composition is valuable, and there should be some significant interest in this paper as a result. ",
    "weaknesses": "I want to see a baseline that tests spelling bee on representations specifically optimized for morphology or spelling, so that we can see what the performance of this probe would be on complete information about spelling. As is, it's not clear what a true upper-bound performance would be for such a probe.\n I felt that the conclusions drawn from the attempt to train with additional spelling information were not well justified. After all, if training with the additional information actually damaged performance, the authors would not have concluded that the model doesn’t use the information or that the information was somehow harmful or misleading. Instead, they would rightly assume that the particular procedure they were using to add that information was not providing it in a structure that the model could easily use. However, because it doesn’t change  performance at all, the authors conclude that the model is able to acquire everything it needs without direct observation of the spelling. It’s not clear to me that these results contradict the idea that some information about character composition might be able to help a model.\nThere needs to be more detail on the implementation of spelling bee. ",
    "comments": "I felt that this paper could do with more citations to the existing literature on morphological/character composition in neural models (e.g., https://aclanthology.org/P17-1184/) ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]