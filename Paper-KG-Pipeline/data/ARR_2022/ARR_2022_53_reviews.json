[
  {
    "review_id": "d598c7732476060a",
    "paper_id": "ARR_2022_53",
    "reviewer": "Zhiqing Sun",
    "paper_summary": "The paper proposes an encoder-decoder-based framework for document re-ranking. Instead of using a dual-encoder framework, the paper regards documents and the input and queries as output, and uses the generative likelihood of queries to retrieve documents. For compressing pre-computed document embeddings, the paper proposes to Funnel Transformer. Empirical results on MS MARCO dataset show that the proposed method achieves the best effectiveness-efficiency trade-off. ",
    "strengths": "1. The idea of using the generative likelihood of queries to retrieve documents seems to be novel.\n2. The proposed method ED2LM archives a better MRR-Latency trade-off on MS MARCO dataset. ",
    "weaknesses": "1. The paper says \"The normalised scores from the true and false tokens are combined as in (Nogueira et al., 2020).\" ( line 249) It's unclear to the reviewer how the scores are combined in the paper. Besides, the paper does not show how the cross-entropy loss and query likelihood loss affect the model performance, which requires an ablation study.\n2. According to the Bayes' theorem, $p(d|q) \\propto \\frac{p(q|d)}{p(q)}$. However, the $p(q)$ term, i.e., the language modeling likelihood of queries is modeled in the paper, which makes the proposed method not mathematically sound. ",
    "comments": "- ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work."
  },
  {
    "review_id": "7b6573d6b9c2ee5b",
    "paper_id": "ARR_2022_53",
    "reviewer": "Yuntian Deng",
    "paper_summary": "This paper aims at reducing the inference computational cost of transformer-based information retrieval, which is high in traditional encoder-only approaches since the model needs to be run on all document-query combinations. To reduce the computational cost, this work adapts an encoder-decoder model to this task, such that the encoder only needs to be run once per document and the computational cost decreases from `num_documents * num_queries * (encoder_cost + decoder_cost)` to `num_documents * encoder_cost + num_documents * num_queries * decoder_cost`. To improve ranking performance, this work follows Nogueira et al 2020 and generates a special boolean token after generating the query, while training is a combination of classification loss and language modeling loss. Experiments show that the proposed approach achieves similar performance to Nogueira et al 2020 while being a few times faster. ",
    "strengths": "1. The proposed paradigm allows caching encoder representations and suits well the typical information retrieval scenarios where running the encoder is more expensive than running the decoder, since the query is usually shorter than the documents in the database. \n2. Empirically this work achieves similar performance (using statistical tests) while being a few times faster than Nogueira et al 2020. ",
    "weaknesses": "1. The main claim of this work is faster inference compared to the baselines, at a slight cost of accuracy, However, one can achieve the same goal by using the more accurate (but slower) baselines, but initially retrieving a smaller candidate set (e.g., instead of using 1k candidates, use BM25 to retrieve 500). This might be a baseline worth comparing to to show that it improves the Pareto frontier of the speed/accuracy tradeoff. \n2. Some claims rely on having a difference not statistically significant, but the test sets are small and it's not surprising to not get statistical significance. ( In the extreme if the test set has only a few examples, then BM25 might be the fastest algorithm while \"its effectiveness is not significantly different from baselines\".) I think it's more convincing to draw conclusions based on statistically significant results, such as ED2LM-base being statistically better than T5-small (unfortunately T5-small seems more efficient). As one example, line 468 states that \"For ED2LM-base, its effectiveness is not significantly different from T5-large on both TREC DL Tracks and under-performs by 0.7 (38.7 vs 39.4) on MS MARCO, while providing a 6.2X speed up.\", but the same can be said when comparing to T5-base although at a less impressive speedup. \n3. The change from previous encoder-decoder ranking models such as Zhuang et al 2021b seems incremental: instead of using the likelihood of the query, this work follows Nogueira et al 2020 and uses a special boolean token in the end. ",
    "comments": "L237: question, I think you meant query here? ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]