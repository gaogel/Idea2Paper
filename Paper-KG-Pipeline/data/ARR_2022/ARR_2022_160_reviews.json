[
  {
    "review_id": "7bd274f90b743235",
    "paper_id": "ARR_2022_160",
    "reviewer": null,
    "paper_summary": "This paper introduces **natural instructions**, a collection of 61 tasks in natural language processing, featuring the actual instructions used in the crowdsourcing process of the original NLP tasks. These instructions are mapped to a unified schema which include definition, prompt, positive/negative examples etc. ( Fig. 4). Extensive experiments are done with BART and GPT-3. Results suggest that (1) instructions help improve cross-task generalization; (2) increasing the number of tasks leads to better generalization; (3) negative examples are unexpectedly not helpful. ",
    "strengths": "1. \tThe paper studies the interesting and important problem of cross-task generalization (generalization to held-out unseen tasks). The findings may be useful to the ACL audience. \n2. \tNatural instructions re-uses existing resources (instructions to crowd-workers) smartly. It provides a realistic learning/instruction-taking environment. Natural instructions is a valuable and unique resource complementary to the concurrent work of PromptSource and FLAN. \n3. \tCareful experiment design (three aspects of generalization: category, dataset, task). Detailed ablation and analysis in the appendix. ",
    "weaknesses": "Below are some minor suggestions or questions.\n- In the future version, if space allows,    * please add examples of each category (table 2) so the reader can better understand the difficulty of each category. For the analysis in Line 469-486, itâ€™s hard to understand the hypothesis without looking at what the instruction looks like for each category. \n  * please move some interesting analysis in the appendix to the main body (ablation of elements, error analysis).  - The datasets included in the paper are mainly related to QA. The classification category is also quite specific, e.g., classifying answer type/answerability. I wonder if the schema is extensive enough for general text classification tasks and general NLP tasks. ",
    "comments": "See comments above. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "08afb6e46460902c",
    "paper_id": "ARR_2022_160",
    "reviewer": null,
    "paper_summary": "The paper introduces NATURAL INSTRUCTIONS - a benchmark to measure cross-task generalization of models that learn tasks from natural language instructions. To prepare the benchmark, the authors consider several downstream tasks (and corresponding subtasks) from various existing benchmarks. To create the task instructions, the authors solicit the crowdsourcing instructions from authors of existing benchmarks, modify and adapt them into a common schema. The authors empirically find that providing instructions about a task in natural language (in a structured format specifying role of each section in an instruction, such as definition, prompt, etc.) improves model performance on cross-task generalization. ",
    "strengths": "1. Strong benchmark to judge cross-task generalization and also study human-like training setup for AI systems. \n2. Empirical results show that when instructions in natural language when properly structured (like definition, task prompt, positive examples, etc.) are provided to a model (BART and GPT-3) improves cross-task generalization performance. \n3. The authors discuss benchmark creation steps in detail. \n4. Error analysis and ablation of various components of task instructions are provided in detail which can be helpful to the community. ",
    "weaknesses": "1. \" In comparison to GPT3, the BART model trained on seen tasks achieves stronger performance despite being x 1k smaller than GPT3\" --  this seems a bit wrong/misguiding as I think the BART model is fine-tuned whereas GPT3 is not? \n2. I would suggest adding examples of the \"Verification\" task type in the paper(main or appendix) as this task type has been discussed in error analysis. Without actual examples of the task, it's difficult to follow the text in the paper. \n3. For leave-one-out-task experiments it's unclear why the numbers are much lower than Table 3. It's mentioned that \"Note that the absolute values, across different encodings, are lower than the numbers in Table 3 which is likely due to the difficulty of this setup compared to the random split\" - but won't the leave-one-out-task be easier or at least comparable to random split as now the training task collection contains all other dataset as well as other tasks from the same dataset ? ",
    "comments": "L#538:  cases --> case L#540: works --> work ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]