[
  {
    "review_id": "7b615d609f85242d",
    "paper_id": "ARR_2022_206",
    "reviewer": "Sanqiang Zhao",
    "paper_summary": "The paper proposes a method to incorporate hierarchical structure information into text summarization tasks. The method encodes hierarchical structure information, such as section titles, sentence positions, and section positions. The method achieves SOTA Rouge performance for three public popular summarization datasets. ",
    "strengths": "The performance of the proposed model achieves SOTA Rouge performance on three public popular summarization datasets. ",
    "weaknesses": "I am concerned about the novelty of the methods. This method simply encodes additional structure information into the Transformer models in a standard way. In this paper, both hierarchical position embedding and section title embedding are borrowed from other papers, so I would say the contribution of this paper is not substantial.\nThe paper has some unclear parts: (1) In the line 232, I think DPE is never defined. \n(2) Segment Embeddings in Figure 1 is not discussed. I believe it is not same to the segment embedding in BERT. \n(3) Regarding section title embedding, I believe section titles are not single vector, how to add it to BOS? ",
    "comments": "Since this is a text generation task and automatic metrics (such as rouge) are not very reliable, I was wondering if you had a human evaluation result? ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work."
  },
  {
    "review_id": "d702266401cd2a77",
    "paper_id": "ARR_2022_206",
    "reviewer": null,
    "paper_summary": "This paper proposes to incorporate the hierarchical structure of source sentences into the extractive summarization system models. This hierarchy would allow for modeling inter-sentence relationships, which should be captured to improve the final system as the authors claim. Specifically, the authors propose to utilize hierarchical position (HP) and Section title (ST) of sentences to refine the output embeddings based on the inter sentential and sectional information. They run their model on three datasets, namely CNN/DM, arXiv, and Pubmed, showing their system staying competitive (in some cases outperforms model-wise baselines). ",
    "strengths": "While the idea of hierarchical modeling the input documents has a long history (Cohan et al., 2018), this paper has approached the problem with a novel way of utilizing section titles, as well as hierarchical position (HP) information of the sentences. \nThe paper is well-structured and well-written in most parts. \nPromising results are reported when comparing with the baseline without injection of HiStruct information (e.g., longformer-base/large vs. HiStruct+ longformer-base/large and etc.). ",
    "weaknesses": "-Why did the authors decide to experiment with their model on CNN/DM? Hierarchical modeling of documents make sense on long documents such as arXiv and Pubmed. Is there any reason to indicate that it also work on shorter input documents such as CNN/DMâ€¦?\n-In the analysis section, BERTSUMEXT has been compared with HiStruct+ (Roberta-base) with the claim that HiStruct+ improves the SOTA. Is it a fair comparison as the BERTSUMEXT and HiStruct+ (Roberta-base) do not use the same LMS. In other words, it is still unclear where the improvements of HiStruct+ (Roberta-base) vs. BERTSUMEXT are coming from? Are they because of the hierarchical info injection, or simply because of Roberta-base LM?\n-The paper does not include the human evaluation process, which is an integral part of the summarization system setup. This gets, especially, higher priority as the results of the HiStruct+ model vs. baselines are close to each other. \n  -While results are promising when comparing HiStruct+ with the reproduced baselines (Table 2), the proposed model still does nor outperform the abstractive SOTA. ",
    "comments": "The methodology section needs re-working and ideas could be better presented. \nSome important information such as the number of extracted oracle sentences should come directly under the experimental setup, not in the Appendix. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]