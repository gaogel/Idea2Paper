[
  {
    "review_id": "ac111729bd87f7f3",
    "paper_id": "ARR_2022_292",
    "reviewer": "Rik Koncel-Kedziorski",
    "paper_summary": "This paper describes a contrastive learning approach to automatically solving math word problems (MWP) and investigates multilingual approaches to the problem. Additionally, it provides further evidence that the top layers of BERT will learn task-specific patterns, as shown in prior works.  This paper treats MWP solving as a text-to-equation-tree translation problem using an encoder-decoder architecture. To motivate the use of contrastive learning, the paper opens with an analysis of the effect of training epoch and encoder layer on the clustering of MWPs by prototype equation. t-SNE plots show expected clustering effects as layer/epoch increases. Analysis of raw high dimensional representations show that problems with similar lexical semantics or topic are given different representations when the prototype equation differs, especially in layer 12, while problems with the same prototype equation are embedded closer together. Moreover, it is shown that MPWs that are represented closer to the center of the cluster of problems with the same prototype equation are more likely to be correctly solved.  The contrastive learning approach proposed here involves finding difficult negative examples, which is done by choosing structurally similar equation trees with different operations in the intermediate nodes. Additional positive examples come from either trees or subtrees which consist of the same structure and operations as the target equation. For the multilingual approach, mBERT is substituted as the encoder. Results show that the contrastive learning method improves MWP solving in both the monolingual and multilingual settings compared to recent baselines. Ablations show the value of choosing difficult negative examples and other design decisions. Analysis shows that the contrastive learning objective results in well defined clusters. Accuracy is especially improved for examples farther from the cluster center. ",
    "strengths": "The contrastive learning for MWP solving seems to improve performance ",
    "weaknesses": "Technique is limited to problems that can be modeled by equation trees.  A lot of paper real estate is given to an analysis that basically shows:  - undertrained models don’t work - only using part of the encoding function (the bottom N layers) doesn’t work I don’t think this analysis will be of much use to the ACL community.  It seems like the cosine similarity of lower layers in figure 3 are relatively high, while the t-SNE visualizations in Figure 2 are more mixed. Do you think t-SNE is accurately representing the latent space? ",
    "comments": "The paper would benefit from connections to prior work on BERTology. An intro to this line of research can be found at https://huggingface.co/docs/transformers/bertology ",
    "overall_score": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "213b9cca0e60464a",
    "paper_id": "ARR_2022_292",
    "reviewer": "Artur Kulmizev",
    "paper_summary": "This paper employs contrastive learning to improve performance on Math Word    Problems (MWP) tasks. The use of contrastive learning is motivated by the    observation that ``semantically similar'' problems appear closer together in    the embedding space, despite belonging to different problem classes. After    demonstrating this phenomenon via several angles, the authors train a seq2seq    MWP model (BERT encoder + tree decoder) supplemented with a contrastive loss    --- mono- and multilingually. They show that, in learning from both positive    and negative examples, their model outperforms many baselines, as well as the    non-contrastive model. Furthermore, they demonstrate that employing    contrastive learning a multilingual scenario improves performance w.r.t. the    multilingual non-contrastive case. These results are supplemented by thorough    analyses. ",
    "strengths": "This paper is generally thorough. After introducing the    problem (albeit very opaquely), the authors proceed to provide a simple    solution and show strong empirical evidence in its support. They a offer    unique application of contrastive learning to an under-explored problem and    highlight how its usage can be generalized. The analysis before and after    training is strong and insightful. ",
    "weaknesses": "The main weakness of this paper is the motivation, which is extremely    imprecise. The pedagogical literature cited in lines 36-46 is opaquely    summarized and does not lend sufficient grounding for introducing the    problem: \"mathematics is about patterns and not merely about numbers\";    \"...students explore patterns, not just memorize procedures\". The authors    distill these insights into their hypothesis, where they posit that    \"semantics affects problem-solving\". The word \"semantics\" is used    extremely liberally here, and it is was unclear to me what the authors    actually meant here (vs. prototype equations). The analysis and discussion    offered by Section 3 is difficult to follow because of this, and it is not    clear how instances \"similar semantics\" are tracked or computed (see, e.g.    Figure 3). Upon several re-readings, it became apparent that what the authors    actually refer to is lexical overlap (at least as evidenced by Probs C and D    in Figure 1). Due to this, the representation clusters for each prototype    equation tend to be expanded by the problems with high lexical overlap across    classes. This is more or less straightforward, but more care needs to be    devoted to making the language precise and less lofty. ",
    "comments": "Comments:    * When investigating cosine similarity, authors should consider the work that    investigates MLM embedding spaces, namely    https://aclanthology.org/D19-1006.pdf and    https://aclanthology.org/2021.emnlp-main.372.pdf, which discuss the    anisotropy phenomenon.     * It would be interesting to see how the embedding spaces look like for BERT      out of the box.\n   * It is important to make distinction between `procedures' and 'patterns'    Questions:    * What layer are the plots in the top row in Figure 2 from? Conversely, what      epoch are the plots in the bottom from?\n   * Relevant to above point: how are \"similar semantics\" calculated? What is      meant by `semantics'? This sort of language not useful. ( lines 216-20)    * Lines 250-52: what does this mean?     Style/typos:    * Line 46 - hypothesize instead of `think'    * Line 55 - citation for encoder/decoder    * Line 155 - representations ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "18fd3b2c5a51dd2d",
    "paper_id": "ARR_2022_292",
    "reviewer": "Younes Samih",
    "paper_summary": "The paper describes an approach for improving  Math Word Problem (MWP) solving. \nDue to the non-distinction of MWP patterns, current methods, which are based on neural networks, perform poorly. The authors  propose a contrastive learning method to assist  the model to correctly separate confused patterns. The proposed methods outperform  two baselines in both monolingual and multilingual settings. ",
    "strengths": "1) The paper is written well and is a pleasure to read (although some details are missing) 2) The paper presents detailed results, which outperform the two baselines in both monolingual and multilingual settings. \n3)  The coupling of both semantic encoding (Bert encoder) and contrastive learning is  a key contribution and strength of the paper, in my opinion. ",
    "weaknesses": "1) Lack of interpretability: There could be more of a discussion of why \"semantic encoder understands semantics in lower layers and gathers the prototype equations in higher layers\".  This aspect could be discussed in more detail. consequently, the paper leaves many questions open while not giving definite answers about others. \n2) It will be interesting to how this method scales with respect to more complex mathematical questions. \n3) The authors have not motivated their choice of (Bert ) as the sole semantic encoder in their experimental settings. There are battery of models to chose from. ",
    "comments": "The paper could be further improved by including more discussion about interpretability as it difficult to explain the model's  behavior. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]