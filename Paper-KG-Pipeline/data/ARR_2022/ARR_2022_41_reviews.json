[
  {
    "review_id": "d8202d06a56e9216",
    "paper_id": "ARR_2022_41",
    "reviewer": null,
    "paper_summary": "This paper explores the effects of using a generative model that provides question prompts to human annotators in either the standard or adversarial data collection for training a QA model. The proposed approach combines the benefits of dynamic adversarial data collection, i.e. human annotators generating data points for the model and generative models that are traditionally used to augment datasets with adversarial data points. \nThis paper provides a thorough analysis of using Generative Annotation Assistants (GAAs) during data annotation process, examining the effects on annotation speedup, effectiveness in fooling the model, and performance improvement on downstream tasks. This paper collects data points from several experimental settings to compare the effects of sampling strategies, using model-in-the-loop (standard vs adversarial data collection), datasets used to train the GAAs, and whether answer prompts are provided or not. \nThis paper shows that the use of GAA without adversarial model in the loop is can achieve near-adversarial accuracy but at much higher speed of generating the adversarial dataset. Thus this paper addresses limitations of the adversarial data collection process, which is the large amount of time needed to collect the dataset. \nThis paper shows that use of GAA in both the standard and adversarial data collection helps improve model performance on downstream tasks. Additionally, in the standard data collection setting, GAA provides a measurable speedup in collecting the dataset in general, but also the effective examples that fool the model by a significant margin. \nDespite the exhaustive experimentation, all the experimental settings are done using one model-in-the-loop, one question prompt generator and 2 datasets of similar domains. This limitation has been called out in the paper. ",
    "strengths": "- The major takeaway is that the use of GAAs in the standard data collection process results in annotation efficiency, as measured by vMER, i.e. model error rate on the collected data, and time per model fooling example. Additionally, this results in significant improvement in model performance on downstream QA task, which is almost as performant as adversarial data collection without use of GAAs.\n- Authors demonstrate effectiveness of using GAAs by exploring effects on the adversarial data collection process as well. Speedup isn't observed here as much as the increase in downstream task performance.\n- Authors even experiment with providing answer prompts in addition to question prompts and observe a performance improvement on most downstream tasks for most of the experimental settings. So overall, providing answer prompts in addition to question prompts helps.\n- This is the first work to investigate the use of generative assistants for crowdworkers doing the data annotation.\n- This paper uses several experimental settings, albeit only few data points per setting to conduct the study.\n- The paper has clear language, explaining in detail all the components used in the experimental setup and evaluation strategy. This paper also provides the motivation behind the experimental settings which makes the objective of the exercise much clear. ",
    "weaknesses": "- This approach is compute-intensive as it requires pretraining of the GAA model on the same dataset as the QA model needs to be trained on. It assumes that the resultant GAA model is performant enough to provide meaningful prompts to annotators. If this assumption doesn't hold true, then it can result in larger annotation times, thus bringing into question the speedup claim of the proposed approach.\n- From table 3, it looks like using GAA with adversarial data collection hurts out-of-domain generalization, as seen by the dip in performance on MRQA dataset.\n- This approach has been tested only on the SQuAD dataset for all experimental settings, and just additionally MRQA for evaluation. While it is understandable that all these experimental settings require lot of time and human effort to conduct, the fact that this has been tested only on one dataset raises concerns about generalizability of this framework. Especially when use of GAAs seems to hurt performance on the out of domain dataset MRQA for adversarial data collection. So in situations where adversarial data collection cost doesn't matter as much as quality of resultant QA model, using GAAs won't work.\n- While some mild correlations emerge between sampling strategy and downstream performance, overall it seems like the best experimental setting in terms of a trade-off between annotation speed, efficiency and downstream performance depends highly on the downstream task at hand. Practically, its not feasible to try out all these experimental settings for data annotation for QA datasets for industrial/real-world applications. ",
    "comments": "- In the setting where annotators were provided with both the question and answer prompts, doesn't this limit the example diversity? This is referring to the drawbacks of generative models mentioned in the introduction section. I think maybe more data is needed to answer this. If we have some metrics showing how many times users accepted the suggestions by GAA, and if we see the most of the GAA suggestions were accepted, then it means that it is the GAA that is generating data points for the most part, with minimal human intervention.\n- In table 4, there is a spike in model performance in terms of annotation efficiency for likelihood sampling strategy in the Adversarial QA dataset. Such a trend was not seen for question prompts only setting, for both standard and adversarial data collection processes. Can you please provide a reason/explanation for this observation? ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]