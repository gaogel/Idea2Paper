[
  {
    "review_id": "3c5f1606df31ea38",
    "paper_id": "ARR_2022_76",
    "reviewer": "Sheikh Muhammad Sarwar",
    "paper_summary": "This paper addresses some key issues of the Entity Set Expansion (ESE) problem; from the evaluation metrics and dataset perspective. The authors mentioned that existing datasets do not consider entities that belong to multiple concepts, are non-named and vaguely attached to the concepts they belong to. They show how their benchmark datasets capture these properties. They also argue that MAE at top-20, which is a standard evaluation metric for ESE approaches, is not sufficient to understand their effectiveness. They propose MAE. at gold-k as an alternative. ",
    "strengths": "The paper proposes three benchmark datasets that are helpful for evaluating ESE approaches. The authors focus on three key issues of ESE datasets and address those in constructing their own datasets. ",
    "weaknesses": "The study is not well motivated. The paper does not consider the application perspective of ESE approaches and the arguments about evaluation metrics is not strong. The description of the dataset construction process is inadequate. ",
    "comments": "What is the motivation behind applying ESE methods on user-generated text? Why would someone apply set expansion approaches to construct a dictionary from customer reviews? For example, a customer might mention that the room type of a hotel is small. How does that help in constructing a dictionary for the room type concept?   “We first select concepts for the seed by referring to the features on the corresponding websites, to ensure their relevance for immediate downstream tasks.” - This is how the authors define the relevance for immediate downstream tasks. But, I worry if ESE has application in such scenarios. In fact, ESE is a useful mechanism when a concept is complicated enough to be expressed as a query. The concepts selected by the authors in this paper are well-defined by the site owners.  The argument about the new metric MAP at gold-k is not solid. From the retrieval perspective MAP at top-20 still makes sense. If someone is interested to retrieve all the entities belonging to a concept, they can look at the top-20 retrieved entities, annotate them, and then get more seeds to train a model to retrieve more. Based on this iterative process, one can find all the relevant entities belonging to a concept. There is a Total Recall track at the Text Retrieval Conference (TREC) that tries to achieve this. That’s why we should not disregard MAP at top-20, but we can explore the performance of a method at gold-k for reference. How does the map at gold-k makes sense from the application perspective? If an ESE approach returns a ranked list of entities, at which depth a user should go to find all the relevant entities? The user does not have any idea about gold-k and could only go to some pre-defined depth such as 20. Different depths could be explored considering how much effort a user is interested to put in, but MAP at top-20 should never be ignored.  “Existing evaluation metrics tend to overestimate the real-world performance of ESE methods and may be unreliable for evaluating concepts with large entity sets.” — I do not agree with this takeaway because of the above statement.  The effectiveness of ensemble approaches is not surprising.  The performance difference between CGExpan and LM-Base (proposed by the authors) is striking even if the underlying techniques are very similar. Can the authors provide more insight into this? Why does LM-base perform so well compared to CGExpan in user-generated datasets? I wonder if there could be hyperparameters for CGExpan that are not well-tuned.  I find the dataset description to be inadequate. Specifically, the source of the data is not clearly mentioned. ",
    "overall_score": "2.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]