[
  {
    "review_id": "3f1a09e155c657a1",
    "paper_id": "ARR_2022_330",
    "reviewer": "Yoshihiko Hayashi",
    "paper_summary": "This paper deals with intriguing and important issues of answer uncertainty and unanswerability in multiple-choice MRC. \nIf the evaluation is conducted under a negative marking scheme, it is sensible to choose an answer or abstain from giving an answer if the system is not very confident with any of the answer options. \nOr, if the QA setting includes the none-of-the-above (NOA) option, how to choose this option may pose a problem: the system may rely on the confidence to select one of the non-NOA options, or it could try to detect the unanswerability of the question explicitly. \nTo investigate these issues, this paper proposes to use an expected entropy as the measure of predictive uncertainty. \nIn addition, the authors devised a dedicated benchmark dataset by modifying the ReClor corpus while removing the correct answer from a subset of possible answers. \nThis paper presents empirical results from the extensive experiments using this dataset and concludes that the uncertainty measure could be used to detect questions that the system is not confident about. \nMoreover, the experimental results show that this approach is better than a system explicitly trained with unanswerable examples. ",
    "strengths": "- This paper investigates the issues of answer uncertainty and unanswerability in multiple-choice MRC, which are intriguing from the perspective of language understanding yet crucial in developing an MRC/QA system that works in more realistic scenarios than experimental settings.\n- This paper empirically demonstrated that the expected entropy could be better used to determine answer-or-not-to-answer (in a negative marking scheme) and choose the NOA option (if it is included in the QA setting).\n- These main results and other convincing trends are obtained from well-organized experiments that exploit the benchmark dataset devised by altering the ReClor corpus. ",
    "weaknesses": "- The proposed measure of uncertainty (expected entropy) should/could be compared with alternatives in the experiments.\n- Although the conclusions may be reliable and valuable, they are drawn from the experiments with the particular dataset (modified ReClor). The experiments with other datasets, such as RACE, could enhance the generalizability of the results. ",
    "comments": "- Answer options are shown with numbers in Fig.1, while they are presented as Option A, B, and C in the last paragraph of section 5.\n- The notions of data uncertainty and knowledge uncertainty should be more clearly defined/explained. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "94d472d4e386f376",
    "paper_id": "ARR_2022_330",
    "reviewer": "Rajkumar Pujari",
    "paper_summary": "Summary:     This paper aims to explore unanswerability in multiple-choice MRC. Specifically, they aim to use uncertainty measures to identify unanswerable questions based on the probability distribution of the answer choices provided. They use ReColr MRC dataset for their experiments. First, they create a data variation that replaces one of the choices with 'None of the Above' to create a dataset with 25% unanswerable questions. Then, they perform experiments with Electra PLM using expected entropy over the answer choices as the metric that measures uncertainty. First, they show that under a harsh negative-penalty (3:5) for wrong answers, abstaining from answering a certain amount of questions improves the overall performance. They observe that it doesn't really help in case of more common/usual negative penalty schemes such as 3:1. Then, they use the data manipulation to train explicit and implicit models to see if the uncertainty measure would help the implicit model perform reasonably in comparison to the explicit model. They observe that the implicit model shows some meaningful gains over a weaker MAP model.  Contributions: 1) Exploring uncertainty measures in detecting unanswerable questions in MRC 2) Demonstrating the use of explicit entropy uncertainty measure for this use case ",
    "strengths": "Strengths: 1) Detecting unanswerability in MRC questions is an important research question and using uncertainty measures over output probability distribution is a valid experimental endeavor 2) The insights obtained via the analysis in the paper are interesting ",
    "weaknesses": "Weaknesses: 1) The paper doesn't present any clear hypothesis and experimental set-up to validate their hypothesis. Neither do the authors explore the issue of unanswerability in MRC in exhaustive detail. They fail to account for previous works on this task (https://ojs.aaai.org/index.php/AAAI/article/download/4619/4497) 2) The claims made in the abstract are unsupported in the results. The authors claim that \"it is shown that uncertainty outperforms a system explicitly built with an NOA option\". But, the results of the Implicit system are inferior to that of the Explicit system. The explicit: option A system is not a meaningful system in itself. \n3) The results of the negative penalty scheme show that the proposed uncertainty measure is not that useful in common cases such as 3:1. The authors fail to explore reasons for this, neither do they attempt to experiment with other MRC datasets to see if this is ubiquitous. This makes the exploration incomplete. \n4) The writing of the paper is quite convoluted. The paper is not divided into motivation, hypothesis, proposal, experimental design, results, discussion and related work clearly. Most of the sections are a mix of several of these componenets. This undermines the readability of the paper significantly. ",
    "comments": "1) Kindly include the details of the expected entropy calculations in the main paper to make the paper self-contained. \n2) Consider adding an explicit related work section which also discusses work on out-of-distribution example detection, which is a closely related more general problem to the questions this paper aims to explore. ",
    "overall_score": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "83332f14ef0ffd23",
    "paper_id": "ARR_2022_330",
    "reviewer": null,
    "paper_summary": "The paper investigates the problem of identifying unanswerable questions in multiple choice MRC. It proposes two ways of tackling this problem: Firstly, by explicitly augmenting training data with unanswerable examples and secondly, by thresholding on (estimated) prediction uncertainty. The paper goes on to show that this can help to identify and abstain from (falsely) predicting hard examples and to identify unanswerable questions on a constructed dataset. ",
    "strengths": "The stregth of the paper is that it touches upon a topic that appears under-explored in the literature. The paper is written well and the results are presented clearly. Evaluation metrics are well motivated and discussed in detail, which is important as they deviate from the usual F1/Accuracy measures used for evaluating MC-MRC. ",
    "weaknesses": "I have identified some weaknesses in the paper:  - How general are the obtained results? From what I can tell, most of the analysis is performed on the results of one optimised model (ensemble), on one dataset (ReClor). I believe the methodology is general enough to be applied to other MC-MRC datasets. Why was ReClor and only ReClor chosen? It would be interesting to see, whether the reported results pertain across different datasets. For example CosmosQA (https://arxiv.org/pdf/1909.00277.pdf) comes with built-in unanswerable questions. This work should be mentioned. \n - While most of the results are clear, I had difficulties interpreting the figures. What makes it difficult is that they all have different axes and threshold over different values. I believe compacting section 2 that discusses the high-level overview of the MC-MRC task to create more room for more thorough explanations of the results would be beneficial. This could be done by providing more informative captions of the figures or linking back to the equations and introduced names (e.g. beta). ",
    "comments": "What follows are some minor remarks, questions and comments: - The hyperparameter section is rather terse. It is not clear which hyper-parameters are selected from. Appropriate information should be added, at least in the appendix.\n- What is \"Fraction unanswerable\" in Figure 5 and how is it obtained?\n- I don't believe it is fair to make the comparison in Table 4. While the manuscript acknowledges that, it does not mention the precise nature of the unfairness. The point is that for MAP, the %UNANS is based on model predictions, it's not a parameter that can be freely chosen, unlike the Implicit method. Here, the dev-mixed set was used both for reporting the final accuracy comparison and to select the best %UNANS threshhold for implicit. It would be more fair to either estimate %UNANS from the training data (i.e. 25%) or to split the DEV-mixed in half, use one half to estimate the best %UNANS threshhold and the other half to compare Implicit with the chosen threshhold to MAP. This wouldn't change much of the argument as from what I can tell from Figure 5, Implicit is still better than MAP with %UNANS of 25. ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "5ae14ebd325342b4",
    "paper_id": "ARR_2022_330",
    "reviewer": null,
    "paper_summary": "This paper describes modifications made to the ReClor data set and ELECTRA, a system built to perform machine reading comprehension (MRC) on that data set, so that both the data and the system handle uncertainty in the selection of multiple-choice answers and abstain from providing an answer when the question posed is unanswerable.  ReClor is augmented to contain four variations of the same content/question/answer-choice triplets but with three versions of the same triplet containing a \"None of the above\" answer choice.  Then ELECTRA is modified to produce confidence scores for its answer selections using one of several metrics, the best-performing of which is expected entropy. ",
    "strengths": "The authors of this paper are correct in that most multiple-choice data sets like these don't include a \"None of the above\" option, and that most standardized tests, especially those that are adaptive (e.g. GRE) don't penalize test-takers for not answering a given question.  In order to mimic that, data sets need uncertainty built into them, which can be provided using the augmentation strategy here, if needed. ",
    "weaknesses": "I think many members of our community will find this work rather niche.  It might be better suited for BEA.\nThe authors build on top of a system called ELECTRA which is a shared task entry that has not yet been released, or even published.  Therefore, it's difficult to understand the modifications made to ELECTRA described in this paper.\nThe authors should consider doing evaluations using other data sets besides ReClor to see if their augmentation strategy generalizes well. ",
    "comments": "Your abstract is way too long.  You can shorten it starting at the sentence, \"This paper investigates both of these issues by making use of predictive uncertainty\", remove everything until you get to \"It is shown\", then replace \"uncertainty. It is shown\" with \"uncertainty, and shows\".  Also, in the last sentence of your abstract, \"it is shown that\" should be \"we show that\".\nOn line 050, replace \"consistently observed\" with \"been used to create\".\nOn lines 066-067, remove \", and of course...\" to the end of the sentence.\nOn lines 074-076, you can remove everything prior to \"answer uncertainty\" and begin this sentence with \"Answer uncertainty\" for clarity.\nOn line 192, insert \"give\" between \"and\" and \"no\".\nOn line 194, \"discourages\" should just be \"discourage\".\nOn line 234, \"increase demand\" should be \"increased demand\".\nOn line 273, \"ensembled-based\" should just be \"ensemble-based\".\nOn lines 338-339, change \"with unanswerable examples too\" to \"that also contain unanswerable examples\".\nIn Table 2, can you clarify what \"Paper\" and \"Others\" are?  Which papers do these come from?\nOn line 466, insert \"and\" in front of \"only\".\nCan you provide some examples content/question/answer-choice triplets from ReClor?  What is ReClor's domain? ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]