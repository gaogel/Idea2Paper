[
  {
    "review_id": "64d426bdd2237b1a",
    "paper_id": "ARR_2022_311",
    "reviewer": null,
    "paper_summary": "This paper proposes, as a form of data augmentation, to replace the one-hot sequences that are typically consumed by text classification models with an interpoloation between this one-hot distribution and a distribution over word-types obtained by running the sequence through BERT. The authors show that this form of augmentation helps on its own as well as when combined with other standard data augmentation techniques. ",
    "strengths": "- The paper obtains good results with a straightforward approach.\n- The paper is written fairly clearly. ",
    "weaknesses": "- The paper needs some light editing (especially Section 4.1) but I don't see any significant weaknesses. ",
    "comments": "- One thing I wasn't sure I understood is whether the \"smoothed\" inputs are used on their own to train the model, or if they're used in addition to the standard inputs. Lines 236-239 make me think they're used in addition. This is fine, but should be emphasized more explicitly.\n- Since, as the authors note, they are are merely approximating the token-distribution given by BERT (by not using any MASK tokens), it might be interesting to see whether this approximation is in fact hurting the performance or not. That is, if we obtain token-level distributions by masking each token in the input in turn, and then use the resulting smoothed representations, is this better or worse for augmentation than the approximation the authors propose? ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "0ac9820142b39e37",
    "paper_id": "ARR_2022_311",
    "reviewer": null,
    "paper_summary": "The paper proposes a data augmentation method using a controllable smoothed representation, which is obtained by combining the one-hot representation and the smooth representation through masked language modeling. The authors showed the effectiveness of the proposed method on low-resourced sentence classification task. They also found that the smooth representation can be used on other data augmentation method to achieve better results. ",
    "strengths": "- The paper is well-structured. The authors explained the motivation and the methodology clearly. Figure 1 and 2 are informative and help the readers understand better understand the method.\n- It is nice that the text smoothing can be combined with other data augmentation approaches to achieve better performances. ",
    "weaknesses": "- The main weaknesses of the paper are the experiments, which is understandable for a short paper but I'd still expect it to be stronger. First, the setting is only on extremely low-resource regime, which is not the only case we want to use data augmentation in real-world applications. Also, sentence classification is an easier task. I feel like the proposed augmentation method has potential to be used on more NLP tasks, which was unfortunately not shown.\n- The proposed mixup strategy is very simple (Equation 5), I wonder if the authors have tried other ways to interpolate the one-hot vector with the MLM smoothed vector. ",
    "comments": "- How does \\lambda influence the performances?\n- How does the augmentation method compare to other baselines with more training data? ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "c6ac27ad9dd330a6",
    "paper_id": "ARR_2022_311",
    "reviewer": "Sanqiang Zhao",
    "paper_summary": "The paper introduces a text smoothing approach and uses it in different downstream tasks. Different types of sentence classification tasks are improved by using such text smoothing method. ",
    "strengths": "The paper demonstrate improvement from using proposed text smoothing method. ",
    "weaknesses": "This paper does not make a significant contribution. Smoothed Representation and Mixup Strategy are not proposed by the authors. Moreover, the strategy is too simple to combine the one-hot representation and smoothed representation with a weight parameter lambda.\nThree low-resource sentence classification tasks are used in the experiments. Even more lower-resource task classifications are missed, such as MRPC in GLUE.  It lacks the configuration of lambda, I believe this can greatly affect performance.\nI am still unsure about the motivation, the high probability of \"average\" is only due to MLM. Nevertheless, the semantic meaning of \"average\" is learned by MLM and a downstream task like sentiment analysis would only care about its semantic meaning. ",
    "comments": "One of the biggest problems is that motivation is not convincing. I think it would be better to have examples of different predictions using your text smoothing approach. In Figure 2, how does the \"average\" likely affect the sentiment analysis label?\nThough some of the tasks aren't very useful, I believe more classification tasks should be conducted. In my experience of GLUE, SST and MNLI are two of the most resourceful sentence classification tasks compared to other tasks in GLUE. ",
    "overall_score": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "ee5e4ca21e0a5335",
    "paper_id": "ARR_2022_311",
    "reviewer": "Shuhuai Ren",
    "paper_summary": "This paper proposes a data augmentation technique named Text Smoothing, which converts sentences from their one-hot representations to controllable smoothed representations. Specifically, the authors multiply the output of a pre-trained BERT with the word embedding matrix to get the smoothed representation of an input token. Then the smoothed representation is combined with the one-hot representation by mixup to get the mixed representation. Using such mixed representation instead of the one-hot representation as the token input can be regarded as a kind of data augmentation and can significantly improve the model performance in the low-resource regime. ",
    "strengths": "__1. The paper is well organized and easy to follow.__\n__2. The proposed method is novel and interesting:__ The idea of mixing the one-hot representation and the LM (smoothed) representation for an input token is very different from other works of data augmentation. It uses the knowledge of pre-trained BERT to integrate and compress information of multiple related words, providing richer semantics that a single token in the one-hot form cannot provide.  __3. The experimental results are impressive:__ The improvement brought by text smoothing is very significant and surpasses many strong baselines. Besides, the text smoothing can be also well combined with various data augmentation methods, showing great practicability and universality. ",
    "weaknesses": "__1. Lack of significance test:__      I'm glad to see the paper reports the standard deviation of accuracy among 15 runs. However, the standard deviation of the proposed method overlaps significantly with that of the best baseline, which raises my concern about whether the improvement is statistically significant. It would be better to conduct a significance test on the experimental results. \n     __2. Anomalous result:__ According to Table 3, the performance of BARTword and BARTspan on SST-2 degrades a lot after incorporating text smoothing, why?\n__3. Lack of experimental results on more datasets:__ I suggest conducting experiments on more datasets to make a more comprehensive evaluation of the proposed method. The experiments on the full dataset instead of that in the low-resource regime are also encouraged.\n__4. Lack of some technical details:__ __4.1__. Is the smoothed representation all calculated based on pre-trained BERT, even when the text smoothing method is adapted to GPT2 and BART models (e.g., GPT2context, BARTword, etc.)? \n     __4.2__. What is the value of the hyperparameter lambda of the mixup in the experiments? Will the setting of this hyperparameter have a great impact on the result? \n     __4.3__. Generally, traditional data augmentation methods have the setting of __augmentation magnification__, i.e., the number of augmented samples generated for each original sample. Is there such a setting in the proposed method? If so, how many augmented samples are synthesized for each original sample? ",
    "comments": "1. Some items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.\n2. The number of BARTword + text smoothing and BARTspan + text smoothing on SST-2 in Table 3 should NOT be in bold as they cause degeneration of the performance.\n3. I suggest Listening 1 to reflect the process of sending interpolated_repr into the task model to get the final representation ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]