[
  {
    "review_id": "0b48ae9564885ff8",
    "paper_id": "ARR_2022_227",
    "reviewer": null,
    "paper_summary": "The paper addresses a fundamental shortcoming in evaluating conversational question answering (CQA) models based on the gold-annotated question-answer pairs. The existing evaluation procedure ignores the impact of the model’s output on upcoming exchanges, thereby detracting from real-world scenarios. Specifically, when a model flounders, the conversation is likely to be progressed differently than what is already annotated. The current evaluation mechanism does not account for these cases.\nThe paper first highlights this problem by using human annotators to converse with 4 CQA models, trained on QuAC. After collecting QA conversations, another group of annotators check the validity of conversations by (1) checking whether the question at each turn is valid and answerable, and (2) verifying whether the models’ answers are correct. This experiment reveals that human evaluation substantially differs from gold-answer evaluation when the relative performance of the 4 models is compared using each evaluation method.\nThe paper finds that using predicted answers while retaining existing questions suffers from 3 drawbacks: unresolved coreference, incoherence, and change in the correct answer. Since the most frequent issue was related to coreference, the paper proposes a question-rewrite method to amend references by replacing them with the original entities, mentioned in the history. This method is empirically shown to be most aligned with human evaluation when compared to other evaluation methods.\nFinally, the paper suggests 3 critical areas that future models should focus on: (1) ability to detect question dependencies, (2) ability to detect unanswered questions, (3) evaluation under real-world settings. ",
    "strengths": "- The paper focuses on an interesting problem and the insights provided in this work can be of interest to the community and spark future research.\n- The analysis of CQA models is much-needed in the literature. Once we understood where our models fall short, we can make meaningful progress in building more effective CQA models.\nOverall, I think the paper is a solid work. ",
    "weaknesses": "- My main concern with this paper is the proposed question-rewrite strategy. I’m not convinced this method covers a sufficiently large number of questions because out of 100 passages (or \\~3.7K questions based on Table 4 in the appendix), 23% become invalid (\\~850 questions), out of which 44% are attributed to unresolved coreference (\\~375 questions). As reported at L373, the proposed method rewrites 12% of the questions for all models (\\~45 questions per model), which hardly has a significant effect.\n- I don’t understand why generating question-in-context lags behind the question-rewrite method. The question-rewrite method may make a question ungrammatical (as pointed out in $\\S$B) because the original entity can be a clause or the coreference resolution tool may make mistakes. On the other hand, the question-replace model is trained to generate contextualized questions and its output is more likely to be grammatically correct. ",
    "comments": "L238-L241: The sentence seems to be repeated but with different agreements. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "973824caabb9511d",
    "paper_id": "ARR_2022_227",
    "reviewer": null,
    "paper_summary": "This paper focuses on the task of Conversational Question Answering (CQA), which envisions a back-and-forth question answering scenario with an information-seeking goal. The work shows how the most common way of automatically evaluating CQA systems (on a popular instantiation of the task with one specific English dataset) is empirically different from how humans would evaluate the systems in an actual real-world scenario. The models use a dataset of human-human conversations and evaluation is commonly done by using the 'gold-standard' conversational history. But the human-machine conversations, which would result when the models are deployed or used, represent a fundamentally different distribution from human-human conversations. This means the current evaluation paradigm does not accurately capture the use-case and therefore, tracking modeling improvement with that evaluation paradigm will not be meaningful. The work's primary contribution is showing this discrepancy by conducting a human evaluation study where CQA systems get to interact with humans - the result human-machine conversational QA dataset is also a contribution. Further analysis shows how automated evaluation differs from human evaluation, and what human evaluation tells about current CQA modeling strategies including insights for future work. The work also puts forth a new automated evaluation paradigm that mirrors human evaluation results more closely than the existing evaluation strategy of using the gold answers. ",
    "strengths": "- The paper highlights a fundamental discrepancy between the dominant automated evaluation paradigm and human judgment of CQA models when situated in the real-world scenario - this exposition is meaningful for the CQA community in order to take stock of whether modeling improvements can be claimed with an evaluation strategy that seems to be quite broken.  - Further, the exposition of this problem in automated evaluation is carried out rigorously by conducting a user study where humans interact with different CQA models - and the resulting human-machine dataset is another important contribution. The work does also shows the different ways in which the problem with the current automated evaluation paradigm manifests itself.  - The difference in the distribution of human-human and human-machine conversations is made clear, and what that means for current automated model evaluation (such as models being ranked differently in terms of automated and human evaluation) is explained well.  - The insights derived for different modeling strategies and how they fare on human evaluation can help better understand differences and relative strengths and weaknesses of different CQA models. These insights can inform future work on CQA modeling and some solid recommendations are indeed put forth in this work.  - The paper is written clearly and organized well. The main ideas and contributions, and their importance, are presented early and quite clearly. ",
    "weaknesses": "1. The case made for adopting the proposed strategy for a new automated evaluation paradigm - auto-rewrite (where the questions that are not valid due to a coreference resolution failure in terms of the previous answer get their entity replaced to be made consistent with the gold conversational history) - seems weak. While the proposed strategy does seem to do better in terms of being closer to how humans evaluated the 4 models (all in the context of one specific English dataset), it is not clear how the proposed strategy -      a) does better than the previously proposed strategy of using model-predicted history (auto-pred). Looking at the comparison results for different evaluations - in terms of table 1, there definitely does not seem to be much difference between the two strategies (auto-rewrite and auto-pred). In fig 5, for some (2/6) pairs, the pred-history strategy has higher agreement than the proposed auto-rewrite strategy while they are all at the same agreement for 1/6 pairs.      b) gets to the fundamental problem with automated evaluation raised in the paper, which is that \"when placed in realistic settings, the models never have access to the ground truth (gold answers) and are only exposed to the conversational history and the passage.\" The proposed strategy seems to need gold answers as well, which is incompatible with the real-world use case. The previously proposed auto-pred strategy, however, uses only the questions and the model's own predictions to form the conversational history - which seems to be more compatible with the real-world use case.  In summary, it is not clear why the proposed new way of automatically evaluating CQA systems is better or should be adopted as opposed to the previously proposed automated evaluation method of using a model's predictions as the conversational history (auto-pred), and the comparison between the results for these two automated strategies seems to be a missing exploration and discussion. ",
    "comments": "Questions to the authors (which also act as suggestions):  Q1. - Line 151: \"four representative CQA models\" - what does representative mean here? representative in what sense? In terms of types or architectures of models? This needs clarification and takes on importance because the discrepancy, in terms of how models get evaluated on human vs automated evaluation, depends on these four models in a sense.  Q2. Line 196: \"We noticed that the annotators are biased when evaluating the correctness of answers\" - are any statistics on this available?  Q3. Section 3.1: For Mechanical Turk crowdsourcing work, what was the compensation rate for the annotators? This should be mentioned, if not in the main text, then add to appendix and point to it in the main text. Also, following the work in Card et al (2020) (\"With Little Power Comes Great Responsibility.\") - were there any steps taken to ensure the human annotation collection study was appropriately powered? ( If not, consider noting or discussing this somewhere in the paper as it helps with understanding the validity of human experiments) Q4. Lines 264-265: \"The gap between HAM and ExCorD is significant in Auto-Gold\" - how is significance measured here?\nQ5. Lines 360-364: \"We determine whether e∗_j = e_j by checking if F1(s∗_{j,1}, s_{j,1}) > 0 ....  .... as long as their first mentions have word overlap.\" Two questions here -      5a. It is not clear why word overlap was used and not just an exact match here? What about cases where there is some word overlap but the two entities are indeed different, and therefore, the question is invalid (in terms of coreference resolution) but deemed valid? \n    5b. How accurate is this invalid question detection strategy? In case this has not already been measured, perhaps a sample of instances where predicted history invalidates questions via unresolved coreference (marked by humans) can be used to then detect if the automated method catches these instances accurately. Having some idea of how well invalid question detection happens is needed to get a sense of if or how many of the invalid questions will get rewritten.  Comments, suggestions, typos:  - Line 031: \"has the promise to revolutionize\" - this should be substantiated further, seems quite vague.  - Line 048: \"extremely competitive performance of\" - what is 'performance' for these systems? ideally be specific since, at this point in the paper, we do not know what is being measured, and 'extremely competitive' is also quite vague.  - The abstract is written well and invokes intrigue early - could potentially be made even better if, for \"evaluating with gold answers is inconsistent with human evaluation\" - an example of the inconsistency, such as models get ranked differently is also given there.  - Line 033: \"With recent development of large-scale datasets\" -> the* recent development, but more importantly - which languages are these datasets in? And for this overall work on CQA, the language which is focused on should be mentioned early on in the introduction and ideally in the abstract itself.  - Line 147: \"more modeling work has been done than in free-form question answering\" - potential typo, maybe it should be \"maybe more modeling work has been done 'in that'\" - where that refers to extractive QA?\n- Line 222: \"In total, we collected 1,446 human-machine con- versations and 15,059 question-answer pairs\" - suggestion: It could be reasserted here that this dataset will be released as this collection of conversations is an important resource and contribution and does not appear to have been highlighted as much as it could.  - Figure 2: It is a bit unintuitive and confusing to see the two y-axes with different ranges and interpret what it means for the different model evaluations. Can the same ranges on the y-axes be used at least even if the two metrics are different? Perhaps the F1 can use the same range as Accuracy - it would mean much smaller gold bars but hopefully, still get the point across without trying to keep two different ranges in our head? Still, the two measures are different - consider making two side-by-side plots instead if that is feasible instead of both evaluations represented in the same chart.  - Lines 250-252: \"the absolute numbers of human evaluation are much higher than those of automatic evaluations\" - saying this seems a bit suspect - what does absolute accuracy numbers being higher than F1 scores mean? They are two different metrics altogether and should probably not be compared in this manner. Dropping this, the other implications still hold well and get the point across - the different ranking of certain models, and Auto-Gold conveying a gap between two models where Human Eval does not.  - Line 348: \"background 4, S∗ - latex styling suggestion, add footnote marker only right after the punctuation for that renders better with latex, so - \"background,\\footnote{} ...\" in latex.\n- Footnote 4: 'empirically helpful' - should have a cite or something to back that there.  - Related Work section: a suggestion that could make this section but perhaps also the broader work stronger and more interesting to a broader audience is making the connection to how this work fits with other work looking at different NLP tasks that looks at failures of the popular automated evaluation strategy or metrics failing to capture or differing significantly from how humans would evaluate systems in a real-world setting. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "48f30939f1c93eba",
    "paper_id": "ARR_2022_227",
    "reviewer": null,
    "paper_summary": "The paper conducts a large-scale human evaluation of human-machine conversations using four state-of-the-art CQA systems on the QuAC dataset. The motivation behind this study is that using gold history does not generalize well to real-world applications. In terms of model ranking, the study shows disagreement between human and gold history evaluations, which is expected, given that humans are interactive and will have followup questions based on model's outputs. An intuitive solution is using predicated history, however, this invalidates some questions. To this end, the paper also proposes a simple question re-writing mechanism based on co-reference resolution to correct invalid questions when the predicted history is used. The paper reports many interesting findings. ",
    "strengths": "The paper is well-written and easy to follow.\nThe motivation is clear and the experimental setup is sound The human evaluation of human-machine conversations and the findings in the paper are insightful and would improve how CQA models are evaluated, and how future CQA datasets are constructed. ",
    "weaknesses": "It seems that the proposed re-writing approach makes re-written questions context-independent and self-contained (i.e., easier for the models). For example, \"What songs were in Parade\" could be answered without history information. Moreover, the proposed mechanism seems to kill the naturality of a conversation. Can you please elaborate on this?\nWith human evaluation (shown in Table 2), the number of unanswerable questions is higher across all models. I wonder if this is an artefact of humans conversing with models or the fact that humans (annotators) did not have access to the full passage. ",
    "comments": "As a followup work, it would be insightful to conduct a similar study on different CQA datasets ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]