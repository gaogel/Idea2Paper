[
  {
    "review_id": "26bc970b4186bbf8",
    "paper_id": "ARR_2022_103",
    "reviewer": null,
    "paper_summary": "My understanding of this paper is that they adapt a framework of “metamorphic relations” for use in probing an LM. This is a relation between inputs, follow up inputs based on the internal structure of those inputs, and outputs. The distinction between this and previous work is that they consider relations between inputs, rather than looking at single inputs.  The first experiment they run is applying a particular transformation, in the form of extra words, to a pair of texts. They then run a sentiment classifier and see whether that classifier maintains are ranking between the pair of sequences after their transformation. If it maintains a ranking, this indicates that the model is systematic. The next experiment also involves transformations in NLI inputs, which lead to specific changes in inference labels. Finally, they test if transitive relations of hypernymy and synonymity, when applied to pairs (x1,x2) and (x2,x3), also apply to (x1,x3). ",
    "strengths": "Overall I really like these complex rule based methods. I think that this is more principled and clearly considered than probing methods which use learned peripheral models. ",
    "weaknesses": "I’m skeptical that systematicity requires that the same transformation always makes the same adjustment geometrically on the output. Consider the following examples:  “I just bought this new software.” \n“I just bought this new mascara.”\nNow we append the same transformation to the text: “It doesn’t run!” We shouldn’t expect that this transformation is going to have the same effect on both reviews, and yet you would consider this to not be a systematic combination. A transformation that has different effects based on context is not necessarily a systematicity violation.\n“we can conclude that pairwise-systematicity testing reveals a different issue in the model f than classic non-metamorphic testing.” Where is the classic testing that you would contrast this with to check whether this issue shows up?\nIn general, it seems like one advantage of this method *should* be avoiding the expense of gold standard labels that would be used for another kind of probe, but if instead you need to have a bunch of information about hypernyms and hyponyms, that doesn’t seem like a significant gain. ",
    "comments": "This paper needs more examples of the kind of relation they are looking at, earlier in the paper as well as throughout the paper. Without examples, it’s hard to evaluate whether the transforming the source inputs with T is more expensive than using ground truth data.  Should the section on the geometry of hypernyms possibly have a citation to http://proceedings.mlr.press/v97/allen19a/allen19a.pdf ? I’m not necessarily pushing it but maybe you should check.\nIf you don’t have any further geometric analysis, it’s not clear to me that section 4.2 adds any information, as it seems to just be a set of definitions that are not later used.\nI’m not clear on why Yanaka et al. wouldn’t be considered a metamorphic test as well? ( line 520).\nI found several points where I was extremely confused. I don’t understand how you are avoiding having any kind of ground truth labels for things like hypernymy? If you don’t have hypernym information for a particular word set, how will you be able to know whether they are maintaining the output property at all? ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "ac032c5ee706b214",
    "paper_id": "ARR_2022_103",
    "reviewer": "William Merrill",
    "paper_summary": "This paper proposes using metamorphic testing to verify the behavior of NLP models. One appeal of this approach is that it can be used in some cases without collecting any training data, verifying that the outputs obey some desired relation between inputs, rather than being “correct” in the sense that they match the ground truth. The authors outline a general metamorphic testing framework (with a useful graphical model formalism to describe different variants of it), and show how most past work falls narrowly into the category of robustness tests. A robustness test is where a transformation is made to the input, and then the test verifies that some property (e.g., label invariance) holds between the original and modified inputs. In contrast, they propose three different extensions of this approach beyond robustness for systematicity, compositionality, and transitivity. Systematicity testing essentially generalizes robustness testing to take a model’s performance on related instances as a ground truth. Compositionality testing as formulated here is testing some level of faithfulness: does the representation of some semantic property in the model encode whether or not the model outputs reflect that property? Finally, transitivity testing tests whether relations between inputs that should be transitive obey transitivity. Overall, I find the general framework and specific results to be valuable contributions. ",
    "strengths": "1. The paper is well written and organized. The framework seems like a useful high-level contribution for testing NLP models, and the graphical model formalism seems like a useful formalism for visualizing and comparing different testing approaches. \n2. The systematicity testing is an interesting novel evaluation paradigm. I did not expect out-of-the-box NLP models to perform so well on this (+90%). \n3. The compositionality test also seems useful, and it is probably easier to interpret the numbers coming out of it in absolute terms compared to probing, since we should expect a strong model to achieve 100% performance. \n4. It is interesting that the transitivity test – which formalizes whether the model predictions satisfy transitivity when they should (independent of whether they are correct) suggests that models are highly non-transitive. This result is a valuable contribution, especially given that more primitive fully supervised transitivity tests have reached the opposite conclusion. ",
    "weaknesses": "1. The compositionality and transitivity tests require labeled data to train the probes, which defeats some of the utility of the method. \n2. Compositionality typically means something like “the meaning of a full expression can be computed recursively as a function of its parts, where the structure of the recursion is modulated by the syntactic structure.” I don’t think that’s exactly what your “compositionality” test is formalizing, though it is somewhat related. I would suggest changing the name to something more precise; perhaps “faithfulness”, since what you’re really testing is the correspondence between the relations the model encodes and the relations that exist in the output? Or maybe I’m missing something, and you can elaborate on how this test reflects a more general definition of compositionality. \n3. There is an inconsistency in the presentation of the systematicity test. In Equation 4, the test checks whether Psrc => Pflw. Later on, at line 340: this is condition is written as being bidirectional: Psrc ⇔ Pflw. Which one of these is right? \n4. It's not obvious how humans would do under this kind of linguistic evaluation. It would be interesting to do human evaluations of this method. In other words, use human annotators in the role of the model, and see how well they do compared to neural models as a baseline. ",
    "comments": "- What is the training data for your probes in the compositionality and transitivity experiments?\n- For the transitivity experiments, does 50% constitute a random baseline? If so, do you have any potential explanation why models are so substantially undershooting random guessing?\n- The notation of x1 and x2 vs. x = (xa, xb) in the compositionality section is a bit confusing, especially when x1 and x2 are already themselves pairs. I would suggest changing this. Maybe you could have x and x’, or rewrite xa as x_{11} and xb as x_{12}. It was unclear whether x_1 was a pair or single sentence on my first read.\n- Nit: In Def 2.2, is v a constant across all R, or can we pick a different v for each R? This is only a technical concern when the number of examples is countably infinite.\n- Line 405: This last sentence seems a bit circular. You’ve defined systematicity such that the geometric property is satisfied if and only if f is systematic. If this is all you mean, I think the sentence could be clarified, but currently, it seems like it may be making a stronger claim than this.  - Line 530: Use \\citep not \\citet ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "77d66f94629e1c83",
    "paper_id": "ARR_2022_103",
    "reviewer": "Yixin Nie",
    "paper_summary": "This paper uses metamorphic testing to check the safety of neural NLP models and proposes three new classes of metamorphic relations to test the NLP model's systematicity, compositionally, and transitivity. The method can increase the number of test cases by a polynomial factor. The paper also proposes to use a graphical notation to summarize the inner structure of metamorphic relations. ",
    "strengths": "The proposed method is flexible and can yield a much larger number of test cases than existing methods. ",
    "weaknesses": "The biggest concern regarding this paper is the lack of human verification. That is treating humans (or a group of annotators) as a black-box (same as neural network model) and checking whether the proposed transformation indeed produces valid comparison. The transformation is all based on intuitions and without human validation,  it's hard to determine whether it will be valid and therefore we do not know whether the satisfy value is indeed meaningful. ",
    "comments": "In the introduction, the paper says \"reliance on ground truth data limits the quantity and quality of test cases we can produce\". In my opinion, ground truth data will increase the quality of the test cases. I was wondering why the author believes that annotation limits the quality of the test cases. ",
    "overall_score": "2.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]