[
  {
    "review_id": "21f74d3b60a2101f",
    "paper_id": "ARR_2022_327",
    "reviewer": null,
    "paper_summary": "This paper works on multi-parallel word alignment (MPWA) through graph neural network (GNN), in particular the graph attention network (GAT). Instead of limiting the word alignment model training to parallel text (bitext), the authors follow the line of work which exploits multi-parallel corpora, which enables recovering missing alignments between a source and a target language. For this, the authors leveraged the community detection (CD) algorithm for graph, which makes the sub-graph within a community complete and while disconnecting inter-community edges, for training data labels.\nThe GNN proposed in the paper takes as input the node features which take into account the graph structure, community membership, and content embeddings which include word, position, and language. To optimize the resulting alignment edges, the authors propose two strategies to post-process the alignments based on the grow-diag-final-and employed in the statistical MT alignment, Thresholding GDFA (TGDFA) and TGDFA+orig ",
    "strengths": "The approach of multi-parallel word alignment (MPWA) is interesting not only because word alignment has largely been not worked on since the advent of neural machine translation (NMT) and only recently resurfaced, but also because of the growing interest of interpretable neural network models, which the current NMT does not really have. In addition, the SMT word alignment also benefits low-resource languages for which parallel or even monolingual corpora are not adequate.\nThe experimental result shows that the proposed graph neural network (GNN) approach outperforms the strong statistical word alignment baseline and prior graph-based and matrix-based approaches in terms of the overall alignment error rate and F1. ",
    "weaknesses": "Although the paper is interesting and strongly supported by experimental results, understanding the details is tough. For one, the difference between TGDFA+orig and TGDFA is not very clear in what is meant by the original bilingual GDFA. An illustration (page limit permitting) may be helpful to understand the difference.\nIt is unclear if HELFI training set uses 3 languages or 84 as listed in the appendix. ",
    "comments": "Apart from the concerns above, it is better to have some insights on how more other language pairs not in the test set correlates with increased performance. This can be achieved by experimental setups which include or exclude other languages not in the test set. For example, in HELFI, there can be one setup which includes only Finnish, Greek, and Hebrew and another that includes all the 84 languages in the Appendix A. The Appendix A is awkward as the section heading is not in one page with the entire content, which can be achieved by the typesetting. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]