[
  {
    "review_id": "25ecf77df8e811b8",
    "paper_id": "ARR_2022_185",
    "reviewer": null,
    "paper_summary": "This paper argues that analyze contextual embeddings is particularly effective with paraphrases, because it allows experiments to control for meaning which is presumably the same in paraphrases. The authors draw several key conclusions about BERT via this paraphrase-based analysis. ",
    "strengths": "The idea of controlling for meaning with paraphrases is unique and promising. It is clever in that instead of focusing on the ways in which context should change the meaning of words, it focuses on the specific contexts which should cause the meanings to converge. This allows for more precise control over what we expect should happen. ",
    "weaknesses": "The meaningfulness of these conclusions strongly depend on PPDB being true paraphrases. If they aren't paraphrases in all dimensions and in the strictness sense, it's unclear that one is truly controlling for meaning. In many example paraphrases shown in the paper, it's clear that one wouldn't not expect representation to be the same for even an ideal representation. For example, \"are mad\" and \"'re out of your mind\" can and often do have different meanings. Even for the highest rated paraphrases such as \"where did they come from?\" vs \"where are they from\" can mean drastically different things, and it's unclear whether a representation that has an identical interpretation of those two phrases is what we really want. ",
    "comments": "I would recommend revisiting strong assumptions being made in this paper. No two sentences mean exactly the same thing, even before discounting noise from PPDB. How these analyses are affected by differences in sentence or phrase meanings should be a big part of the discussion. ",
    "overall_score": "2.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "d684cce5e50cb1b2",
    "paper_id": "ARR_2022_185",
    "reviewer": null,
    "paper_summary": "This paper evaluates BERT representations of words in a paraphrase corpus (PPDB), mostly analyzing the extent to which representations of equivalent words are consistent between their respective sentences, demonstrating that word form carries more weight in BERT than the semantic neighborhood. ",
    "strengths": "The idea is nice and, as far as I am aware, novel, presenting insights that are both useful and straightforward. The paper is well-written and the methods are well-applied. ",
    "weaknesses": "N/A --- I reviewed this paper in its previous iteration and it appears the authors have quelled pretty much all of my concerns. Well done! ",
    "comments": "\"BERT does consistently represent phrases that are paraphrases\" (l.41) has a different meaning from \"BERT does represent phrases that are paraphrases consistently\", which is probably what you meant. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]