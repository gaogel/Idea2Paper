[
  {
    "review_id": "c699ea82d7e1a59a",
    "paper_id": "ARR_2022_251",
    "reviewer": null,
    "paper_summary": "This paper proposes a trainable subgraph retriever (SR) that acts as a plug-in module to enhance subgraph-oriented knowledge base question answering (KBQA) frameworks. The authors decouple the subgraph retriever from the reasoner based on the probabilistic formalization of KBQA. The subgraph retriever constructs the subgraph by expanding paths from topic entities and merging the trees. The authors explore weak supervision and distant supervision for training the subgraph retriever and also propose an end-to-end training method to jointly learn the retriever and reasoner. Extensive experiments show that the proposed method shows significant improvement over existing subgraph-oriented KBQA models. ",
    "strengths": "1. \tThe proposed method is very novel. The decoupled retriever and reasoner are substantially different from existing works. Different training strategies and an end-to-end variant are also studied. \n2. \tThe proposed method shows significant improvement over previous methods. \n3. \tThe experiments are very comprehensive and solid. \n4. \tThe paper is well-written and easy to follow. \n5. \tCode is provided and well-organized. ",
    "weaknesses": "1. \tWhile the proposed method is more effective than the previous methods, the efficiency (e.g. training and inference speed) is not compared. ",
    "comments": "Question: 1. \tFor the end-to-end variant, are the retrieving and the reasoning also intertwined as in previous works? Any intuition on why the proposed method is better?\nTypos and Formatting Issues: 1. \tLine 53: the publication year is missing from the citation for WebQSP. \n2. \tThe caption of the table should be put on top. \n3. \tLine 435: a space is missing between “statement.” and “SR”. \n4. \tLine 576: proves -> prove. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "cef38cf1ce556879",
    "paper_id": "ARR_2022_251",
    "reviewer": "Nan Shao",
    "paper_summary": "In this paper, the authors propose a trainable subgraph retriever (SR) decoupled from the subsequent reasoning process, which enables a plug-and-play framework to enhance any subgraph-oriented KBQA model. The paper also propose a weakly supervised pre-training for SR and a unsupervised pre-training method for the retriever. Extensive experiments are conducted on two KBQA datasets: WebQSP and CWQ. \nThe experimental results show the proposed subgraph retriever and training strategy significantly improve retrieval perfomance and achieve the SOTA performance.\nThe paper is a standard ACL-level paper and I tend to recommend to accept it. ",
    "strengths": "1. The intuition that decouple subgraph retriever and reasoner seems reasonable. \n2. The paper gives clear problem deﬁnition and formalized discussion, which makes the proposed method more convincing. \n3. Extensive experiments, clear and significant improvements. \n3. Good writting. ",
    "weaknesses": "1. The method is too complicated, may be hard to follow. ( but the authors public their code) 2. The  proposed weakly supervised pre-training method and unsupervised pre-training method seem not very related to the main contribution of the paper. ",
    "comments": "It's better to also append the results of proposed weakly supervised pre-training method and unsupervised pre-training method  to the main table (Table 2). ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work."
  },
  {
    "review_id": "fd6f572f1c527952",
    "paper_id": "ARR_2022_251",
    "reviewer": "Chen Zhang0",
    "paper_summary": "This paper proposes a trainable subgraph retrieving (SR) approach for KBQA, which can be combined with existing reasoning modules.  The SR approach uses a dual-encoder to expand paths to induce the subgraph and stop the expansion automatically. It can be trained in a weakly supervised way or in an unsupervised way and be fine-tuned in an end-to-end manner. This paper conducts experiments on two benchmark datasets with the proposed SR module.  Experiments show that the SR module leads to smaller subgraphs with higher answer recall. When combined with existing reasoning modules such as GraftNet and NSM, the approach achieves SOTA performance. ",
    "strengths": "1. The paper points out a promising direction of improving retrieving subgraphs for KBQA, which receives little attention currently.\n2. The weakly supervised and unsupervised pre-training strategies can alleviate the problem of scarcity in training data, which is an important issue in KBQA.\n3. The proposed trainable SR approach is effective in retrieving subgraphs with higher coverage and smaller sizes. The performance on two KBQA datasets is impressive. ",
    "weaknesses": "1. The proposed path expanding method might not involve much novelty. It is very similar to the UHop model in [1], which also adopts an iterative style to extract reasoning path until reaching the END, and updates question representation with historical expanded relations.  2. The trainable SR module is based on RoBERTa, while NSM and GraftNet do not employ large pretrained language models. Since RoBERTa is generally considered more powerful than LSTM and MLP, it is likely that the high performance largely results from RoBERTa rather than the trainable SR strategy itself. Then the comparison between the setting with and without SR may be unfair. Also, it is questionable whether it is worth the much larger computation resources to use a RoBERTa-based SR module instead of the simple and effective PRR.  3. There may be some problems with the mathematical formulas. In equation 6, when the similarity s(q,r) is larger, the probability of r given q should be larger rather than smaller.\n[1] UHop: An Unrestricted-Hop Relation Extraction Framework for Knowledge-Based Question Answering. NAACL’19 ",
    "comments": "1. I recommend citing the UHop paper, which is closely related to the proposed SR method.\n2. In the experiments of this paper, it is assumed that the golden topic entities are given (as indicated in lines 187-188). So it is better that you mark whether the baseline models use golden topic entities or entity linking results in the result table, which is a variable need to be controlled when we compare the performance of KBQA models. For example, BAMNet, GraftNet, and PullNet use entity linking results on WebQSP, according to original paper.\n3. The EmbedKGQA reports 66.4 Hits@1 in the original paper while Table 2 shows 46.0 Hits@1, which is much lower. Is there any explanation for this contradiction? ",
    "overall_score": "2.5 ",
    "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work."
  }
]