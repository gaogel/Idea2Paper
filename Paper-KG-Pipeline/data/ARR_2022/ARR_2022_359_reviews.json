[
  {
    "review_id": "4e676df011164c73",
    "paper_id": "ARR_2022_359",
    "reviewer": null,
    "paper_summary": "This paper proposes a unified SpeechT5 framework, which models various spoken language processing tasks as a speech/text to speech/text format based on the encoder-decoder architecture. For input speech or text, the modal-specific pre-net processes it into a shared space. Then the shared encoder-decoder network performs the sequence-to-sequence conversion. Finally, the modal-specific post-nets generate the corresponding output. For learning cross-modal alignment, a vector quantization method is proposed. Experiments on several spoken language processing tasks demonstrate the effectiveness. ",
    "strengths": "1. \tThis paper proposes a new framework, which treats various spoken language processing tasks as a unified format. With the help of the task-specific pre/post-nets, the encoder-decoder network is shared across all tasks. This makes the efficient training. \n2. \tThis paper proposes a vector quantized method for cross-modal alignment. This technique may inspire related studies about cross-modal learning. \n3. \tThe proposed SpeechT5 achieves multiply state-of-the-art results on spoken language processing tasks. The released models will promote the development of the related community. ",
    "weaknesses": "Although this paper proposes a unified framework, the design of the pre/post-nets for each task is not novel. In addition, the quantized method has been explored in the previous work. The primary contribution of this paper is to integrate the existed strategies into a unified framework. ",
    "comments": "The amount of the training data and the model size are important for pre-training. The dominant pre-trained methods usually report the results and release the models with the different settings. I suggest the authors do it, which will improve the influence of the work. ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "6bd292c3f083f1a2",
    "paper_id": "ARR_2022_359",
    "reviewer": null,
    "paper_summary": "This paper proposes SpeechT5, a multimodal transformer model that pretrains on a variety of speech/text related tasks, including speech recognition, speech translation, speech synthesis and voice conversion, etc. The authors adapts T5 (an encoder-decoder transformer) to speech pretraining by jointly learning speech and texts. The joint learning are said to improve the performance of downstream tasks. While evaluated on a variety of speech related tasks, SpeechT5 demonstrates impressive performance compared to other pretrained models (Wav2Vec2 & HuBERT).   This paper contains non-trivial contributions. As the authors noted, this is the first model that try to combine speech recognition and generation tasks in speech pre-training. This alone will be an important contribution to the field of speech processing, as previous pretrained models like Wav2Vec2 & HuBERT models do not focus on speech generation tasks. This could be a step towards uniting the pipeline of speech processing. ",
    "strengths": "- A novel appriach to combine speech recognition and generation through jointly training on speech & text.\n- Exhaustive evaluation and strong performance across a variety of speech related tasks.  - Good documentation of experiments.\n- The writing is clear and easy to follow. ",
    "weaknesses": "- I do not see any major weaknesses of this paper but there are a minor points.\n- The authors evaluated the model on speech-to-text, text-to-speech and speech-to-speech tasks but did not include text-to-text tasks in the evaluation. Yet this evaluation could be important for understanding the advantages/limitations of SpeechT5. ",
    "comments": "- Vased on what is written in this paper, it seems that the authors do not state any plans to release the code and the pre-trained weights. SpeechT5 is a complex model with several pretraining tasks and ultilizes a lot of computing resources. It might not be easy to replicate the model for a significant number of researchers. If the authors can consider releasing the model, it will benefit more of the research community.  - As speech and texts are projected into the same latent space, it will be interesting to see an analysis of the joint embedding space to better understand how the model learns. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]