[
  {
    "review_id": "0c2af0134351deb5",
    "paper_id": "ARR_2022_168",
    "reviewer": "Marco Gaido",
    "paper_summary": "The paper discusses the introduction into direct speech translation models of contrastive learning methods that aim at reducing the difference between the encoded representations of speech and text with the same content. The main goal is to increase the transfer learning benefits of cross-modal systems, trained both with textual and audio data, and hence obtaining better performance especially in ST, leveraging large parallel textual corpora, which are easily available on the contrary of ST data. The effectiveness of the proposed approach is validated in different data conditions, comparing it with several previous works and with a thorough analysis targeted to validate the claim that similarity of the representations of the same sentences with different modalities is actually increased. ",
    "strengths": "The approach proposed in the paper is interesting and the results reported are promising. The main strength of the paper in my opinion is the analysis section on the modality, which demonstrates that the proposed method actually brings the benefits it was introduced for. Moreover, it motivates a possible future adoption of the proposed architecture in application scenarios where the similarity between the representations of textual and audio versions of the same content is needed. ",
    "weaknesses": "The main problem of the paper in my opinion is related to the data Augmentation: I have not been able to understand exactly which are the data augmentation methods used, and their impact on the scores (line 461-470 were quite obscure to me) and how this affects the comparison with other methods. In other words, the higher scores obtained with respect to other works cited in Table 1 are due to the contrastive learning method or to the different data augmentation methods? The comparison with some of the reported works is already not fair due to the different data condition (the usage of external speech data). It would have been better to compare with the other architectures/works repeating their experiments with the same data conditions (including data augmentation methods).\nThe reproducibility of this work would not be easy, since the code is not open sources and some details of the experimental settings are not very clear:  - line 184-186: the 2 convolutions are said to have stride 4, and that in total they shrink the input by a factor of 4. The two claims are contrasting. I guess the 2 convolutions actually have stride 2. 4 is the kernel size maybe? \n - line 236-251: the same letters (u and v) are used for different things and v is sometimes treated as a variable, sometimes as a function. This section could be more clear with a simpler notation. \n - line 342-344: Why using 10k as vocabulary size? Most of the works in literature (and also cited in table 1) use 8k (eg. Wang et al. 2020a), as also suggested in https://aclanthology.org/2020.amta-research.13/. In addition, it is not clear to me on which data the vocabulary was built: on MuST-C or on the WMT corpus? ",
    "comments": "Why did you introduce a parallelism between neural networks and the human brain in the introduction? This parallelism is questionable, as there are contrasting feelings and opinions about it. I am not sure it is adding anything to the motivation of the work, but it may rise concerns.\nIn the related works, among the many multi-task frameworks the addition of the CTC loss is not cited.\nIn lines 370-374, the paper claims that MT performance is not affected, but there is no evidence of it in the paper: the MT results are missing.\nIn the caption of table 2, what does strong mean? There are stronger cascade systems (e.g. you cited Bentivogli et al., 2020, whose cascade scores 28.8).\nIn line 152 the citation format is not correct. \nIn line 162, \"in\" -> \"into\". \nIn line 172, \"fout\" -> \"four\" In line 264 I guess a part of the sentence is missing: how is the contrastive loss computed? ",
    "overall_score": "3.5 ",
    "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work."
  }
]