[
  {
    "review_id": "b2323ea6a2acb091",
    "paper_id": "ARR_2022_182",
    "reviewer": null,
    "paper_summary": "This paper addresses the problem of how to weight task-specific losses in multi-task learning and presents an algorithm to adapt weights of loss functions, each of which corresponds to a task, during training.\nThe key idea is to update loss weights so that the weighted sum of task-specific gradients moves model parameters to the direction which reduces all task-specific losses computed on held-out data.\nThe proposed algorithm is expected to (a) avoid undesirable task performance trade-offs (i.e, sacrificing performance on one task to improve on another task) and (b) facilitate better generalization.\nThe experiments showed that the proposed method achieved meaningful improvements in most of the tested individual tasks, and also achieved significantly higher performances on average. ",
    "strengths": "1. The proposed method directly and naturally addresses the trade-off problem of multi-task learning. The design motivation is easy to understand thus can facilitate  further, deeper research easily.\n2. The presented experiments clearly demonstrated the performance superiority in two different types of text classification tasks. Thus it is arguable that the proposed method will have broader applications. ",
    "weaknesses": "1. The experimental settings may be in favor of the proposed method and the actual performance gap in the averaged loss may not be as significant as presented in the paper. The loss function of the “Uniform” baseline uses a constant weight across tasks, which matches the weights used in the evaluation of average performance. Thus I expect that the baseline’s performance is not very much behind the other methods including the proposed method *for the averaged score*. However Figure 2 and 3 shows it performed poorly. This may suggest that the baseline, and other baselines,  might overfit to the training data. There is no mention of the criteria to stop training so I cannot tell whether the authors did early stopping using held-out data. Since the proposed method uses a held-out dataset (called “query” in the paper) to control training, I think it is fair to use such a dataset to control training of baseline methods.\n2. The paper doesn’t provide insights about the adapted weights except they looked to keep adapted during training. I think it is of general interest whether the proposed method actually controlled task weights nicely and/or the other baseline methods assigned task weights poorly and resulted in worse performances and due to that. The scaling of Figure 8 and 9 makes it hard to compare weights between compared methods, especially MGDA and GradNorm, because these absolute values are much smaller than others and see *relative magnitudes” between tasks. ( For task weighting, absolute magnitudes don’t matter.) I think the paper doesn’t provide strong evidence that the observed good performances were actually due to better task weighting.\n3. Related to #2, no reason is provided to support why the baselines were chosen to compare. The readers who follow the technical area closely might understand how informative contrasting the proposed method and the baselines are, but for others like me, they look like random choices. Explanations are required to describe why they were chosen. ",
    "comments": "1. The theoretical analysis doesn’t provide much insight beyond that a loss computed on a held-out dataset (= the query set) is a better estimator of the expected loss than that computed on the training data, which is well known. It could be fully removed to make spaces for other discussions.\n2. Scatter plots in Figure 2 and 3 are not very suitable because it is hard to see how each run is distributed when they overlap. Box plots like in Figure 4 and 5 would be better. ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]