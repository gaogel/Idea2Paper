[
  {
    "review_id": "22d03e07f270cb7d",
    "paper_id": "ARR_2022_339",
    "reviewer": null,
    "paper_summary": "This paper focuses on an approach of Non-Autoregressive Generation (NAR) for summarization tasks, it includes three parts. The first part is using search-based method to the training data, the second part is training an encoder-only model, the third part includes the training strategy and a length-control algorithm. ",
    "strengths": "1. This work combines an unsupervised approach and NAR generation on a summarization task. The experiment shows some gain over baselines.  2. The experiment settings, ROUGE score and speed are detailed. ",
    "weaknesses": "1. The proposed NAR model is almost an encoder part of a transformer model, papers [1,2] are used an encoder-only model for NAR. The novelty is not clear described.  [1] Non-Autoregressive Text Generation with Pre-trained Language Models [2] Fine-Tuning by Curriculum Learning for Non-Autoregressive Neural Machine Translation 2. Lack of an ablation analysis to show the gain from each change. For example, with the special blank token (the difference between this model and [1]) vs without it. ",
    "comments": "1. The three observations taking a lot of space in the introduction are very general, the reader may want to know more about the specific issue this paper is targeted, and the contributions made in this work. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "a7425617c20ec8b8",
    "paper_id": "ARR_2022_339",
    "reviewer": null,
    "paper_summary": "The paper proposed an unsupervised summarization method. It first generates pseudo summaries with a search-based method. A non-autoregressive model is then trained to predict these pseudo summaries. CTC loss is used during the training. Finally, a length-control inference method is proposed to decode short summaries from the model. ",
    "strengths": "1. Experiment results show strong unsupervised parsing performance on Gigaword and DUC2004 datasets. \n2. The encoder-only architecture is intuitively coherent with the nature of summarization. Ablation studies also show that NAUS is better than an encoder-decoder baseline. \n3. The proposed method is more efficient than search or autoregressive baselines. ",
    "weaknesses": "1. The novelty of this method seems to be marginal. The only novelty component proposed in this paper is Length-Control inference. ",
    "comments": "1. It would be better to separate the description of the previous methods and the proposed method in section 2. \n2. Some examples could help the audience to better understand the importance of length-control inference. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]