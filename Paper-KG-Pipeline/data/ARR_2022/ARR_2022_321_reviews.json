[
  {
    "review_id": "909d810280c389fb",
    "paper_id": "ARR_2022_321",
    "reviewer": null,
    "paper_summary": "This paper proposes a bidimensional leaderboard, or Billboard, that measures progress both on generation tasks and automatic evaluation of NLG simultaneously. The authors observe that while much progress has been made on automatic evaluation metrics, NLG modeling papers often still report very old metrics like BLEU and ROUGE. The Billboard is an attempt to bring the best practices of the metrics and modeling communities into one place. Generation models submitted to the Billboard would automatically get evaluated on new metrics as they are submitted, and a global ensemble metric (chosen to maximize correlation with human judgments) would also update as metrics are submitted, thus updating the ranking of generation systems. The paper presents Billboards for translation, summarization, and image captioning, as well as initial results on which metrics contribute most to the best ensemble metric.\nThis is an excellent idea and I think could be very impactful for the community. The paper is well-written and thorough, for example conducting a meta-analysis of which human annotations to use when learning the global ensemble metric. I see no significant risks in accepting this paper. ",
    "strengths": "- Great idea to organize generation and metrics into a single joint leaderboard - Has potential to improve community practices, in particular making it easier to use better evaluation metrics when writing generation papers - Insights into which existing metrics today perform best, in terms of correlation with human judgments - Paper is well-written and clear ",
    "weaknesses": "- Less of a weakness, but I do slightly disagree with the authors' focus on instance-level rather than system-level correlation (see below) ",
    "comments": "In Section 2.4, the authors argue that metrics should be ranked in terms of instance-level judgments. While I acknowledge the clear advantages of this, particularly related to sample size, I would encourage the authors to also include system-level scores in the leaderboard. Ultimately, in the other half of the leaderboard, metrics will be used to make system-level comparisons. Even if there is high instance-level correlation, there could be different patterns at the system-level, for example if a metric has a small but consistent bias for or against a particular system's outputs. I wonder if perhaps there is some sort of compromise way to rank metrics that incorporates both instance-level and system-level scores. ",
    "overall_score": "4.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "ebfe58b10dee02a1",
    "paper_id": "ARR_2022_321",
    "reviewer": "Nora Hollenstein",
    "paper_summary": "This paper proposes a leaderboard to join the progress reporting of the two important factors in automatic text generation: the output of generation models and the metrics used to evaluate the output. The authors propose a leaderboard were both model outputs and metrics can be submitted. The strength of submitted metrics is analyzed by their complementarity to human-based metrics. The current version of the leaderboard includes three downstream tasks, including machine translation, summarization and image captioning. Such a multidimensional leaderboard is a good idea especially for tasks such as language generation, where more than one criterion is important to be evaluated. ",
    "strengths": "- Design choices are based on an extensive case study that emphasises the gap between model developers and the use of recently proposed evaluation metrics.\n- New submitted metrics are aligned to human judgements rather than to ROUGE.\n- Reproducibility to previous versions is ensured.\n- The paper takes in account the differences between experts and crowd-workers.\n- The paper is well structured and written clearly. ",
    "weaknesses": "- Number of instances in a test set: Did the authors analyze if this affects the rankings?\n- Section 3: There are no baselines presented for summarisation and image captioning.\n- It does not become clear why automatic metrics tend to overrate machine-generated text? ",
    "comments": "Line 473: missing word - “depending on the downstream task of interest” ",
    "overall_score": "4.5 ",
    "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work."
  }
]