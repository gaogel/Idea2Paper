[
  {
    "review_id": "178fc26935bf42f6",
    "paper_id": "ARR_2022_226",
    "reviewer": "Yilin Yang",
    "paper_summary": "This paper propose a two-stage training for the complete many-to-many Multilingual NMT, where the model is first pretrained on the complete multilingual dataset, then finetuned only with the same target language data. With the proposed training schedule, author reports a significant improvement over both bilingual and pivot-based baselines. ",
    "strengths": "1. The proposed two-stage training is novel and highly effective. Demonstrated on two large-scale datasets, this approach is able to achieve significant improvement over the baselines. \n2. Their idea is simple and clearly conveyed. With the experimental success, it is well-suited for a short paper. ",
    "weaknesses": "1. The \"baseline\" may not be appropriate. Their approach should be compared with the baseline setup where N many-to-one models are trained with only their corresponding target languages. The approach in the paper ends up with N models, thus it's unfair to compare against the Many-to-Many baseline (one model) nor the bilingual baseline (one dataset). \n2. Since their approach ends up solving a Many-to-Many translation problem with N Many-to-One models, it greatly drops the deployment efficiency provided by Multilingual NMT, and scales linearly to the number of languages. \n3. The experiment section lacks comparison to existing works, e.g. the winning system of WMT-21 4. paper writing needs improvement (see below) ",
    "comments": "L76: incomplete sentence \"... and .\" \nI suggest changing all XY, EX, XE notations to X-Y, En-X, X-En, since they could be confusing. ",
    "overall_score": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "ea50f866851bc980",
    "paper_id": "ARR_2022_226",
    "reviewer": "Yuan Cao",
    "paper_summary": "This paper aims to improve the quality of arbitrary translation directions (arbitrary \"XY\" translation) for a multilingual NMT (MNMT) system. The proposed approach consists of two stages: 1) Pretrain an NMT model on all available parallel data between any languages; 2) Finetune the model on \"many-to-one\" data for all target languages (multilingual many-to-one finetuning). The paper also studied large-scale training with in-house settings, including En-centric vs. multilingual-centric pretraining, and light-weight decoder architecture. Experimental results on WMT21 demonstrated that the proposed approach outperformed direct bi-lingual systems and pivot translation systems. ",
    "strengths": "This paper addresses and important problem of improving arbitrary translation directions between any language pairs in an NMT system, which is not well solved yet. The proposed two stage training (pretraining + finetuning) is potentially powerful and provides a new direction in solving this problem. ",
    "weaknesses": "1. It remains questionable how the scalable the proposed approach will be, as it seems for each target language a different model needs to be finetuned. \n2. Baselines chosen by the paper for comparison appear to be weak. \n3. The paper could also have been better organized. ",
    "comments": "Regarding the proposed approach 1. The proposed approach requires producing |L| finetuned models, one for each target language. While this seems to be helpful in improving quality, it remains questionable how scalable this approach is, especially when extending the system to handle hundreds of languages. It would be more interesting and feasible to study the possibility of at least grouping several similar languages into one model during finetuning. \n2. If pretraining on multi-centric data performs better than En-centric data (Table 1), seems the problem can be better solved by preparing more diverse and high quality data. It remains unclear whether the improvement comes from the proposed 2-stage training, or simply collecting more relevant data.\nRegarding experiments 1. I would suggest adding one or more existing end-to-end MNMT models that enable XY translations into the baselines for comparison. Bilingual and pivot baselines are kind of weak in the multiway translation setup.\nRegarding paper organization: 1. I would suggest adding a summary of contributions in the last paragraph of Sec. 1 for clarity. \n2. It seems Sec. 2.1 should be a standalone section discussing experimental setups and results. In addition, Sec. 2.2 can also be a new section talking about in-house training procedure and results. It makes the paper a little confusing to nest everything under Sec. 2. ",
    "overall_score": "2.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "98e038cc6cfd29e7",
    "paper_id": "ARR_2022_226",
    "reviewer": "Fabien Cromieres",
    "paper_summary": "This paper discusses some practical aspects of Multilingual MT.  1- Observing that many-to-many multilingual systems have issues with target language generation, they propose to fine-tune a N-to-N multilingual MT system into N   N-to-1 many-to-one MT system. This represents a middleground: N systems have to be managed (vs N^2 bilingual system or 1 many-to-many system), but a large pretraining with all language direction has to be done only once. They report good BLEU improvement on the WMT21 multilingual translation task.\n2- They also propose to improve the inference lattency of the systems by using an \"unbalanced\" Transformer architecture with 9 encoding layers but only 2 decoding layers. This architecture can reduce decoding lattency by 50% over a \"balanced\" architecture.\n3- They also make some experiments to establish the usefulness of mult-centric pretraining over english-centric pretraining. ",
    "strengths": "I found the paper quite interesting , even if at times a bit unclear. The idea of fine-tuning many-to-many MT system into several many-to-one systems is simple but is shown to work well over two wmt21 tasks comprising 30 translation directions each.  Reducing the decoder depth for improving lattency do not appear so innovative to me, but it is at least interesting to see some number for the BLEU/lattency compromises this implies in a real large-scale setting.\nOverall, I think there is enough to learn from this, given it is a short paper. ",
    "weaknesses": "I regret that the authors do not make comparisons with training N  N-to-1 systems from scratch. Training would be more costly, but would BLEU be better? ( or worse, due to the models never seeing some language directions).\nGenerally speaking, there is nothing strikingly innovative in the paper.\nThe text has many typos; and the paper could be clearer (see my recommendations below). ",
    "comments": "The way you present your results do not sell them very well. It is a bit hard to tell at first glance from figure 1 if your method is working or not. At the very least, you should present an average of the improvements over all the languages (or maybe a separate average for english-centric and non-english centric pairs). The caption of figure 1 mention some number but you do not explain what they are: are they the average improvement over all languages?\nYou say your training data is 3.7B, but is that 3.7 B words? 3.7MB? 3.7B sentence pairs?\nYou mention doing some preliminary experiments showing that \"complete many-to-many training is still as challenging as one-to-many\". I would have liked that you detail a bit more your findings.\nIt is not clear to me why results in Table 2 and Table 1 are different for the pivot-based baselines. Is it because they are actually distilled in table 2?\nI found section 2.2 a bit confusing. You seem to be trying a lot of unrelated things and presenting them simultaneously (training strategies, decoder architecture,...). And then you suddenly mention distillation in the Result section. You should restructure this a bit (maybe with one or two additional sections) to make clearer what you are doing and why/. Many typos: l076: \"and .\"\nFigure 1 caption: baslines -> baselines l123: \"graduation accumulation\" -> gradient accumulation l127:  \"graduation accumulation\" -> gradient accumulation l 212:  \"the the issue\" l254: \"Table 1\"  -> I think this should be Table 2 ",
    "overall_score": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]