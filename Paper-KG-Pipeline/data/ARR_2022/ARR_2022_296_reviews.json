[
  {
    "review_id": "3f55670117f852d4",
    "paper_id": "ARR_2022_296",
    "reviewer": "Yaojie Lu",
    "paper_summary": "This paper proposes a data-efficient end-to-end event extraction method by designing event-specific generation templates. \nThese event-specific templates describe the event types in a natural language way. \nThe proposed method can use pre-trained language models and exploit label semantics from event type definitions. \nExperimental results show that DEGREE has better performance on low resource event extraction and can be compared with SOTA under the full supervised setting. ",
    "strengths": "This paper designs an event-specific template for generative event extraction. \nCompared with the previous methods, the proposed method is more consistent with natural language generation and achieves better performance in the low-resource setting. ",
    "weaknesses": "- The proposed framework is based on the previous template-based information extraction method. \nThe template-based method's main drawback is the significantly increased training/inference cost (corresponding to the number of event types). The increase in the category of events (such as MAVEN with 100+ types) will exacerbate the problem.\n- For extreme low-resource settings (1%), different data sampling may heavily affect the experimental results. \nIt is better to conduct experiments multi-times on different subset samples. ",
    "comments": "Some detailed questions about model inference: - Multi-events: Can DEGREE and DEGREE(ED) deal with multiple events of the same type in the same sentence? Although we can predict the same input sentence for different types one-by-one if a sentence contains multiple events of the same type, the same input (template and sentence) will correspond to various outputs. For example, some sentences have two death events. This is not a problem for other generation methods because they are trigger-driven (TANL, BART-GEN), or generate all extracted events in one step (Text2Event).\n- Converting generated results to events: Are there some generated results that cannot be parsed into events? Generation methods are more uncontrollable than span extraction and span classification methods for event extraction. For example, the generated span maybe not appear in the input sentence.\nThere are many template methods for information extraction. \nIt is suggested that authors should cite these articles: - Template-Based Named Entity Recognition Using BART. Findings of ACL_IJCNLP 2021 - Reading the Manual: Event Extraction as Definition Comprehension. spnlp@EMNLP 2020 ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "1731e84e1beb6d67",
    "paper_id": "ARR_2022_296",
    "reviewer": "Hongming Zhang",
    "paper_summary": "This paper proposes a new template-based generation model for the event extraction task. Specifically, with the help of event type definition, event detection template, and keywords, the proposed model can outperform previous supervised models in the low-resource setting. ",
    "strengths": "1. The paper is clearly written and easy to follow. \n2. This paper conducts comprehensive experiments to clearly present almost all perspectives of the proposed model. ",
    "weaknesses": "My main concern is the limited Effect in real applications.  From the results in table 1, it seems like the proposed model is more likely to help when 5%-10% of training data are provided. ( By the way, it seems weird that OneIE will drop performance when 10% of the data is provided.) However, annotating that amount of data is also labor-intensive and hard to get for novel classes we might want to extract in the future.  Besides that, the used template seems to be a powerful prior knowledge. For example, as discussed in table 4, using the keywords only can lead to almost the same performance of the model. Can you provide more discussion on how many tested triggers are covered by the provided keywords? I suspect that there might be a big overlap.  Compared with using 10% of data, zero-shot is a more interesting and realistic data-scare setting. ( If we can afford to annotate some data, why not just annotate more). Unfortunately, experiments in the appendix do not compare with existing zero-shot approaches for the event extraction tasks [1, 2, 3] Last but not least, the high-quality template and keywords may not always be available in real applications. ( we actually need domain experts to design that). Is there a way to automatically generate those templates with minimum efforts (e.g., event type name)?\nDue to the limitation of ACE, which is highly biased (almost 40% of the test data is \"attack\"), maybe the results on a more diverse and balanced dataset [4] are more convincing. A potential challenge of applying the proposed model on MAVEN [1] Lifu Huang, Heng Ji, Kyunghyun Cho, Ido Dagan, Sebastian Riedel, and Clare Voss. Zero-Shot Transfer Learning for Event Extraction. ACL 2018.\n[2] Hongming Zhang, Haoyu Wang, and Dan Roth. Zero-shot Label-Aware Event Trigger and Argument Classification. ACL 2021 Findings.  [3] Qing Lyu, Hongming Zhang, Elior Sulem, and Dan Roth. Zero-shot Event Extraction via Transfer Learning: Challenges and Insights. ACL 2021.\n[4] Xiaozhi Wang, Ziqi Wang, Xu Han, Wangyi Jiang, Rong Han, Zhiyuan Liu, Juanzi Li, Peng Li, Yankai Lin, Jie Zhou. MAVEN: A Massive General Domain Event Detection Dataset. EMNLP 2020. ",
    "comments": "Great writing. ",
    "overall_score": "2.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "cb1c7a25620a6ec7",
    "paper_id": "ARR_2022_296",
    "reviewer": null,
    "paper_summary": "This paper presents a method for identifying event types and their arguments in a low-resource scenario, i.e., with a limited amount of training examples. The method uses distant supervision signals given in the form of templates that define event types and their arguments that were manually curated based on the definition of events in the ACE dataset. \nThe authors demonstrate the impressive ability of their model to outperform the current sota and baselines in the low-resource scenario and comparable ability in the high-resource scenario. They also perform ablation studies to emphasize the contribution of every suggested component.  Although this is the paper summary section and criticism should not appear here - I feel obligated to mention here that the authors did not report any statistical analysis of their result! This makes the entire result section meaningless and I implore the authors to add the significance testing results to their paper before publication. ",
    "strengths": "- A new model for event and argument detection and typing that uses distant supervision signals in the form of additional prompt templates.\n- The model presents impressive performance (though its impressiveness is questionable until statistical analysis results will be presented) and comparable scores to sota and strong baseline models.\n- The paper is well written and detailed. ",
    "weaknesses": "The main weakness is that this method can only be applied to the event types and argument roles defined in the ACE dataset (which are not a lot). There are newer event-type datasets such as MAVEN (https://aclanthology.org/2020.emnlp-main.129/) that include many more events but cannot be used in this framework because they don't include the type definitions that are required for the prompting design.\nNo statistical analysis of the results is reported although a comparison between models is made. Sentences like \"The performance gap becomes more significant for the extremely low-resource situation.\" should not be written without statistical proof of significance. ",
    "comments": "I would suggest trying to come up with templates based on MAVEN types too, this will cover many more event types. ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]