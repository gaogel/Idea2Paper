[
  {
    "review_id": "7d67dcb3c0915c61",
    "paper_id": "ARR_2022_167",
    "reviewer": null,
    "paper_summary": "This paper proposes an LM pre-training method for bi-encoder models. The formal definition of their pre-training task includes: 1. the pre-training task is the same as the task definition used in extractive QA models 2. the model architecture is the bi-encoder architecture. First, one encoder computes contextualized vector representation of tokens in the given passage. Next, the other encoder computes vector representation of the given question. Then the model predicts the start and end indices by computing the dot-product similarity between the question vector and the token vectors. \n3. The pre-training dataset is generated jointly using question generation and a student-teacher framework. The generated question-answer pairs are then used for the pre-training.\nThis paper evaluates their pre-trained encoder on five downstream tasks: extractive QA, zero-shot paraphrase ranking, few-shot paragraph classification, few-shot named entity recognition, zero-shot sentiment analysis. The results show that the proposed pre-training method is sufficient to be used by bi-encoders in zero/few-shot settings. ",
    "strengths": "1. The importance of bi-encoder in QA and other NLP fields is increasing. This paper proposes a novel pre-training approach for bi-encoder models. \n2. The experimental results of this paper show the efficacy of their pre-training method. \n3. This paper shows an interesting finding on their proposed pre-training method; in the zero-shot setting, pre-training bi-encoders with the proposed pre-training method is better than using cross-encoder     1. Evidence: Table2 (QUIP + cross-encoder vs QUIP)     2. Evidence: Table 4 (QUIP + Cross-student vs QUIP)     3. Evidence: Table 5 4. This paper provides prompt methods for using their pre-trained encoder to downstream tasks. ",
    "weaknesses": "The motivation described in this paper is not well aligned with some experimental designs of this paper. For example, it is unclear why some downstream tasks are in zero/few-shot settings. ",
    "comments": "It would be great if the authors provided experimental analysis on the reason bi-encoders outperform cross-encoders when the encoders are pre-trained on their method. ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "4eeedac91e83ad9a",
    "paper_id": "ARR_2022_167",
    "reviewer": null,
    "paper_summary": "This paper propose a new pre-training objective for train Bi-encoder based QA model. The main claim is that the proposed pre-training objectives helps in improving the model's performance on non-QA downstream tasks like paraphrase detection, sentiment analysis etc. without extensive finetuing.  The main hypothesis is that the proposed way of pre-training helps the model in learning better token level representation needed for zero shot and few shot tasks. ",
    "strengths": "Synthetically generated questions have been nicely used for pre-training QA model to improve performance on tasks like Sentiment analysis, paraphrase detection in a zero shot, few shot setup. A good analysis of using QA as the pre-training objective rather than MLM. ",
    "weaknesses": "The paper has incremental or limited novelty as the question generation from passages using BART is already a known, Bi-encoder for QA already exists. \nContributions of the paper are quite trivial. \nBi-encoder+Unsupervised QA didn't perfrom great. So whether choosing QA as the pre-training is helping QA task or not is not very clear. \nwhat is the motivation behind independently encoding questions and passage is not very clear. isn't that a shared common encoder would help the model learning better question tokens to passage token matching? \nFor QA task, It seems that the cross-encoder + MRQA is doing better than proposed QUIP. ",
    "comments": "Highlight the best numbers in table 7.  - Have a strategy to filter out best quality question from the set of synthetically generated questions.\n- May be design a curriculum learning paradigm for pre-training so that the batch sampling is done in a manner that helps the pre-training learn from easy to complex questions. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "a08c0e0dafb5d746",
    "paper_id": "ARR_2022_167",
    "reviewer": null,
    "paper_summary": "This paper proposes a pre-training objective based on QA, aiming at learning general-purpose contextual representations. They use BART model to generate QA pairs with context passages and use a cross-encoder RoBERTa to relabel the examples. After that, bi-encoder model is trained to match the cross-encoder predictions on the generated questions. And this paper conducts many experiments on multiple datasets of four downstream tasks. ",
    "strengths": "1. The experimental effect is considerable under four different downstream tasks. \n2. Experiments require a lot of computing resources and time. ",
    "weaknesses": "1. The article is very trivial and didn't write a paragraph to summarize the contribution of the paper, which makes it difficult to focus on the core ideas. \n2. It is suggested to combine pictures and other forms to assist the description, which will be clearer to understand. \n3. Will the code and dataset of this paper be published? ",
    "comments": "Please see my comments and questions in the core review. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]