[
  {
    "review_id": "8a618d10cbf3a6fd",
    "paper_id": "ARR_2022_340",
    "reviewer": null,
    "paper_summary": "This paper describes an advancement in counterfactual probes that test whether neural nets make use syntactic information. The goal of counterfactual probes is show that models actually use syntactic info rather than just encoding it. The paper identifies a potential problem in previous work (summarized neatly in Fig. 1), namely that information may be encoded redundantly, and the probe may attend to the wrong redunant representation relative to the model itself, thus underestimating the presence/utility of the information. They solve this by introducing a dropout probe. ",
    "strengths": "The paper is overall clear in identifying the problem, though I wish they had described what a counterfactual probe was more clearly and earlier than they did (2.2), and I believe from the paper that they have solved the particular problem that they set out to: the issue of probes missing redundantly encoded information. ",
    "weaknesses": "My main concern is that they haven't quite demonstrated enough to validate the claim that these are demonstrating a causal role for syntactic knowledge. Two crticisms in particular: 1) The dropout probe improves sensitivity. It finds a causal role for syntactic representations where previous approaches would have missed it. Good. But all other things being equal, one should worry that this also increases the risk of false positives. I would think this should be a substantial part of the discussion.  2) Relating to the possibility of false positives, what about the probe itself? The counterfactual paradigm assumes that the probe itself is capturing syntactic structure, but I worry that training on templates could allow both the probe and the model to \"cheat\" and look for side information. Templates exacerbate this problem by presenting a relatively invariant sentence string structure.\nFinally, there's the possiblity that syntactic structure (or not quite, see point 2)) is encoded and plays some causal role in the model's predictions, but it's secondary to some other information.\nReally, the criticism comes down to a lack of baselines. How do we know that this approach isn't now overestimating the causal role of syntax in these models? Testing with a clearly non-syntactic proble, destroying syntax but keeping lexical effects by scrambling word order, or probing a model that certainly cannot encode this information. This is most of the way towards being an excellent paper, but it drops the ball in this respect. ",
    "comments": "I got garden pathed reading line 331 \"One could define other Z1 and Z2...\" The author seems to really like the bigram \"prior art.\" ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "27b39805e4e0c55f",
    "paper_id": "ARR_2022_340",
    "reviewer": "William Merrill",
    "paper_summary": "The authors entertain the hypothesis that syntactic information is redundantly encoded in neural language models, which has two important implications for probing and causal analysis: 1. Probes may arbitrarily choose one place a syntactic property is encoded, and the model may choose another. \n2. Changing a representation based on a probe may therefore only partially change whether the property is encoded in the model, and might not influence the model decision, even if the model is relying on syntax.\nFor contributions, the authors show first that syntactic information is redundantly encoded in language models, based on estimates of mutual information between different parts of the embeddings. They then propose a simple method to make probing more robustly rely on all the information in the representation, by adding dropout to the probe. They show that this leads to altered representations that are more influential on changing the model's behavior. ",
    "strengths": "1. Well-written and clear hypothesis: redundant encoding of syntactic information poses a problem for drawing conclusions about whether models rely on the syntactic information encoded by probes.\n2. The authors show that different parts of the hidden representations in different trained networks redundantly encode information correlated with syntax.\n3. The dropout method for addressing this issue is both simple and motivated, which makes it appealing.\n4. The experimental results, in the case of the QA models, tell a different story than past work that suffered from the redundancy vulnerability. Specifically, this work finds evidence that QA models do rely on syntactic information, while past work suggested they didn't. In the case of NLI, this work agrees with past works in finding a negative result. ",
    "weaknesses": "1. There is some imprecision in the discussion of the results in 4.2.2: “In contrast to the standard probes, the dropout probes, plotted in the right column, revealed much larger effects of syntactic interventions.” Is this conclusion really justified? Visually, it is not clear that the red line is “above” the green line any more with dropout than without. Rather, it may just have more variance (so more causal influence, but maybe not in the right direction). In any case, the clean summary you give here seems at odds with the trend in the figure, and should be made more precise.\n2. There may be some issue with the counterfactual intervention experiment with NLI-HANS, if I understand that part correctly. In 4.3, if the NLI-HANS model is already at 99% performance, why would you expect its accuracy to change significantly after counterfactual modification? You are already very close to the ceiling, and it should be hard to see any benefit of the intervention.\n3. “ First, we found that language models redundantly encoded syntactic information in their embeddings” — Section 4.1 seems like solid evidence that networks redundantly encode information that is informative about syntax. But, to be a bit pedantic, it doesn’t have to be syntactic information; if $D$ is highly correlated with something non-syntactic, then that property could be expressed redundantly, and the MI would still be high. If we believe that there are dataset artifacts in syntactic parsing, then this is a concern. ",
    "comments": "It seems to me that redundancy may only be a problem for the specific gradient-based representation alteration method you consider, rather than other ways of producing counterfactual interventions. For example, would [AlterRep](https://arxiv.org/abs/2105.06965) automatically handle the issue of redundancy?\n417: why is “as” included along with “were” and “are”?\n295: What do you mean by “conservative but tight estimate of mutual information”? This is pretty unclear to me 188: typo at “represented by within” ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]