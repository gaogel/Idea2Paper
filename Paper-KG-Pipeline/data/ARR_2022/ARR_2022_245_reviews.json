[
  {
    "review_id": "802e97a5cf30c788",
    "paper_id": "ARR_2022_245",
    "reviewer": null,
    "paper_summary": "The paper presents a new task called: Spoken Conversational Question Answering (SCQA). The proposed dataset consists of multi-turn conversations and passages in both text and speech form. The authors claim that SCQA is more challenging than only text-based tasks; and indeed, the performance of existing SOTA models such as BERT-based and ALBERT-based degrade. The paper suggests a dual attention technique to tackle the underperformance and shows that indeed the performance increases. The main idea of the dual attention is that model can benefit from learning on two language modalities (in this case, text and speech) ",
    "strengths": "•\tThe paper is well written; the experiments are sound, and the task seem interesting. \n•\tI really like the idea of exploiting the two language modalities to improve existing models. I am not familiar with this kind of usage, so the novelty here is good. Also, I like the teacher-student approach (called Knowledge Distillation) which enable them to overcome the noise that came from ASR. \n•\tThe mathematical formulation of the task and the different approaches are nice and clear •\tThe baseline comparisons are also good ",
    "weaknesses": "The authors use the term “significant” several times in the paper but in fact, did not use any statistical test to check that the results are indeed significantly decreed. \nThis problem gets bigger when looking at Tables 5 and 6; it is hard to see that the suggested techniques truly improve the models. Personally, I am not convinced that the Dual attention and knowledge distillation technique, suggested by the authors, improved the models (see for example lines 5 and 8 in table 5, the change in F1 score is ~2 points) I am wondering if you had a perfect ASR, was it still beneficial to use the two language modalities? I am not an expert on speech embedding but does it capture things like tone, speed, and nervousness? If the answers is negative, I think that this is a major weakness ",
    "comments": "I think that the main strength of the paper is the use of the two language modalities. The dataset\\task is nice, but it is not the main contribution in my opinion. \nI think that if the paper will be accepted then it will be required to perform the right statistical test in order to prove that indeed the methods works ",
    "overall_score": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "94fa38294cafb35d",
    "paper_id": "ARR_2022_245",
    "reviewer": "Shang-Wen Li",
    "paper_summary": "The paper discussed the problem of Spoken Conversational Question Answering (SCQA). On the one hand, conversational question answering (CQA) has been a challenging problem in language understanding and of many applications. On the other hand, speech modality in a natural form for conversation and contains additional information in audio signals. It is natural to explore the intersection of the two, i.e., SCQA. There are several existing works, but the research is usually limited by available dataset. This paper collects a Spoken-CoQA dataset based on CoQA and Google TTS for generating the training set and human speakers for collecting the test set audio. The authors also proposed DDNET approach which is a unification of knowledge distillation and dual attention techniques, and showed the efficacy of DDNET empirically by applying DDNET to various spoken QA models. ",
    "strengths": "- The paper investigates a novel and challenging problem, Spoken Conversational Question Answering (SCQA). The problem is crucial for understanding natural/spoken conversation. The area is relevant to several ACL communities and could be interesting for industry application.  - The authors compile a dataset (Spoken-CoQA) to evaluate SCQA problem with much bigger sizes than previously available datasets. If the dataset is released as the authors promised, it will signifcantly benefit the research and benchmarking in SCQA problem.\n- The authors proposed a novel approach, DDNET. The approach leverages knowledge distillation and dual attention techniques to combine text representation and audio representation. The authors further showed the efficacy of DDNET empirically by applying DDNET to various spoken QA models and benchmarking with Spoken-CoQA dataset and a public available spoken SQuAD dataset. ",
    "weaknesses": "- Although the authors showed consistent improvement with proposed DDNET approach over the baselines, it is difficult to interpret how good the results are. It would be great if the authors can provide some qualitiative analysis to help readers understanding the quality of result. Also, it would be interesting to see how the audio modality helps the answer prediction to better interpret the pros and cons of proposed method and the addition of audio modality.\n- There is a recent trend to bypass ASR, in order to make the underlying methods more scalable to low resourced languages. The proposed method still relies on ASR and it would be interesting to see if SCQA can achieve reasonable performance without ASR system. ",
    "comments": "See above weakness section ",
    "overall_score": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]