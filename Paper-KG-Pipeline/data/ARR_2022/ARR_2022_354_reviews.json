[
  {
    "review_id": "9bc6d4dd35f310f3",
    "paper_id": "ARR_2022_354",
    "reviewer": null,
    "paper_summary": "This paper studies the routing fluctuation problem in the mixture of experts (MoE) Transformers: existing MoE methods assign the same input to different experts even when the training almost ends. This fluctuation hurts sample efficiency and leads to slower convergence.  It introduces a two-stage training routing strategy called StableMoE to stabilize the routing: first learns a lightweight and more balanced router via a balanced loss and the router distillation, then freezes the router for stable token-to-expert assignment for second-stage training that only has the task loss. The novelty is in the decoupled two-stage training strategy and the use of a stable routing strategy. The proposed balanced loss is not distinct from the one in Switch Transformer.\nExperimental results on language model and multilingual machine translation tasks show StableMoE achieves better performance than other MoE methods and converges faster. The ablation study further confirms the two-stage training stabilizes the routing and verify the effectiveness of StableMoE. It remains unclear whether the total budget for training (i.e. total compute resources) is smaller compared to exiting MoE methods. It also misses the inference speedup results. ",
    "strengths": "- This paper identifies the routing fluctuation problem and provides a thorough analysis as indicated in the ablation study.  - The two-stage training strategies in StableMoE are simple and effective to reduce the routing fluctuation during training. The lightweight distilled router is novel and useful.\n- The empirical results show strong task performance improvements and faster training convergence over existing MoE methods. ",
    "weaknesses": "- Experiments need further improvements. StableMoE converges faster than other MoE methods, but the total compute budgets are not revealed and remain unclear. The inference speed comparisons are also missing.\n- The overall contribution of StableMoE is strong, but the contribution of the balance loss is marginal. Comparison (either empirical results or theoretical justifications) to existing load-balancing methods are missing. ",
    "comments": "- Does the routing fluctuation problem exist in common MoE models or just the one compared (BASE Layer)? The experiment baselines include Hash Layer and Switch Transformer, but the paper only reports the token-to-expert assignment changes for the BASE Layer model.   - How is the balance loss different from the one in Switch Transformer? The two losses are both encouraging the model to assign the tokens to experts in a more balanced way.  - The main results compare the training FLOPS, how about the total compute budget/costs for convergence? And also the inference speed is unclear.\n- For \"still uses original assignment scores as input, so the gate signal can also be learned in training stage 2\" (line 215 to 216), the routing strategy is fixed, so how the gate single could be also learned during stage 2? ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]