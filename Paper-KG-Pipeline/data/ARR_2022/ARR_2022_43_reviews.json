[
  {
    "review_id": "dda72a5f50547fe9",
    "paper_id": "ARR_2022_43",
    "reviewer": null,
    "paper_summary": "This paper attempts to address potential catastrophic forgetting, data imbalance and rare word issues, for intent detection in the medical domain. These three concerns are addressed with appropriate losses (L_pl, L_fl, L_co; Sections 3.2/3.3). Evaluations were performed against prior models, for various numbers of classes. ",
    "strengths": "Strengths of the paper include its addressing of several longstanding issues in incremental learning, including the rare word problem particularly relevant to the medical domain. ",
    "weaknesses": "Some concerns remain over the empirical evaluation, as detailed in the comments. ",
    "comments": "1. The fairness of comparison against prior methods might be clarified. For example, EMR is stated to \"randomly stor[e] a few examples of old classes\" (Line 254); one might therefore expect the performance of EMR to possibly improve (significantly), as the quantity of stored examples of old classes increases.\nBasically, it is difficult to confidently evaluate the superiority of the proposed CRN over prior methods, without taking into consideration the amount of data available to each of these methods. For example, CRN implements memory by storing the top n (prototype) examples (Line 160) for each class - do the other methods such as EMR also have access to n examples?\nIn summary, the relationship between storage capacity (n, apparently maxing out at Upperbound in Figure 2) and performance appears an important factor to explore for the various memory-based models, since the memory-performance tradeoff is central to such models (as otherwise there appears no need to innovate if all old samples can just be stored). While a brief exploration is provided in Figure 3 in the Appendix, it is against a single model on a single dataset, and only up to a relatively limited range of storage (i.e. n=200) 2. For the ablation experiments in Table 2, it might be considered to include various (valid) combinations of exclusions (e.g. w/o PL and w/o FL); in particular, the performance for the baseline memory-based model (i.e. without any of the proposed extensions) would be important in benchmarking against the comparison models.\n3. The effect of the various losses towards their intended function has not been thoroughly covered. For example, it is claimed that \"The performance drops with removing Lco (\"w/o CO\"). It demonstrates that it is helpful to handle medical rare words\" (Line 292). However, this is not substantiated with an analysis of performance on actual items with rare words. These instances might be addressed.\nMinor grammatical/spelling concerns: (Line 97) \"Medica intent\" (Line 124) \"Contrastive Reply Networks\" (reply or replay?) \n(Line 161) \"...top n example(s)\" etc. ",
    "overall_score": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]