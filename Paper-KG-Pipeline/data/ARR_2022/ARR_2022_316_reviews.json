[
  {
    "review_id": "7e2200824612618f",
    "paper_id": "ARR_2022_316",
    "reviewer": null,
    "paper_summary": "This paper introduces an approach to improve code language models so that the code they produce is compilable. The approach involves two techniques: 1) it uses policy gradients with KL-control _alla_ Ziegler et al. (2018) by translating the compiler's output into a +1/-1 reward. 2) it uses the generated sequences to train a discriminator that learns to detect whether a sequence compiles or not. Interestingly, the generator backpropagates into the generators' representation, possibly allowing it to learn to encode features that inform a sequence's compilability and which are useful to the generator. ",
    "strengths": "1. While the RL approach is the same as in Ziegler et al. (2018), the joint training of the discriminator with the generator looks rather novel and even an exciting development. \n2. The evaluation takes care to evaluate not only compilability but also the generalization of the model and its general fluency. \n3. The results look strong (but lookout for some caveats below) ",
    "weaknesses": "1. There is no associated code that could guarantee the reproducibility of these results. \n2. The evaluation is done on (as far as I can see) no standard splits of the datasets. Without making these splits available, future work will not be able to compare to these results. \n3. The compared methods are not on equal grounds. The proposed approach does a beam search (with k=5) using the discriminator output to select compilable sequences. For the comparison to be fairer, the authors could consider extracting k=5 samples from the other models and filter for the ones that compile (+ use the model probabilities to break ties) or, almost equivalently, doing rejection sampling on the distribution P(c)*1[compiler(c)]. One can argue that this gives an advantage to the baselines because they have access to the compiler, but on the other hand, it would be possible to train an independent discriminator that predicts whether a sequence compiles or not, as it was done for the proposed model, and no access to the compiler would be needed. Alternatively, IR metrics such as MAP or Prec@k could be used. \n4. The claim that this is the first work to integrate the compiler feedback to improve compilability is not correct. Korbak et al. (2021) cited at the end of the related work section also learn from the compiler feedback signal. While this could be easily amended, it would be better if this paper would compare their approach to this earlier work for the task studied in this other study, namely, the code completion task. ",
    "comments": "Other comments: 1. Levenshtein is a distance metric (lower is better), not a similarity (higher is better) one afaik. It would be good if the authors explained how the authors convert Levenshtein distance into similarity (I have checked the cited paper, and it does not provide any clue neither). \n2. L337: It's not clear why the authors chose to use 41k pairs and not the full dataset.\nTypos/grammar/etc (not exhaustive): (in general) \"code\" is uncountable in English. I believe you cannot say \"codes\". \nL101: comprehensive experiment -> comprehensive experiments L231: This fine-tuning process incorporates a (..) discriminator will... -> which will L240: We hope the generator will have more perceiving to -> more perception power? \nL298: same as above Eq. 8: c in the summation is not used anywhere in the terms. Does it expand into t and s? Please, clarify. ",
    "overall_score": "2.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]