[
  {
    "review_id": "db6797e7fe34857e",
    "paper_id": "ARR_2022_285",
    "reviewer": null,
    "paper_summary": "The present paper investigates the Word-in-Context task in a few-shot setting. Utilising a prompt-based approach, rather than optimising models to predict the expected label (i.e. whether two word senses are different), this paper proposes to predict (embeddings of) words synonymous to those in question and compare the distances of these embeddings. Models optimised in this way reach the performance of models finetuned on the whole training set and the approach is argued to generalise to some other tasks. ",
    "strengths": "The method is surprisingly simple, the results are convincing as they significantly improve upon the state-of-the-art on the WiC task in the few-shot setting. The paper shows that the methodology is applicable to some other few-shot tasks as well. Claims are substantiated and backed by evidence. ",
    "weaknesses": "The main weakness of the paper is that it's very terse, partly due to the 4-page limit of a short submission. This leads to potentially important information being omitted, see detailed comments below. \nSome of the mentioned points could be easily alleviated by utilising the additional page that is provided upon acceptance in a venue. ",
    "comments": "The following are points where i would like to see further clarifications: (NB: I do not have access to the forum/comments of the previous submission so some of my comments might have been addressed earlier.)\n- Why are SST-2 and SICK-E chosen as representative tasks to show that the proposed method generalises to other few-shot settings? Other papers seem to go for the full SuperGLUE suite. Similarly, for those tasks, why is Autoprompt chosen as the only reference approach to compare against? Admittedly, the difference in performance to other approaches (at least for the SST-2 task) appears to not differ too much.  - It is great to see that confidence scores were reported for the obtained results, but how exactly are they calculated?\n- The high-level description helps to understand the approach intuitively, but a more detailed (e.g. mathematical) formulation, for example in the appendix, would be helpful as well. Similarly, the figure is supposed to help to understand the problem better, but I find it confusing in two ways: First, the figure is too abstract for me. Maybe having more text labels would help. Second, depicting sentiment analysis, it does not align well with the main contribution of the paper, improvements on the WiC task. Maybe reworking the figure to depict the WiC task would help with both problems.\n- Qualitative error analysis in the Appendix is great, but the manuscript so far lacks a more detailed analysis of the results. One could wonder, for the WiC task, are the errors always due to models predicting \"matched\" for \"not matched\" GT? Is this similar to other approaches? For example, a by-class accuracy breakdown could answer some of these questions. ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]