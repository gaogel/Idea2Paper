[
  {
    "review_id": "30d20ad4e8024174",
    "paper_id": "ARR_2022_169",
    "reviewer": null,
    "paper_summary": "This paper deals with the Cross-Lingual Event Detection (CLED) task where only source language data is available for training but the model is then tested on a target language. The paper claims that their intuition is using unlabelled data in training (line 097-101). They use adversarial language adaptation (ALA) to learn language-invariant representations out of unlabeled bilingual text data. The main contribution of the paper is it proposes to use Optimal Transport (OT) for selecting samples for ALA training. Introducing ALA into CLED itself can be seen as a minor contribution. ",
    "strengths": "1. The paper proposes to leverage Optimal Transport (OT) to find similar samples. This is new knowledge to me. Previously I only know Optimal Transport (OT) or Wasserstein Distance is used in Wasserstein GAN (WGAN) which aligns real and fake image spaces. I did not see papers that use OT to search similar data samples before. I also appreciate the way that the paper draw samples in equation (3) and calculate p(s) and p(t) in 2.4.2 which is clever.  2. The main results in 3.2 looks promising. ",
    "weaknesses": "1. The paper claims that it exploits unlabelled target language data. However, in line 363, it seems that the paper actually uses event-presence labels $e_{i}$ for each target language sample. First, $e_{i}$ is probably extracted directly from labels $y_{i}$; it is when $y_{i}$ says that some word is an event trigger that one can know that $e_{i}=1$. So, for target language data, their labels are actually used in an indirect way. Thus the method is not totally using pure \"unlabelled\" target language data as the paper claims. Second, I think $e_{i}$ provides super crucial information which might be responsible for most of the gain derived.  To make fair comparisons with the baselines, I think baseline methods BERT-CRF in section 3.2 and the BERT-CRF+MLM in 3.4 should also see $e_{i}$ labels.  2. Also concerning $e_{i}$ in weakness point 1 above, it is not known how $e_{i}$ and $e_{i}$'s distributions look like at all. I could only guess $e_{i}$ is 0,1 binary variables? Since all SRC and TRG data comes from Cross-Lingual Event Detection datasets, maybe most samples do have an event trigger and thus most $e_{i}$s equal 1.  3. It is confusing in line 339: s$\\sim$p(s) and t$\\sim$p(t). Do p(s) and p(t) here the ones calculated and updated in equation (6-7) in lines 369-370? Or maybe it is fixed since each sample already has a ground truth $e_{i}$. If it is the former case, I think it might be a little weird to predict the p(s) and p(t) which the paper uses to draw samples, because p(s) and p(t) are already given since $e_{i}$s are known for all samples?  4. The authors did not justify why Optimal Transport (OT) is used and did not elaborate on what are OT's advantages. One simple substitute for OT is average euclidean distance or average cosine similarity, which can be used to replace the paper's equation (8). There are more substitutes to OT, such as the KL divergence or the Jensen-Shannon divergence (which are commonly used to make comparisons with OT). It is worth comparing OT with say, Euclidean distance, KL divergence as a side experiment. All these simple substitutes are probably super-efficient and quicker to compute than OT.  5. It is not known if the OT sample selection process in 2.4.3 only runs once or runs iteratively as EP module is updated during the training steps. Are optimizing the loss of equation (10), i.e. the training steps, and solving OT in equation (3) conducted by turns iteratively? It will be much easier for readers to know the whole process if more details and a flow chart can be added. Furthermore, what is the runtime for solving the entropic regularized discrete OT problem, and the runtime for OT sample selection?\n6. It is claimed in lines 128-132 that \"it would be beneficial for the LD to be trained with examples containing events\". The statement in lines 137-148 also focuses only on LD. Why only LD benefits from seeing examples containing events? Do the text encoders also benefit from seeing these examples? A clue that the encoder might benefit from unlabelled data is in 3.4's result where simply MLM fine-tuning can derive considerable gains.\n7. In section 3.4, the result shows that simple MLM fine-tuning on unlabelled target language data derives considerable gains against BERT-CRF baseline. I was curious if the authors could do BERT-CRF + MLM + EP like in equation (10), can the performance be better than ALA? If true, it might show that a simple MLM is better than adversarial training. ",
    "comments": "1. The writing is not fluent enough. Some typos and awkward/redundant/unnatural sentences such as lines 019, 041-043. \n2. Using Optimal Transport (OT), or more specifically leveraging the Wasserstein Distance, in GAN is first seen in the Wasserstein GAN paper, i.e. WGAN (Arjovsky et al. ICML 2017). It might be beneficial to discuss WGAN a bit or even add WGAN as a baseline method. \n3. The paper should elaborate on OT in both the introduction and the methodology parts and should provide more details and justifications for OT. \n4. In equation (4), L2 distance is used. In OT,  earth mover's distance is more common. What is the benefit of L2 distance? \n5. I hope to see the authors' response in resubmission (if rejected) or clarifications in the camera-ready (if accepted) to remove my concern. ",
    "overall_score": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]