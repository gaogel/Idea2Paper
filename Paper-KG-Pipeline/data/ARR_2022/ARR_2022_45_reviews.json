[
  {
    "review_id": "9f16d7fbb89be40b",
    "paper_id": "ARR_2022_45",
    "reviewer": null,
    "paper_summary": "This paper proposes add copy probability in a transformer-based model to improve the generation quality on question answering task. Specifically, the attention distribution from the last encoder-decoder attention layer is used as the copy probability. In addition, the word embedding and the output representation are used together to compute generation probability. The experiment results show the proposal method outperforms baseline model on two benchmark datasets. ",
    "strengths": "1. The paper is well organized, and it explains the proposed solution clearly. ",
    "weaknesses": "1. The idea is incremental. Applying the copy mechanism in the point generator paper on transformer model has been studied in many papers.\n    [1] Deaton, Jon, et al. \"Transformers and pointer-generator networks for abstractive summarization.\" ( 2019).\n    [2] Prabhu, Nikhil, and Katharina Kann. \" Making a Point: Pointer-Generator Transformers for Disjoint Vocabularies.\"\n2. The experimental results are not convincing enough. Only compared to the base version of FiD/FiD-KD model, and lack of result on a large version. ",
    "comments": "1. Whatâ€™s the performance of the proposed method on the Yes/No answer which cannot be extracted from passage? ",
    "overall_score": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]