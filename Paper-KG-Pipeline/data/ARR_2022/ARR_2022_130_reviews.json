[
  {
    "review_id": "3331de31268882a7",
    "paper_id": "ARR_2022_130",
    "reviewer": null,
    "paper_summary": "The paper proposes a new dataset for reading comprehension with multi-span answers, MultiSpanQA. \nIn addition, a new model for MultiSpanQA is introduced.\nThe **dataset** is created using a re-annotation of questions with multiple answers from Natural Questions. \nThis re-annotation process consists of two phases: - First, each instance is given one of four categories: (1) Good example, (2) Bad question, (3) Bad answer span(s), and (4) Bad QA pair.\n- Second, instances are categorized to different \"semantic structures\" (for example, whether all answer spans are needed to correctly answer a question, or each span is an answer on its own). \nAn expanded version of the dataset also includes single-span and unanswerable questions for better simulation of real-world scenarios, The **proposed model** is a sequence tagging module (BIO), augmented by three modules: - Semantic structure prediction - Span number prediction - Span adjustment module that combines the predicted number of spans with preliminary predicted spans.\nThe authors demonstrated that their proposed model improves over single-span models (generalized for multi-span QA) and over vanilla sequence tagging models. ",
    "strengths": "- The problem of multi-span QA is very important and helps deviating from the limiting single-span setting - The dataset is potentially useful for the QA community - To the best of my knowledge, the 5-way semantic annotation scheme proposed by the authors is novel (and I found it elegant and important to better understand the task) - The proposed model improves over all baselines ",
    "weaknesses": "1. Section 4 (Models) misses some details about the proposed model. For example, what is the exact inference procedure over $O_p$ (personally I prefer equations over textual descriptions) 2. Evaluation metrics: This subsection is difficult to read and not rigorous. ",
    "comments": "Comments & questions: - Abstract: The sentence in lines 12-17 (\"After multi-span re-annotation, MultiSpanQA consists of over a total of 6,0000 multi-span questions in the basic version, and over 19,000 examples with unanswerable questions, and questions with single-, and multi-span answers in the expanded version\") is cumbersome and can be made clearer.  - Do you perform re-annotation for the expanded dataset as well? The text now says \"..and applying the same preprocessing\" (line 355) - this point can be made more clear.\n- What is the inference procedure for single-span (v1)? Is the prediction a long span like in training?\nTypos: - Line 47: \"constitinga\" -> \"consisting\" - Line 216: \"classifies\" -> \"classify\" ",
    "overall_score": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "8b153255696c4dbb",
    "paper_id": "ARR_2022_130",
    "reviewer": "Kazutoshi Shinoda",
    "paper_summary": "This paper tackles question answering under a new setting, where models should extract multiple spans from textual context as an answer set. For this problem, the authors propose a new dataset, MultiSpanQA, consisting of 6.4k questions and 19k unanswerable questions. The context and questions of MultiSpanQA are extracted from Natural Questions and the answer sets are re-annotated by three trained annotators. \nThe authors cast this problem as a sequence tagging problem and show its efficacy by comparison with a simple baseline. ",
    "strengths": "- This paper proposed a multiple-span answer setting for extractive question answering and a dataset, MultiSpanQA, consisting of 6.4k multi-span questions for this problem based on the re-annotation of the Natural Questions dataset.\n- The proposed joint training with span number prediction and structure prediction, and span adjustment module can improve the QA scores on MultiSpanQA compared to a pure sequence tagging baseline. ",
    "weaknesses": "- How the proposed multiple-span answer setting is essential in real-world applications is unclear to me. \n  * If multiple answer spans for a question often exist within one sentence consecutively as shown in Table 2, just extracting the whole sentence as a single-span answer may be enough to answer a question and easier to model than the proposed multi-span setting. \n  * If there is a non-negligible number of cases where multiple answer spans are distributed across multiple sentences or passages in the datasets, the proposed framework may be important and worth being studied.\n- The number of questions with multi-span answers in the proposed dataset is small. \n  * The sizes are Train: 6465, Val: 646, Test: 646. \n  * The conventional QA datasets usually consist of more than hundreds of thousands of QA pairs for train and tens of thousands for test. \n  * Only evaluating on such a small test set is likely to overlook the overfitting or shortcut learning of a QA model. ",
    "comments": "#### Minor comments: - L228: I think the grammatical errors in questions in QA datasets are not necessarily problematic but rather important characteristics because a QA model is sometimes required to be robust to these errors in real-world applications as claimed by [1].\n[1] Ravichander et al. 2021. NoiseQA: Challenge Set Evaluation for User-Centric Question Answering. In EACL.\n#### Typos: - L47: consistinga of -> consisting of - L348: real-word -> real-world ",
    "overall_score": "2.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "a2b8425216b033d2",
    "paper_id": "ARR_2022_130",
    "reviewer": "Kang Liu",
    "paper_summary": "This paper focused on the task of multi-span question answering. The main contribution is to propose a dataset and its expansion for the focused task. The authors also proposed some baselines and show the basic performance of this task. ",
    "strengths": "Multi-span question answering is a new problem in machine reading comprehension. Constructing such a dataset is interesting and beneficial for this research area.\nThe constructed dataset is large, and the passages and questions come from the real texts. ",
    "weaknesses": "I wonder whether some multi-span answer structures are really needed or not? For example, in Table 2, in Conjunction, Multi-part-disjunction (Redunant) and Shared Structure, multi answers are separated by \"and\". If we do not separate them, I think is also ok by regarding them as a single answer.  As mentioned in the abstract part, a new evaluation metric is proposed. What is it? ",
    "comments": "1. In line 047, \"consistinga\" should be \"consisting\". \n2. In Table 5, the performance of \"DESCRIPTION\" is very low. What is that reason? ",
    "overall_score": "2.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]