[
  {
    "review_id": "7857e9075c709a77",
    "paper_id": "ARR_2022_264",
    "reviewer": "Gabriella Lapesa",
    "paper_summary": "The paper tackles the known issue of multi-label hierarchical extreme classification, the scenario in which each item can be assigned to multiple classes from a large set of possible classes (in the order of thousands) which are hierarchically structured. The paper surveys existing metrics very thoroughly and then proceeds to define the formal desiderata for metrics designed for the targeted problem and evaluates existing metrics with respect to these properties. A new metric is proposed, ICM, which is based on information content and satisfies all desiderata but one (which is already an improvement with respect to existing metrics. The proposed metric is then evaluated on synthetic data and in a case study from the automatic encoding of discharge reports.  I really like the paper and I think should be accepted. More detailed feedback is nevertheless provided below. ",
    "strengths": "- Very clear and useful survey, accompanied by a formal definition and assessment of the desiderata for a proper evaluation metric - Novel metric proposed - Evaluation on both synthetic data and a real-world case study ",
    "weaknesses": "The case study does not provide very strong evidence for the superiority of ICM with respect to the other measures. A paper like this one, whose  ambition is to make a strong methodological contribution, should have a better coverage of real-world evaluation.  It would be made stronger if the authors could test their metric in one or two further domains, and extend the pool of evaluated classifiers to architecture in which the distribution of labels in the development set also plays a role. ",
    "comments": "While defining the formal desiderata, probably for reasons of space, the authors do not provide enough intuitive description of what the target of the different principles is. I believe that the paper would be made much stronger if the authors would keep the somewhat didactic attitude that characterizes the overview. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "5e3486ca3ce459c4",
    "paper_id": "ARR_2022_264",
    "reviewer": null,
    "paper_summary": "This paper first stipulates desirable properties for metrics measuring the performance of multi-label classification systems where the label set has a hierarchical structure. Then it introduces the Information Contrast Model (ICM) and compares it with other metrics which are commonly used in the literature to evaluate multi-label classification.  It is argued ICM satisfies more of the desirable properties than others.\nThe paper presents two experiments using synthetic and real-world text classification datasets. The synthetic data experiment results shows ICM distinguished better class assignments from worse assignments robustly for various types of differences. The real-world data experiment shows ICM ranked tested classifiers similar to the existing metrics but penalizes them differently. ",
    "strengths": "1. The paper made explicit desirable properties of an evaluation metric for hierarchical multi-label classification tasks, and analyzed existing and proposed metrics using the concepts. I think it is an important contribution for metric research, which often ends up in a competition for better correlations to another “golden” metric. The formulation can facilitate in-depth discussions based on metric aspects and allow metric users to choose one based on their demands. ",
    "weaknesses": "1. I cannot fully understand the settings for the synthetic data experiments, thus cannot be sure what kind of evidence these experiments provide. \n    * For the “Sensitivity to Error Rate” test, the authors wrote “we randomly remove selected (i, c) assignments” but didn’t explain how they selected. They might mean removing assignments with a probability X but I cannot tell for sure if my understanding is correct from the statement. \n    * For the “Hierarchical Similarity” test, the authors mentioned “single labeled items with leaf categories”, which implies there are items with non-leaf categories. But non-leaf categories never seemed to be assigned when  generating the synthesized datasets. I’m confused. \n    * For the “Item Specificity” test, the authors wrote “For the worst output, we randomly select an item i, and we take one if its assignments (i, c).” The “if” clause looks unfinished and I cannot guess what was, probably unintentionally, dropped.\n2. I cannot see what kind of new values ICM will have practically for selecting and/or designing multi-label classifiers. The real-world data experiment showed ICM’s ranking is aligned with some other existing metrics. It is argued that ICM penalizes some classifiers differently after score normalization. But the normalization seems to depend on absolute scores of compared classifiers thus the penalty statement may not hold when more classifiers are added to comparison. ",
    "comments": "I have nothing to mention here. ",
    "overall_score": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]