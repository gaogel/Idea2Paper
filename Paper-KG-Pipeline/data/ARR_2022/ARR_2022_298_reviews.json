[
  {
    "review_id": "82d6c7a265215702",
    "paper_id": "ARR_2022_298",
    "reviewer": null,
    "paper_summary": "This paper proposes an approach for few-shot text classification. The authors propose a Siamese Network to encode the input text and a textual representation of a label. Once encoded, the cosine similarity of the two encodings is computed and it's used in a Textual Entailment like task to decide the label of the input. The main contribution of the paper is in the Label Tuning technique, which allows to 1) re-use the architecture for multiple tasks and ii) to save computational cost at inference time. ",
    "strengths": "The paper is in a well-know area in Computational Linguistics which is gaining attention in the past years. The paper introduces an interesting perspective on the usage of Cross Attention models for few-shot settings, demonstrating that a much simpler (and faster approach) can achieve a comparable accuracy in many different tasks/datasets. The authors show multiple results in English and multi-lingual settings that seems convincing. ",
    "weaknesses": "The main weak point of the paper is that at it is not super clear. There are many parts in which I believe the authors should spend some time in providing either more explanations or re-structure a bit the discussion (see the comments section). ",
    "comments": "- I suggest to revise a bit the discussion, especially in the modeling section, which in its current form is not clear enough. For example, in section 2 it would be nice to see a better formalization of the architecture. If I understood correctly, the Label Embeddings are external parameters; instead, the figure is a bit misleading, as it seems that the Label Embeddings are the output of the encoder.\n- Also, when describing the contribution in the Introduction, using the word hypothesis/null hypothesis really made me think about statistical significance. For example, in lines 87-90 the authors introduce the hypothesis patterns (in contrast to the null hypothesis) referring to the way of representing the input labels, and they mention \"no significant\" difference, which instead is referring to the statistical significance. I would suggest to revise this part.\n- It is not clear the role of the dropout, as there is not specific experiment or comment on the impact of such technique. Can you add some details?\n- On which data is fine-tuned the model for the \"Knowledge Distillation\" - Please, add an intro paragraph to section 4.\n- The baseline with CharSVM seems disadvantaged. In fact, a SVM model in a few-shot setting with up to 5-grams risks to have huge data-sparsity and overfitting problems. Can the authors explain why they selected this baseline? Is a better (fair) baseline available?\n- In line 303 the authors mention \"sentence transformers\". Why are the authors mentioning this? Is it possible to add a citation?\n- There are a couple of footnotes referring to wikipedia. It is fine, but I think the authors can find a better citation for the Frobenius norm and the Welch test.\n- I suggest to make a change to the tables. Now the authors are reporting in bold the results whose difference is statistical significant. Would it be possible to highlight in bold the best result in each group (0, 8, 64, 512) and with another symbol (maybe underline) the statistical significant ones? \n- ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "4ae7e5a89a0e8818",
    "paper_id": "ARR_2022_298",
    "reviewer": null,
    "paper_summary": "This paper presents new methods for zero/few-shot classification focusing on improving inference and training time costs. Few-shot classification is a challenging setting where very little training data is available (5-100 examples per label). Recent work has seen that entailment models are effective at zero/few-shot classification, where the input sentence is used as the premise and label strings are used as hypothesis. Inference from these types of approaches typically need a number of forward passes (equal to the number of labels) through a large pretrained model, which involves an expensive cross-attention step between the input sentence and hypothesis.\nThe first method proposed in the paper uses a dual-encoder / Siamese network to separately encode the premise and hypothesis. This is much more efficient at inference, since label vectors can be cached and only the input sentences need to be encoded once (rather than as many times as the number of labels). The paper further proposes \"label tuning\", where the premise encoder is kept fixed, and only the label embeddings are tuned. This helps significantly reduce training time cost, and allows deployment of a single shared premise encoder for input sentences across various different tasks (saving memory costs).\nThe authors validate their approach on several classification datasets in English, German and Spanish. Siamese Networks perform similar to their cross-attention counter-parts. Label tuning is not as effective, but a lot of performance can be recovered using knowledge distillation. ",
    "strengths": "1. Zero/few-shot classification is a very important and practically relevant problem. This paper is well-grounded into state-of-the-art research in few-shot learning and has an excellent literature survey.\n2. The paper identifies an important limitation of current few-shot methods using entailment approaches (slow inference time), and does a thorough analysis comparing the proposed more efficient Siamese networks with the expensive cross-attention models, finding little difference in performance. Moreover, the authors have an interesting finding that simply using the label strings themsleves (instead of hand-crafted prompts / strings) are equally effective.\n3. The authors further improve training/inference time time/memory efficiency using label tuning, where the input encoder parameters are kept fixed, and only label vectors are finetuned. Since this leads to a performance drop compared to approaches which fine-tune the encoder network, the authors propose using knowledge distillation from the fine-tuned model to recover part of the performance.\n4. The paper has several experiments in both English and multilingual settings. A total of 16 datasets are studied, and results tables comprehensively compare different variants of the models. The paper also has good analysis experiments studying inference time speed, performance by token length, the effect of negation markers. ",
    "weaknesses": "No major concerns, added minor points in the Comments section. I thought this was a well-written paper overall with good contributions. The only thing I would have liked to see was similar few-shot experiments in non-classification tasks too, like some kind of structured prediction, QA, text generation. However, I agree that this is probably out of the scope of the current work. ",
    "comments": "1. Start Section 4 by mentioning you are listing baseline models, it's a bit abrupt currently. \n2. Any reason you did not consider the other GLUE / SUPERGLUE tasks? \n3. I think you should rename \"null hypothesis\" to something else, since \"null hypothesis\" could have a different meaning and is not specific to label verbalization. \n4. The results section would be easier to read if there were bold-marked takeaways as paragraph headers, or at the beginning / end of each paragraph. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]