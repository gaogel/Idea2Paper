[
  {
    "review_id": "aea5e35400998885",
    "paper_id": "ARR_2022_309",
    "reviewer": null,
    "paper_summary": "Note - I reviewed this paper in the past and had a positive criticism about it. The authors also addressed my previous comments and I keep my positive review from before.\nThis paper discusses methods for improving multi-domain training for dialog response generation. The authors experiment with several approaches to improve multi-domain models, namely (1) \"Interleaved Learning\", when data from multiple domains/corpora is concatenated and used for training, (2) \"Labeled Learning\" where each example is encoded using an additional corpus-specific embedding/label that guides the model, (3) \"Multi-Task Labeled Learning\" where the model has an additional classification head that determines the domain/corpora label based on the given context, and (4) \"Weighted Learning\" where the authors propose a weighted loss function that give more weight on words that are especially salient in a given domain.\nThe authors run experiments that evaluate the different approaches using 4 dialog datasets (PersonaChat, OpenSubtitles, Ubuntu and Twitter) where they show the effect of each approach on the resulting model as measured using BLEU, perplexity and F1. While the experiments show that there is no single best approach on all metrics, the proposed approaches improve the results over simple corpora concatenation or single-corpora training. A human evaluation showed that the proposed \"Weighted Learning\" approach was favorable in comparison to the other methods. ",
    "strengths": "The main strengths of the paper are as follows: The highlighted task of multi-domain dialog generation is important, practical and relatively understudied. \nTo the best of my knowledge, the proposed \"Weighted Learning\" approach is novel The experiments are thorough and convincing, especially as they include a human evaluation ",
    "weaknesses": "The main weakness of the paper is that some of the proposed approaches lack novelty - \"interleaved learning\", \"labeled learning\", \"multi-task labeled learning\" were studied extensively in the MT community. Having said that, I am not aware of works applying those approaches to open-domain dialog generation. ",
    "comments": "line 230 - \"learning material\" --> \"training data\" ",
    "overall_score": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]