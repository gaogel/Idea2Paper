[
  {
    "review_id": "37280326dd784256",
    "paper_id": "ARR_2022_357",
    "reviewer": null,
    "paper_summary": "This work tackles the automatic identification of human values in arguments. \nHuman values can provide the underlying motives behind arguments and can explain why someone takes a position or what goals the person sees as worth striving for. \nOpposing values or different ways of prioritizing these values lead to conflicts and disagreements. If the underlying values can be made explicit, valuable insights can be gained in the analysis of different discourses. In other contexts arguments can be automatically generated for a target audience based on appropriate matching values and thus be more convincing. This motivates the authors to tackle the task and initially address the following problems: they systematically evaluate existing taxonomies from social and psychological science. Based on this they develop a framework that integrates the values from the various theories and that provides different levels of granularity, thus making an automatic classification of these values more feasible. The framework is used to create a resource with arguments from different geographical regions annotated with human values. The authors conduct some classification experiments with transformer-based models and discuss the results in an analysis. ",
    "strengths": "The authors provide the following contributions: - a systematic literature review of existing theories about the classification of human values in argumentation - a theoretically-informed framework to annotate human values in arguments. Different levels of granularity provide flexibility for the annotation of new data and is useful to do automatic classification of human values, especially when data is scarce and the classes are imbalanced - a resource annotated with their new framework that is made accessible for the research community. The annotation process is described in detail. The resource contains arguments from different cultural regions.  - experiments for the automatic classification of human values using the new data set. These can serve as a baseline for future research and they provide some initial insights about the difficulty of the task and the robustness ",
    "weaknesses": "Section 3 could benefit from a more clear structure and some insights from the background section that lead to the design of the proposed taxonomy. It took me some time to understand that the schemata introduced in section 2.1 are also considered when designing the new taxonomy (Table 1), making this more explicit (already in section 2.1) would help to make this more clear. To me it is not clear how the beginning of section 3 (claim 1: human values are implicit, claim 2: human values make arguments more persuasive, claim 3: the implicit connection has to me made explicit) provide justifications for the new design. To me the motivation for the proposed taxonomy is a) that it combines different theories therefore is more complete, missing values have been added and b) that is has different levels of granularity which is useful for both, quantitative analyses and machine learning. Although this was hinted at in the introduction, I am missing the clear motivation / advantages for the new taxonomy in this section. \nOn top of that there is a missing reference that is relevant, as it tackles the automatic classification of human values using the Schwartz taxonomy (though in social media and using feature-based approaches, no Deep Learning): A Societal Sentiment Analysis: Predicting the Values and Ethics of Individuals by Analysing Social Media Content (https://aclanthology.org/E17-1069.pdf) ",
    "comments": "This paper has some weaknesses in section 2 / 3 but these can be resolved by some rewording / making things more clear / adding the missing reference. The theoretical review and development of the new taxonomy, the resource and the experiments are a strong contribution and will be valuable for the research community.  minor remarks: 212-218: unclear, what is meant by 'task'? The 'task' for the ML model? \n247-255: sounds more like future work or has to be rephrased to make clear that this is a motivation for using cross-cultural data 289: how was it translated? manually? \nWhy only 50 arguments from Africa but 100 from India and China? \ntypos: 155 (present*s*), 268 conclusion(s) ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "b576530cde6bfac3",
    "paper_id": "ARR_2022_357",
    "reviewer": null,
    "paper_summary": "This paper addresses the issue of automatically mining values behind arguments. Relying on a taxonomy of values based on literature and consistent with those used in social sciences, and collecting data across 4 cultural domain, the paper presents a robust study on the automatic detection of values with very encouraging results. ",
    "strengths": "The paper is exquisitely well-written and presented. The experimental design is sound, and the data collection follows good practice, leading to very encouraging results.  The background literature review is excellent.  Overall, I think this is an excellent paper which I would like to see at ACL. Addressing the weaknesses below would strengthen the paper even further. ",
    "weaknesses": "There is a slight imbalance between the background and the description and discussion of results in the paper. Given the very strong set up, the final part of the paper seems somewhat underwhelming. Further discussion on the results and their significance, with more space allocated to what they mean for further work and applications would have made for an even stronger paper.\nAt alpha=.49 inter-annotator agreement is low, which is not unexpected given the complexity of the task. However, further discussion on how this could be addressed (e.g. with better annotation manuals or training) is missing. ",
    "comments": "Line 458: equally effected -> equally affected Overall, no other issues with the writing, which is excellent. I would advise the authors to dedicate a bit more space on discussing the results and their significance for further work and applications, perhaps shortening the background sections slighlty. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "7f07c676946fce33",
    "paper_id": "ARR_2022_357",
    "reviewer": "Gabriella Lapesa",
    "paper_summary": "The paper introduces a dataset targeted at computational argumentation, more specifically at implicit assumptions which are often involved in premises supporting arguments. Such implicit assumptions, labelled as “human values” in the paper and in the social sciences, are shared beliefs regarding the positive assessment of some dimension, e.g., “having a comfortable life is good”, “being polite is good”, etc. The dataset released with the paper contains 5270 English arguments annotated with a 54-level taxonomy of human values, which is  organized hierarchically and covers four geographical cultures (USA, China, India, Africa). The release is complemented by modeling experiments which set the baseline for future work on the data.\nThe main strengths of the paper are in 1) the novelty of the dataset and the fine-grained taxonomy it adopts, 2) the interdisciplinary nature of the adopted taxonomy, and 3) the size and coverage (for the U.S. part).  The main weaknesses are 1) the cursory description of the annotation schema, 2) the need for a more detailed discussion of the modeling results (what works, what doesn’t), 3) the lack of ground for cross-cultural generalization which is claimed as a contribution.  For these reasons, I consider the paper good but, acceptance-wise, I am not fully convinced by in its current version. I recommend the authors address the points summarized above and discussed below, and I look forward to read the next version of the paper, either as a resubmission or in its final version if it is accepted. ",
    "strengths": "- As a strong supporter of the introduction of interdisciplinary work in NLP, I really liked the theoretical motivation behind the paper. Moreover, when it comes to computational argumentation, I believe such implicit assumptions like the ones at the core of the paper are of enormous relevance and still a bottleneck in the NLP/Argument Mining handling of real-world data.\n- I also strongly appreciate the fine-grainedness of the adopted taxonomy and the annotation effort it required.  - The US subset, which covers the vast majority of the dataset, is indeed very large and commonly employed in Argument Mining, allowing for robust generalization. ",
    "weaknesses": "- Precisely because the annotation is the major contribution of the dataset, I would have expected a more structured discussion of its different layers. Figure 1, in the Introduction, is extremely complex and not discussed at all. Similarly, Table 1 is incredibly rich and the text of section 3 basically focusses on the comparison between the adopted annotation schema and previous work/theories that the NLP reader is not supposed to be familiar with.  - The modeling results, albeit meant as baseline experiments to accompany the dataset, should have been discussed more thoroughly. What is the effect of frequency of a specific annotated human value on the performance in modeling it? Besides excluding the extremely rare human value from the test set, was the sampling of training/development/test also conducted taking care of the general distribution of the categories?  - I believe that the cross-cultural claim put forward by the authors should be smoothened for two reasons: 1) not only the non US subsets are very small, but the sample of users who writes on English forums cannot be considered representative of the culture of the respective geographical locations; 2) even bypassing 1, the authors do not provide any discussion of the distribution of values in the 4 involved geographic locations — which is what I was very much looking forward to as a reader, since the onset of the paper. ",
    "comments": "- Write-up: The contextualization provided in 2.2 would benefit from examples of the comparison terms (framing in the Entman sense; framing in the Argument Mining sense) and by more explicit connections between those argumentation schemes and the ones adopted in the datasets. E.g., care in moral foundation theory, Benevolence: caring in the dataset).  - Modeling: BERT is multi-label, SVM multi-class. Can the authors elaborate on how this (paired with the skewed frequency coming with the high-granular dataset) affects the modeling?  - Conceptually: Human values are a phenomenon that has its own standing independently from argumentation. They are expected to vary across social groups, geographical locations, etc. The authors should probably acknowledge this explicitly, and I think they could make their motivation/contribution even stronger by reflecting on the gain of both sides of this interdisciplinary study. Human values can of huge interest for computational argumentation (main motivation of the paper). But what is the specific perspective/contribution that the focus on argumentative text can give on the study of human values? ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "7bb3a2bd4904204c",
    "paper_id": "ARR_2022_357",
    "reviewer": null,
    "paper_summary": "The paper presents a novel dataset of arguments annotated with human values from a taxonomy of 54 values. The dataset contains 5270 arguments annotated by 3 crowdworkers each. More precisely, the dataset contains arguments from different cultures, namely 50 arguments for Africa, 100 for China, 100 for India and 5020 for USA (from an existing dataset). In addition, the authors propose some baselines to automatically establish the link between the considered human values and the natural language arguments. ",
    "strengths": "- the paper tackle an interesting issue for the ACL community, and in particular, for the Argument Mining community. Human values may help to improve the research in the area about assessing the quality of natural language arguments.\n- the paper is the first that aims to identify values behind arguments.\n- an annotated dataset is build for this task, which has an indubitable value for the whole community.\n- the paper is well written and the motivation and methodology is clearly detailed.\n- results are sound. ",
    "weaknesses": "- the presented dataset is highly unbalanced from the culture point of view, where the western countries are leading (the USA in this case). This leads to un unbalanced dataset from the point of view of the values too, meaning that the dataset captures mostly western country human values and not all of them. In general, the China-India-Africa part of the dataset is not convincing in terms of impact on the obtained results. This also regards the structure of the arguments, which is far from being homogeneous (e.g., in the case of Africa arguments). Also the basic structure of the arguments limits the impact of the presented dataset, i.e., one premise + one conclusion.\n- the methods used to automatically link the arguments to their implicit human value(s) are basic and not novel (BERT, SVM, 1-baseline). They mostly represent baselines for this computational task. This means that the main contribution of the paper is the annotated resource.\n- No error analysis is provided. ",
    "comments": "- the dataset should be improved concerning the eastern country representativeness, as well as its impact on the structure of the arguments.\nTypos: - one conclusions --> conclusion ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]