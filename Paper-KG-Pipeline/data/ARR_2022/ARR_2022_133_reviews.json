[
  {
    "review_id": "3ad7968944d1e229",
    "paper_id": "ARR_2022_133",
    "reviewer": "Mengzhou Xia",
    "paper_summary": "The paper introduces a pre-trained vision language model (FewVLM) for prompt-based few-shot vision language tasks such as image captioning and vision question answering. The model is pre-trained with a combined objective of masked language modeling and prefix language modeling. Compared to giant pre-trained vision language models, FewVLM is relatively smaller, but it achieves significantly better zero-shot and few-shot performances, as reported. The authors also conducted a fine-grained analysis understanding the effect of different prompts, data sizes, and pre-training objectives. Their findings include that 1) zero-shot tasks are more sensitive to prompt crafting than few-shot tasks. 2) low-quality prompt also learn fast when increasing data size 3) the masked language modeling objective helps vqa more while the prefix language modeling objective boosts captioning performance. ",
    "strengths": "- The idea is straightforward and the results presented are solid and strong. It shows that with the proper objective for pre-training, the pre-trained models could be more performant on zero-shot and few-shot tasks even when the model size is much smaller than those giant pre-trained vision language models - The analysis is comprehensive and interesting, and some of the conclusions align well with the findings in NLP tasks. For example, prompt crafting is essential for zero-shot prediction, which inspires better prompt searching. ",
    "weaknesses": "- The baselines are not very well explained in the paper, making it hard to understand the difference between the proposed model and the baselines. It would be much better if the authors could add some brief introductions for each baseline model.  - The paper also lacks analysis or an intuitive explanation as to why the proposed model outperforms large pre-trained models like Frozen. The numbers look strong, but the analysis focus on how different factors affect FewVLM instead of why FewVLM outperforms baselines. ",
    "comments": "- I also wonder why some numbers are missing from table 2-5? Is it because these numbers are not reported in the original papers? ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "ce8f55e360259259",
    "paper_id": "ARR_2022_133",
    "reviewer": "Jialu Li",
    "paper_summary": "This paper proposes a moderate-sized vision-and-language model for low-resource learning of several vision-and-language tasks. Specifically, the proposed methods includes first pre-training a sequence to sequence transformer based model with prefix language modeling and masked language modeling. Then, the pre-trained model can be adapted to several vision-and-language tasks (e.g., VQAv2) in a zero-shot or few-shot manner with hand-craft prompts. Empirical results show that the proposed methods could achieve comparable results to PICa on VQA tasks. This paper also analyzes the effect of using different hand-craft prompts/noisy prompts for zero-shot/few-shot performance and the effect of two pre-training objectives on different tasks. ",
    "strengths": "1. The proposed method is effective regarding zero-shot and few-shot performance, achieving comparable performance to larger vision-and-language models (i.e., PICa) on VQA tasks. On most tasks, the proposed method gets large improvement compared with VL-T5, which has almost the same architecture (same parameter number). \n2. Analysis shows that different prompt design will affect the zero-shot performance a lot. \n3. Interesting analysis demonstrating the effect of different pre-training objectives on different tasks, suggesting that the zero-shot and few-shot performance on downstream tasks can get better if including similar pre-training objective. ",
    "weaknesses": "1. The few-shot performance is evaluated over 5 randomly sampled different training and dev splits, it's better to also report the performance variance across 5 splits. If the variance is high, it might be better to include an analysis of how to pick training/dev data for few-shot learning / what kind of data will be more helpful for the performance. \n2. The proposed method gets large improvement compared with VL-T5, which has almost the same architecture. I'm also interested in how much of the improvement comes from different pre-training objective and how much of the improvement comes from better hand-craft prompt design (if we consider VL-T5 as using dataset-specific prefixes as hand-craft prompt). ",
    "comments": "Typos: Line 166: \"or significantly improves\", no \"or\"?\nSuggestions: Related paper: Learning to Prompt for Vision-Language Models. Zhou et al. ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "0711fd7db80f8adb",
    "paper_id": "ARR_2022_133",
    "reviewer": null,
    "paper_summary": "In this paper, the author(s) propose a new method, FewVLM, for learning vision-language tasks using a few examples. FewVLM is pre-trained using a mixture of PrefixLM and MaskLM losses on the MDCOCO and Visual Genome datasets. When adapting to downstream tasks of VQA and Image Captioning, the author(s) study the role of prompts in improving performance in the zero-shot and few-shot settings. Their analyses reveal that zero-shot performance can vary significantly with different prompts, but the performance stabilizes with the availability of a few examples. In their main results, the author(s) show that FewVLM can outperform Frozen and PICa while having far fewer parameters. ",
    "strengths": "- The paper studies an interesting question of low-resource adaptation of vision language models to downstream tasks. This is perhaps one of the first works to explore the role of prompts in vision language models.\n- FewVLM is a (relatively) small scale model that can be run in academic settings and still manages to significantly outperform Frozen that is about one order of magnitude larger. Further, it can achieve results almost on par with PICa which uses GPT-3 as a backbone.\n- Extensive ablation analysis of different prompts and settings in addition to the role of the pre-training objectives. ",
    "weaknesses": "In section 6.3, the author(s) mention that Unified VLP underperforms FewVLM on the captioning task. However, the conjecture mentioned is not clear. Why does the architecture or the pre-training make the model underperform? Particularly when the objective seems well-aligned with the downstream task? ",
    "comments": "line 47: model → model. \nline 275: to the text → to the text as in ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]