[
  {
    "review_id": "7a37c234f8021057",
    "paper_id": "ARR_2022_2",
    "reviewer": "Ashkan Kazemi",
    "paper_summary": "This paper introduces a new dataset and a different problem setting for automatic fact-checking. The authors argue that it makes more sense to formulate automatic fact-checking as an entailment problem in which the veracity of the claim is evaluated against existing evidence. Authors demonstrate that by modifying a recent dense passage retrieval method proposed by Karpukhin et. al. (2020), they can improve over a TF-IDF baseline. ",
    "strengths": "Authors offer a more realistic problem setting for automated fact-checking using existing data sources.\nEvaluation framework (prequential evaluation) used in this study is the more appropriate evaluation framework that takes into account the temporal aspect of fact-checking, than static accuracy and F1 scores. Aside from evaluation, the paper also follows a sound research design in general. ",
    "weaknesses": "This dataset is very similar to the FEVER formulation of fact-checking, it's just data from a different and more noisy domain, which is of course a more realistic problem setting for automated fact-checking. The approach the authors propose is still useful but not very novel.  At least some version of this data has been previously released in other publications. See https://aclanthology.org/2021.acl-short.86/ , https://aclanthology.org/2020.emnlp-main.623/ .\nBriefly at the end of the abstract and In 5.1 (Stage-1 Results) in detail, you mention that your dense passage retrieval approach improves over a TF-IDF baseline. Why is that improvement significant enough to be published? Since you are only comparing with a simple baseline, It is difficult to tell how much improvement is good improvement.\nThe negative pairs used for training the dense passage retrieval model appear to be selected at random. This presents a known issue about negative sampling into the problem setting: because it's easier to distinguish negative example pairs than positive ones, your final model ends up learning to do the easier task. Since learning the discrepancy between irrelevant and randomly selected <claim, article sentence> pairs is much simpler than learning the similarity between relevant <claim, article sentence> pair, the model ends up not learning very much. ",
    "comments": "How do you handle false negatives in your problem formulation? I couldn't find any references to automatic or manual deduplication of the claims in your data, and this data clearly may contain different versions of the same claim fact-checked multiple times. How do you make sure that the false negatives in your dataset do not disturb your evaluation? i.e., what if model A is performing better than model B in your task, but because model A is picking more on those false negatives and you only count performance based on true positives, you end up showing model B superior, where in reality model A is superior but your evaluation is noisy? ",
    "overall_score": "2.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "3c49820f93e3e837",
    "paper_id": "ARR_2022_2",
    "reviewer": "Yan Zhang",
    "paper_summary": "The paper provides a benchmark dataset that can be used for training & evaluation of automated fact checking systems. The major contribution of this paper is that they provide a large collection of 33,697 claims with associated review articles and premise articles. In the experiment, this work presents a two-stage detection framework, including evidence sentence extraction and claim veracity inference.  LSTM-based baselines and RoBERTa-based baselines are included and compared. ",
    "strengths": "1. The idea of using premises articles for claim inference in automated fact checking is interesting. \n2. The paper is overall well-structured and the methods are explained clearly. ",
    "weaknesses": "1. The methods are not novel as they are largely borrowing from existing work. \n2. It would be nice to have more detailed descriptions of the data collection process, e.g., label mapping, and data statistics, (how many articles per claims? how many sentences per articles? sentence length?) If not enough space on the main text, these information could be added on appendix. \n3. It would be better if the authors evaluate more state-of-the-art methods on this benchmark dataset. \n4. In section 3.3, the authors claim that the disadvantage of using web search is indirect data leak. Can we eliminate the data leak through filtering with publishing time. ",
    "comments": "1. The prequential evaluation is well-written.  It would be interesting to see more such analysis and discussion of the datasets. \n2. Did you try the combination of TF-IDF and dense retrieval for evidence sentence extraction? \n3. As your dataset is imbalanced, it would be better to see some analysis of the outputs. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "d5a6f95cdf5edc2a",
    "paper_id": "ARR_2022_2",
    "reviewer": null,
    "paper_summary": "The paper introduces a new fact-checking dataset, containing claims from a variety of fact-checking websites. No annotations are performed. Instead, the dataset combines each claim with its review written by a human fact-checker (working for the respective fact-checking site) and all background articles mentioned in the review. The main task to be solved using the dataset is claim verification based on background articles. The full dataset contains over 30k claims.\nThe authors experiment with various baselines models for solving this task in a two-step approach. The first is sentence retrieval with the goal to retrieve those sentences from all background articles that belong to background articles of a claim. The authors compare TF-IDF with a BERT architecture, where the latter shows better performance. In a second step, the top retrieved sentences are used for veracity prediction. Here, a Bi-LSTMs, hierarchical attention networks, and RoBERTa are compared, with the RoBERTa model outperforming the others.\nThe authors also show that there is a time-dependent topic shift in the dataset, which affects performance. That is, if the model was to be applied to new data collected in the future, a drop in performance is to be expected due to the topic shift. ",
    "strengths": "-\tA large new dataset for fact-checking is introduced that will be made publicly available. \n-\tA two-step approach with various different baseline models is evaluated on the new dataset. \n-\tAn analysis regarding data-shift is performed, which is very relevant for the real-world application of models trained on the dataset. \n-\tThe paper is well written and easy to follow. ",
    "weaknesses": "-\tThe authors claim that this is the first fact-checking dataset considering background documents. However, both the FEVEROUS dataset (“FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information” by Aly et al. 2021) and the UKP Snopes Corpus (“A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking” by Hanselowski et al. 2019) consider background documents. Particularly the latter dataset considers links found in the review articles just as in the WatClaimCheck datast, the only difference being that only the Snopes website is used. \n-\tGiven that the main contribution of the paper is the introduction of a new dataset, more analysis of the dataset would have been useful. For example, is there any overlap of claims, background documents, or (more generally) topics between the claims extracted from the various different sources? ",
    "comments": "-\tIt would be interesting to know if there is a particularly reason that the authors frame the retrieval task (stage 1) as sentence retrieval rather than as document retrieval. \n-\tIt was not completely clear to me whether the retrieval model is trained only on the new dataset or if the model pre-trained on the QA task is further fine-tuned on the dataset. \n-\tIn addition to the two datasets mentioned in the “Weakness” section there are others that could be useful to compare against, see e.g. “Automated fact-checking: A survey” by Zeng et al. 2021 or “A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking” by Hanselowski et al. 2019 for further references. \n-\tIn Table 1, it is stated that the Vlachos and Riedel 2014 dataset has no Evidence, however in the paper it is stated that “we also collected the sources used by the journalists in the analysis provided for the verdict. Common sources include tables with statistics and reports from governments, think tanks and other organisations, available online”. \n-\tIn Section 4.2.2, is it correct that for each claim-true_sentence_from_review pair, n=number_of_claims incorrect sentences are chosen? Furthermore, are these incorrect sentences chosen from one other review article or each sentence from a different review article? \n-\tIn Section 4.2.2, it is unclear what the “top scoring” sentences (line 417) are. Do you choose a certain number of sentences or do you define a cut-off threshold? \n-\tThe fact that the claimant is included in the veracity prediction hints at the authors’ assumption that certain claimants are more or less likely to make true/false claims. It would be useful here to investigate this relationship between claimant and veracity as well as to exclude the claimant from the prediction (only use claim and evidence) to figure out what the weight of the claimant in this prediction is (maybe the model does not actually learn to predict the veracity from the evidence but rather from the claimant). \n-\tIn Section 4.3.3, how do you deal with the RoBERTa token limit? \n-\tIn Section 5.1 it would be useful to report the total number of sentences in the test set so that the reader has an idea of the difficulty of the retrieval task. \n-\tIn Table 3 the authors report the performance based on the review of a claim as the upper bound. However, it seems to me that a better upper bound would be the prediction based on true background articles. \n-\tIt would be interesting to also report or mention the class-wise F1 scores for veracity prediction and to analyse the confusion between the classes as a confusion between the True and False is much more severe than between True (or False) and the Partially True (False) labels. ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]