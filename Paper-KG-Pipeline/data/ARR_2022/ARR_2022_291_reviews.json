[
  {
    "review_id": "2a9d16fb890611d1",
    "paper_id": "ARR_2022_291",
    "reviewer": "Ni Lao",
    "paper_summary": "This paper proposes DAR, which improves dense retrieval models with MixUp interpolations and dropout perturbation. MixUp (Zhang+ 2018) was initially proposed for image models, which creates augmented examples by linearly interpolating both the representations and labels between positive-negative sample pairs.  This work applies this technique to dense  text retrieval, and augments the original contrastive loss with soft binary cross-entropy losses. Furthermore, this paper also explores perturbing the representations with 0.1 dropout rate, and this technique can be combined with the MixUp strategy.\nExperiment on the DPR framework shows that DAR significantly outperforms vanilla DPR and its combinations with different data augmentation strategies (QA,DA,AR). The improvement is more prominent on unlabeled documents (not seen during training).  Oblation study shows that perturbation is only marginally helpful by itself, but a lot more helpful when combined with MuxUp.  Experiment on the ANCE framework shows that combining ANCE with DAR significantly improves MRR, and marginally improves R@1k. ",
    "strengths": "This work is well motivated from the need to generalize unseen documents, and shows good empirical results. ",
    "weaknesses": "However, I find it less motivated by the computational cost of other data augmentation approaches, which are not really that expensive as shown in Table 7. ",
    "comments": "It seems that the proposed document augmentation approach is not mutually exclusive to other approaches such as query generation (QA, DA, AR in Table 1), and we should evaluate the combined performance of them with DAR. ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "4983c10973bc21ac",
    "paper_id": "ARR_2022_291",
    "reviewer": "Kai Hui",
    "paper_summary": "The paper proposed two document augmentation methods for training data augmentation, namely, the interpolation with mixup and the stochastic perturbation with dropout. In the interpolation, a dummy document is created by interpolating the positive and negative documents which is associated with a soft label; In the dropout, multiple dummy document representations are  generated by masking the document representation with stochastic masks. ",
    "strengths": "- The description of the motivation, and method are clear and easy to follow - The proposed methods are intriguing - The reported results are promising. Especially, the boost in terms of the success rate @ 20, 5, 1 on NQ looks good. ",
    "weaknesses": "- The biggest concerns/confusions during the reading are the lack of implementation details of the proposed methods. They should have been described in the implementation details in Section 4.1.   1) For the interpolation method, how the \\lambda has been set? \n2) For the dropout, thru the reading of the response letter, my understanding is that multiple stochastic masks (w/ 0 and 1) are applied to a document presentation from an encoder. Herein, what is the dropping rate? How many masks have been generated?\n- I am not sure TQA is a good enough benchmark for the proposed method.  It can be seen from Table 1, none of the competing deep models could outperform BM25. Though the proposed DAR could boost DPR, I am not sure if such gain is meaningful. ",
    "comments": "- The proposed methods are novel and intriguing. The confusion mostly comes from the implementation details and the results. Why not extend the efforts to a full paper and properly add all of the details given the extra space? ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]