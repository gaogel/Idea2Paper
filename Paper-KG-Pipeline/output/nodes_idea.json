[
  {
    "idea_id": "idea_0",
    "description": "通过多原型嵌入统一文本和知识表示",
    "tech_stack": [
      "多原型嵌入",
      "实体表示学习",
      "联合向量空间建模"
    ],
    "input_type": "文本实体提及及知识图谱实体",
    "output_type": "统一向量空间中的实体嵌入",
    "source_paper_ids": [
      "ACL_2017_104"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_1",
    "description": "利用硬性单调注意力机制提升形态词形生成效果",
    "tech_stack": [
      "神经网络",
      "硬性单调注意力",
      "字符级序列建模"
    ],
    "input_type": "词根及其形态句法属性",
    "output_type": "目标词形（变形后的单词）",
    "source_paper_ids": [
      "ACL_2017_105"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_2",
    "description": "通过弱监督实现跨语言命名实体识别的高效标注与表示投射",
    "tech_stack": [
      "弱监督学习",
      "跨语言表示投射",
      "命名实体识别"
    ],
    "input_type": "源语言带注释文本及目标语言未标注文本",
    "output_type": "目标语言的命名实体识别结果",
    "source_paper_ids": [
      "ACL_2017_107"
    ],
    "pattern_ids": [
      "pattern_7"
    ]
  },
  {
    "idea_id": "idea_3",
    "description": "利用多重图结构建模，实现重叠实体的识别。",
    "tech_stack": [
      "多重图建模",
      "序列标注",
      "深度学习"
    ],
    "input_type": "自然语言文本序列",
    "output_type": "包含重叠关系的实体识别结果",
    "source_paper_ids": [
      "ACL_2017_108"
    ],
    "pattern_ids": [
      "pattern_7"
    ]
  },
  {
    "idea_id": "idea_4",
    "description": "提出改进的神经关系检测方法提升KBQA准确率",
    "tech_stack": [
      "神经网络",
      "关系检测",
      "知识库问答"
    ],
    "input_type": "自然语言问题",
    "output_type": "知识库中的关系标签",
    "source_paper_ids": [
      "ACL_2017_117"
    ],
    "pattern_ids": [
      "pattern_33"
    ]
  },
  {
    "idea_id": "idea_5",
    "description": "利用神经网络自动追踪对话状态，实现数据驱动的对话管理。",
    "tech_stack": [
      "神经网络",
      "端到端学习",
      "概率建模"
    ],
    "input_type": "用户对话文本及上下文信息",
    "output_type": "对话状态的概率分布（belief state）",
    "source_paper_ids": [
      "ACL_2017_122"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_6",
    "description": "利用知识引导结构化注意力网络提升对话系统理解能力",
    "tech_stack": [
      "知识引导",
      "结构化注意力网络",
      "自然语言理解"
    ],
    "input_type": "用户语音或文本输入，相关知识库",
    "output_type": "结构化语义表示或任务指令",
    "source_paper_ids": [
      "ACL_2017_128"
    ],
    "pattern_ids": [
      "pattern_25"
    ]
  },
  {
    "idea_id": "idea_7",
    "description": "结合句法类型与启发式规则识别文本中的时间表达式",
    "tech_stack": [
      "句法分析",
      "启发式规则",
      "信息抽取"
    ],
    "input_type": "自然语言文本",
    "output_type": "时间表达式及其结构化信息",
    "source_paper_ids": [
      "ACL_2017_12"
    ],
    "pattern_ids": [
      "pattern_26"
    ]
  },
  {
    "idea_id": "idea_8",
    "description": "结合复杂网络与词嵌入分析语言特征检测轻度认知障碍。",
    "tech_stack": [
      "复杂网络分析",
      "词嵌入",
      "语言特征提取"
    ],
    "input_type": "患者语音转录文本",
    "output_type": "轻度认知障碍检测结果",
    "source_paper_ids": [
      "ACL_2017_130"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_9",
    "description": "用神经网络端到端学习自动挖掘文本中的论证结构",
    "tech_stack": [
      "神经网络",
      "端到端学习",
      "序列标注"
    ],
    "input_type": "自然语言文本",
    "output_type": "论证结构及其关系标签",
    "source_paper_ids": [
      "ACL_2017_134"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_10",
    "description": "通过词向量将语义相近的词映射到相近的向量空间位置",
    "tech_stack": [
      "词向量",
      "神经网络",
      "大规模语料库训练"
    ],
    "input_type": "单词或文本数据",
    "output_type": "词的连续向量表示",
    "source_paper_ids": [
      "ACL_2017_145"
    ],
    "pattern_ids": [
      "pattern_1"
    ]
  },
  {
    "idea_id": "idea_11",
    "description": "分析阅读理解评估指标对前置技能和文本可读性的影响。",
    "tech_stack": [
      "自然语言处理",
      "阅读理解评估",
      "技能分析"
    ],
    "input_type": "开放域文档与相关问题",
    "output_type": "系统在阅读理解任务中的表现评估",
    "source_paper_ids": [
      "ACL_2017_148"
    ],
    "pattern_ids": [
      "pattern_33"
    ]
  },
  {
    "idea_id": "idea_12",
    "description": "通过学习词形结构实现字符级神经机器翻译",
    "tech_stack": [
      "字符级编码器",
      "神经网络",
      "词形学建模"
    ],
    "input_type": "源语言字符序列",
    "output_type": "目标语言字符序列",
    "source_paper_ids": [
      "ACL_2017_150"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_13",
    "description": "自动标注和评估语法纠错中的错误类型",
    "tech_stack": [
      "自动化标注",
      "错误类型分类",
      "系统评估"
    ],
    "input_type": "带有语法错误的文本及系统输出",
    "output_type": "带错误类型标签的纠错结果及评估报告",
    "source_paper_ids": [
      "ACL_2017_169"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_14",
    "description": "利用事件论元信息，通过监督注意力机制提升事件检测效果",
    "tech_stack": [
      "事件检测",
      "监督注意力机制",
      "论元信息建模"
    ],
    "input_type": "包含事件及论元的文本句子",
    "output_type": "检测并分类事件及其触发词",
    "source_paper_ids": [
      "ACL_2017_16"
    ],
    "pattern_ids": [
      "pattern_14"
    ]
  },
  {
    "idea_id": "idea_15",
    "description": "通过离散分布聚类定量评估词向量带来的增益",
    "tech_stack": [
      "词嵌入",
      "离散分布聚类",
      "定量分析"
    ],
    "input_type": "词向量与传统文本特征表示",
    "output_type": "词向量相较于传统方法的增益度量",
    "source_paper_ids": [
      "ACL_2017_173"
    ],
    "pattern_ids": [
      "pattern_1"
    ]
  },
  {
    "idea_id": "idea_16",
    "description": "弱监督下联合嵌入概念、短语和词语，提升语义表示。",
    "tech_stack": [
      "弱监督学习",
      "嵌入表示",
      "向量空间模型"
    ],
    "input_type": "文本数据，包括词语、短语和概念",
    "output_type": "统一的词语、短语和概念向量表示",
    "source_paper_ids": [
      "ACL_2017_178"
    ],
    "pattern_ids": [
      "pattern_1"
    ]
  },
  {
    "idea_id": "idea_17",
    "description": "提出网络黑产论坛产品识别任务及数据集，探索细粒度领域适应方法。",
    "tech_stack": [
      "自然语言处理",
      "领域适应",
      "数据标注"
    ],
    "input_type": "论坛帖子文本数据",
    "output_type": "帖子中产品类别及相关标注",
    "source_paper_ids": [
      "ACL_2017_180"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_18",
    "description": "通过建模话语间上下文关系提升多模态情感分析效果",
    "tech_stack": [
      "多模态融合",
      "上下文建模",
      "深度学习"
    ],
    "input_type": "视频中的文本、音频和视觉信息序列",
    "output_type": "每个话语的情感类别或情感分数",
    "source_paper_ids": [
      "ACL_2017_182"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_19",
    "description": "提出Attention-over-Attention机制提升阅读理解模型对文本和问题的匹配能力。",
    "tech_stack": [
      "神经网络",
      "注意力机制",
      "深度学习"
    ],
    "input_type": "文章文本和填空式问题",
    "output_type": "问题的预测答案",
    "source_paper_ids": [
      "ACL_2017_18"
    ],
    "pattern_ids": [
      "pattern_33"
    ]
  },
  {
    "idea_id": "idea_20",
    "description": "提出跨语言适用的语义标注体系UCCA，支持多语言快速稳定标注。",
    "tech_stack": [
      "UCCA语义标注",
      "基础语言学理论",
      "认知语言学"
    ],
    "input_type": "文本语料（多语言）",
    "output_type": "结构化语义标注结果",
    "source_paper_ids": [
      "ACL_2017_193"
    ],
    "pattern_ids": [
      "pattern_3"
    ]
  },
  {
    "idea_id": "idea_21",
    "description": "通过大规模伪训练数据提升零代词消解性能，缓解标注数据稀缺问题。",
    "tech_stack": [
      "伪数据生成",
      "监督学习",
      "零代词消解"
    ],
    "input_type": "未标注文本或含零代词的语料",
    "output_type": "零代词的指代消解结果",
    "source_paper_ids": [
      "ACL_2017_19"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_22",
    "description": "比较不同上下文类型与表示对词向量学习效果的影响",
    "tech_stack": [
      "词嵌入模型",
      "上下文窗口设计",
      "向量空间分析"
    ],
    "input_type": "文本语料库",
    "output_type": "低维词向量表示",
    "source_paper_ids": [
      "ACL_2017_201"
    ],
    "pattern_ids": [
      "pattern_1"
    ]
  },
  {
    "idea_id": "idea_23",
    "description": "提出宏观-微观统一的主次关系建模方法以解析话语结构。",
    "tech_stack": [
      "宏观话语结构建模",
      "主次关系识别",
      "语义分析"
    ],
    "input_type": "话语文本",
    "output_type": "话语主次结构及语义关系",
    "source_paper_ids": [
      "ACL_2017_214"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_24",
    "description": "通过引入分段机制提升LDA模型的话题连贯性",
    "tech_stack": [
      "LDA",
      "主题建模",
      "分段算法"
    ],
    "input_type": "文本语料库",
    "output_type": "分段后具有更高话题连贯性的主题分布",
    "source_paper_ids": [
      "ACL_2017_216"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_25",
    "description": "利用传导式非线性方法提升中文实体的上位词预测准确性",
    "tech_stack": [
      "传导式学习",
      "非线性模型",
      "中文自然语言处理"
    ],
    "input_type": "中文实体及其上下文信息",
    "output_type": "实体对应的上位词（hypernym）",
    "source_paper_ids": [
      "ACL_2017_21"
    ],
    "pattern_ids": [
      "pattern_12"
    ]
  },
  {
    "idea_id": "idea_26",
    "description": "研究如何在NLP中识别和处理转喻现象。",
    "tech_stack": [
      "自然语言处理",
      "命名实体识别",
      "语义分析"
    ],
    "input_type": "包含转喻表达的文本",
    "output_type": "识别并恢复真实指代的实体或概念",
    "source_paper_ids": [
      "ACL_2017_220"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_27",
    "description": "提出新标注方案，实现实体与关系的联合抽取",
    "tech_stack": [
      "序列标注",
      "联合抽取模型",
      "深度学习"
    ],
    "input_type": "非结构化文本",
    "output_type": "实体及其关系对",
    "source_paper_ids": [
      "ACL_2017_222"
    ],
    "pattern_ids": [
      "pattern_28"
    ]
  },
  {
    "idea_id": "idea_28",
    "description": "提出非英语语义模型评估数据集的构建流程",
    "tech_stack": [
      "分布式语义模型",
      "数据集构建",
      "语句配对与标注"
    ],
    "input_type": "待评估语言的语句对",
    "output_type": "用于语义模型验证的标注数据集",
    "source_paper_ids": [
      "ACL_2017_226"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_29",
    "description": "基于多维向量空间的新情感倾向度量方法",
    "tech_stack": [
      "多维向量空间",
      "情感分析",
      "无监督方法"
    ],
    "input_type": "文本数据（如句子、短语或单词）",
    "output_type": "情感倾向分数或类别",
    "source_paper_ids": [
      "ACL_2017_237"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_30",
    "description": "强调用高效、简单监督任务评估词嵌入的数据效率。",
    "tech_stack": [
      "词嵌入",
      "监督学习",
      "表示学习评估"
    ],
    "input_type": "文本数据（如词或句子）",
    "output_type": "词嵌入在下游任务中的表现指标",
    "source_paper_ids": [
      "ACL_2017_239"
    ],
    "pattern_ids": [
      "pattern_1"
    ]
  },
  {
    "idea_id": "idea_31",
    "description": "用数学方法解释Skip-Gram词向量的加法组合性原理",
    "tech_stack": [
      "Skip-Gram模型",
      "词向量",
      "数学形式化"
    ],
    "input_type": "文本语料库",
    "output_type": "具有可加性特征的词向量",
    "source_paper_ids": [
      "ACL_2017_251"
    ],
    "pattern_ids": [
      "pattern_1"
    ]
  },
  {
    "idea_id": "idea_32",
    "description": "利用条件变分自编码器提升对话系统的话语层多样性",
    "tech_stack": [
      "Conditional Variational Autoencoder",
      "Neural Dialog Models",
      "Discourse-level Decision Modeling"
    ],
    "input_type": "新话语及对话上下文",
    "output_type": "多样化的话语层对话决策",
    "source_paper_ids": [
      "ACL_2017_256"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_33",
    "description": "利用任务相关语料训练词向量提升情感分类效果",
    "tech_stack": [
      "词嵌入",
      "预训练",
      "情感分类"
    ],
    "input_type": "文本语料，待分类句子",
    "output_type": "情感类别标签",
    "source_paper_ids": [
      "ACL_2017_266"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_34",
    "description": "端到端模型结合跨注意力机制实现知识库问答",
    "tech_stack": [
      "端到端神经网络",
      "跨注意力机制",
      "知识库嵌入"
    ],
    "input_type": "自然语言问题",
    "output_type": "知识库中的答案",
    "source_paper_ids": [
      "ACL_2017_26"
    ],
    "pattern_ids": [
      "pattern_25"
    ]
  },
  {
    "idea_id": "idea_35",
    "description": "通过改进LSTM结构提升自然语言推理任务表现",
    "tech_stack": [
      "增强型LSTM",
      "深度学习",
      "自然语言推理"
    ],
    "input_type": "句子对或文本对，用于推理关系判断",
    "output_type": "推理关系标签（如蕴含、矛盾、中立）",
    "source_paper_ids": [
      "ACL_2017_270"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_36",
    "description": "结合半监督和多任务学习提升序列标注性能",
    "tech_stack": [
      "半监督学习",
      "多任务学习",
      "神经网络"
    ],
    "input_type": "文本序列",
    "output_type": "序列标签",
    "source_paper_ids": [
      "ACL_2017_276"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_37",
    "description": "不同写作任务会显著影响文本的语言风格。",
    "tech_stack": [
      "文本风格分析",
      "ROC Story Cloze Task",
      "统计方法"
    ],
    "input_type": "写作任务下的文本数据",
    "output_type": "语言风格变化的分析结果",
    "source_paper_ids": [
      "ACL_2017_288"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_38",
    "description": "利用义原知识增强词表示学习，提高语义表达能力。",
    "tech_stack": [
      "语义知识库",
      "词向量模型",
      "义原注释"
    ],
    "input_type": "词语及其义原信息",
    "output_type": "包含义原语义的词向量表示",
    "source_paper_ids": [
      "ACL_2017_318"
    ],
    "pattern_ids": [
      "pattern_1"
    ]
  },
  {
    "idea_id": "idea_39",
    "description": "利用深度神经网络自动识别事件的事实性属性",
    "tech_stack": [
      "深度神经网络",
      "事件表示学习",
      "语义分析"
    ],
    "input_type": "包含事件的文本数据",
    "output_type": "事件的事实性分类标签（如事实、可能、虚构）",
    "source_paper_ids": [
      "ACL_2017_31"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_40",
    "description": "利用神经网络建模句子间局部连贯性以区分连贯与不连贯文本",
    "tech_stack": [
      "神经网络",
      "句子表示",
      "局部上下文建模"
    ],
    "input_type": "一段或多段文本（句子序列）",
    "output_type": "文本连贯性评分或连贯/不连贯判定",
    "source_paper_ids": [
      "ACL_2017_323"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_41",
    "description": "利用对抗多准则学习统一不同分词标准的中文分词模型",
    "tech_stack": [
      "对抗学习",
      "多任务学习",
      "神经网络"
    ],
    "input_type": "带有多种分词标准标注的中文文本",
    "output_type": "符合指定分词标准的中文分词结果",
    "source_paper_ids": [
      "ACL_2017_326"
    ],
    "pattern_ids": [
      "pattern_34"
    ]
  },
  {
    "idea_id": "idea_42",
    "description": "用概念图结构化和总结大型文档集合，提升信息检索效率。",
    "tech_stack": [
      "多文档摘要",
      "概念图生成",
      "自然语言处理"
    ],
    "input_type": "多篇相关文本或文档集合",
    "output_type": "包含主要概念及关系的概念图",
    "source_paper_ids": [
      "ACL_2017_331"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_43",
    "description": "通过选择性编码机制提升句子抽象式摘要生成效果",
    "tech_stack": [
      "神经网络",
      "选择性编码",
      "序列到序列模型"
    ],
    "input_type": "单个原始句子文本",
    "output_type": "简短的抽象式句子摘要",
    "source_paper_ids": [
      "ACL_2017_333"
    ],
    "pattern_ids": [
      "pattern_27"
    ]
  },
  {
    "idea_id": "idea_44",
    "description": "提出门控自匹配网络提升阅读理解与问答性能",
    "tech_stack": [
      "门控机制",
      "自匹配注意力",
      "神经网络"
    ],
    "input_type": "文章段落和问题文本",
    "output_type": "文章中答案的文本片段",
    "source_paper_ids": [
      "ACL_2017_335"
    ],
    "pattern_ids": [
      "pattern_33"
    ]
  },
  {
    "idea_id": "idea_45",
    "description": "通过联合嵌入文本与行为特征解决评论冷启动垃圾检测问题",
    "tech_stack": [
      "嵌入学习",
      "文本分析",
      "行为建模"
    ],
    "input_type": "评论文本及用户行为数据",
    "output_type": "评论是否为垃圾的判定结果",
    "source_paper_ids": [
      "ACL_2017_338"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_46",
    "description": "通过语言学规则对LSTM进行正则化以提升情感分类效果",
    "tech_stack": [
      "LSTM",
      "语言学正则化",
      "情感分类"
    ],
    "input_type": "文本数据（如句子或评论）",
    "output_type": "情感类别标签（如正面、负面等）",
    "source_paper_ids": [
      "ACL_2017_33"
    ],
    "pattern_ids": [
      "pattern_32"
    ]
  },
  {
    "idea_id": "idea_47",
    "description": "利用丰富预训练提升神经网络中文分词性能",
    "tech_stack": [
      "神经网络",
      "预训练模型",
      "深度学习"
    ],
    "input_type": "未分词的中文文本序列",
    "output_type": "分词后的中文文本序列",
    "source_paper_ids": [
      "ACL_2017_343"
    ],
    "pattern_ids": [
      "pattern_34"
    ]
  },
  {
    "idea_id": "idea_48",
    "description": "提出事件抽取方法，实现事件检测与论元识别",
    "tech_stack": [
      "自然语言处理",
      "事件检测",
      "论元识别"
    ],
    "input_type": "自然语言文本",
    "output_type": "事件类型及其相关论元角色",
    "source_paper_ids": [
      "ACL_2017_350"
    ],
    "pattern_ids": [
      "pattern_14"
    ]
  },
  {
    "idea_id": "idea_49",
    "description": "利用神经网络实现多任务学习以提升单任务性能",
    "tech_stack": [
      "神经网络",
      "多任务学习",
      "深度学习"
    ],
    "input_type": "多任务相关数据集",
    "output_type": "各任务的预测结果",
    "source_paper_ids": [
      "ACL_2017_352"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_50",
    "description": "通过神经网络建模多谓词间互动，提升日语省略论元识别效果",
    "tech_stack": [
      "神经网络",
      "多谓词交互建模",
      "谓词论元结构分析"
    ],
    "input_type": "日语句子文本，包含谓词和论元信息",
    "output_type": "句子中各谓词的论元角色及填充结果",
    "source_paper_ids": [
      "ACL_2017_355"
    ],
    "pattern_ids": [
      "pattern_17"
    ]
  },
  {
    "idea_id": "idea_51",
    "description": "通过学习发音来改进历史文本规范化中的注意力机制。",
    "tech_stack": [
      "注意力机制",
      "发音建模",
      "神经网络"
    ],
    "input_type": "历史文本及其拼写变体",
    "output_type": "标准化或现代化的文本形式",
    "source_paper_ids": [
      "ACL_2017_365"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_52",
    "description": "探讨自动评价指标在自然语言生成系统中的有效性与局限性",
    "tech_stack": [
      "BLEU",
      "自动评价指标",
      "NLG系统分析"
    ],
    "input_type": "自然语言生成系统的输出文本",
    "output_type": "自动评价分数或系统性能评估结果",
    "source_paper_ids": [
      "ACL_2017_367"
    ],
    "pattern_ids": [
      "pattern_13"
    ]
  },
  {
    "idea_id": "idea_53",
    "description": "利用深度学习提升SMT中的形态生成质量",
    "tech_stack": [
      "统计机器翻译",
      "深度学习",
      "自然语言处理"
    ],
    "input_type": "源语言句子及其形态信息",
    "output_type": "目标语言的正确形态句子",
    "source_paper_ids": [
      "ACL_2017_369"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_54",
    "description": "在语言模型中融入隐含嵌套结构和丰富的子词结构信息。",
    "tech_stack": [
      "统计语言模型",
      "神经网络语言模型",
      "结构化建模"
    ],
    "input_type": "自然语言文本序列",
    "output_type": "概率分布或下一个词预测",
    "source_paper_ids": [
      "ACL_2017_371"
    ],
    "pattern_ids": [
      "pattern_29"
    ]
  },
  {
    "idea_id": "idea_55",
    "description": "结合上下文信息进行网络嵌入以建模节点关系",
    "tech_stack": [
      "网络嵌入",
      "上下文建模",
      "低维表示学习"
    ],
    "input_type": "网络结构及节点上下文信息",
    "output_type": "节点的上下文感知低维向量表示",
    "source_paper_ids": [
      "ACL_2017_375"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_56",
    "description": "利用事件驱动递归神经网络自动提取并聚合国际联盟关系。",
    "tech_stack": [
      "事件驱动建模",
      "递归神经网络",
      "关系抽取"
    ],
    "input_type": "国际新闻或事件文本数据",
    "output_type": "结构化的国际联盟关系网络",
    "source_paper_ids": [
      "ACL_2017_376"
    ],
    "pattern_ids": [
      "pattern_28"
    ]
  },
  {
    "idea_id": "idea_57",
    "description": "提出顺序匹配网络提升多轮对话回复选择效果",
    "tech_stack": [
      "深度学习",
      "神经网络",
      "序列建模"
    ],
    "input_type": "多轮对话历史与候选回复",
    "output_type": "最佳回复的选择或排序",
    "source_paper_ids": [
      "ACL_2017_37"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_58",
    "description": "探讨并分类NLG微规划训练语料库的构建方法",
    "tech_stack": [
      "语料库构建",
      "专家标注",
      "众包数据收集"
    ],
    "input_type": "数值、形式或语言输入",
    "output_type": "自然语言文本",
    "source_paper_ids": [
      "ACL_2017_382"
    ],
    "pattern_ids": [
      "pattern_4"
    ]
  },
  {
    "idea_id": "idea_59",
    "description": "通过修饰词组合提升细粒度IsA关系抽取准确性",
    "tech_stack": [
      "Hearst模式",
      "修饰词组合",
      "自动知识抽取"
    ],
    "input_type": "文本语料库，包含实例和类别短语",
    "output_type": "细粒度IsA关系（如“Charles Mingus 是1950年代美国爵士作曲家”）",
    "source_paper_ids": [
      "ACL_2017_384"
    ],
    "pattern_ids": [
      "pattern_8"
    ]
  },
  {
    "idea_id": "idea_60",
    "description": "利用眼动数据学习认知特征提升情感与讽刺分类效果",
    "tech_stack": [
      "眼动追踪数据处理",
      "卷积神经网络",
      "特征融合"
    ],
    "input_type": "文本评论与对应眼动数据",
    "output_type": "情感类别与讽刺类别标签",
    "source_paper_ids": [
      "ACL_2017_387"
    ],
    "pattern_ids": [
      "pattern_32"
    ]
  },
  {
    "idea_id": "idea_61",
    "description": "提出UDEPLAMBDA框架，将UD树库用于语义解析。",
    "tech_stack": [
      "Universal Dependencies",
      "语法分析",
      "语义解析"
    ],
    "input_type": "UD格式句法树",
    "output_type": "可机器处理的语义表示",
    "source_paper_ids": [
      "ACL_2017_388"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_62",
    "description": "用深度强化学习生成多义词的多感知词向量表示",
    "tech_stack": [
      "深度学习",
      "强化学习",
      "词向量表示"
    ],
    "input_type": "文本语料库，包含多义词",
    "output_type": "每个词的多感知词向量",
    "source_paper_ids": [
      "ACL_2017_395"
    ],
    "pattern_ids": [
      "pattern_1"
    ]
  },
  {
    "idea_id": "idea_63",
    "description": "利用神经网络实现跨语言范式补全的一次性迁移学习。",
    "tech_stack": [
      "神经网络",
      "迁移学习",
      "跨语言建模"
    ],
    "input_type": "少量高资源语言范式数据及目标语言单个样本",
    "output_type": "目标低资源语言的完整范式补全结果",
    "source_paper_ids": [
      "ACL_2017_419"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_64",
    "description": "研究主流语言与本地语言融合形成克里奥尔语的演化机制",
    "tech_stack": [
      "语言演化分析",
      "语法结构比较",
      "语料库研究"
    ],
    "input_type": "语言历史与语料数据",
    "output_type": "克里奥尔语演化模型与案例分析",
    "source_paper_ids": [
      "ACL_2017_433"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_65",
    "description": "利用上下文神经网络判别因果词语的具体语义",
    "tech_stack": [
      "神经网络",
      "上下文建模",
      "因果关系识别"
    ],
    "input_type": "包含因果词的文本及其上下文",
    "output_type": "因果词语的语义消歧结果",
    "source_paper_ids": [
      "ACL_2017_435"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_66",
    "description": "利用supertagging提升CCG句法分析效率与准确性",
    "tech_stack": [
      "Supertagging",
      "A*搜索",
      "Combinatory Categorial Grammar (CCG)"
    ],
    "input_type": "句子文本",
    "output_type": "CCG句法分析树",
    "source_paper_ids": [
      "ACL_2017_440"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_67",
    "description": "提出评估创意语言生成的新方法，特别针对说唱歌词代写。",
    "tech_stack": [
      "自然语言处理",
      "文本生成评估",
      "风格与创造力分析"
    ],
    "input_type": "机器生成的说唱歌词文本及参考人类作品",
    "output_type": "创意性、风格和准确性等多维度评价结果",
    "source_paper_ids": [
      "ACL_2017_444"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_68",
    "description": "利用神经话语结构提升文本分类性能",
    "tech_stack": [
      "神经网络",
      "话语结构理论",
      "注意力机制"
    ],
    "input_type": "文本数据",
    "output_type": "文本类别标签",
    "source_paper_ids": [
      "ACL_2017_447"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_69",
    "description": "利用词嵌入和情感分析预测财务披露文本中的波动性",
    "tech_stack": [
      "词嵌入",
      "信息检索模型",
      "情感分析"
    ],
    "input_type": "财务披露文本及历史股价数据",
    "output_type": "公司或市场的波动性预测值",
    "source_paper_ids": [
      "ACL_2017_462"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_70",
    "description": "利用多语言词嵌入实现跨语言任务与迁移学习",
    "tech_stack": [
      "多语言词嵌入",
      "迁移学习",
      "自然语言处理"
    ],
    "input_type": "多语言文本数据",
    "output_type": "统一的词向量表示或跨语言模型",
    "source_paper_ids": [
      "ACL_2017_467"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_71",
    "description": "探索神经网络词表示是否有效捕捉词形结构信息",
    "tech_stack": [
      "神经网络",
      "词嵌入",
      "形态学分析"
    ],
    "input_type": "单词或字符序列",
    "output_type": "连续词表示向量",
    "source_paper_ids": [
      "ACL_2017_477"
    ],
    "pattern_ids": [
      "pattern_1"
    ]
  },
  {
    "idea_id": "idea_72",
    "description": "将语言与视觉信息结合以提升理解能力",
    "tech_stack": [
      "视觉问答模型",
      "图像描述生成",
      "多模态学习"
    ],
    "input_type": "文本和图像",
    "output_type": "答案或图像描述文本",
    "source_paper_ids": [
      "ACL_2017_481"
    ],
    "pattern_ids": [
      "pattern_22"
    ]
  },
  {
    "idea_id": "idea_73",
    "description": "利用指针网络自动识别和抽取文本中的论证结构关系",
    "tech_stack": [
      "指针网络",
      "深度学习",
      "自然语言处理"
    ],
    "input_type": "带有论证结构的文本数据",
    "output_type": "论证结构中各要素及其关系的抽取结果",
    "source_paper_ids": [
      "ACL_2017_483"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_74",
    "description": "利用深度学习提升自动语音识别系统的各模块性能",
    "tech_stack": [
      "自动语音识别",
      "深度学习",
      "概率噪声信道模型"
    ],
    "input_type": "语音信号",
    "output_type": "文本转录结果",
    "source_paper_ids": [
      "ACL_2017_484"
    ],
    "pattern_ids": [
      "pattern_9"
    ]
  },
  {
    "idea_id": "idea_75",
    "description": "结合分布式语义与指称信息，通过跨模态映射和直接词预测提升物体命名效果",
    "tech_stack": [
      "分布式语义建模",
      "跨模态映射",
      "直接词预测"
    ],
    "input_type": "视觉场景中的物体图像及相关上下文",
    "output_type": "物体的名称（单词或短语）",
    "source_paper_ids": [
      "ACL_2017_489"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_76",
    "description": "用简单语言规则微调词向量空间以提升词形相关性",
    "tech_stack": [
      "词向量微调",
      "语言特定规则",
      "分布式语义模型"
    ],
    "input_type": "预训练词向量，语言特定的形态规则",
    "output_type": "形态结构更合理的词向量空间",
    "source_paper_ids": [
      "ACL_2017_494"
    ],
    "pattern_ids": [
      "pattern_1"
    ]
  },
  {
    "idea_id": "idea_77",
    "description": "分析神经机器翻译模型对形态学知识的学习能力",
    "tech_stack": [
      "神经网络",
      "序列到序列模型",
      "注意力机制"
    ],
    "input_type": "源语言文本序列",
    "output_type": "目标语言文本序列",
    "source_paper_ids": [
      "ACL_2017_496"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_78",
    "description": "提出基于块的解码器提升神经机器翻译质量",
    "tech_stack": [
      "神经机器翻译",
      "编码器-解码器结构",
      "块级解码机制"
    ],
    "input_type": "源语言文本序列",
    "output_type": "目标语言文本序列",
    "source_paper_ids": [
      "ACL_2017_49"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_79",
    "description": "提出视觉-语言双模态理解任务，选出最佳场景描述文本。",
    "tech_stack": [
      "多模态学习",
      "图像理解",
      "自然语言处理"
    ],
    "input_type": "一张图片及多个相似文本描述选项",
    "output_type": "最符合图片内容的文本描述",
    "source_paper_ids": [
      "ACL_2017_501"
    ],
    "pattern_ids": [
      "pattern_22"
    ]
  },
  {
    "idea_id": "idea_80",
    "description": "提出概率化正则图语言以更好建模自然语言的语义结构。",
    "tech_stack": [
      "概率模型",
      "正则图语言",
      "语义图表示"
    ],
    "input_type": "自然语言文本或语义图",
    "output_type": "概率化语义图语言表示",
    "source_paper_ids": [
      "ACL_2017_503"
    ],
    "pattern_ids": [
      "pattern_31"
    ]
  },
  {
    "idea_id": "idea_81",
    "description": "提出多词锚点方法提升交互式主题建模的可控性与表达力",
    "tech_stack": [
      "主题模型",
      "锚点算法",
      "多词锚定"
    ],
    "input_type": "大规模文本语料库，用户指定的多词锚点",
    "output_type": "主题-词概率矩阵，改进的主题分布",
    "source_paper_ids": [
      "ACL_2017_516"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_82",
    "description": "提出SHAPEWORLD，用合成数据评测多模态语言理解能力",
    "tech_stack": [
      "深度学习",
      "合成数据生成",
      "多模态评测"
    ],
    "input_type": "图像与文本描述",
    "output_type": "模型对多模态理解的准确性评估",
    "source_paper_ids": [
      "ACL_2017_520"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_83",
    "description": "比较多种HPSG鲁棒解析方法以提升覆盖率与精度",
    "tech_stack": [
      "HPSG语法",
      "鲁棒解析算法",
      "约束求解"
    ],
    "input_type": "自然语言句子或文本",
    "output_type": "句法结构分析结果",
    "source_paper_ids": [
      "ACL_2017_524"
    ],
    "pattern_ids": [
      "pattern_3"
    ]
  },
  {
    "idea_id": "idea_84",
    "description": "利用字符视觉特征学习字符级组合语义，提高稀有词表示能力",
    "tech_stack": [
      "神经网络",
      "字符级建模",
      "视觉特征提取"
    ],
    "input_type": "文本字符及其视觉图像",
    "output_type": "字符或单词的语义表示向量",
    "source_paper_ids": [
      "ACL_2017_543"
    ],
    "pattern_ids": [
      "pattern_29"
    ]
  },
  {
    "idea_id": "idea_85",
    "description": "提出跨语境词义分析通用框架，比较词义随语境变化。",
    "tech_stack": [
      "词嵌入",
      "上下文建模",
      "语义变化检测"
    ],
    "input_type": "文本语料（含不同语境信息）",
    "output_type": "词语语义变化、上下文敏感性分析结果",
    "source_paper_ids": [
      "ACL_2017_553"
    ],
    "pattern_ids": [
      "pattern_1"
    ]
  },
  {
    "idea_id": "idea_86",
    "description": "通过可扩展贝叶斯方法提升RNN在语言建模中的泛化能力",
    "tech_stack": [
      "贝叶斯学习",
      "循环神经网络",
      "语言建模"
    ],
    "input_type": "文本序列上下文",
    "output_type": "下一个词或字符的概率分布",
    "source_paper_ids": [
      "ACL_2017_554"
    ],
    "pattern_ids": [
      "pattern_29"
    ]
  },
  {
    "idea_id": "idea_87",
    "description": "结合端到端神经网络与全局优化提升关系抽取性能",
    "tech_stack": [
      "神经网络",
      "端到端关系抽取",
      "全局优化算法"
    ],
    "input_type": "未结构化文本",
    "output_type": "抽取的实体及其关系",
    "source_paper_ids": [
      "ACL_2017_557"
    ],
    "pattern_ids": [
      "pattern_28"
    ]
  },
  {
    "idea_id": "idea_88",
    "description": "利用双向语言模型进行半监督序列标注，提高上下文表示能力。",
    "tech_stack": [
      "双向语言模型",
      "半监督学习",
      "预训练词嵌入"
    ],
    "input_type": "未标注和部分标注的文本序列",
    "output_type": "每个序列中词的标签（如POS、NER等）",
    "source_paper_ids": [
      "ACL_2017_561"
    ],
    "pattern_ids": [
      "pattern_29"
    ]
  },
  {
    "idea_id": "idea_89",
    "description": "将关系抽取任务转化为阅读理解，实现零样本关系抽取。",
    "tech_stack": [
      "关系抽取",
      "阅读理解建模",
      "零样本学习"
    ],
    "input_type": "文本语料与关系描述（问题形式）",
    "output_type": "抽取出的关系三元组或答案片段",
    "source_paper_ids": [
      "ACL_2017_562"
    ],
    "pattern_ids": [
      "pattern_28"
    ]
  },
  {
    "idea_id": "idea_90",
    "description": "利用词向量空间分析和建模语义关系",
    "tech_stack": [
      "词嵌入",
      "分布式语义模型",
      "向量空间分析"
    ],
    "input_type": "文本语料库或单词列表",
    "output_type": "语义关系的向量表示或相似度评分",
    "source_paper_ids": [
      "ACL_2017_563"
    ],
    "pattern_ids": [
      "pattern_1"
    ]
  },
  {
    "idea_id": "idea_91",
    "description": "提出Grid Beam Search以支持带词汇约束的序列生成",
    "tech_stack": [
      "Grid Beam Search",
      "Beam Search",
      "序列生成模型"
    ],
    "input_type": "待生成序列的输入x及词汇约束",
    "output_type": "满足词汇约束的最优输出序列",
    "source_paper_ids": [
      "ACL_2017_564"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_92",
    "description": "通过Ngram共现统计提升词向量表示质量",
    "tech_stack": [
      "Ngram统计",
      "词嵌入",
      "深度学习"
    ],
    "input_type": "大规模未标注文本语料",
    "output_type": "改进的低维词向量表示",
    "source_paper_ids": [
      "ACL_2017_56"
    ],
    "pattern_ids": [
      "pattern_1"
    ]
  },
  {
    "idea_id": "idea_93",
    "description": "提出一种鲁棒的增量神经语义图解析方法，实现深层语义理解。",
    "tech_stack": [
      "神经网络",
      "增量解析",
      "语义图表示"
    ],
    "input_type": "自然语言句子",
    "output_type": "结构化语义图",
    "source_paper_ids": [
      "ACL_2017_578"
    ],
    "pattern_ids": [
      "pattern_31"
    ]
  },
  {
    "idea_id": "idea_94",
    "description": "提出MinIE方法以简化和最小化OIE抽取的事实三元组。",
    "tech_stack": [
      "开放信息抽取",
      "无监督学习",
      "三元组简化算法"
    ],
    "input_type": "自然语言文本",
    "output_type": "简化后的主体-关系-客体三元组",
    "source_paper_ids": [
      "ACL_2017_579"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_95",
    "description": "结合分层LSTM与外部知识库预测罕见实体",
    "tech_stack": [
      "分层LSTM",
      "外部知识库集成",
      "实体预测"
    ],
    "input_type": "自然语言文本与结构化知识库信息",
    "output_type": "罕见实体的预测结果",
    "source_paper_ids": [
      "ACL_2017_588"
    ],
    "pattern_ids": [
      "pattern_29"
    ]
  },
  {
    "idea_id": "idea_96",
    "description": "结合神经网络与符号推理，实现弱监督下的语义解析器学习",
    "tech_stack": [
      "神经网络",
      "符号推理",
      "弱监督学习"
    ],
    "input_type": "自然语言问题",
    "output_type": "可在Freebase执行的符号程序",
    "source_paper_ids": [
      "ACL_2017_606"
    ],
    "pattern_ids": [
      "pattern_31"
    ]
  },
  {
    "idea_id": "idea_97",
    "description": "自动获取大规模词汇和短语释义资源以提升NLP应用效果",
    "tech_stack": [
      "自动释义挖掘",
      "语义匹配",
      "自然语言处理"
    ],
    "input_type": "文本语料库",
    "output_type": "释义词汇和短语对",
    "source_paper_ids": [
      "ACL_2017_614"
    ],
    "pattern_ids": [
      "pattern_12"
    ]
  },
  {
    "idea_id": "idea_98",
    "description": "构建带注释的修订语料库以研究议论文写作过程",
    "tech_stack": [
      "语料库构建",
      "修订注释",
      "写作过程分析"
    ],
    "input_type": "多版本议论文文本及修订信息",
    "output_type": "结构化修订语料库及分析数据",
    "source_paper_ids": [
      "ACL_2017_619"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_99",
    "description": "端到端强化学习训练对话智能体以提升信息访问能力",
    "tech_stack": [
      "端到端学习",
      "强化学习",
      "自然语言处理"
    ],
    "input_type": "用户自然语言查询",
    "output_type": "智能体生成的自然语言回复",
    "source_paper_ids": [
      "ACL_2017_627"
    ],
    "pattern_ids": [
      "pattern_25"
    ]
  },
  {
    "idea_id": "idea_100",
    "description": "利用迭代扩张卷积实现高效准确的序列标注",
    "tech_stack": [
      "扩张卷积",
      "迭代结构",
      "GPU并行计算"
    ],
    "input_type": "文本序列",
    "output_type": "每个序列元素的标签（如词性或实体类别）",
    "source_paper_ids": [
      "ACL_2017_636"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_101",
    "description": "自动化评估对话回复质量，模拟图灵测试标准。",
    "tech_stack": [
      "机器学习",
      "自然语言处理",
      "对话系统评估"
    ],
    "input_type": "对话回复文本",
    "output_type": "回复质量评分或评价",
    "source_paper_ids": [
      "ACL_2017_649"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_102",
    "description": "利用端到端深度学习模型提升语义角色标注，无需依赖句法分析。",
    "tech_stack": [
      "深度神经网络",
      "端到端学习",
      "语义角色标注"
    ],
    "input_type": "自然语言句子或文本",
    "output_type": "谓词-论元结构及语义角色标签",
    "source_paper_ids": [
      "ACL_2017_654"
    ],
    "pattern_ids": [
      "pattern_17"
    ]
  },
  {
    "idea_id": "idea_103",
    "description": "通过解释神经网络理解价值观申明作文中的书面理由",
    "tech_stack": [
      "神经网络",
      "自然语言处理",
      "模型可解释性分析"
    ],
    "input_type": "价值观申明作文文本",
    "output_type": "模型对作文理由的解释与理解",
    "source_paper_ids": [
      "ACL_2017_657"
    ],
    "pattern_ids": [
      "pattern_18"
    ]
  },
  {
    "idea_id": "idea_104",
    "description": "利用神经网络自动生成具备韵律的英文诗歌",
    "tech_stack": [
      "神经语言模型",
      "音素编码",
      "深度学习"
    ],
    "input_type": "英文语料库或诗歌文本",
    "output_type": "自动生成的韵律诗歌文本",
    "source_paper_ids": [
      "ACL_2017_660"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_105",
    "description": "利用主系统自动生成易记的数字助记词编码",
    "tech_stack": [
      "主系统映射",
      "自然语言处理",
      "算法生成"
    ],
    "input_type": "数字序列",
    "output_type": "易记的助记词单词或短语",
    "source_paper_ids": [
      "ACL_2017_66"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_106",
    "description": "通过二进制编码预测优化神经机器翻译输出层",
    "tech_stack": [
      "神经机器翻译",
      "二进制编码",
      "输出层优化"
    ],
    "input_type": "源语言文本序列",
    "output_type": "目标语言二进制编码词序列",
    "source_paper_ids": [
      "ACL_2017_676"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_107",
    "description": "通过融合学习架构自动构建语义层级体系",
    "tech_stack": [
      "融合学习",
      "语义分析",
      "本体构建"
    ],
    "input_type": "文本语料或词汇集合",
    "output_type": "语义层级结构（如本体或词库）",
    "source_paper_ids": [
      "ACL_2017_67"
    ],
    "pattern_ids": [
      "pattern_12"
    ]
  },
  {
    "idea_id": "idea_108",
    "description": "利用完形填空式大规模数据集提升机器阅读理解能力",
    "tech_stack": [
      "监督学习",
      "机器阅读理解",
      "自动化数据集构建"
    ],
    "input_type": "包含上下文的文档及相关问题",
    "output_type": "针对问题的准确答案",
    "source_paper_ids": [
      "ACL_2017_684"
    ],
    "pattern_ids": [
      "pattern_33"
    ]
  },
  {
    "idea_id": "idea_109",
    "description": "提出越南语文本可读性评估的新公式",
    "tech_stack": [
      "可读性公式设计",
      "文本特征分析",
      "统计方法"
    ],
    "input_type": "越南语文本",
    "output_type": "可读性分数或等级",
    "source_paper_ids": [
      "ACL_2017_68"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_110",
    "description": "利用本体知识增强的词嵌入预测介词短语附着关系",
    "tech_stack": [
      "本体知识",
      "词嵌入",
      "预训练模型"
    ],
    "input_type": "文本中的介词短语及其上下文",
    "output_type": "介词短语的附着预测结果",
    "source_paper_ids": [
      "ACL_2017_691"
    ],
    "pattern_ids": [
      "pattern_1"
    ]
  },
  {
    "idea_id": "idea_111",
    "description": "自动从长文本中提取高质量关键短语以表达主旨",
    "tech_stack": [
      "自然语言处理",
      "关键短语提取",
      "文本表示学习"
    ],
    "input_type": "长文本，如科学论文",
    "output_type": "关键短语或关键词列表",
    "source_paper_ids": [
      "ACL_2017_699"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_112",
    "description": "通过交互式学习将编程语言自然化，实现自然语言到程序的转换。",
    "tech_stack": [
      "语义解析",
      "语法归纳",
      "交互式学习"
    ],
    "input_type": "用户自然语言指令或问题",
    "output_type": "对应的程序代码或执行结果",
    "source_paper_ids": [
      "ACL_2017_706"
    ],
    "pattern_ids": [
      "pattern_31"
    ]
  },
  {
    "idea_id": "idea_113",
    "description": "利用Wikipedia文本直接回答开放域事实性问题",
    "tech_stack": [
      "文档检索",
      "神经阅读理解模型",
      "端到端训练"
    ],
    "input_type": "自然语言问题",
    "output_type": "基于Wikipedia的简明答案",
    "source_paper_ids": [
      "ACL_2017_715"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_114",
    "description": "跨多语言信息抽取并知识库对齐以消除语言壁垒",
    "tech_stack": [
      "多语言信息抽取",
      "实体识别",
      "知识库对齐"
    ],
    "input_type": "多语言文本数据",
    "output_type": "用户可访问的本地化知识信息",
    "source_paper_ids": [
      "ACL_2017_71"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_115",
    "description": "提出结合正字法与其他特征的形态素切分新方法",
    "tech_stack": [
      "形态素切分",
      "自然语言处理",
      "特征融合"
    ],
    "input_type": "原始文本序列",
    "output_type": "切分后的形态素序列",
    "source_paper_ids": [
      "ACL_2017_723"
    ],
    "pattern_ids": [
      "pattern_34"
    ]
  },
  {
    "idea_id": "idea_116",
    "description": "通过用户反馈逐步优化神经语义解析器，实现快速部署NLIDB。",
    "tech_stack": [
      "神经网络",
      "语义解析",
      "用户反馈学习"
    ],
    "input_type": "自然语言查询",
    "output_type": "数据库查询语句",
    "source_paper_ids": [
      "ACL_2017_726"
    ],
    "pattern_ids": [
      "pattern_31"
    ]
  },
  {
    "idea_id": "idea_117",
    "description": "利用行为和社交信息弱监督集体分类推断Twitter政治话语立场",
    "tech_stack": [
      "弱监督学习",
      "集体分类",
      "社交网络分析"
    ],
    "input_type": "Twitter上的用户行为和社交互动数据",
    "output_type": "用户或推文的政治立场分类结果",
    "source_paper_ids": [
      "ACL_2017_727"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_118",
    "description": "利用传播结构和核学习方法检测微博谣言",
    "tech_stack": [
      "传播结构建模",
      "核学习",
      "谣言检测算法"
    ],
    "input_type": "微博帖子及其传播路径数据",
    "output_type": "谣言判定结果（真假标签）",
    "source_paper_ids": [
      "ACL_2017_729"
    ],
    "pattern_ids": [
      "pattern_6"
    ]
  },
  {
    "idea_id": "idea_119",
    "description": "从同义词图自动归纳同义词集合（synsets）",
    "tech_stack": [
      "图论",
      "聚类算法",
      "词义消歧"
    ],
    "input_type": "同义词关系构成的词汇图",
    "output_type": "自动归纳出的同义词集合（synsets）",
    "source_paper_ids": [
      "ACL_2017_741"
    ],
    "pattern_ids": [
      "pattern_12"
    ]
  },
  {
    "idea_id": "idea_120",
    "description": "用序列到序列模型实现AMR解析与生成，突破数据稀疏限制。",
    "tech_stack": [
      "序列到序列模型",
      "神经网络",
      "AMR图表示"
    ],
    "input_type": "自然语言文本或AMR图",
    "output_type": "AMR图或自然语言文本",
    "source_paper_ids": [
      "ACL_2017_752"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_121",
    "description": "联合建模对话内容与话语关系以提升关键信息抽取效果",
    "tech_stack": [
      "机器学习",
      "联合建模",
      "自然语言处理"
    ],
    "input_type": "会议或对话文本",
    "output_type": "关键信息和话语关系结构",
    "source_paper_ids": [
      "ACL_2017_759"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_122",
    "description": "利用神经网络提升自然语言处理多任务性能",
    "tech_stack": [
      "神经网络",
      "深度学习",
      "自然语言处理"
    ],
    "input_type": "文本数据，如句子或文档",
    "output_type": "标签、分类结果或翻译文本",
    "source_paper_ids": [
      "ACL_2017_760"
    ],
    "pattern_ids": [
      "pattern_29"
    ]
  },
  {
    "idea_id": "idea_123",
    "description": "在具体上下文中检测词汇蕴含关系，提升语义理解。",
    "tech_stack": [
      "上下文建模",
      "语义表示",
      "深度学习"
    ],
    "input_type": "带有上下文的词对或句子",
    "output_type": "词汇是否存在蕴含关系的判断",
    "source_paper_ids": [
      "ACL_2017_768"
    ],
    "pattern_ids": [
      "pattern_1"
    ]
  },
  {
    "idea_id": "idea_124",
    "description": "通过动态知识图谱嵌入实现对称协作对话智能体的学习",
    "tech_stack": [
      "动态知识图谱嵌入",
      "对称协作对话建模",
      "深度学习"
    ],
    "input_type": "对话历史与知识图谱信息",
    "output_type": "生成的协作对话响应",
    "source_paper_ids": [
      "ACL_2017_769"
    ],
    "pattern_ids": [
      "pattern_25"
    ]
  },
  {
    "idea_id": "idea_125",
    "description": "利用几何上下文模型识别未见隐喻表达",
    "tech_stack": [
      "几何表示学习",
      "上下文建模",
      "隐喻识别算法"
    ],
    "input_type": "文本语料，隐喻候选短语",
    "output_type": "隐喻识别结果（是否为隐喻）",
    "source_paper_ids": [
      "ACL_2017_775"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_126",
    "description": "利用矩阵分解从推文中建模用户在不同话题上的偏好关联。",
    "tech_stack": [
      "矩阵分解",
      "社交媒体数据分析",
      "用户偏好建模"
    ],
    "input_type": "用户推文及话题标签数据",
    "output_type": "用户在各话题间的偏好关系模型",
    "source_paper_ids": [
      "ACL_2017_777"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_127",
    "description": "通过教师-学生框架实现零资源神经机器翻译",
    "tech_stack": [
      "神经机器翻译",
      "教师-学生框架",
      "无监督学习"
    ],
    "input_type": "无平行语料的源语言文本",
    "output_type": "目标语言翻译文本",
    "source_paper_ids": [
      "ACL_2017_779"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_128",
    "description": "利用双向LSTM结合词汇与语义信息生成词嵌入",
    "tech_stack": [
      "双向LSTM",
      "词嵌入",
      "语义建模"
    ],
    "input_type": "文本序列",
    "output_type": "词嵌入向量",
    "source_paper_ids": [
      "ACL_2017_792"
    ],
    "pattern_ids": [
      "pattern_29"
    ]
  },
  {
    "idea_id": "idea_129",
    "description": "提出可解释的知识迁移模型以补全知识库缺失信息",
    "tech_stack": [
      "知识迁移",
      "可解释模型",
      "知识库补全"
    ],
    "input_type": "知识库中的实体及关系三元组",
    "output_type": "补全后的知识库三元组",
    "source_paper_ids": [
      "ACL_2017_79"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_130",
    "description": "提出基于连续序列的文本相似度度量方法TextFlow。",
    "tech_stack": [
      "文本序列分析",
      "相似度计算",
      "连续序列建模"
    ],
    "input_type": "两段或多段文本数据",
    "output_type": "文本相似度分数",
    "source_paper_ids": [
      "ACL_2017_805"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_131",
    "description": "从自然语言中推断动作与物体的相对物理属性知识",
    "tech_stack": [
      "联合推断",
      "自然语言处理",
      "无结构文本分析"
    ],
    "input_type": "无结构自然语言文本",
    "output_type": "动作和物体在物理属性维度上的相对知识",
    "source_paper_ids": [
      "ACL_2017_818"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_132",
    "description": "利用词语位置信息无监督提取学术文献关键词",
    "tech_stack": [
      "图模型",
      "PageRank算法",
      "无监督学习"
    ],
    "input_type": "学术文献全文",
    "output_type": "文献的关键词列表",
    "source_paper_ids": [
      "ACL_2017_87"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_133",
    "description": "提出一种完全非单调的转换系统，实现无约束的非投射句法分析。",
    "tech_stack": [
      "依存句法分析",
      "转换系统",
      "统计模型"
    ],
    "input_type": "句子或文本序列",
    "output_type": "依存句法树结构",
    "source_paper_ids": [
      "ACL_2017_94"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_134",
    "description": "用基于情感的单语机器翻译方法解释讽刺语句含义",
    "tech_stack": [
      "情感分析",
      "单语机器翻译",
      "自然语言处理"
    ],
    "input_type": "包含讽刺的用户生成文本",
    "output_type": "讽刺语句的直接含义解释",
    "source_paper_ids": [
      "ACL_2017_96"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_135",
    "description": "提出一种基于Winograd式推理的任务，用于评估大语言模型对新颖隐喻理解和推断能力。",
    "tech_stack": [
      "自回归语言模型",
      "概率推断",
      "Winograd schema",
      "GPT-2",
      "GPT-neo",
      "GPT-3"
    ],
    "input_type": "成对的隐喻表达及其对应的解释选项",
    "output_type": "模型对隐喻解释正确性的概率判断与准确率",
    "source_paper_ids": [
      "ARR_2022_0"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_136",
    "description": "提出将视觉知识通过文本和跨模态迁移方法融入预训练语言模型以提升其常识推理能力。",
    "tech_stack": [
      "预训练语言模型",
      "文本知识迁移",
      "跨模态知识迁移",
      "图像编码器",
      "文本编码器",
      "中间预训练",
      "数据增强"
    ],
    "input_type": "图像-文本配对数据（如图像及其描述）、多样化文本语料",
    "output_type": "增强视觉常识推理能力的语言模型输出，如文本分类或推理任务结果",
    "source_paper_ids": [
      "ARR_2022_100"
    ],
    "pattern_ids": [
      "pattern_22"
    ]
  },
  {
    "idea_id": "idea_137",
    "description": "提出基于最优传输的分析方法和新距离度量，提升句子相似性模型的可解释性。",
    "tech_stack": [
      "预训练语言模型",
      "句子嵌入",
      "最优传输",
      "对比学习",
      "上下文嵌入空间"
    ],
    "input_type": "两句话或句子对，用于语义相似性分析",
    "output_type": "句子相似性分数及跨句子词对贡献的可解释性分析",
    "source_paper_ids": [
      "ARR_2022_101"
    ],
    "pattern_ids": [
      "pattern_10"
    ]
  },
  {
    "idea_id": "idea_138",
    "description": "首次提出并公开带有毒性片段标注的数据集TOXICSPANS，并系统研究文本中毒性片段的检测方法与评估框架。",
    "tech_stack": [
      "序列标注",
      "注意力机制二分类器",
      "F1评分",
      "自监督学习",
      "有监督学习"
    ],
    "input_type": "包含用户评论或帖子内容的文本数据",
    "output_type": "文本中被识别为有毒的具体字符或词语片段的偏移位置集合",
    "source_paper_ids": [
      "ARR_2022_102"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_139",
    "description": "提出并分析了NLP模型测试中基于变形关系的系统性和鲁棒性测试方法，拓展了测试维度。",
    "tech_stack": [
      "无监督学习",
      "变形测试（Metamorphic Testing）",
      "鲁棒性测试",
      "系统性测试",
      "语义和句法组成分析"
    ],
    "input_type": "自然语言文本输入及其变形版本",
    "output_type": "模型输出的一致性或系统性表现的评估结果",
    "source_paper_ids": [
      "ARR_2022_103"
    ],
    "pattern_ids": [
      "pattern_4"
    ]
  },
  {
    "idea_id": "idea_140",
    "description": "提出了一种用二值选择操作和L1距离替代乘法的高能效注意力机制E-ATT。",
    "tech_stack": [
      "注意力机制",
      "模型压缩",
      "复杂度优化",
      "二值化操作",
      "L1距离",
      "Transformer"
    ],
    "input_type": "机器翻译任务中的文本序列",
    "output_type": "翻译后的文本序列及能耗分析结果",
    "source_paper_ids": [
      "ARR_2022_104"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_141",
    "description": "提出了一种统一模型UniTE，实现对REF、SRC和SRC+REF三类机器翻译评价任务的统一评估。",
    "tech_stack": [
      "多语言预训练语言模型（PLM）",
      "层级协调（layerwise coordination）",
      "单调区域注意力（Monotonic Regional Attention, MRA）",
      "多任务学习",
      "基于排序的数据标注策略"
    ],
    "input_type": "包含假设译文、源语言文本和参考译文的文本序列",
    "output_type": "翻译质量的自动化评估分数",
    "source_paper_ids": [
      "ARR_2022_105"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_142",
    "description": "提出了一种自动化生成自然且难以检测的对话系统对抗触发器的新方法。",
    "tech_stack": [
      "Universal Adversarial Trigger (UAT)",
      "语言模型",
      "自动化触发器生成",
      "异常检测规避"
    ],
    "input_type": "自然语言对话输入，包括文本和语音识别结果",
    "output_type": "对话系统生成的响应文本，特别关注是否被触发生成有毒内容",
    "source_paper_ids": [
      "ARR_2022_106"
    ],
    "pattern_ids": [
      "pattern_19"
    ]
  },
  {
    "idea_id": "idea_143",
    "description": "提出了一种基于变分概率建模的多任务组dropout方法，实现自动学习任务间相似性并共享子网络结构。",
    "tech_stack": [
      "多任务学习",
      "变分概率建模",
      "组dropout",
      "神经网络",
      "端到端训练"
    ],
    "input_type": "多语言、多领域的机器翻译任务数据",
    "output_type": "针对不同任务自动分配子网络的机器翻译模型及其翻译结果",
    "source_paper_ids": [
      "ARR_2022_107"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_144",
    "description": "提出并系统分析了多语言文档级神经机器翻译中的跨语言零样本迁移方法。",
    "tech_stack": [
      "多语言建模",
      "迁移学习",
      "文档级神经机器翻译（DocNMT）",
      "简单串联法",
      "回译（Back-Translation）",
      "深度学习模型训练与微调"
    ],
    "input_type": "多语言平行句对和/或文档对，包括部分语言仅有句对、部分语言有文档对的数据集",
    "output_type": "针对目标语言对的文档级翻译结果及其性能评估",
    "source_paper_ids": [
      "ARR_2022_108"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_145",
    "description": "提出并纠正了文本匹配模型中的长度分歧偏差，通过对抗测试集和训练方法提升模型泛化能力。",
    "tech_stack": [
      "文本匹配模型",
      "对抗测试集构建",
      "对抗训练",
      "SentLen probing",
      "BERT",
      "MatchPyramid",
      "BiMPM",
      "ESIM"
    ],
    "input_type": "成对的文本数据用于语义相似性判定",
    "output_type": "文本对的语义相似性分类结果",
    "source_paper_ids": [
      "ARR_2022_109"
    ],
    "pattern_ids": [
      "pattern_19"
    ]
  },
  {
    "idea_id": "idea_146",
    "description": "通过生成受语法约束的抽象语法树（AST），提升自然语言到Python代码生成的准确性和可执行性。",
    "tech_stack": [
      "Transformer",
      "预训练语言模型（BERT, GPT, BART）",
      "Seq2Seq",
      "抽象语法树（AST）",
      "TranX架构"
    ],
    "input_type": "自然语言描述的编程需求或问题",
    "output_type": "符合语法约束的Python代码（通过AST生成）",
    "source_paper_ids": [
      "ARR_2022_10"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_147",
    "description": "提出CLEAR方法，通过跨语言和环境无关的表示提升多语言视觉-语言导航任务的泛化能力。",
    "tech_stack": [
      "跨语言表示学习",
      "环境无关视觉表示",
      "模仿学习",
      "强化学习",
      "视觉-语言导航",
      "预训练语言模型"
    ],
    "input_type": "多语言自然语言指令和环境视觉观测数据",
    "output_type": "智能体在新环境中的导航路径或动作序列",
    "source_paper_ids": [
      "ARR_2022_110"
    ],
    "pattern_ids": [
      "pattern_20"
    ]
  },
  {
    "idea_id": "idea_148",
    "description": "定量分析结构化预测模型对对抗攻击的敏感性，并揭示现有方法的局限性。",
    "tech_stack": [
      "对抗攻击",
      "对抗训练",
      "词替换攻击",
      "结构化预测",
      "依存句法分析",
      "序列标注"
    ],
    "input_type": "自然语言文本输入（如句子），用于结构化预测或分类任务",
    "output_type": "模型输出的结构化结果（如依存句法结构）或分类标签",
    "source_paper_ids": [
      "ARR_2022_111"
    ],
    "pattern_ids": [
      "pattern_19"
    ]
  },
  {
    "idea_id": "idea_149",
    "description": "提出并公开了首个带注释指南的技能与知识成分跨度级数据集SKILLSPAN，并系统评估了多种预训练语言模型在岗位技能抽取任务中的表现。",
    "tech_stack": [
      "SKILLSPAN数据集",
      "SpanBERT",
      "BERT",
      "领域自适应预训练",
      "序列标注",
      "多任务学习"
    ],
    "input_type": "来自招聘信息的非结构化文本数据",
    "output_type": "文本中标注的技能和知识成分的跨度及类别",
    "source_paper_ids": [
      "ARR_2022_112"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_150",
    "description": "提出一种基于改进的最佳优先搜索和路径重组的解码框架，以高效生成大规模多样化文本候选。",
    "tech_stack": [
      "最佳优先搜索（Best-First Search）",
      "深度优先路径补全",
      "候选路径重组",
      "文本生成模型（如BART, mBART）"
    ],
    "input_type": "文本生成相关任务的数据，如新闻摘要、机器翻译输入文本",
    "output_type": "大规模、多样化的文本生成候选集合（以lattice结构编码）",
    "source_paper_ids": [
      "ARR_2022_113"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_151",
    "description": "提出了一种结合条件模仿学习和任务重标注的端到端对话系统微调方法，实现目标导向的对话生成。",
    "tech_stack": [
      "端到端语言模型",
      "条件模仿学习",
      "任务重标注",
      "POMDP",
      "辅助损失",
      "任务预训练",
      "离线强化学习"
    ],
    "input_type": "包含对话历史和任务上下文的离线对话数据",
    "output_type": "目标导向、高奖励的对话生成文本",
    "source_paper_ids": [
      "ARR_2022_115"
    ],
    "pattern_ids": [
      "pattern_25"
    ]
  },
  {
    "idea_id": "idea_152",
    "description": "只微调大型语言模型中的偏置参数即可实现高效且任务无关的模型适配。",
    "tech_stack": [
      "Transformer",
      "BERT",
      "偏置参数微调",
      "参数冻结",
      "多任务学习"
    ],
    "input_type": "任务特定的监督训练数据（如文本分类、问答等NLP任务数据）",
    "output_type": "针对特定任务优化后的语言模型参数，提升任务性能",
    "source_paper_ids": [
      "ARR_2022_116"
    ],
    "pattern_ids": [
      "pattern_5"
    ]
  },
  {
    "idea_id": "idea_153",
    "description": "提出两项探测任务系统分析中文BERT模型在字符级理解上的能力，并比较CLM与WWM预训练方式。",
    "tech_stack": [
      "BERT",
      "Transformer",
      "Character-level Masking (CLM)",
      "Whole Word Masking (WWM)",
      "Masked Language Modeling",
      "Texsmart分词工具",
      "ADAM优化器"
    ],
    "input_type": "带有字符替换或插入需求的中文句子或语料",
    "output_type": "模型对字符级错误的纠正结果（如替换或插入正确字符）",
    "source_paper_ids": [
      "ARR_2022_117"
    ],
    "pattern_ids": [
      "pattern_23"
    ]
  },
  {
    "idea_id": "idea_154",
    "description": "提出了通过显式词汇-逻辑对齐提升Text-to-SQL解析泛化能力的两阶段神经框架。",
    "tech_stack": [
      "词汇-逻辑对齐预测",
      "序列到序列模型（seq2seq）",
      "注意力机制",
      "提示信息注入",
      "领域泛化",
      "组合泛化"
    ],
    "input_type": "自然语言问题与关系数据库表结构",
    "output_type": "可执行的SQL查询语句",
    "source_paper_ids": [
      "ARR_2022_119"
    ],
    "pattern_ids": [
      "pattern_2"
    ]
  },
  {
    "idea_id": "idea_155",
    "description": "提出并验证了无需手动选择答案片段的自动问题生成方法，提升教育场景下问题生成效率。",
    "tech_stack": [
      "T5语言模型",
      "SQuAD数据集微调",
      "多任务学习（问答、问题生成、答案抽取）",
      "自动摘要生成"
    ],
    "input_type": "原始文本、人工或自动生成的文本摘要",
    "output_type": "自动生成的相关性高的问题",
    "source_paper_ids": [
      "ARR_2022_11"
    ],
    "pattern_ids": [
      "pattern_15"
    ]
  },
  {
    "idea_id": "idea_156",
    "description": "本论文系统评估并适应多种预训练Transformer模型于法律文本处理任务，提升法律领域自然语言理解性能。",
    "tech_stack": [
      "Transformer",
      "BERT",
      "RoBERTa",
      "DeBERTa",
      "预训练语言模型",
      "微调（fine-tuning）"
    ],
    "input_type": "法律文本数据（如判决书、法规、合同等）及相关自然语言理解任务",
    "output_type": "针对法律任务的文本分类、信息抽取、问答等自然语言理解结果",
    "source_paper_ids": [
      "ARR_2022_120"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_157",
    "description": "提出了一种结合BERT多尺度（文档、分段、词级）表示与回归预测的自动作文评分模型。",
    "tech_stack": [
      "BERT",
      "LSTM",
      "Attention",
      "Dense Regression Layer"
    ],
    "input_type": "学生作文文本数据",
    "output_type": "作文的自动评分数值",
    "source_paper_ids": [
      "ARR_2022_121"
    ],
    "pattern_ids": [
      "pattern_13"
    ]
  },
  {
    "idea_id": "idea_158",
    "description": "提出了一种基于句子级嵌入空间的局部聚合梯度特征归因方法（LAFA），用于提升NLP模型解释性。",
    "tech_stack": [
      "梯度归因",
      "句子级嵌入",
      "相似文本聚合",
      "深度学习模型解释"
    ],
    "input_type": "自然语言文本输入及其对应的深度学习模型",
    "output_type": "输入文本中各特征（如词语、短语）的归因分数",
    "source_paper_ids": [
      "ARR_2022_122"
    ],
    "pattern_ids": [
      "pattern_18"
    ]
  },
  {
    "idea_id": "idea_159",
    "description": "通过多分辨率训练激活原始Transformer实现端到端文档级神经机器翻译，无需修改模型结构。",
    "tech_stack": [
      "Transformer",
      "多分辨率训练",
      "子词分割",
      "Adam优化器",
      "Horovod分布式训练",
      "标签平滑"
    ],
    "input_type": "成对的源语言和目标语言文档级平行语料",
    "output_type": "完整目标语言文档的自动翻译结果",
    "source_paper_ids": [
      "ARR_2022_123"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_160",
    "description": "提出了基于内部对话分解子问题的视觉问答新框架Co-VQA，提升复杂问题的推理能力与可解释性。",
    "tech_stack": [
      "Transformer",
      "自注意力机制",
      "多模态融合",
      "子问题生成",
      "对话式推理"
    ],
    "input_type": "图像与自然语言问题",
    "output_type": "候选答案集合中的最终答案",
    "source_paper_ids": [
      "ARR_2022_124"
    ],
    "pattern_ids": [
      "pattern_22"
    ]
  },
  {
    "idea_id": "idea_161",
    "description": "提出在语义依存句法分析中结合高效的O(n²)双仿射架构与上下文表示以提升性能。",
    "tech_stack": [
      "语义依存句法分析",
      "双仿射架构（biaffine architecture）",
      "Transformer-based上下文表示",
      "图结构建模"
    ],
    "input_type": "自然语言句子或文本",
    "output_type": "语义依存图（predicate-argument dependency graph）",
    "source_paper_ids": [
      "ARR_2022_125"
    ],
    "pattern_ids": [
      "pattern_17"
    ]
  },
  {
    "idea_id": "idea_162",
    "description": "提出了可结构化控制的元评论生成方法，并构建了包含结构控制信号的新数据集MReD。",
    "tech_stack": [
      "神经网络生成技术",
      "编码器-解码器架构",
      "结构化控制信号",
      "多输入线性化"
    ],
    "input_type": "多条评论文本与结构控制信号",
    "output_type": "符合指定结构的元评论文本",
    "source_paper_ids": [
      "ARR_2022_126"
    ],
    "pattern_ids": [
      "pattern_21"
    ]
  },
  {
    "idea_id": "idea_163",
    "description": "提出RICON模型，通过正则性感知和非感知模块联合挖掘中文实体识别中的内部规律信息。",
    "tech_stack": [
      "正则性感知模块",
      "正则性非感知模块",
      "任务特定编码器",
      "优化目标",
      "BERT预训练模型"
    ],
    "input_type": "中文文本序列用于命名实体识别任务",
    "output_type": "实体识别结果，包括实体的边界和类别",
    "source_paper_ids": [
      "ARR_2022_127"
    ],
    "pattern_ids": [
      "pattern_7"
    ]
  },
  {
    "idea_id": "idea_164",
    "description": "将worst-case aware自动课程学习方法应用于多语言依存句法分析以提升零样本迁移性能。",
    "tech_stack": [
      "worst-case aware curriculum learning",
      "multi-task learning",
      "multilingual pretrained language models",
      "dependency parsing",
      "Universal Dependency treebanks"
    ],
    "input_type": "多语言依存句法分析任务中的多语言文本及其句法结构数据",
    "output_type": "提升的多语言依存句法分析模型在零样本语言上的解析性能",
    "source_paper_ids": [
      "ARR_2022_128"
    ],
    "pattern_ids": [
      "pattern_20"
    ]
  },
  {
    "idea_id": "idea_165",
    "description": "提出并构建了专为教育领域设计的高质量叙事理解数据集FairytaleQA，并开发了三步式自动问答生成系统。",
    "tech_stack": [
      "自动问答生成（QAG）",
      "SOTA语言模型",
      "启发式答案抽取",
      "教育学框架",
      "专家标注数据集"
    ],
    "input_type": "经典童话故事文本片段",
    "output_type": "高质量、教育导向的问答对（QA-pairs）",
    "source_paper_ids": [
      "ARR_2022_129"
    ],
    "pattern_ids": [
      "pattern_15"
    ]
  },
  {
    "idea_id": "idea_166",
    "description": "提出了MultiSpanQA多跨度阅读理解数据集及一种结合序列标注与结构预测的新模型，实现多跨度答案抽取。",
    "tech_stack": [
      "多跨度阅读理解数据集",
      "序列标注",
      "跨度数量预测",
      "跨度结构预测",
      "跨度调整模块",
      "人工语义标注",
      "新评测指标"
    ],
    "input_type": "包含问题和长文本段落的问答对",
    "output_type": "包含多个不重叠答案跨度及其逻辑结构的预测结果",
    "source_paper_ids": [
      "ARR_2022_130"
    ],
    "pattern_ids": [
      "pattern_33"
    ]
  },
  {
    "idea_id": "idea_167",
    "description": "提出了同步多模态融合模块，实现视觉和语言模态的同步交互以提升指代图像分割性能。",
    "tech_stack": [
      "Synchronous Multi-Modal Fusion Module (SFM)",
      "Hierarchical Cross-Modal Aggregation Module (HCAM)",
      "CNN",
      "LSTM"
    ],
    "input_type": "一张图像和对应的自然语言指代表达",
    "output_type": "与指代表达对应的像素级分割掩码",
    "source_paper_ids": [
      "ARR_2022_131"
    ],
    "pattern_ids": [
      "pattern_22"
    ]
  },
  {
    "idea_id": "idea_168",
    "description": "提出SimpDefiner多任务框架，实现无监督生成易懂词语定义以辅助语言学习者。",
    "tech_stack": [
      "多任务学习",
      "定义生成",
      "文本重构",
      "语言建模",
      "参数共享"
    ],
    "input_type": "词语及其上下文信息",
    "output_type": "简明易懂的词语定义",
    "source_paper_ids": [
      "ARR_2022_132"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_169",
    "description": "提出FEWVLM中等规模视觉-语言模型，通过提示学习实现低资源零/少样本任务。",
    "tech_stack": [
      "序列到序列Transformer",
      "Prefix Language Modeling",
      "Masked Language Modeling",
      "Encoder-Decoder架构",
      "Faster R-CNN",
      "Prompt-based Learning"
    ],
    "input_type": "图像和文本输入，用于视觉问答、图像描述等任务",
    "output_type": "生成目标文本，如答案或描述",
    "source_paper_ids": [
      "ARR_2022_133"
    ],
    "pattern_ids": [
      "pattern_22"
    ]
  },
  {
    "idea_id": "idea_170",
    "description": "提出自动化框架大规模识别NLP模型中的伪相关（shortcuts）而非依赖预定义模式。",
    "tech_stack": [
      "模型可解释性方法",
      "注意力分数",
      "集成梯度",
      "跨数据集分析",
      "知识感知扰动"
    ],
    "input_type": "已训练的NLP模型及其任务相关数据集",
    "output_type": "被模型利用的重要token及其“genuine”或“spurious”分类",
    "source_paper_ids": [
      "ARR_2022_134"
    ],
    "pattern_ids": [
      "pattern_19"
    ]
  },
  {
    "idea_id": "idea_171",
    "description": "首次系统性比较多种熵估计器在自然语言数据上的表现，推荐更适合语言分布的估计方法。",
    "tech_stack": [
      "信息论",
      "熵估计",
      "统计分析",
      "自然语言处理",
      "均方误差评估"
    ],
    "input_type": "合成数据和自然语言分布数据（如unigram分布）",
    "output_type": "不同熵估计器在各类数据上的性能评估结果（如均方误差）及方法推荐",
    "source_paper_ids": [
      "ARR_2022_135"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_172",
    "description": "首次提出了面向中文生物医学语言理解的综合性评测基准CBLUE，并系统评估了多种中文预训练模型。",
    "tech_stack": [
      "生物医学自然语言处理（BioNLP）",
      "评测基准（Benchmark）",
      "预训练语言模型",
      "命名实体识别",
      "信息抽取",
      "短文本分类",
      "问答系统",
      "语义相似度计算"
    ],
    "input_type": "多种中文生物医学文本任务的数据，包括实体识别、信息抽取、诊断归一化、分类、问答等文本输入",
    "output_type": "各项任务的模型性能评估结果与基准分数",
    "source_paper_ids": [
      "ARR_2022_136"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_173",
    "description": "提出利用高阶ODE方法替代传统残差网络中的一阶欧拉方法，以提升深层神经网络的数值稳定性和参数效率。",
    "tech_stack": [
      "残差网络（Residual Networks）",
      "常微分方程（ODE）",
      "高阶数值方法",
      "Transformer",
      "欧拉方法",
      "参数共享"
    ],
    "input_type": "自然语言处理任务中的序列数据，如机器翻译或语言建模输入",
    "output_type": "改进的神经网络模型输出，如翻译文本或预测的语言序列",
    "source_paper_ids": [
      "ARR_2022_137"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_174",
    "description": "提出将信息抽取任务形式化为文本到表格的序列到序列生成问题，并实现自动化表格结构抽取。",
    "tech_stack": [
      "序列到序列模型（seq2seq）",
      "预训练语言模型",
      "表格约束（Table Constraint, TC）",
      "表格关系嵌入（Table Relation Embeddings, TRE）"
    ],
    "input_type": "包含文本与对应表格对的训练数据，以及待抽取信息的长文本输入",
    "output_type": "结构化表格序列，自动生成包含多表的抽取结果",
    "source_paper_ids": [
      "ARR_2022_138"
    ],
    "pattern_ids": [
      "pattern_2"
    ]
  },
  {
    "idea_id": "idea_175",
    "description": "将知识图谱补全和问答统一建模为序列到序列任务，并用Transformer实现高效、可扩展的端到端方法。",
    "tech_stack": [
      "序列到序列建模（seq2seq）",
      "Transformer",
      "T5-small",
      "知识图谱嵌入（KGE）",
      "文本化实体与关系",
      "多任务学习"
    ],
    "input_type": "知识图谱中的三元组（实体、关系）和自然语言问句的文本表示",
    "output_type": "预测的知识图谱三元组（如缺失实体或关系）或问题答案的文本",
    "source_paper_ids": [
      "ARR_2022_139"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_176",
    "description": "提出一种新的CAKE框架，提升知识图谱补全的负采样质量和推理准确性。",
    "tech_stack": [
      "知识图谱嵌入(KGE)",
      "负采样(negative sampling)",
      "逻辑规则学习",
      "路径推理",
      "链接预测"
    ],
    "input_type": "知识图谱中的实体、关系和三元组数据",
    "output_type": "预测缺失实体或关系的高质量三元组",
    "source_paper_ids": [
      "ARR_2022_140"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_177",
    "description": "提出基于句子级方面匹配和聚合的科学文献相似性新模型，利用共引句作为监督信号。",
    "tech_stack": [
      "多向量表示",
      "上下文句子嵌入",
      "共引句监督",
      "多实例学习",
      "最小L2距离",
      "最优传输（Earth Mover's Distance）"
    ],
    "input_type": "科学论文摘要的句子级文本及共引句信息",
    "output_type": "文献间的整体相似性分数及可选的方面条件检索结果",
    "source_paper_ids": [
      "ARR_2022_141"
    ],
    "pattern_ids": [
      "pattern_10"
    ]
  },
  {
    "idea_id": "idea_178",
    "description": "提出并构建了QuALITY长文本多项选择问答数据集，促进长文档理解模型的评估与发展。",
    "tech_stack": [
      "Longformer",
      "Longformer Encoder-Decoder (LED)",
      "检索方法",
      "ROUGE-1",
      "余弦相似度"
    ],
    "input_type": "2k–8k英文长文档及相关问答问题",
    "output_type": "多项选择题的答案选项",
    "source_paper_ids": [
      "ARR_2022_142"
    ],
    "pattern_ids": [
      "pattern_30"
    ]
  },
  {
    "idea_id": "idea_179",
    "description": "提出基于聚类信号的特征压缩与剪枝方法，优化kNN-MT中的datastore以提升检索效率和语义分布质量。",
    "tech_stack": [
      "kNN-MT",
      "datastore pruning",
      "feature compression",
      "cluster-based pruning",
      "translation probability pruning",
      "semantic clustering"
    ],
    "input_type": "神经机器翻译模型生成的上下文特征及目标语料库",
    "output_type": "经过剪枝和压缩优化的datastore及改进的翻译结果",
    "source_paper_ids": [
      "ARR_2022_143"
    ],
    "pattern_ids": [
      "pattern_10"
    ]
  },
  {
    "idea_id": "idea_180",
    "description": "提出并评估了基于道德框架生成针对特定受众的论证文本的方法。",
    "tech_stack": [
      "BERT模型",
      "道德基础理论",
      "远程监督",
      "Project Debater",
      "词典方法"
    ],
    "input_type": "包含争议话题、立场和道德集合的文本输入",
    "output_type": "针对指定道德框架和立场生成的高质量论证文本",
    "source_paper_ids": [
      "ARR_2022_144"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_181",
    "description": "提出Flooding-X方法，无需生成对抗样本即可显著提升BERT模型的对抗鲁棒性。",
    "tech_stack": [
      "Flooding正则化",
      "梯度一致性分析",
      "BERT微调",
      "对抗训练"
    ],
    "input_type": "自然语言处理任务中的文本数据，如问答或自然语言推断数据集",
    "output_type": "提升对抗鲁棒性的深度模型性能指标（如准确率）",
    "source_paper_ids": [
      "ARR_2022_145"
    ],
    "pattern_ids": [
      "pattern_19"
    ]
  },
  {
    "idea_id": "idea_182",
    "description": "提出了结合领域知识与常识知识的反思性心理咨询回复生成方法，并通过知识增强的生成模型提升回复质量。",
    "tech_stack": [
      "BERT-based检索",
      "COMET知识生成",
      "BART生成模型",
      "软位置编码",
      "掩码自注意力",
      "Rouge/METEOR/BLEU/BertScore评测"
    ],
    "input_type": "包含心理咨询对话上下文的文本及相关知识库（常识与领域知识）",
    "output_type": "知识增强的心理咨询反思性回复文本",
    "source_paper_ids": [
      "ARR_2022_147"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_183",
    "description": "提出通过相似用户数据插值构建个性化语言模型，提升少量用户数据下的预测效果。",
    "tech_stack": [
      "个性化语言模型",
      "用户相似性计算",
      "插值模型",
      "Bidirectional LSTM",
      "用户嵌入",
      "模型微调",
      "Adam优化器",
      "交叉熵损失"
    ],
    "input_type": "新用户的少量文本数据及大规模语料库中其他用户的文本数据",
    "output_type": "针对新用户的个性化语言模型输出（如下一个词的概率分布）",
    "source_paper_ids": [
      "ARR_2022_148"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_184",
    "description": "提出了自适应专家混合框架（SaMoE），针对表格事实核查任务自适应分配不同类型推理子任务。",
    "tech_stack": [
      "Mixture of Experts",
      "特征提取器",
      "管理模块",
      "表格语义建模",
      "自适应专家分配"
    ],
    "input_type": "表格与陈述（table-statement pair）作为输入，需进行事实核查",
    "output_type": "陈述与表格之间一致性（如支持、反对、中立）的分类结果",
    "source_paper_ids": [
      "ARR_2022_150"
    ],
    "pattern_ids": [
      "pattern_6"
    ]
  },
  {
    "idea_id": "idea_185",
    "description": "提出MoDIR方法，通过对抗性域不变表示学习提升零样本稠密检索模型的泛化能力。",
    "tech_stack": [
      "Dense Retrieval",
      "Dual-Encoder",
      "Adversarial Domain Adaptation",
      "Momentum Encoder",
      "Pre-trained Language Models",
      "t-SNE 可视化"
    ],
    "input_type": "查询和文档对（query-document pairs），包括有标签的源域数据和无标签的目标域数据",
    "output_type": "查询和文档的稠密向量表示及其相关性评分",
    "source_paper_ids": [
      "ARR_2022_151"
    ],
    "pattern_ids": [
      "pattern_10"
    ]
  },
  {
    "idea_id": "idea_186",
    "description": "提出了基于单一预训练模型的Picker-Generator联合训练方法，实现对对话中不完整发言的高效恢复。",
    "tech_stack": [
      "预训练生成模型",
      "联合训练",
      "Picker-Generator架构",
      "启发式重要词构建"
    ],
    "input_type": "包含上下文和不完整发言的多轮对话数据",
    "output_type": "补全后的完整发言文本",
    "source_paper_ids": [
      "ARR_2022_152"
    ],
    "pattern_ids": [
      "pattern_25"
    ]
  },
  {
    "idea_id": "idea_187",
    "description": "提出MCSE，将视觉信息引入多模态对比学习以提升句子嵌入的语义表示能力。",
    "tech_stack": [
      "多模态对比学习",
      "SimCSE",
      "预训练语言模型",
      "视觉-文本联合嵌入",
      "语义文本相似性评估"
    ],
    "input_type": "文本和对应图片的多模态数据",
    "output_type": "高质量的句子嵌入向量，用于语义相似性任务",
    "source_paper_ids": [
      "ARR_2022_154"
    ],
    "pattern_ids": [
      "pattern_10"
    ]
  },
  {
    "idea_id": "idea_188",
    "description": "提出并公开了一个用于多模态社交媒体声明检测的新数据集MM-Claims，涵盖文本与视觉内容。",
    "tech_stack": [
      "多模态数据处理",
      "Transformer模型",
      "语法与语义特征分析",
      "数据集构建"
    ],
    "input_type": "包含文本、图像等多模态社交媒体推文数据",
    "output_type": "推文声明类型分类（无声明、声明但不需核查、需核查声明、需核查且视觉相关声明）",
    "source_paper_ids": [
      "ARR_2022_155"
    ],
    "pattern_ids": [
      "pattern_6"
    ]
  },
  {
    "idea_id": "idea_189",
    "description": "提出结合顺序预测和语法监督的KG-to-text生成方法，提升文本流畅性和语义一致性。",
    "tech_stack": [
      "预训练语言模型",
      "KG顺序预测网络",
      "词性生成器",
      "语义上下文评分",
      "序列到序列生成"
    ],
    "input_type": "结构化知识图谱数据",
    "output_type": "流畅且语义相关的自然语言描述文本",
    "source_paper_ids": [
      "ARR_2022_156"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_190",
    "description": "提出了一种基于知识图谱三元组和共指消解的对话语义理解与生成方法。",
    "tech_stack": [
      "知识图谱三元组",
      "共指消解",
      "GRU神经网络",
      "对话结构建模"
    ],
    "input_type": "包含对话历史和实体信息的文本数据",
    "output_type": "结构化的实体关系三元组及生成的对话响应",
    "source_paper_ids": [
      "ARR_2022_157"
    ],
    "pattern_ids": [
      "pattern_25"
    ]
  },
  {
    "idea_id": "idea_191",
    "description": "本论文系统比较多种NLP任务特征对fMRI脑区活动预测的有效性，揭示不同语言特征与大脑区域的关联。",
    "tech_stack": [
      "Transformer模型",
      "BERT",
      "词嵌入",
      "Ridge回归",
      "fMRI脑编码",
      "K折交叉验证",
      "一元方差分析（ANOVA）"
    ],
    "input_type": "句子或故事等语言刺激及其多种NLP任务特征表示",
    "output_type": "基于不同NLP特征预测的fMRI脑区活动响应",
    "source_paper_ids": [
      "ARR_2022_158"
    ],
    "pattern_ids": [
      "pattern_23"
    ]
  },
  {
    "idea_id": "idea_192",
    "description": "提出DIITO方法，通过因果抽象和交换干预训练提升蒸馏学生模型对教师模型因果动态的对齐。",
    "tech_stack": [
      "模型蒸馏",
      "因果抽象",
      "交换干预训练（IIT）",
      "BERT",
      "MLM预训练"
    ],
    "input_type": "大规模文本语料和预训练语言模型的内部表示",
    "output_type": "学生模型在下游任务上的输出分布（如分类logits）及其与教师模型因果动态的对齐",
    "source_paper_ids": [
      "ARR_2022_159"
    ],
    "pattern_ids": [
      "pattern_11"
    ]
  },
  {
    "idea_id": "idea_193",
    "description": "提出一种利用属性相关的连续前缀向量联合训练以实现高效可控自然语言生成的新方法。",
    "tech_stack": [
      "前缀调优（Prefix-tuning）",
      "GPT2",
      "属性相关性建模",
      "监督学习",
      "无监督学习"
    ],
    "input_type": "带有特定属性标签的文本数据或需控制生成属性的文本生成任务",
    "output_type": "具有目标属性（如情感、主题等）控制的自然语言文本",
    "source_paper_ids": [
      "ARR_2022_15"
    ],
    "pattern_ids": [
      "pattern_21"
    ]
  },
  {
    "idea_id": "idea_194",
    "description": "提出并验证了利用自然语言任务说明提升预训练语言模型对未见任务泛化能力的方法。",
    "tech_stack": [
      "预训练语言模型",
      "BART",
      "GPT-3",
      "多任务训练",
      "自然语言任务说明编码",
      "指令学习"
    ],
    "input_type": "自然语言任务说明与输入实例的文本对",
    "output_type": "根据指令生成的任务输出文本",
    "source_paper_ids": [
      "ARR_2022_160"
    ],
    "pattern_ids": [
      "pattern_23"
    ]
  },
  {
    "idea_id": "idea_195",
    "description": "将构式语法中的论元结构构式（ASC）心理语言学实验方法应用于Transformer语言模型，探究其对构式意义的神经表征。",
    "tech_stack": [
      "Transformer语言模型",
      "BERT",
      "RoBERTa",
      "mBERT",
      "句子嵌入",
      "心理语言学实验设计",
      "模板生成"
    ],
    "input_type": "多语言的句子模板，包含不同动词和构式的句子集合",
    "output_type": "语言模型对句子语义相似性的评分或嵌入距离，用于分析对构式意义的敏感性",
    "source_paper_ids": [
      "ARR_2022_161"
    ],
    "pattern_ids": [
      "pattern_3"
    ]
  },
  {
    "idea_id": "idea_196",
    "description": "首次系统性揭示并分析了密集段落检索中对比学习框架存在的对比冲突问题。",
    "tech_stack": [
      "对比学习",
      "Bi-Encoder结构",
      "预训练语言模型",
      "稠密检索",
      "负样本采样"
    ],
    "input_type": "自然语言问题与大规模文本语料库（段落集合）",
    "output_type": "与输入问题最相关的段落集合",
    "source_paper_ids": [
      "ARR_2022_163"
    ],
    "pattern_ids": [
      "pattern_10"
    ]
  },
  {
    "idea_id": "idea_197",
    "description": "评估BERT在处理荷兰语跨序列依赖等超越上下文无关语法结构上的能力。",
    "tech_stack": [
      "BERT",
      "上下文化表示",
      "注意力机制",
      "人工生成数据集",
      "跨注意力矩阵",
      "掩码聚合",
      "降维投影"
    ],
    "input_type": "带有动词和名词短语标注的荷兰语句子数据集，包含跨序列依赖结构",
    "output_type": "动词与名词短语之间的跨注意力矩阵及其关联映射",
    "source_paper_ids": [
      "ARR_2022_164"
    ],
    "pattern_ids": [
      "pattern_3"
    ]
  },
  {
    "idea_id": "idea_198",
    "description": "提出无需微调语言模型、基于判别器和蒙特卡洛树搜索实现约束文本生成的新方法。",
    "tech_stack": [
      "判别器（Discriminator）",
      "蒙特卡洛树搜索（MCTS）",
      "重排序（Re-ranking）",
      "大型语言模型（LM）",
      "HuggingFace Transformers"
    ],
    "input_type": "带有特定约束条件的文本生成任务输入（如写作风格、情感、事实性等）",
    "output_type": "满足指定约束条件的生成文本",
    "source_paper_ids": [
      "ARR_2022_165"
    ],
    "pattern_ids": [
      "pattern_21"
    ]
  },
  {
    "idea_id": "idea_199",
    "description": "将化学专利的指代消解注释框架泛化到食谱程序文本，构建数据集并探索跨领域迁移学习。",
    "tech_stack": [
      "指代消解",
      "桥接消解",
      "迁移学习",
      "BiLSTM",
      "注意力机制",
      "端到端神经指代模型",
      "特征向量拼接",
      "分类任务"
    ],
    "input_type": "包含丰富指代现象的程序性文本（如食谱、化学专利）",
    "output_type": "文本中实体指代关系的识别与分类结果",
    "source_paper_ids": [
      "ARR_2022_166"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_200",
    "description": "提出基于问答任务的新型预训练损失QUIP，以提升上下文相关的token级表示能力。",
    "tech_stack": [
      "问答预训练（QA-infused pre-training）",
      "bi-encoder模型",
      "cross-encoder模型",
      "知识蒸馏",
      "自动问答生成",
      "自训练"
    ],
    "input_type": "包含短语和相关上下文的文本片段及自动生成的问题对",
    "output_type": "改进的token级上下文表示，用于多种零样本和小样本任务",
    "source_paper_ids": [
      "ARR_2022_167"
    ],
    "pattern_ids": [
      "pattern_23"
    ]
  },
  {
    "idea_id": "idea_201",
    "description": "提出了基于跨模态对比学习的端到端语音翻译方法ConST，有效对齐语音与文本表征。",
    "tech_stack": [
      "跨模态对比学习",
      "Transformer",
      "Wav2vec2.0",
      "多任务学习"
    ],
    "input_type": "语音信号及其文本转录",
    "output_type": "目标语言的文本翻译",
    "source_paper_ids": [
      "ARR_2022_168"
    ],
    "pattern_ids": [
      "pattern_9"
    ]
  },
  {
    "idea_id": "idea_202",
    "description": "提出利用目标语言无标签数据优化跨语言事件检测模型以提升语言不变性。",
    "tech_stack": [
      "BERT-CRF",
      "mBERT",
      "Conditional Random Field (CRF)",
      "对抗训练",
      "迁移学习",
      "无监督学习"
    ],
    "input_type": "多语言文本数据，包含有标签的源语言数据和无标签的目标语言数据",
    "output_type": "句子中事件触发词的识别及其事件类型分类",
    "source_paper_ids": [
      "ARR_2022_169"
    ],
    "pattern_ids": [
      "pattern_20"
    ]
  },
  {
    "idea_id": "idea_203",
    "description": "提出友好型对抗性数据增强（FADA）以提升文本分类模型在对抗攻击下的鲁棒性和准确率。",
    "tech_stack": [
      "对抗训练",
      "文本对抗攻击",
      "友好型对抗性数据增强（FADA）",
      "TextFooler",
      "TextBugger",
      "BAE",
      "RoBERTa",
      "DeBERTa"
    ],
    "input_type": "自然语言文本数据（如句子或评论），用于文本分类任务，并包含对抗性扰动。",
    "output_type": "模型在干净和对抗性测试集上的分类准确率和鲁棒性评估结果",
    "source_paper_ids": [
      "ARR_2022_16"
    ],
    "pattern_ids": [
      "pattern_19"
    ]
  },
  {
    "idea_id": "idea_204",
    "description": "提出可控长度的自解释模型，系统研究不同长度的文本解释对人类理解的影响。",
    "tech_stack": [
      "自解释模型",
      "稀疏性控制",
      "上下文感知",
      "连续文本抽取",
      "文本分类",
      "人类实验",
      "ERASER数据集"
    ],
    "input_type": "文本分类任务中的文档输入（句子或词元）",
    "output_type": "不同长度的文本解释（rationale）及其对应的分类预测结果",
    "source_paper_ids": [
      "ARR_2022_170"
    ],
    "pattern_ids": [
      "pattern_18"
    ]
  },
  {
    "idea_id": "idea_205",
    "description": "提出AdapterBias，通过为每个输入token添加可学习的、token相关的偏置，实现更高效的参数微调。",
    "tech_stack": [
      "Adapter模块",
      "Transformer架构",
      "Token-dependent bias",
      "参数冻结",
      "线性层",
      "GLUE基准评测"
    ],
    "input_type": "下游任务的训练数据，包括文本输入和标签",
    "output_type": "针对下游任务优化后的预训练语言模型输出结果（如分类分数或预测标签）",
    "source_paper_ids": [
      "ARR_2022_171"
    ],
    "pattern_ids": [
      "pattern_5"
    ]
  },
  {
    "idea_id": "idea_206",
    "description": "通过分析注意力机制揭示并检测NLP模型中的Trojan攻击机制及异常行为。",
    "tech_stack": [
      "注意力机制分析",
      "BERT模型",
      "注意力漂移检测",
      "剪枝技术",
      "Trojan检测算法"
    ],
    "input_type": "含有潜在Trojan触发器的文本数据和训练好的语言模型",
    "output_type": "模型是否被Trojan攻击的判定及触发器相关异常行为分析",
    "source_paper_ids": [
      "ARR_2022_172"
    ],
    "pattern_ids": [
      "pattern_19"
    ]
  },
  {
    "idea_id": "idea_207",
    "description": "提出了一种结合目标上下文信息的条件双语互信息（CBMI）指标用于自适应神经机器翻译训练。",
    "tech_stack": [
      "神经机器翻译（NMT）",
      "条件双语互信息（CBMI）",
      "自适应训练",
      "损失重加权",
      "互信息计算"
    ],
    "input_type": "源语言句子与目标语言句子对",
    "output_type": "加权优化后的NMT模型及更准确的目标语言翻译",
    "source_paper_ids": [
      "ARR_2022_173"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_208",
    "description": "提出了基于人类评价数据训练的、无参考的视觉故事生成自动评价指标Vrank。",
    "tech_stack": [
      "SIMCSE",
      "VHED数据集",
      "无参考评价",
      "视觉故事生成",
      "深度学习排序模型"
    ],
    "input_type": "视觉故事生成模型输出的故事文本对及其相关图像",
    "output_type": "对视觉故事文本的自动化质量排序或评分",
    "source_paper_ids": [
      "ARR_2022_174"
    ],
    "pattern_ids": [
      "pattern_13"
    ]
  },
  {
    "idea_id": "idea_209",
    "description": "提出SpellingBee探针，揭示预训练语言模型的词嵌入包含丰富的拼写信息，并探索利用拼写预训练嵌入层的效果。",
    "tech_stack": [
      "SpellingBee probe",
      "预训练语言模型",
      "子词分词算法",
      "嵌入层预训练",
      "生成模型",
      "chrF评估指标"
    ],
    "input_type": "预训练语言模型的词嵌入向量（未上下文化）",
    "output_type": "对应词嵌入的字符组成（拼写）或拼写准确率指标",
    "source_paper_ids": [
      "ARR_2022_175"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_210",
    "description": "通过引入人工标注的文本合理性信息，提高小规模标注数据下的文本分类性能。",
    "tech_stack": [
      "文本分类",
      "合理性标注",
      "多任务学习",
      "注意力机制",
      "逻辑回归",
      "支持向量机",
      "深度学习"
    ],
    "input_type": "带有标签和合理性标注的文本数据",
    "output_type": "文本分类标签及模型性能提升",
    "source_paper_ids": [
      "ARR_2022_176"
    ],
    "pattern_ids": [
      "pattern_16"
    ]
  },
  {
    "idea_id": "idea_211",
    "description": "提出了一种无需词级标注数据、基于模型解释性（rationale extraction）的机器翻译质量估计新方法。",
    "tech_stack": [
      "Quality Estimation (QE)",
      "Rationale Extraction",
      "Feature Attribution",
      "Pre-trained Multilingual Transformers (如BERT, XLM-R)",
      "Post hoc Explanation Methods"
    ],
    "input_type": "源语言序列和目标语言序列（句对）",
    "output_type": "每个目标词的错误/正确二分类标签及句级翻译质量分数",
    "source_paper_ids": [
      "ARR_2022_177"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_212",
    "description": "构建并分析人类定义和模型定义的实例难度数据集，评估神经网络对实例难度的预测能力。",
    "tech_stack": [
      "预训练语言模型（PLM）",
      "多出口BERT",
      "内部分类器",
      "多标签分类",
      "难度估计",
      "早退出（early exiting）"
    ],
    "input_type": "句子级和标注级的文本分类任务数据，包括SNLI和OntoNotes NER数据集",
    "output_type": "实例的多标签难度预测结果（每层是否正确预测的标签）",
    "source_paper_ids": [
      "ARR_2022_178"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_213",
    "description": "提出Masked Label Smoothing方法，解决标签平滑与词汇共享在神经机器翻译中的冲突，提高翻译质量与模型校准。",
    "tech_stack": [
      "Transformer",
      "Label Smoothing",
      "Vocabulary Sharing",
      "Weighted Label Smoothing",
      "Masked Label Smoothing",
      "Model Calibration",
      "ECE Score",
      "BLEU Score"
    ],
    "input_type": "源语言与目标语言文本数据，用于神经机器翻译任务",
    "output_type": "翻译文本及其性能指标（如BLEU分数、模型校准分数）",
    "source_paper_ids": [
      "ARR_2022_179"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_214",
    "description": "提出并验证了SUPERB-SG基准，系统评估多种自监督语音模型在多任务和不同下游结构下的稳健性。",
    "tech_stack": [
      "自监督学习（SSL）",
      "迁移学习",
      "多任务学习",
      "向量量化",
      "Log Mel Filterbank",
      "标准化基准测试"
    ],
    "input_type": "未标注或标注的语音信号数据，涵盖多种语音任务",
    "output_type": "多种语音任务的评测结果与模型性能排名",
    "source_paper_ids": [
      "ARR_2022_17"
    ],
    "pattern_ids": [
      "pattern_9"
    ]
  },
  {
    "idea_id": "idea_215",
    "description": "提出了一种无需额外预训练即可将领域知识高效迁移到新预训练语言模型的知识蒸馏框架DoKTra。",
    "tech_stack": [
      "Transformer",
      "预训练语言模型（PLM）",
      "知识蒸馏",
      "激活边界蒸馏",
      "BERT",
      "BioBERT"
    ],
    "input_type": "已有领域预训练模型及新语言模型，配合领域文本数据",
    "output_type": "具备领域知识的新语言模型参数或模型",
    "source_paper_ids": [
      "ARR_2022_180"
    ],
    "pattern_ids": [
      "pattern_11"
    ]
  },
  {
    "idea_id": "idea_216",
    "description": "提出边界平滑正则化方法，提升span-based神经网络NER模型的性能和置信度校准。",
    "tech_stack": [
      "边界平滑（boundary smoothing）",
      "span-based模型",
      "Transformer预训练嵌入",
      "biaffine解码器",
      "标签平滑（label smoothing）"
    ],
    "input_type": "带有实体边界和类型标注的文本数据（如NER数据集）",
    "output_type": "文本中实体的边界和类型识别结果，并输出置信度分数",
    "source_paper_ids": [
      "ARR_2022_181"
    ],
    "pattern_ids": [
      "pattern_7"
    ]
  },
  {
    "idea_id": "idea_217",
    "description": "提出了一种基于双层优化的任务加权方法MetaWeighting，以提升多任务文本分类的泛化性能。",
    "tech_stack": [
      "多任务学习",
      "任务加权",
      "双层优化（bi-level optimization）",
      "元学习（learning-to-learn）"
    ],
    "input_type": "多任务文本分类问题的数据集（如评论情感分析、新闻主题分类）",
    "output_type": "各任务的最优加权模型及其在测试集上的分类性能",
    "source_paper_ids": [
      "ARR_2022_182"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_218",
    "description": "通过多任务辅助训练方法，减轻机器翻译质量估计中的部分输入偏差而无需大幅修改模型或增加标注数据。",
    "tech_stack": [
      "多任务学习",
      "辅助任务训练",
      "对抗训练",
      "去偏焦损失函数",
      "MonoTransQuest架构"
    ],
    "input_type": "机器翻译生成的译文及其对应的源语言句子",
    "output_type": "对译文质量的自动评分（如句子级质量分数）",
    "source_paper_ids": [
      "ARR_2022_183"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_219",
    "description": "提出以能力为中心的公平性方法，衡量和改进法律NLP模型在不同群体间的性能平等。",
    "tech_stack": [
      "自然语言处理（NLP）",
      "机器学习",
      "公平性评价指标",
      "能力为中心的公平性理论"
    ],
    "input_type": "法律文本数据及相关受保护属性信息",
    "output_type": "不同群体间性能平等性评估结果或改进后的模型输出",
    "source_paper_ids": [
      "ARR_2022_184"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_220",
    "description": "利用带有词对齐的释义句分析BERT等模型的上下文词表示一致性及其影响因素。",
    "tech_stack": [
      "BERT",
      "上下文嵌入",
      "释义句对齐",
      "Paraphrase Database"
    ],
    "input_type": "带有词对齐信息的释义句对（paraphrase pairs with word alignments）",
    "output_type": "关于词和短语表示一致性、多义性、拼写变化等的定量分析结果",
    "source_paper_ids": [
      "ARR_2022_185"
    ],
    "pattern_ids": [
      "pattern_10"
    ]
  },
  {
    "idea_id": "idea_221",
    "description": "提出基于专家指导启发式的高质量对抗性NER数据集，并探索有限数据学习提升模型泛化能力。",
    "tech_stack": [
      "BERT",
      "专家指导启发式对抗样本",
      "Token-Aware Virtual Adversarial Training (TAVAT)",
      "TextFlint文本对抗攻击",
      "Mixup",
      "Dropout"
    ],
    "input_type": "命名实体识别（NER）任务的文本数据及有限专家增强对抗样本",
    "output_type": "NER模型对实体类别的分类标签及其在对抗性和OOD数据集上的性能表现",
    "source_paper_ids": [
      "ARR_2022_186"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_222",
    "description": "提出并验证了一种基于神经网络的成对排序模型用于自动可读性评估及跨语种迁移。",
    "tech_stack": [
      "神经网络",
      "BERT",
      "Pairwise Ranking",
      "Pairwise Logistic Loss",
      "跨语种迁移学习"
    ],
    "input_type": "文档及其对应阅读等级的成对数据",
    "output_type": "文档间可读性排序概率及排序结果",
    "source_paper_ids": [
      "ARR_2022_187"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_223",
    "description": "通过训练历时字符嵌入，利用历史文本中的拼写变化自动追踪和建模语音变化。",
    "tech_stack": [
      "历时字符嵌入",
      "分布式表示",
      "拼写变化建模",
      "相似度度量",
      "语音变化模拟"
    ],
    "input_type": "历史时期的文本数据及其拼写变体",
    "output_type": "语音变化的时序轨迹和拼写变化的分布式表示",
    "source_paper_ids": [
      "ARR_2022_188"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_224",
    "description": "在极少语料条件下为加拿大三种原住民语言开发并验证可用的文本到语音合成系统。",
    "tech_stack": [
      "神经网络语音合成",
      "低资源TTS建模",
      "主观评价方法"
    ],
    "input_type": "配有文本转录的原住民语言音频数据（25分钟至3.5小时）",
    "output_type": "可被教师和学习者接受的原住民语言合成语音音频",
    "source_paper_ids": [
      "ARR_2022_189"
    ],
    "pattern_ids": [
      "pattern_9"
    ]
  },
  {
    "idea_id": "idea_225",
    "description": "提出了触发词显著性归因方法以量化事件类型的上下文依赖性，并据此提升事件检测模型的鲁棒性。",
    "tech_stack": [
      "事件检测",
      "特征归因方法",
      "触发词显著性归因",
      "上下文模式分析",
      "模型训练机制优化"
    ],
    "input_type": "带有事件类型标注的文本句子",
    "output_type": "每个句子的事件类型检测结果及其触发词/上下文依赖性分析",
    "source_paper_ids": [
      "ARR_2022_190"
    ],
    "pattern_ids": [
      "pattern_14"
    ]
  },
  {
    "idea_id": "idea_226",
    "description": "提出了SUMMN多阶段分段摘要框架，实现对超长对话和文档的高效抽象式摘要。",
    "tech_stack": [
      "多阶段摘要框架",
      "分段处理",
      "ROUGE贪婪匹配算法",
      "BART预训练模型",
      "分层自注意力机制"
    ],
    "input_type": "长文本对话或文档（可选带查询）",
    "output_type": "高质量、细粒度的文本摘要",
    "source_paper_ids": [
      "ARR_2022_191"
    ],
    "pattern_ids": [
      "pattern_27"
    ]
  },
  {
    "idea_id": "idea_227",
    "description": "提出了基于智能动态目录的文档导航系统，可按不同用户角色高亮相关内容并以问题引导理解。",
    "tech_stack": [
      "段落级分割",
      "主题聚类",
      "角色兴趣映射",
      "问题生成",
      "智能导航界面"
    ],
    "input_type": "金融或法律领域的长篇文档（如财务报表、合同等）",
    "output_type": "按用户角色高亮相关段落并生成问题导向的动态目录界面",
    "source_paper_ids": [
      "ARR_2022_192"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_228",
    "description": "提出NoisyTune方法，通过在微调前对预训练语言模型参数添加噪声，提升下游任务表现。",
    "tech_stack": [
      "预训练语言模型",
      "参数扰动",
      "矩阵级噪声注入",
      "微调技术",
      "RecAdam",
      "Mixout"
    ],
    "input_type": "下游NLP任务的有限标注数据和预训练语言模型参数",
    "output_type": "在下游NLP任务上的模型性能提升结果",
    "source_paper_ids": [
      "ARR_2022_194"
    ],
    "pattern_ids": [
      "pattern_5"
    ]
  },
  {
    "idea_id": "idea_229",
    "description": "提出并分析基于序列标注和编辑操作的高效语法纠错方法，兼顾准确性与推理速度。",
    "tech_stack": [
      "Transformer",
      "BERT",
      "序列到序列模型",
      "序列标注",
      "编辑操作",
      "神经机器翻译",
      "合成数据生成"
    ],
    "input_type": "包含语法错误的自然语言文本",
    "output_type": "语法纠正后的自然语言文本",
    "source_paper_ids": [
      "ARR_2022_195"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_230",
    "description": "提出两步自动化提示生成方法，将CLIP的零样本能力迁移到视觉-语言理解任务。",
    "tech_stack": [
      "CLIP",
      "自动化提示生成",
      "T5生成模型",
      "依存句法分析",
      "对比损失",
      "prompt engineering"
    ],
    "input_type": "图像与自然语言问题或句子",
    "output_type": "视觉问答或视觉蕴含任务的答案或关系判定",
    "source_paper_ids": [
      "ARR_2022_196"
    ],
    "pattern_ids": [
      "pattern_22"
    ]
  },
  {
    "idea_id": "idea_231",
    "description": "提出了一种基于神经回归模型的G2P方法，通过评估候选发音与正确发音的相似度选择最佳发音。",
    "tech_stack": [
      "神经网络",
      "回归模型",
      "正字法规则生成",
      "发音候选生成",
      "发音相似度评估"
    ],
    "input_type": "单词或短语的字母（字符）序列",
    "output_type": "与输入对应的最佳发音（音素序列）",
    "source_paper_ids": [
      "ARR_2022_197"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_232",
    "description": "提出了跨语言的时间表达式自动归约方法，将模糊的时间词映射为具体小时区间，并分析文化差异。",
    "tech_stack": [
      "语料库方法",
      "语言模型",
      "正则表达式匹配",
      "整数线性规划（ILP）",
      "分布估计",
      "跨语言迁移",
      "Google Translate"
    ],
    "input_type": "自然语言中的时间表达式及相关语料",
    "output_type": "每个时间表达式对应的具体起止小时区间",
    "source_paper_ids": [
      "ARR_2022_198"
    ],
    "pattern_ids": [
      "pattern_26"
    ]
  },
  {
    "idea_id": "idea_233",
    "description": "用带标注的单一对话示例替代服务schema描述，实现更高效和鲁棒的零/少样本迁移对话状态追踪。",
    "tech_stack": [
      "大语言模型（BERT、T5）",
      "schema-guided建模",
      "对话示例驱动方法",
      "对话状态追踪（DST）"
    ],
    "input_type": "带最终状态标注的单一对话示例或对话数据",
    "output_type": "对话状态追踪结果（如意图和槽位的填充状态）",
    "source_paper_ids": [
      "ARR_2022_199"
    ],
    "pattern_ids": [
      "pattern_25"
    ]
  },
  {
    "idea_id": "idea_234",
    "description": "提出Prix-LM，通过预训练语言模型统一表示和丰富多语言知识库结构化知识。",
    "tech_stack": [
      "预训练语言模型",
      "XLM-R",
      "知识图谱",
      "因果语言建模",
      "跨语言对齐",
      "特殊标记序列化"
    ],
    "input_type": "多语言知识库中的结构化三元组和跨语言实体链接",
    "output_type": "统一空间中的多语言知识表示和补全后的知识库内容",
    "source_paper_ids": [
      "ARR_2022_1"
    ],
    "pattern_ids": [
      "pattern_20"
    ]
  },
  {
    "idea_id": "idea_235",
    "description": "通过将语言模型预测的未来信息显式集成到单调注意力机制中，提升同步神经机器翻译的延迟-质量权衡。",
    "tech_stack": [
      "单调注意力机制",
      "语言模型（XLM-R, SLM）",
      "同步神经机器翻译（SNMT）",
      "Masked Language Modeling",
      "Causal Language Modeling",
      "MMA模型"
    ],
    "input_type": "源语言文本序列（如语音转写或视频字幕）和目标语言前缀序列",
    "output_type": "目标语言的同步翻译文本",
    "source_paper_ids": [
      "ARR_2022_200"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_236",
    "description": "提出了多层次相互促进机制，实现推理与句子级解释的深度融合与共同提升。",
    "tech_stack": [
      "Stepwise Integration Mechanism (SIM)",
      "Adversarial Fidelity Regularization (AFiRe)",
      "Transformer 解码器",
      "对抗训练"
    ],
    "input_type": "自然语言推理任务中的文本输入及相关解释数据",
    "output_type": "推理结果标签和具有人类可读性的句子级解释",
    "source_paper_ids": [
      "ARR_2022_201"
    ],
    "pattern_ids": [
      "pattern_18"
    ]
  },
  {
    "idea_id": "idea_237",
    "description": "提出了一种结合弱监督对比学习和原型聚类的方法，以更有效地利用事件共现信息学习事件表示。",
    "tech_stack": [
      "弱监督对比学习",
      "原型聚类",
      "掩码语言模型（MLM）",
      "事件共现关系建模"
    ],
    "input_type": "包含多个事件及其共现关系的文本数据",
    "output_type": "高质量的事件分布式表示向量",
    "source_paper_ids": [
      "ARR_2022_202"
    ],
    "pattern_ids": [
      "pattern_10"
    ]
  },
  {
    "idea_id": "idea_238",
    "description": "提出一种基于部分微调的BERT服务框架，实现多任务高效共享与灵活独立更新。",
    "tech_stack": [
      "BERT",
      "部分微调",
      "知识蒸馏",
      "多任务学习",
      "模型融合"
    ],
    "input_type": "多任务自然语言处理问题及相关文本数据",
    "output_type": "支持多任务的高效、可独立更新的BERT模型",
    "source_paper_ids": [
      "ARR_2022_203"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_239",
    "description": "提出COMMAQA基准，通过与预定义AI代理的自然语言交互，将复杂任务分解为可解子任务以提升推理能力。",
    "tech_stack": [
      "多智能体协作",
      "自然语言交互",
      "任务分解",
      "T5-Large",
      "UnifiedQA-Large",
      "RoBERTa-Large",
      "阅读理解",
      "事实检索"
    ],
    "input_type": "复杂自然语言推理任务及可调用的AI代理的输入样例",
    "output_type": "复杂任务的最终自然语言答案",
    "source_paper_ids": [
      "ARR_2022_204"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_240",
    "description": "提出了一个统一框架GCPG，将词汇和句法控制条件结合用于可控释义生成。",
    "tech_stack": [
      "可控释义生成（CPG）",
      "序列到序列模型",
      "预训练语言模型（PLM）",
      "编码器-解码器结构",
      "关键词拼接",
      "句法特征序列化"
    ],
    "input_type": "包含源句子及可选词汇和句法控制条件的文本序列",
    "output_type": "满足指定词汇和/或句法条件的释义句子",
    "source_paper_ids": [
      "ARR_2022_205"
    ],
    "pattern_ids": [
      "pattern_21"
    ]
  },
  {
    "idea_id": "idea_241",
    "description": "显式提取、编码并注入层次结构信息以提升单文档抽取式文本摘要性能。",
    "tech_stack": [
      "Transformer Language Model (TLM)",
      "BERT",
      "RoBERTa",
      "Longformer",
      "层次结构编码",
      "句子线性位置编码",
      "句子层次位置编码",
      "自注意力机制",
      "堆叠Transformer编码器"
    ],
    "input_type": "包含内部层次结构（如章节、段落、句子）的长文本或科学论文",
    "output_type": "每个句子的摘要包含置信度（二分类标签或分数）",
    "source_paper_ids": [
      "ARR_2022_206"
    ],
    "pattern_ids": [
      "pattern_27"
    ]
  },
  {
    "idea_id": "idea_242",
    "description": "提出了一种结合工作记忆和长期记忆、能适应听者差异的Pragmatic Rational Speaker模型。",
    "tech_stack": [
      "Rational Speech Act (RSA) 模型",
      "工作记忆与长期记忆机制",
      "强化学习",
      "多智能体通信",
      "语用推理"
    ],
    "input_type": "一对图片、目标图片指示和听者能力差异信息",
    "output_type": "针对目标图片和听者差异自适应生成的描述性文本",
    "source_paper_ids": [
      "ARR_2022_207"
    ],
    "pattern_ids": [
      "pattern_25"
    ]
  },
  {
    "idea_id": "idea_243",
    "description": "将分类损失与校准损失联合优化，实现深度学习模型的动态概率校准。",
    "tech_stack": [
      "深度神经网络",
      "交叉熵损失",
      "校准损失",
      "直方图分箱",
      "多任务学习",
      "KL散度"
    ],
    "input_type": "带有真实标签的多类别分类训练数据",
    "output_type": "经过校准的类别概率分布",
    "source_paper_ids": [
      "ARR_2022_208"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_244",
    "description": "提出高效选择和标注目标语言少量数据以优化跨语言少样本迁移学习效果的方法。",
    "tech_stack": [
      "跨语言迁移学习",
      "多语言预训练模型",
      "主动学习",
      "训练数据选择",
      "少样本学习"
    ],
    "input_type": "多语言未标注语料和部分任务相关标注数据",
    "output_type": "目标语言上的任务模型性能提升",
    "source_paper_ids": [
      "ARR_2022_20"
    ],
    "pattern_ids": [
      "pattern_20"
    ]
  },
  {
    "idea_id": "idea_245",
    "description": "提出了mix-GLT方法，无需教师模型即可直接从原始数据集训练非自回归Transformer，有效缓解多模态问题。",
    "tech_stack": [
      "非自回归Transformer (NAT)",
      "Glancing Transformer (GLAT)",
      "离散潜变量建模",
      "分而治之策略",
      "序列到序列建模"
    ],
    "input_type": "原始文本序列对（如机器翻译中的源语言和目标语言句子）",
    "output_type": "生成的目标文本序列（如翻译后的句子）",
    "source_paper_ids": [
      "ARR_2022_212"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_246",
    "description": "首次用因果推断系统性分析翻译语体与数据-模型-训练集方向对机器翻译性能的因果影响。",
    "tech_stack": [
      "因果推断",
      "机器翻译性能分析",
      "数据-模型对齐",
      "翻译语体分析"
    ],
    "input_type": "包含不同人类翻译方向和模型方向的机器翻译训练与测试数据",
    "output_type": "对各因果因素影响机器翻译性能的定量分析和因果效应评估",
    "source_paper_ids": [
      "ARR_2022_213"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_247",
    "description": "对中间层蒸馏（ILD）中层选择策略进行全面评估，提升BERT等预训练模型压缩效率与性能。",
    "tech_stack": [
      "知识蒸馏",
      "中间层蒸馏（ILD）",
      "数据增强",
      "对抗训练",
      "层组合",
      "注意力机制",
      "对比学习"
    ],
    "input_type": "需要压缩的预训练语言模型及其训练数据",
    "output_type": "压缩后的小型学生模型及其在NLU任务上的性能指标",
    "source_paper_ids": [
      "ARR_2022_214"
    ],
    "pattern_ids": [
      "pattern_11"
    ]
  },
  {
    "idea_id": "idea_248",
    "description": "提出了一种改进的Distinct多样性评估指标，通过期望独特词数替代原有缩放因子，提升对对话生成多样性的公平评价。",
    "tech_stack": [
      "Distinct多样性指标",
      "新Distinct指标",
      "相关性分析（Pearson, Spearman, Kendall）",
      "Transformer模型",
      "BERT分词器",
      "Adam优化器"
    ],
    "input_type": "对话生成模型生成的文本响应或自然语料库",
    "output_type": "文本多样性评估分数及与人工评价的相关性分析结果",
    "source_paper_ids": [
      "ARR_2022_216"
    ],
    "pattern_ids": [
      "pattern_13"
    ]
  },
  {
    "idea_id": "idea_249",
    "description": "将CLIP的视觉信息通过跨模态蒸馏方式迁移到预训练语言模型以增强其视觉语义理解能力。",
    "tech_stack": [
      "Transformer",
      "CLIP",
      "BERT",
      "跨模态蒸馏",
      "跨模态编码器",
      "Masked Language Modeling (MLM)",
      "注意力机制"
    ],
    "input_type": "文本数据（如wiki103语料），部分方法涉及视觉语义信息",
    "output_type": "增强视觉语义能力的语言模型输出（如改进的文本表示或下游NLU任务性能）",
    "source_paper_ids": [
      "ARR_2022_217"
    ],
    "pattern_ids": [
      "pattern_22"
    ]
  },
  {
    "idea_id": "idea_250",
    "description": "首次系统评估解释方法在跨领域场景下的忠实性与泛化能力。",
    "tech_stack": [
      "特征归因方法",
      "select-then-predict模型",
      "HardKuma",
      "FRESH",
      "BERT",
      "bi-LSTM"
    ],
    "input_type": "文本分类任务中的输入文本数据，包含跨领域数据集对。",
    "output_type": "模型预测结果及其对应的解释（如重要性标注、rationale mask）",
    "source_paper_ids": [
      "ARR_2022_218"
    ],
    "pattern_ids": [
      "pattern_18"
    ]
  },
  {
    "idea_id": "idea_251",
    "description": "提出PT-BERT，通过伪token语义空间增强对句子语义的对比学习，减少表层特征干扰。",
    "tech_stack": [
      "对比学习",
      "BERT",
      "伪token表示",
      "注意力机制",
      "连续与离散增强",
      "prompt learning"
    ],
    "input_type": "自然语言句子对或单句，用于句子嵌入学习",
    "output_type": "高质量、语义感知的句子嵌入向量",
    "source_paper_ids": [
      "ARR_2022_219"
    ],
    "pattern_ids": [
      "pattern_10"
    ]
  },
  {
    "idea_id": "idea_252",
    "description": "提出了基于句子优先框架的上下文化EDU表示方法用于话语依存分析。",
    "tech_stack": [
      "句子优先（Sent-First）解析框架",
      "上下文化表示",
      "序列标注",
      "依存树构建"
    ],
    "input_type": "包含多个Elementary Discourse Units（EDU）的文档文本",
    "output_type": "完整的话语依存结构树及EDU间关系标签",
    "source_paper_ids": [
      "ARR_2022_21"
    ],
    "pattern_ids": [
      "pattern_17"
    ]
  },
  {
    "idea_id": "idea_253",
    "description": "提出了一种结合GPT-2数据增强与BERT序列标注的攻击性文本片段检测新方法。",
    "tech_stack": [
      "GPT-2",
      "BERT",
      "序列标注",
      "数据增强",
      "REINFORCE算法"
    ],
    "input_type": "带有BIO标注的文本序列",
    "output_type": "每个词的攻击性片段BIO标签序列",
    "source_paper_ids": [
      "ARR_2022_220"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_254",
    "description": "提出并构建了用于层次化表格问答与文本生成的新数据集HiTab，解决复杂表格结构下的推理与实体对齐难题。",
    "tech_stack": [
      "层次化表格处理",
      "实体对齐",
      "表格问答（Table QA）",
      "自然语言生成（NLG）",
      "数据集构建"
    ],
    "input_type": "包含层次化表格及其相关自然语言描述的问题或任务",
    "output_type": "针对层次化表格的问答结果或生成的自然语言文本",
    "source_paper_ids": [
      "ARR_2022_221"
    ],
    "pattern_ids": [
      "pattern_2"
    ]
  },
  {
    "idea_id": "idea_255",
    "description": "提出并系统分析了跨任务和跨模型的软提示迁移方法以提升提示微调效率。",
    "tech_stack": [
      "预训练语言模型",
      "提示微调（Prompt Tuning）",
      "软提示（Soft Prompts）",
      "跨任务迁移",
      "跨模型迁移",
      "提示投影（Prompt Projection）"
    ],
    "input_type": "多种NLP任务的数据及已训练的软提示参数",
    "output_type": "在目标任务或目标模型上的高效软提示及其下游任务性能",
    "source_paper_ids": [
      "ARR_2022_223"
    ],
    "pattern_ids": [
      "pattern_5"
    ]
  },
  {
    "idea_id": "idea_256",
    "description": "通过机器学习方法验证东南亚语言中无标记协调表达的语序可由音系特征高效预测。",
    "tech_stack": [
      "决策树",
      "机器学习",
      "音系特征分析"
    ],
    "input_type": "无标记协调表达的音系特征数据",
    "output_type": "协调表达语序的预测结果",
    "source_paper_ids": [
      "ARR_2022_224"
    ],
    "pattern_ids": [
      "pattern_3"
    ]
  },
  {
    "idea_id": "idea_257",
    "description": "提出实例依赖的自动生成prompt方法，实现高效迁移学习并缓解领域差异。",
    "tech_stack": [
      "Transformer",
      "Prompt Tuning",
      "Prefix Tuning",
      "Adapter",
      "Compacter",
      "PHM Layer",
      "DNN Generator"
    ],
    "input_type": "下游自然语言处理任务的输入文本",
    "output_type": "针对输入实例生成的自适应prompt及下游任务预测结果",
    "source_paper_ids": [
      "ARR_2022_225"
    ],
    "pattern_ids": [
      "pattern_5"
    ]
  },
  {
    "idea_id": "idea_258",
    "description": "提出了一种两阶段训练方法，通过预训练和多对一微调提升多语种神经机器翻译系统在任意语言对间的翻译性能。",
    "tech_stack": [
      "多语种神经机器翻译（MNMT）",
      "两阶段训练",
      "多对多预训练",
      "多对一微调",
      "知识迁移"
    ],
    "input_type": "多语种平行语料数据，包括英为中心和非英为中心的多语言对翻译任务",
    "output_type": "支持任意语言对（XY方向）高质量翻译的多语种神经机器翻译系统",
    "source_paper_ids": [
      "ARR_2022_226"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_259",
    "description": "提出了基于自动问题重写的评估机制，使CQA系统的自动评估更贴近真实人机对话表现。",
    "tech_stack": [
      "BERT",
      "图神经网络",
      "历史注意力机制",
      "问题重写",
      "指代消解模型",
      "人类评测"
    ],
    "input_type": "对话历史、证据段落和用户提出的问题",
    "output_type": "模型生成的答案及其与人类评测的一致性",
    "source_paper_ids": [
      "ARR_2022_227"
    ],
    "pattern_ids": [
      "pattern_25"
    ]
  },
  {
    "idea_id": "idea_260",
    "description": "首次构建并发布了大规模印地语法律文档语料库，并提出其在保释预测任务中的应用方法。",
    "tech_stack": [
      "Doc2Vec",
      "SVM",
      "XgBoost",
      "IndicBERT",
      "Transformer",
      "文本摘要",
      "多任务学习"
    ],
    "input_type": "印地语法律文档文本，特别是用于保释相关案件的原始文档",
    "output_type": "对保释是否批准的自动预测结果",
    "source_paper_ids": [
      "ARR_2022_228"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_261",
    "description": "构建并公开了首个大规模、细粒度时间标注的历史英文文本语料库及NLP基准，用于训练和评估历史语言模型。",
    "tech_stack": [
      "Transformer架构",
      "预训练-微调范式",
      "时间感知语言模型",
      "OCR处理",
      "大规模文本语料构建"
    ],
    "input_type": "带有精确日期标注的大规模历史英文报纸文本数据",
    "output_type": "可用于预训练和评估的历史语言模型及相关NLP基准测试结果",
    "source_paper_ids": [
      "ARR_2022_229"
    ],
    "pattern_ids": [
      "pattern_26"
    ]
  },
  {
    "idea_id": "idea_262",
    "description": "提出基于超链接结构自动生成高质量问答相关性对的预训练方法以提升开放域问答检索效果。",
    "tech_stack": [
      "开放域问答",
      "超链接诱导预训练（HLP）",
      "预训练语言模型",
      "密集检索（Dense Retrieval）",
      "远程监督学习"
    ],
    "input_type": "自然语言查询与大规模文档语料库",
    "output_type": "与查询高度相关的文档或段落",
    "source_paper_ids": [
      "ARR_2022_22"
    ],
    "pattern_ids": [
      "pattern_10"
    ]
  },
  {
    "idea_id": "idea_263",
    "description": "系统性比较BoW、序列和图神经网络三类模型在文本分类任务中的表现，强调BoW模型的有效性。",
    "tech_stack": [
      "Bag of Words (BoW)",
      "Deep Averaging Networks (DAN)",
      "Simple Word Embedding Models (SWEM)",
      "fastText",
      "Graph Neural Networks (GNN)",
      "TextGCN",
      "HeteGCN",
      "TensorGCN",
      "HyperGAT",
      "Transformer",
      "BERT",
      "DistilBERT",
      "LSTM"
    ],
    "input_type": "文本单元（如文档、社交媒体帖子、新闻文章）及其对应的分类任务",
    "output_type": "文本对应的主题类别标签",
    "source_paper_ids": [
      "ARR_2022_230"
    ],
    "pattern_ids": [
      "pattern_32"
    ]
  },
  {
    "idea_id": "idea_264",
    "description": "提出基于元学习的知识蒸馏方法MetaDistil，使教师模型能动态适应学生模型并优化知识传递。",
    "tech_stack": [
      "知识蒸馏",
      "元学习",
      "双层优化",
      "学生感知蒸馏",
      "pilot update机制"
    ],
    "input_type": "大规模神经网络模型及其训练任务（如CV或NLP任务）",
    "output_type": "经过优化的学生模型，具备高效推理能力和较小模型规模",
    "source_paper_ids": [
      "ARR_2022_231"
    ],
    "pattern_ids": [
      "pattern_11"
    ]
  },
  {
    "idea_id": "idea_265",
    "description": "提出并系统研究了消除新闻报道框架偏见的中立多新闻摘要（NEUS）任务与方法。",
    "tech_stack": [
      "多文档摘要（MDS）",
      "BART",
      "PEGASUS",
      "极性度量",
      "多任务学习（MTL）"
    ],
    "input_type": "带有不同政治倾向的多篇新闻标题及其内容",
    "output_type": "消除框架偏见的中立新闻摘要",
    "source_paper_ids": [
      "ARR_2022_232"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_266",
    "description": "提出了首个可解释的知识密集型类比推理基准E-KAR，并评估神经模型的人类类比推理能力。",
    "tech_stack": [
      "人工神经网络",
      "预训练词嵌入",
      "上下文词嵌入",
      "结构映射",
      "知识注入"
    ],
    "input_type": "多项选择类比推理问题，包含查询词组和候选答案",
    "output_type": "最优类比答案及其自然语言解释",
    "source_paper_ids": [
      "ARR_2022_233"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_267",
    "description": "提出两种高效且计算开销低的Transformer不确定性估计方法，提升文本分类和命名实体识别中的误判检测能力。",
    "tech_stack": [
      "Transformer架构",
      "Monte Carlo Dropout",
      "Determinantal Point Process (DPP)",
      "Mahalanobis距离",
      "谱归一化（Spectral Normalization）"
    ],
    "input_type": "自然语言处理任务中的文本数据，如文本分类和命名实体识别数据",
    "output_type": "模型预测的不确定性分数，用于误分类检测等任务",
    "source_paper_ids": [
      "ARR_2022_234"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_268",
    "description": "首次系统性分析对比视觉语义对比预训练对自然语言表示的影响。",
    "tech_stack": [
      "对比学习",
      "视觉语义预训练",
      "Transformer",
      "GPT-2",
      "CLIP",
      "上下文词嵌入"
    ],
    "input_type": "自然语言文本（如单词和句子）",
    "output_type": "自然语言的语义表示（嵌入向量）及其分布特性",
    "source_paper_ids": [
      "ARR_2022_235"
    ],
    "pattern_ids": [
      "pattern_23"
    ]
  },
  {
    "idea_id": "idea_269",
    "description": "通过检测图像与文本的语义不一致，自动识别Twitter上的出境图像虚假信息。",
    "tech_stack": [
      "CLIP",
      "多模态融合",
      "元素乘积",
      "分类器",
      "对比学习"
    ],
    "input_type": "多模态Twitter推文（包含图像和文本）",
    "output_type": "推文是否为真实或出境虚假信息的分类标签",
    "source_paper_ids": [
      "ARR_2022_236"
    ],
    "pattern_ids": [
      "pattern_6"
    ]
  },
  {
    "idea_id": "idea_270",
    "description": "提出EmRel框架，通过嵌入式关系表示和三元组对齐方法提升多三元组关系抽取性能。",
    "tech_stack": [
      "嵌入式关系表示",
      "注意力机制融合模块",
      "Tucker分解对齐函数",
      "联合三元组建模"
    ],
    "input_type": "包含多个实体和关系的自然语言文本",
    "output_type": "结构化的<主体-关系-客体>三元组集合",
    "source_paper_ids": [
      "ARR_2022_237"
    ],
    "pattern_ids": [
      "pattern_28"
    ]
  },
  {
    "idea_id": "idea_271",
    "description": "首次将MAML与基于发音和音系特征的输入结合应用于低资源跨语言TTS任务。",
    "tech_stack": [
      "MAML（模型无关元学习）",
      "发音特征输入",
      "音系特征输入",
      "深度神经网络TTS模型"
    ],
    "input_type": "高资源和低资源语言的音素的发音与音系特征表示",
    "output_type": "对应语言的高质量语音波形",
    "source_paper_ids": [
      "ARR_2022_238"
    ],
    "pattern_ids": [
      "pattern_9"
    ]
  },
  {
    "idea_id": "idea_272",
    "description": "提出结合领域自适应元学习和对抗风格训练的新方法用于低资源文本风格迁移。",
    "tech_stack": [
      "领域自适应元学习（DAML）",
      "模型无关元学习（MAML）",
      "序列到序列预训练模型",
      "对抗训练",
      "风格判别器"
    ],
    "input_type": "不同领域的带风格标签的文本数据，目标领域数据稀缺",
    "output_type": "风格已迁移且内容保持的目标领域文本",
    "source_paper_ids": [
      "ARR_2022_239"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_273",
    "description": "提出了MedLAMA生物医学知识探测基准和基于对比学习的Contrastive-Probe方法以提升多token实体知识探测效果。",
    "tech_stack": [
      "预训练语言模型",
      "知识探测",
      "对比学习",
      "检索式方法",
      "多token实体处理"
    ],
    "input_type": "生物医学领域的知识三元组查询（cloze-style queries）",
    "output_type": "与查询相关的实体答案（多token形式）",
    "source_paper_ids": [
      "ARR_2022_23"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_274",
    "description": "提出一种结合可证明误差界剪枝与渐进模块嫁接的知识蒸馏框架，以降低预训练语言模型剪枝后的过拟合风险。",
    "tech_stack": [
      "知识蒸馏",
      "可证明误差界剪枝",
      "渐进模块嫁接",
      "Transformer",
      "预训练-微调范式"
    ],
    "input_type": "预训练语言模型及其下游任务数据",
    "output_type": "剪枝后具有较低过拟合风险且高效的下游任务模型",
    "source_paper_ids": [
      "ARR_2022_240"
    ],
    "pattern_ids": [
      "pattern_10"
    ]
  },
  {
    "idea_id": "idea_275",
    "description": "提出CogTaskonomy框架，通过认知数据分析构建NLP任务的认知驱动层级结构。",
    "tech_stack": [
      "认知表示分析（CRA）",
      "表征相似性分析（RSA）",
      "认知-神经映射（CNM）",
      "神经网络模型",
      "认知处理信号（如眼动、EEG、fMRI）"
    ],
    "input_type": "NLP模型的任务表示和认知处理数据（如眼动、EEG、fMRI）",
    "output_type": "NLP任务之间的认知驱动层级结构（任务分类/结构图）",
    "source_paper_ids": [
      "ARR_2022_242"
    ],
    "pattern_ids": [
      "pattern_23"
    ]
  },
  {
    "idea_id": "idea_276",
    "description": "本论文创新性地在文本风格迁移任务中引入并利用句法信息以提升内容保持效果。",
    "tech_stack": [
      "文本风格迁移",
      "GPT-2",
      "REINFORCE算法",
      "句法信息",
      "依存句法树",
      "监督学习",
      "无监督学习",
      "重构损失"
    ],
    "input_type": "带有特定风格标签的文本句子（可为成对或非成对）",
    "output_type": "在目标风格下内容保持的生成文本句子",
    "source_paper_ids": [
      "ARR_2022_243"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_277",
    "description": "提出了基于神经模块网络的显式推理结构用于视频语境下的对话任务。",
    "tech_stack": [
      "神经模块网络（NMN）",
      "Transformer",
      "序列到序列模型",
      "多头注意力机制"
    ],
    "input_type": "视频及多轮对话文本问题",
    "output_type": "基于视频和语境的多轮对话答案",
    "source_paper_ids": [
      "ARR_2022_244"
    ],
    "pattern_ids": [
      "pattern_22"
    ]
  },
  {
    "idea_id": "idea_278",
    "description": "提出并构建了多模态口语对话问答任务SCQA，并引入知识蒸馏方法实现语音与文本的跨模态学习。",
    "tech_stack": [
      "多模态学习",
      "知识蒸馏",
      "口语对话问答",
      "语音与文本表示"
    ],
    "input_type": "多轮口语对话的语音和文本数据",
    "output_type": "多模态下的对话问答结果（文本答案）",
    "source_paper_ids": [
      "ARR_2022_245"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_279",
    "description": "首次利用语言学分布（如Zipf定律）调整训练权重，以提升WSD任务中对稀有和零样本词义的泛化能力。",
    "tech_stack": [
      "深度神经网络",
      "BERT",
      "Z-reweighting策略",
      "Zipf定律",
      "多义性分布建模"
    ],
    "input_type": "带有上下文的多义词语料数据（如SemCor）",
    "output_type": "针对给定上下文的词义判别结果",
    "source_paper_ids": [
      "ARR_2022_246"
    ],
    "pattern_ids": [
      "pattern_1"
    ]
  },
  {
    "idea_id": "idea_280",
    "description": "提出了一种基于预训练语言模型的高效任务型对话NLU方法，提升低资源场景下的意图识别与槽位标注能力。",
    "tech_stack": [
      "预训练语言模型（PLM）",
      "SQuAD微调",
      "意图检测（Intent Detection）",
      "槽位标注（Slot Labeling）"
    ],
    "input_type": "用户对话语句及领域本体信息",
    "output_type": "结构化的意图标签和槽位值",
    "source_paper_ids": [
      "ARR_2022_247"
    ],
    "pattern_ids": [
      "pattern_25"
    ]
  },
  {
    "idea_id": "idea_281",
    "description": "本论文系统性比较并提升了语音NER任务中的pipeline与端到端方法，利用多种外部数据显著提升性能。",
    "tech_stack": [
      "端到端模型（E2E）",
      "pipeline系统",
      "自监督表示（SSR）",
      "ASR（自动语音识别）",
      "BERT",
      "字符级预测"
    ],
    "input_type": "带有实体标注的语音音频及其文本转录，或纯文本/纯音频等多种外部数据",
    "output_type": "文本序列中实体及其类别的识别结果",
    "source_paper_ids": [
      "ARR_2022_248"
    ],
    "pattern_ids": [
      "pattern_7"
    ]
  },
  {
    "idea_id": "idea_282",
    "description": "提出X-GEAR，通过语言无关模板实现零样本跨语言事件参数抽取。",
    "tech_stack": [
      "多语言预训练生成模型",
      "语言无关模板",
      "生成式结构化预测",
      "复制机制"
    ],
    "input_type": "包含事件触发词和辅助信息的文本段落",
    "output_type": "事件参数及其角色的结构化预测结果",
    "source_paper_ids": [
      "ARR_2022_249"
    ],
    "pattern_ids": [
      "pattern_20"
    ]
  },
  {
    "idea_id": "idea_283",
    "description": "通过合成语言预训练分析语言模型结构归纳偏置对自然语言任务迁移的影响。",
    "tech_stack": [
      "预训练语言模型",
      "合成语言设计",
      "结构归纳偏置测试（TILT）",
      "LSTM",
      "Transformer",
      "因果语言建模",
      "掩码语言建模",
      "句法任务迁移"
    ],
    "input_type": "合成语言和自然语言的文本数据",
    "output_type": "下游自然语言任务的性能评估（如词性标注、依存句法分析）",
    "source_paper_ids": [
      "ARR_2022_24"
    ],
    "pattern_ids": [
      "pattern_23"
    ]
  },
  {
    "idea_id": "idea_284",
    "description": "通过分析和利用预训练语言模型不同层对谓词及论元结构的编码特性，提升语义角色标注性能。",
    "tech_stack": [
      "预训练语言模型（PLM）",
      "BERT",
      "BiLSTM",
      "层权重加权",
      "语义角色标注（SRL）"
    ],
    "input_type": "自然语言句子，需进行谓词及论元结构分析",
    "output_type": "包含谓词及其论元角色的结构化标注结果",
    "source_paper_ids": [
      "ARR_2022_250"
    ],
    "pattern_ids": [
      "pattern_17"
    ]
  },
  {
    "idea_id": "idea_285",
    "description": "提出了一种与推理器解耦的可训练子图检索器，用于提升知识库问答的检索与推理效果。",
    "tech_stack": [
      "知识库问答（KBQA）",
      "子图检索",
      "双编码器（dual-encoder）",
      "嵌入方法",
      "神经网络推理"
    ],
    "input_type": "自然语言事实性问题及结构化知识库",
    "output_type": "对应问题的知识库实体或答案",
    "source_paper_ids": [
      "ARR_2022_251"
    ],
    "pattern_ids": [
      "pattern_10"
    ]
  },
  {
    "idea_id": "idea_286",
    "description": "提出以人类标注的rationales为核心的双重鲁棒性学习框架，提升少样本学习中模型对伪模式的抵抗力。",
    "tech_stack": [
      "人类标注rationales",
      "半事实数据生成",
      "同义词替换",
      "动态人机纠正",
      "上下文分解敏感性分析"
    ],
    "input_type": "带有文本内容的少量标注数据及人类标注的rationales",
    "output_type": "增强后的训练数据和鲁棒性提升的分类模型",
    "source_paper_ids": [
      "ARR_2022_252"
    ],
    "pattern_ids": [
      "pattern_19"
    ]
  },
  {
    "idea_id": "idea_287",
    "description": "通过分析CharacterBERT对真实词、伪词和随机字符n-gram的嵌入，提出并验证了信息轴与词具体性轴在语义空间中的正交性。",
    "tech_stack": [
      "CharacterBERT",
      "UMAP降维",
      "Markov模型",
      "分布式语义",
      "词具体性评分"
    ],
    "input_type": "真实词、伪词和随机生成的字符n-gram序列",
    "output_type": "字符n-gram在嵌入空间中的分布结构及信息轴和具体性轴的关系",
    "source_paper_ids": [
      "ARR_2022_253"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_288",
    "description": "提出利用AMR语义级扰动生成更真实对话不连贯样本，提升对话系统连贯性评估。",
    "tech_stack": [
      "Abstract Meaning Representation (AMR)",
      "RoBERTa-large",
      "对话负样本生成",
      "语义级扰动",
      "Adam优化器",
      "对比学习",
      "自动评价指标"
    ],
    "input_type": "人机或人类对话文本数据及其AMR表示",
    "output_type": "对话连贯性自动评估分数或二分类判定",
    "source_paper_ids": [
      "ARR_2022_254"
    ],
    "pattern_ids": [
      "pattern_25"
    ]
  },
  {
    "idea_id": "idea_289",
    "description": "提出应对多标签文档分类中类别不平衡和时间概念漂移的新方法。",
    "tech_stack": [
      "多标签分类",
      "类别不平衡处理",
      "时序数据分割",
      "概念漂移检测"
    ],
    "input_type": "包含大量标签的文档数据，具有类别不平衡和时间变化特性",
    "output_type": "为每个文档分配最相关的标签子集",
    "source_paper_ids": [
      "ARR_2022_255"
    ],
    "pattern_ids": [
      "pattern_8"
    ]
  },
  {
    "idea_id": "idea_290",
    "description": "提出了一种多任务多教师知识蒸馏模型，通过结合实体识别和相似性评估提升跨语言命名实体识别性能。",
    "tech_stack": [
      "多任务学习",
      "知识蒸馏",
      "mBERT",
      "实体识别",
      "相似性评估",
      "教师-学生模型"
    ],
    "input_type": "跨语言命名实体识别任务中的源语言标注数据和目标语言未标注数据",
    "output_type": "目标语言的命名实体识别标签和实体间相似性评估结果",
    "source_paper_ids": [
      "ARR_2022_256"
    ],
    "pattern_ids": [
      "pattern_7"
    ]
  },
  {
    "idea_id": "idea_291",
    "description": "系统性分析注意力机制作为神经网络解释方法的有效性及其评估标准。",
    "tech_stack": [
      "注意力机制",
      "加性注意力",
      "缩放点积注意力",
      "解释方法比较",
      "软最大函数"
    ],
    "input_type": "自然语言处理任务中的文本数据或分类问题",
    "output_type": "模型决策的解释性权重或解释方法的评估结果",
    "source_paper_ids": [
      "ARR_2022_257"
    ],
    "pattern_ids": [
      "pattern_18"
    ]
  },
  {
    "idea_id": "idea_292",
    "description": "提出了一种将NLP数据集映射到地理空间以评估其代表性的新方法，并分析其与经济和人口等因素的相关性。",
    "tech_stack": [
      "地理映射",
      "数据代表性分析",
      "实体链接",
      "多语言模型",
      "社会经济相关性分析"
    ],
    "input_type": "多语言NLP数据集及其对应的地理和经济信息",
    "output_type": "数据集地理代表性可视化、代表性评估结果及实体链接性能分析",
    "source_paper_ids": [
      "ARR_2022_259"
    ],
    "pattern_ids": [
      "pattern_4"
    ]
  },
  {
    "idea_id": "idea_293",
    "description": "提出了一种能够为开放性知识问答自动生成详细、个性化反馈的评测方法，提升自动评测的解释性和信任度。",
    "tech_stack": [
      "Transformer模型",
      "自动短答案评分（ASAG）",
      "自然语言处理",
      "解释型反馈生成"
    ],
    "input_type": "学生针对开放性知识问题的自然语言回答",
    "output_type": "针对每个回答的分数/标签和详细、个性化的解释性反馈",
    "source_paper_ids": [
      "ARR_2022_25"
    ],
    "pattern_ids": [
      "pattern_13"
    ]
  },
  {
    "idea_id": "idea_294",
    "description": "提出针对Seq2Seq Transformer模型在摘要生成中的知识蒸馏方法，减少模型体积并优化伪标签质量。",
    "tech_stack": [
      "Seq2Seq Transformer",
      "知识蒸馏",
      "伪标签生成",
      "注意力分布分析",
      "ROUGE评分"
    ],
    "input_type": "长文档及其对应的摘要对",
    "output_type": "精简版摘要生成模型及其生成的摘要",
    "source_paper_ids": [
      "ARR_2022_260"
    ],
    "pattern_ids": [
      "pattern_27"
    ]
  },
  {
    "idea_id": "idea_295",
    "description": "首次定量分析会议举办地与参与多样性及碳排放之间的关系。",
    "tech_stack": [
      "自然语言处理（NLP）",
      "文献解析",
      "地理位置识别"
    ],
    "input_type": "学术论文元数据，包括作者单位和会议举办地信息",
    "output_type": "会议举办地、参与多样性和碳排放的定量分析结果",
    "source_paper_ids": [
      "ARR_2022_262"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_296",
    "description": "提出了一种变分层次模型，通过显式建模翻译和摘要的层次关系来提升跨语言摘要性能。",
    "tech_stack": [
      "Conditional Variational Auto-Encoder (CVAE)",
      "Transformer",
      "多任务学习",
      "层次潜变量建模"
    ],
    "input_type": "源语言（如英文）文档",
    "output_type": "目标语言（如中文）的摘要",
    "source_paper_ids": [
      "ARR_2022_263"
    ],
    "pattern_ids": [
      "pattern_20"
    ]
  },
  {
    "idea_id": "idea_297",
    "description": "提出了一种基于信息理论的ICM指标，用于统一评估多标签、层次化和极端分类问题。",
    "tech_stack": [
      "信息对比模型（ICM）",
      "信息论",
      "相似性度量",
      "合成数据测试",
      "多标签分类",
      "层次化分类",
      "极端分类"
    ],
    "input_type": "多标签、层次化或极端分类任务中的预测标签与真实标签集合",
    "output_type": "用于评估分类系统性能的相似性分数或评价指标",
    "source_paper_ids": [
      "ARR_2022_264"
    ],
    "pattern_ids": [
      "pattern_8"
    ]
  },
  {
    "idea_id": "idea_298",
    "description": "提出了一种结合问题类型预测与事件中心摘要的自动生成儿童故事书高认知需求教育性问题的框架。",
    "tech_stack": [
      "问题类型分布学习",
      "事件中心摘要生成",
      "教育性问题生成",
      "伪标签",
      "神经网络模型"
    ],
    "input_type": "儿童故事书的文本段落",
    "output_type": "按类型划分的高认知需求教育性问题集合",
    "source_paper_ids": [
      "ARR_2022_265"
    ],
    "pattern_ids": [
      "pattern_15"
    ]
  },
  {
    "idea_id": "idea_299",
    "description": "将多个预训练的黑盒专家模型组合为能量模型，通过Gibbs-Metropolis-Hastings采样实现属性可控的文本生成。",
    "tech_stack": [
      "能量模型（Energy-based Model）",
      "Gibbs采样",
      "Metropolis-Hastings算法",
      "预训练判别器（如情感分类器）",
      "BERT等双向语言模型",
      "Bertscore/Bleurt等距离度量"
    ],
    "input_type": "需要生成满足特定属性（如情感、流畅性、上下文一致性等）约束的文本生成任务描述或上下文信息。",
    "output_type": "满足指定属性控制的高质量自然语言文本序列。",
    "source_paper_ids": [
      "ARR_2022_266"
    ],
    "pattern_ids": [
      "pattern_21"
    ]
  },
  {
    "idea_id": "idea_300",
    "description": "提出利用角色间信息交互提升角色导向对话摘要质量的新方法。",
    "tech_stack": [
      "Pointer-Generator Network (PGN)",
      "LSTM",
      "Copy Mechanism",
      "Coverage Mechanism",
      "Cross Attention",
      "Role Attention",
      "Transformer",
      "BERTAbs"
    ],
    "input_type": "多角色对话文本",
    "output_type": "每个角色的主要观点摘要",
    "source_paper_ids": [
      "ARR_2022_268"
    ],
    "pattern_ids": [
      "pattern_27"
    ]
  },
  {
    "idea_id": "idea_301",
    "description": "提出CrossAligner及多种辅助对齐损失，实现零样本跨语言任务型NLU的高效迁移与性能提升。",
    "tech_stack": [
      "CrossAligner",
      "Translate-Intent",
      "Contrastive Alignment",
      "XLMRoBERTa",
      "Multilingual BERT",
      "辅助损失函数",
      "对比学习",
      "无监督平行数据"
    ],
    "input_type": "多语言任务型NLU数据，包括用户意图分类和实体识别的原始语句及无标签平行语料",
    "output_type": "目标语言下的用户意图分类标签和实体识别结果",
    "source_paper_ids": [
      "ARR_2022_269"
    ],
    "pattern_ids": [
      "pattern_25"
    ]
  },
  {
    "idea_id": "idea_302",
    "description": "提出并探索自动检测社交媒体中自杀相关事件的新任务以辅助自杀风险识别与理解。",
    "tech_stack": [
      "自然语言处理",
      "事件检测",
      "深度神经网络",
      "主题建模",
      "Latent Dirichlet Allocation (LDA)"
    ],
    "input_type": "社交媒体自杀相关文本数据",
    "output_type": "自杀相关事件的触发词及事件类别标签",
    "source_paper_ids": [
      "ARR_2022_26"
    ],
    "pattern_ids": [
      "pattern_14"
    ]
  },
  {
    "idea_id": "idea_303",
    "description": "提出并系统分析了LM-based社交聊天机器人中的隐私泄露问题，并设计了针对persona推断攻击的防御策略。",
    "tech_stack": [
      "GPT-2",
      "黑盒攻击",
      "多层感知机（MLP）",
      "PersonaChat数据集",
      "嵌入表示",
      "隐私保护机制"
    ],
    "input_type": "用户对话语句及其对应persona信息",
    "output_type": "对话嵌入泄露的persona属性推断结果及防御效果",
    "source_paper_ids": [
      "ARR_2022_270"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_304",
    "description": "通过引入以否定为中心的数据增强和掩码策略，提升预训练语言模型对否定检测的跨领域能力。",
    "tech_stack": [
      "预训练语言模型",
      "数据增强",
      "否定特定掩码策略",
      "NegBERT",
      "迁移学习"
    ],
    "input_type": "包含否定表达的自然语言文本数据",
    "output_type": "否定触发词检测和否定范围识别的标签或标注结果",
    "source_paper_ids": [
      "ARR_2022_272"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_305",
    "description": "提出了一种无需结构化知识库，通过生成式知识提示提升大模型常识推理能力的方法。",
    "tech_stack": [
      "生成式知识提示（Generated Knowledge Prompting）",
      "大规模预训练语言模型",
      "few-shot学习",
      "自然语言知识生成"
    ],
    "input_type": "常识推理相关的自然语言问题或任务输入",
    "output_type": "包含生成知识提示的增强型模型推理结果",
    "source_paper_ids": [
      "ARR_2022_273"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_306",
    "description": "提出了一种基于BERT的自适应推理机制，通过中间层置信度动态决定提前终止推理以提升效率。",
    "tech_stack": [
      "BERT",
      "自适应推理（Adaptive Inference）",
      "中间层分类器",
      "熵置信度度量",
      "早停机制"
    ],
    "input_type": "文本分类任务中的文本输入或句子",
    "output_type": "类别标签的概率分布或最终分类结果",
    "source_paper_ids": [
      "ARR_2022_274"
    ],
    "pattern_ids": [
      "pattern_5"
    ]
  },
  {
    "idea_id": "idea_307",
    "description": "首次探索并优化GPT模型在中文拼音输入法中的应用，提升缩写拼音输入下的字符预测准确率。",
    "tech_stack": [
      "Transformer",
      "GPT",
      "拼音上下文增强",
      "拼音约束训练",
      "自动回归预测"
    ],
    "input_type": "中文拼音输入（包括完整拼音和缩写拼音）",
    "output_type": "对应拼音的中文字符预测结果",
    "source_paper_ids": [
      "ARR_2022_275"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_308",
    "description": "提出了Aligned Constrained Training (ACT)算法，通过对齐和约束训练提升非自回归机器翻译中术语翻译的准确性和一致性。",
    "tech_stack": [
      "非自回归机器翻译 (NAT)",
      "Levenshtein Transformer (LevT)",
      "对齐提示 (Alignment Prompting)",
      "约束训练 (Constrained Training)"
    ],
    "input_type": "带有预定义术语约束的源语言文本",
    "output_type": "包含指定术语且上下文准确的目标语言翻译文本",
    "source_paper_ids": [
      "ARR_2022_276"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_309",
    "description": "提出并系统分析NLP领域中‘underclaiming’现象，探讨其危害及减少方法。",
    "tech_stack": [
      "模型性能评估",
      "写作与引用规范分析",
      "基准测试工具改进",
      "模型性能预测"
    ],
    "input_type": "NLP模型能力评估相关文献、写作和引用实践",
    "output_type": "对‘underclaiming’类型的分类、危害分析及改进建议",
    "source_paper_ids": [
      "ARR_2022_277"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_310",
    "description": "提出了一种结合跨语句条件VAE和BERT嵌入的TTS系统，实现更细粒度和可控的韵律建模。",
    "tech_stack": [
      "Conditional Variational Autoencoder (CVAE)",
      "Cross-Utterance Embedding",
      "BERT Sentence Embedding",
      "Multi-Head Attention",
      "FastSpeech 2",
      "Speaker Information Integration"
    ],
    "input_type": "带有上下文语句的文本输入及说话人信息",
    "output_type": "具有更自然和可控韵律的合成语音",
    "source_paper_ids": [
      "ARR_2022_278"
    ],
    "pattern_ids": [
      "pattern_9"
    ]
  },
  {
    "idea_id": "idea_311",
    "description": "提出一种无需依赖中间依存表示、直接从文本预测情感图的新型结构化情感分析方法。",
    "tech_stack": [
      "结构化情感分析",
      "图解析",
      "Permutation-Invariant Graph-Based Parsing",
      "PERIN模型",
      "多种图编码方法"
    ],
    "input_type": "原始文本句子，包含需提取的情感结构信息",
    "output_type": "包含极性表达、持有者、目标及极性的情感图结构",
    "source_paper_ids": [
      "ARR_2022_279"
    ],
    "pattern_ids": [
      "pattern_17"
    ]
  },
  {
    "idea_id": "idea_312",
    "description": "提出ELLE方法，通过函数保持的模型扩展，实现高效终身预训练以适应不断增长的新数据。",
    "tech_stack": [
      "预训练语言模型",
      "终身学习",
      "函数保持初始化（FPI）",
      "模型宽度和深度扩展",
      "知识刺激与解耦"
    ],
    "input_type": "流式到来的多领域文本语料数据",
    "output_type": "高效适应新知识且避免遗忘的扩展型预训练语言模型",
    "source_paper_ids": [
      "ARR_2022_27"
    ],
    "pattern_ids": [
      "pattern_5"
    ]
  },
  {
    "idea_id": "idea_313",
    "description": "提出Domain Confused Contrastive Learning，通过域困扰和对比学习提升无监督领域适应的文本分类性能。",
    "tech_stack": [
      "对比学习",
      "自监督学习",
      "域困扰（domain puzzles）",
      "深度表示学习",
      "一致性损失",
      "情感分类损失"
    ],
    "input_type": "带标签的源域文本和无标签的目标域文本",
    "output_type": "目标域上的文本分类预测结果",
    "source_paper_ids": [
      "ARR_2022_281"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_314",
    "description": "提出了相对槽准确率作为对话状态跟踪模型的新评价指标，以更真实地评估模型性能。",
    "tech_stack": [
      "对话状态跟踪",
      "MultiWOZ数据集",
      "评价指标设计",
      "相对槽准确率"
    ],
    "input_type": "多轮对话数据及其累积信念状态",
    "output_type": "对话状态跟踪模型的性能评估指标（包括相对槽准确率）",
    "source_paper_ids": [
      "ARR_2022_282"
    ],
    "pattern_ids": [
      "pattern_25"
    ]
  },
  {
    "idea_id": "idea_315",
    "description": "提出了一种基于句子级隐私的新文档嵌入方法，并设计了DeepCandidate机制实现强隐私保护。",
    "tech_stack": [
      "句子隐私定义",
      "DeepCandidate机制",
      "句子编码器",
      "高维嵌入采样",
      "无监督学习"
    ],
    "input_type": "包含多个句子的文档文本",
    "output_type": "满足句子级隐私保护的文档嵌入向量",
    "source_paper_ids": [
      "ARR_2022_283"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_316",
    "description": "提出了IMAGECODE数据集和基于上下文的多模态图像检索任务，推动模型对细粒度语境的理解与推理。",
    "tech_stack": [
      "多模态模型",
      "CLIP",
      "ViLBERT",
      "上下文增强训练",
      "Transformer",
      "元素级特征融合"
    ],
    "input_type": "包含上下文描述和高度相似图像集合的检索问题",
    "output_type": "模型从候选图像集合中检索出与描述最匹配的目标图像",
    "source_paper_ids": [
      "ARR_2022_284"
    ],
    "pattern_ids": [
      "pattern_22"
    ]
  },
  {
    "idea_id": "idea_317",
    "description": "提出了一种基于相似度的多提示（Similarity Prompting, SP）方法，以提升预训练语言模型在对比类任务中的少样本学习效果。",
    "tech_stack": [
      "预训练语言模型（PLM）",
      "Prompt-based Learning",
      "Cloze-style 问题",
      "Similarity-based Prompting",
      "特征提取",
      "多提示融合"
    ],
    "input_type": "包含一个或多个文本序列的任务特定输入，如需要判断词义是否一致的上下文对。",
    "output_type": "针对输入的分类结果，如二分类判断目标词在不同上下文中的含义是否相同。",
    "source_paper_ids": [
      "ARR_2022_285"
    ],
    "pattern_ids": [
      "pattern_10"
    ]
  },
  {
    "idea_id": "idea_318",
    "description": "提出并验证了检测大规模语言模型中Stolen Probability现象的精确算法，并实证其在实际模型中的影响有限。",
    "tech_stack": [
      "softmax层分析",
      "低秩矩阵理论",
      "Stolen Probability检测算法",
      "神经网络模型评测"
    ],
    "input_type": "预训练语言模型或机器翻译模型的参数和词汇表",
    "output_type": "模型中是否存在Stolen Probability现象及相关不可达token的列表",
    "source_paper_ids": [
      "ARR_2022_286"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_319",
    "description": "通过多任务学习和强化学习选择相关数据，提升低资源环境下的刻板印象检测性能。",
    "tech_stack": [
      "多任务学习",
      "强化学习",
      "预训练语言模型"
    ],
    "input_type": "带有刻板印象及相关攻击性语言标签的文本数据",
    "output_type": "对文本中刻板印象及攻击性语言的检测与分类结果",
    "source_paper_ids": [
      "ARR_2022_287"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_320",
    "description": "提出并验证了针对政治冲突领域的BERT模型ConfliBERT以提升事件抽取和分类效果。",
    "tech_stack": [
      "BERT",
      "Transformer",
      "自监督学习",
      "迁移学习",
      "自然语言处理"
    ],
    "input_type": "大规模无标签新闻文本和冲突相关语料",
    "output_type": "结构化冲突事件数据及分类结果",
    "source_paper_ids": [
      "ARR_2022_288"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_321",
    "description": "提出了一种基于自适应注意力机制的提示迁移方法，实现数据稀缺下文本生成任务的高效迁移。",
    "tech_stack": [
      "预训练语言模型（PLMs）",
      "提示学习（Prompt-based Learning）",
      "自适应注意力机制",
      "多键记忆网络",
      "BART-LARGE"
    ],
    "input_type": "少量标注数据的新文本生成任务及其输入文本",
    "output_type": "针对新任务生成的自然语言文本",
    "source_paper_ids": [
      "ARR_2022_289"
    ],
    "pattern_ids": [
      "pattern_5"
    ]
  },
  {
    "idea_id": "idea_322",
    "description": "提出了一种基于表填充和结构约束的管道式OpenIE系统COMPACTIE，用于从句子中提取紧凑三元组。",
    "tech_stack": [
      "Open Information Extraction (OpenIE)",
      "神经网络",
      "表填充模型",
      "结构约束",
      "依存句法分析"
    ],
    "input_type": "原始自然语言句子",
    "output_type": "结构化的(主语; 谓语; 宾语)三元组",
    "source_paper_ids": [
      "ARR_2022_290"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_323",
    "description": "通过对文档表示进行插值和扰动，扩展查询-文档对以提升密集检索模型对未标注文档的泛化能力。",
    "tech_stack": [
      "密集检索",
      "双编码器结构",
      "负采样",
      "表示插值（Mixup）",
      "表示扰动"
    ],
    "input_type": "查询与文档对，包含少量标注和大量未标注文档",
    "output_type": "增强后的查询-文档对及改进的检索模型表示",
    "source_paper_ids": [
      "ARR_2022_291"
    ],
    "pattern_ids": [
      "pattern_10"
    ]
  },
  {
    "idea_id": "idea_324",
    "description": "提出基于对比学习的神经网络方法，通过识别和聚合数学应用题的模式提升解题能力。",
    "tech_stack": [
      "BERT",
      "Encoder-Decoder模型",
      "Tree Decoder",
      "对比学习",
      "T-SNE可视化"
    ],
    "input_type": "自然语言描述的数学应用题",
    "output_type": "对应的数学方程（解题表达式）",
    "source_paper_ids": [
      "ARR_2022_292"
    ],
    "pattern_ids": [
      "pattern_2"
    ]
  },
  {
    "idea_id": "idea_325",
    "description": "提出一种基于数学语法图和语法感知记忆网络的方法，实现对数学问题文本与公式的深度融合理解。",
    "tech_stack": [
      "预训练语言模型（PLM）",
      "BERT",
      "图注意力网络（GAT）",
      "数学语法图",
      "语法感知记忆网络",
      "持续预训练"
    ],
    "input_type": "包含文本描述和数学公式的数学问题数据",
    "output_type": "融合文本与公式信息的数学问题语义表示",
    "source_paper_ids": [
      "ARR_2022_293"
    ],
    "pattern_ids": [
      "pattern_2"
    ]
  },
  {
    "idea_id": "idea_326",
    "description": "提出并实证分析了连续提示与其离散文本解释之间的显著不一致性（Prompt Waywardness假设）。",
    "tech_stack": [
      "连续提示（continuous prompts）",
      "离散提示（discrete prompts）",
      "最近邻投影（nearest-neighbor projection）",
      "大语言模型（large language models）",
      "分类任务分析"
    ],
    "input_type": "分类任务中的文本数据及其对应的连续提示",
    "output_type": "投影到指定离散文本的连续提示及其在任务上的性能表现",
    "source_paper_ids": [
      "ARR_2022_294"
    ],
    "pattern_ids": [
      "pattern_10"
    ]
  },
  {
    "idea_id": "idea_327",
    "description": "首次提出并发布专为古韩文（Hanja）历史文献设计的预训练语言模型和评测数据集，提升文献理解与分析能力。",
    "tech_stack": [
      "预训练语言模型",
      "多语言BERT",
      "AnchiBERT",
      "微调",
      "命名实体识别",
      "主题分类",
      "零样本学习",
      "输入信息增强（实体遮蔽、文档年代拼接）"
    ],
    "input_type": "古韩文（Hanja）历史文献文本及相关任务（如国王预测、主题分类、命名实体识别、摘要检索）",
    "output_type": "针对各任务的模型预测结果，如国王身份、主题类别、命名实体标签和摘要内容",
    "source_paper_ids": [
      "ARR_2022_295"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_328",
    "description": "提出了基于生成式模型和模板提示的低资源事件抽取方法DEGREE。",
    "tech_stack": [
      "生成式模型",
      "模板提示",
      "事件抽取",
      "弱监督学习",
      "端到端学习"
    ],
    "input_type": "包含事件的文本段落及人工设计的提示模板",
    "output_type": "事件触发词及其参与者（论元）及角色的结构化抽取结果",
    "source_paper_ids": [
      "ARR_2022_296"
    ],
    "pattern_ids": [
      "pattern_14"
    ]
  },
  {
    "idea_id": "idea_329",
    "description": "提出基于问答驱动的注释方法，自动生成否定语句的肯定解释。",
    "tech_stack": [
      "QASRL（问答语义角色标注）",
      "模板生成",
      "自然语言处理",
      "语义注释"
    ],
    "input_type": "包含动词否定的自然语言句子",
    "output_type": "肯定解释的自然语言陈述",
    "source_paper_ids": [
      "ARR_2022_297"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_330",
    "description": "提出了只微调标签嵌入的Siamese Network方法，实现高效可迁移的零样本和小样本文本分类。",
    "tech_stack": [
      "Siamese Networks",
      "Transformer",
      "Label Tuning",
      "Knowledge Distillation",
      "Textual Entailment",
      "Cross Attention"
    ],
    "input_type": "输入文本和标签的文本描述（如标签名或简要描述）",
    "output_type": "文本分类标签或类别",
    "source_paper_ids": [
      "ARR_2022_298"
    ],
    "pattern_ids": [
      "pattern_16"
    ]
  },
  {
    "idea_id": "idea_331",
    "description": "提出一种同步修正与生成目标词的方法，提升自回归语言生成模型的输出质量。",
    "tech_stack": [
      "encoder-decoder框架",
      "自回归解码",
      "同步修正机制",
      "注意力机制",
      "掩码选择"
    ],
    "input_type": "源语言文本或待生成文本的输入序列",
    "output_type": "经过同步修正的目标语言生成序列",
    "source_paper_ids": [
      "ARR_2022_299"
    ],
    "pattern_ids": [
      "pattern_21"
    ]
  },
  {
    "idea_id": "idea_332",
    "description": "提出LongT5模型，通过同时扩展输入长度和模型规模，并引入TGlobal注意力机制和PEGASUS预训练策略，提升长文本任务性能。",
    "tech_stack": [
      "Transformer",
      "T5架构",
      "TGlobal注意力机制",
      "局部/全局稀疏注意力",
      "PEGASUS预训练策略"
    ],
    "input_type": "需要处理长文本序列的自然语言处理任务输入，如文档、问题等",
    "output_type": "针对输入任务的生成式文本输出，如摘要、答案等",
    "source_paper_ids": [
      "ARR_2022_29"
    ],
    "pattern_ids": [
      "pattern_30"
    ]
  },
  {
    "idea_id": "idea_333",
    "description": "提出了基于证据的两阶段系统，通过从前提文章中选择证据并推断声明的真实性，解决自动事实核查中的声明推断问题。",
    "tech_stack": [
      "TF-IDF",
      "Dense Passage Retrieval",
      "Bi-directional Recurrent Networks",
      "Transformers",
      "Deep Learning"
    ],
    "input_type": "声明文本及其关联的前提文章（包含多个句子）",
    "output_type": "声明的真实性判定（如真、部分真/假、假）",
    "source_paper_ids": [
      "ARR_2022_2"
    ],
    "pattern_ids": [
      "pattern_6"
    ]
  },
  {
    "idea_id": "idea_334",
    "description": "提出利用轻量化Transformer模型提升主动学习在文本标注任务中的效率与实用性。",
    "tech_stack": [
      "Active Learning",
      "Transformer",
      "BERT",
      "ELECTRA",
      "XLNet",
      "DistilBERT",
      "CNN-BiLSTM-CRF",
      "Transfer Learning",
      "Model Distillation"
    ],
    "input_type": "大规模未标注文本数据和少量已标注文本实例",
    "output_type": "经过主动学习筛选后需人工标注的高信息量文本实例",
    "source_paper_ids": [
      "ARR_2022_300"
    ],
    "pattern_ids": [
      "pattern_11"
    ]
  },
  {
    "idea_id": "idea_335",
    "description": "系统性量化了NLP任务中训练和测试数据时间不一致对模型性能的影响，并提出了衡量性能退化速率的新指标。",
    "tech_stack": [
      "预训练-微调范式",
      "GPT2",
      "任务性能退化率指标",
      "领域自适应",
      "Huggingface实现"
    ],
    "input_type": "跨多个时间段和领域的文本数据集，用于不同NLP任务（如摘要、实体类型识别等）",
    "output_type": "模型在不同时间段测试集上的任务性能指标（如F1分数）及其随时间变化的退化速率",
    "source_paper_ids": [
      "ARR_2022_301"
    ],
    "pattern_ids": [
      "pattern_26"
    ]
  },
  {
    "idea_id": "idea_336",
    "description": "提出在固定标注预算下，用单一多语言预训练模型结合主动学习高效提升多语言NLP任务性能。",
    "tech_stack": [
      "多语言预训练语言模型（MPLM）",
      "multilingual-BERT (mBERT)",
      "主动学习（Active Learning）",
      "零样本迁移（Zero-shot Transfer）",
      "图结构双仿射注意力解析器（Bi-affine Attention Parser）"
    ],
    "input_type": "多语言下的带有限标注预算的NLP任务数据（如分类、序列标注、依存句法分析）",
    "output_type": "多语言NLP任务的模型预测结果（如类别标签、序列标签、依存关系）",
    "source_paper_ids": [
      "ARR_2022_302"
    ],
    "pattern_ids": [
      "pattern_20"
    ]
  },
  {
    "idea_id": "idea_337",
    "description": "提出并标注了面向引文的相关工作生成数据集CORWA，并联合多任务模型区分异质文本片段。",
    "tech_stack": [
      "Transformer编码器",
      "多任务学习",
      "段落级句子标注",
      "联合标签解码"
    ],
    "input_type": "NLP论文的相关工作章节文本及其句子、引文标注信息",
    "output_type": "带有引文范围、引文类型和话语标签的相关工作章节标注",
    "source_paper_ids": [
      "ARR_2022_303"
    ],
    "pattern_ids": [
      "pattern_21"
    ]
  },
  {
    "idea_id": "idea_338",
    "description": "扩展了语言否定属性（LNP）到词汇语义层面，系统评估并揭示大规模预训练语言模型在否定和反义等语义扰动下的表现缺陷。",
    "tech_stack": [
      "预训练语言模型（PLMs）",
      "否定属性测试（LNP probing）",
      "词汇语义扰动（同义词/反义词替换）",
      "数据增强",
      "否定表达分析"
    ],
    "input_type": "包含原始句子及其否定或语义扰动（如反义、同义）后的文本输入",
    "output_type": "模型对原始与扰动输入的输出结果及其一致性/差异性分析",
    "source_paper_ids": [
      "ARR_2022_304"
    ],
    "pattern_ids": [
      "pattern_23"
    ]
  },
  {
    "idea_id": "idea_339",
    "description": "提出了一种结合事实性评估与离线强化学习的摘要系统训练方法，以区分并抑制非事实性幻觉。",
    "tech_stack": [
      "事实性分类器",
      "离线强化学习（Offline RL）",
      "最大似然估计（MLE）",
      "实体识别",
      "重要性采样"
    ],
    "input_type": "包含源文档和人工摘要的文本数据",
    "output_type": "事实性增强的自动生成摘要",
    "source_paper_ids": [
      "ARR_2022_305"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_340",
    "description": "首次提出基于指令的统一模型，实现多种生物医学NLP任务的高效泛化。",
    "tech_stack": [
      "指令学习",
      "多任务学习",
      "BART模型微调",
      "meta-dataset构建"
    ],
    "input_type": "带有任务指令和输入实例的文本数据",
    "output_type": "针对各生物医学NLP任务的文本输出结果",
    "source_paper_ids": [
      "ARR_2022_306"
    ],
    "pattern_ids": [
      "pattern_23"
    ]
  },
  {
    "idea_id": "idea_341",
    "description": "提出并评估利用多模态信息对无序任务步骤进行排序的方法及数据集。",
    "tech_stack": [
      "多模态编码器",
      "顺序解码器",
      "自监督预训练",
      "掩码语言模型（MLM）"
    ],
    "input_type": "无序的多模态任务步骤（文本和图像）",
    "output_type": "有序排列的任务步骤序列",
    "source_paper_ids": [
      "ARR_2022_307"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_342",
    "description": "首次系统性审查知识对话基准数据集中的幻觉现象，并分析主流模型对幻觉的放大作用。",
    "tech_stack": [
      "知识对话基准数据集注释",
      "幻觉检测与分类",
      "大规模预训练语言模型（GPT2, BART, CTRL）",
      "McNemar检验",
      "ROUGE指标"
    ],
    "input_type": "知识对话数据集中的对话文本及模型生成的回复",
    "output_type": "对话回复的幻觉率、主观/客观信息分类、模型放大幻觉的定量分析结果",
    "source_paper_ids": [
      "ARR_2022_308"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_343",
    "description": "提出多语料多域开放域对话生成的新训练与评估方法，包括语料嵌入和基于领域词频的加权学习。",
    "tech_stack": [
      "LSTM Seq2Seq with Attention",
      "GPT-2",
      "Interleaved Learning",
      "Labeled Learning",
      "Multi-task Labeled Learning",
      "Domain-specific Frequency Weighted Learning",
      "Corpus Embedding"
    ],
    "input_type": "多来源、跨领域的对话语料数据",
    "output_type": "针对不同语料域生成相关性强的开放域对话回复",
    "source_paper_ids": [
      "ARR_2022_309"
    ],
    "pattern_ids": [
      "pattern_25"
    ]
  },
  {
    "idea_id": "idea_344",
    "description": "提出了一种基于双语文本填充（BiTI）的交互式神经机器翻译方法，提升了翻译灵活性和效率。",
    "tech_stack": [
      "双语文本填充（Bilingual Text-Infilling）",
      "Transformer",
      "序列到序列模型（Seq2Seq）",
      "数据增强",
      "神经机器翻译（NMT）",
      "词汇约束解码（LCD）"
    ],
    "input_type": "包含源语言句子和带有空白的部分译文（编辑模板）",
    "output_type": "填补空白后的完整目标语言译文",
    "source_paper_ids": [
      "ARR_2022_310"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_345",
    "description": "利用预训练语言模型生成平滑词嵌入，实现高效加权的数据增强方法。",
    "tech_stack": [
      "预训练语言模型",
      "Masked Language Modeling (MLM)",
      "词嵌入",
      "Dropout",
      "数据增强"
    ],
    "input_type": "自然语言文本句子，尤其用于低资源场景的数据增强",
    "output_type": "包含平滑词嵌入的增强文本数据，用于提升下游任务表现",
    "source_paper_ids": [
      "ARR_2022_311"
    ],
    "pattern_ids": [
      "pattern_16"
    ]
  },
  {
    "idea_id": "idea_346",
    "description": "提出了一种推理时的coherence boosting方法，通过混合不同长度上下文的专家预测，提高大语言模型对长距离依赖的建模能力。",
    "tech_stack": [
      "coherence boosting",
      "log-linear expert ensemble",
      "autoregressive language models",
      "negative log-likelihood (NLL)",
      "context length conditioning"
    ],
    "input_type": "自然语言文本序列或对话提示",
    "output_type": "更具长距离上下文一致性的文本生成、概率分布或任务评分",
    "source_paper_ids": [
      "ARR_2022_312"
    ],
    "pattern_ids": [
      "pattern_30"
    ]
  },
  {
    "idea_id": "idea_347",
    "description": "提出基于软提示（soft prompts）的数据增强方法PromDA，用于低资源NLU任务中生成高质量合成数据。",
    "tech_stack": [
      "Prompt Tuning",
      "Pre-trained Language Models (PLMs)",
      "Soft Prompts",
      "Data Augmentation"
    ],
    "input_type": "小规模标注文本数据，用于自然语言理解任务（如句子分类和序列标注）",
    "output_type": "高质量的合成训练样本，用于提升NLU模型性能",
    "source_paper_ids": [
      "ARR_2022_313"
    ],
    "pattern_ids": [
      "pattern_16"
    ]
  },
  {
    "idea_id": "idea_348",
    "description": "提出了一种结合注意力机制和层归一化的全局输入token归因分析方法，提升了Transformer模型解释性。",
    "tech_stack": [
      "Transformer",
      "自注意力机制",
      "层归一化（Layer Normalization）",
      "残差连接",
      "归一化分解",
      "全局归因聚合",
      "BERT",
      "梯度归因方法"
    ],
    "input_type": "Transformer模型（如BERT）的输入token序列及模型参数",
    "output_type": "每个输入token对模型输出的全局归因分数",
    "source_paper_ids": [
      "ARR_2022_314"
    ],
    "pattern_ids": [
      "pattern_18"
    ]
  },
  {
    "idea_id": "idea_349",
    "description": "提出DILR框架，将层次注意力机制与可微分逻辑推理结合，实现多跳自然语言推理。",
    "tech_stack": [
      "层次注意力阅读器",
      "多跳可微分逻辑推理",
      "神经网络",
      "归纳逻辑编程",
      "端到端训练"
    ],
    "input_type": "包含实体、关系和文本描述的多跳自然语言推理问题",
    "output_type": "针对查询关系的候选答案及其概率",
    "source_paper_ids": [
      "ARR_2022_315"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_350",
    "description": "提出利用编译器反馈指导神经网络生成可编译代码的三阶段方法COMPCODER。",
    "tech_stack": [
      "CodeGPT",
      "GPT-2",
      "编译器反馈",
      "交叉熵损失",
      "神经网络微调",
      "可编译性强化",
      "可编译性判别"
    ],
    "input_type": "自然语言描述或部分代码片段",
    "output_type": "可编译的函数级源代码",
    "source_paper_ids": [
      "ARR_2022_316"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_351",
    "description": "提出了一种自动化生成视频-文本对抗性对比集的方法，通过动词和人物实体操控测试模型的语义理解能力。",
    "tech_stack": [
      "预训练语言模型（T5）",
      "Spacy",
      "GPT2-XL",
      "自动化对比集生成",
      "动词短语操控",
      "实体替换"
    ],
    "input_type": "视频及其对应的文本描述",
    "output_type": "包含自动生成对抗性负样本的视频-文本对比测试集",
    "source_paper_ids": [
      "ARR_2022_317"
    ],
    "pattern_ids": [
      "pattern_19"
    ]
  },
  {
    "idea_id": "idea_352",
    "description": "提出了一种结合连续潜变量的预训练对话生成模型DialogVED，提升对话生成的多样性和鲁棒性。",
    "tech_stack": [
      "预训练语言模型",
      "变分自编码器（VAE）",
      "连续潜变量建模",
      "Transformer编码器-解码器结构",
      "多目标预训练优化"
    ],
    "input_type": "多轮对话上下文及相关历史语句",
    "output_type": "针对给定对话上下文生成的多样化自然语言回复",
    "source_paper_ids": [
      "ARR_2022_318"
    ],
    "pattern_ids": [
      "pattern_25"
    ]
  },
  {
    "idea_id": "idea_353",
    "description": "揭示现有归因方法评价指标中的逻辑陷阱，并分析其对模型解释可靠性的影响。",
    "tech_stack": [
      "归因方法",
      "有意义扰动",
      "模型攻击",
      "梯度分析",
      "消融分析"
    ],
    "input_type": "训练好的深度模型及其输入实例",
    "output_type": "归因分数及其评价结果",
    "source_paper_ids": [
      "ARR_2022_319"
    ],
    "pattern_ids": [
      "pattern_18"
    ]
  },
  {
    "idea_id": "idea_354",
    "description": "系统性分析否定在主流自然语言理解任务语料库中的作用及其被忽视的问题。",
    "tech_stack": [
      "语料库分析",
      "自然语言处理",
      "Transformer模型",
      "任务性能评估"
    ],
    "input_type": "自然语言理解任务相关的多种语料库文本数据",
    "output_type": "关于否定在语料库中分布、任务表现及模型挑战性的统计与分析结果",
    "source_paper_ids": [
      "ARR_2022_31"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_355",
    "description": "提出基于triaffine变换融合多种异质特征的嵌套命名实体识别方法。",
    "tech_stack": [
      "triaffine transformation",
      "span-based representation",
      "label-aware representation learning",
      "pretrained language models"
    ],
    "input_type": "包含嵌套实体的自然语言文本",
    "output_type": "文本中所有嵌套命名实体及其类型",
    "source_paper_ids": [
      "ARR_2022_320"
    ],
    "pattern_ids": [
      "pattern_7"
    ]
  },
  {
    "idea_id": "idea_356",
    "description": "提出双维排行榜（BILLBOARDs），同时促进自然语言生成模型和自动评价指标的进步。",
    "tech_stack": [
      "自动评价指标",
      "稀疏回归",
      "线性组合",
      "可执行评测程序",
      "相关性分析"
    ],
    "input_type": "自然语言生成任务的模型和评价指标提交",
    "output_type": "基于与人工判断相关性的模型和评价指标排名及可复现的分数",
    "source_paper_ids": [
      "ARR_2022_321"
    ],
    "pattern_ids": [
      "pattern_13"
    ]
  },
  {
    "idea_id": "idea_357",
    "description": "提出并分析非自回归神经机器翻译模型的评估方法，兼顾译文质量与解码速度。",
    "tech_stack": [
      "非自回归神经机器翻译",
      "条件独立假设",
      "Pareto前沿分析",
      "自动评价指标（BLEU, ChrF, COMET）",
      "知识蒸馏",
      "Transformer模型"
    ],
    "input_type": "源语言文本及标准测试集（如WMT 14/16）",
    "output_type": "目标语言翻译文本及其质量与速度评估结果",
    "source_paper_ids": [
      "ARR_2022_322"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_358",
    "description": "提出了一种增强型多通道图卷积网络（EMC-GCN），结合词间关系和语言特征提升三元组抽取效果。",
    "tech_stack": [
      "BERT",
      "多通道图卷积网络（GCN）",
      "Biaffine Attention",
      "句法依存分析",
      "特征融合"
    ],
    "input_type": "包含多个单词的自然语言句子，需进行方面-观点-情感三元组抽取",
    "output_type": "句子中所有方面-观点-情感三元组的集合",
    "source_paper_ids": [
      "ARR_2022_323"
    ],
    "pattern_ids": [
      "pattern_28"
    ]
  },
  {
    "idea_id": "idea_359",
    "description": "提出将个性化记忆引入知识选择过程，通过概率模型提升知识驱动对话系统的响应质量。",
    "tech_stack": [
      "概率图模型",
      "双向学习",
      "无监督学习",
      "KL散度优化",
      "潜变量建模"
    ],
    "input_type": "包含对话上下文、外部知识候选和个性化记忆的数据",
    "output_type": "结合个性化记忆和知识选择生成的对话响应",
    "source_paper_ids": [
      "ARR_2022_324"
    ],
    "pattern_ids": [
      "pattern_25"
    ]
  },
  {
    "idea_id": "idea_360",
    "description": "首次提出结合神经CRF自动编码器与预训练语言模型表示用于无监督词性标注。",
    "tech_stack": [
      "神经条件随机场自动编码器（Neural CRF-AE）",
      "预训练语言模型（PLM）",
      "ELMo表示",
      "手工特征",
      "ScalarMix"
    ],
    "input_type": "未标注文本数据",
    "output_type": "每个词对应的词性标签（POS tags）",
    "source_paper_ids": [
      "ARR_2022_325"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_361",
    "description": "提出基于有监督聚类和定向最小生成树的训练目标，显式建模实体指称间的共指关系以提升零样本实体链接表现。",
    "tech_stack": [
      "有监督聚类",
      "定向最小生成树（arborescence）",
      "实体表示学习",
      "双编码器",
      "零样本学习",
      "实体链接",
      "共指消解"
    ],
    "input_type": "包含实体指称的自然语言文本及知识库实体信息（如描述、类型、别名）",
    "output_type": "实体指称与知识库实体的高质量向量表示及实体链接预测结果",
    "source_paper_ids": [
      "ARR_2022_326"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_362",
    "description": "提出基于图神经网络的多语种词对齐方法，提升低资源语言的词对齐质量。",
    "tech_stack": [
      "Graph Neural Network",
      "Graph Auto Encoder",
      "Graph Attention Network (GAT)",
      "GATConv",
      "Link Prediction"
    ],
    "input_type": "多语种平行语料中的句子级词对齐图",
    "output_type": "改进后的多语种词对齐边预测结果",
    "source_paper_ids": [
      "ARR_2022_327"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_363",
    "description": "提出针对直播视频转录文本的高质量标点恢复方法以提升可读性和后续NLP任务表现。",
    "tech_stack": [
      "自动语音识别（ASR）",
      "标点恢复（Punctuation Restoration）",
      "自然语言处理（NLP）"
    ],
    "input_type": "无标点的直播视频自动转录文本",
    "output_type": "插入正确标点后的高质量文本",
    "source_paper_ids": [
      "ARR_2022_328"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_364",
    "description": "提出并构建了土耳其语NLP任务的系统化基准和数据集，填补了该语言在NLP领域的研究空白。",
    "tech_stack": [
      "基准数据集构建",
      "任务定义与评估",
      "多任务基线模型",
      "数据集分割",
      "性能评测指标"
    ],
    "input_type": "土耳其语文本数据，涵盖多种NLP任务（如语言建模、句子分割、拼写校正等）",
    "output_type": "针对每个任务的评测结果、基线模型性能、公开数据集与分割",
    "source_paper_ids": [
      "ARR_2022_329"
    ],
    "pattern_ids": [
      "pattern_4"
    ]
  },
  {
    "idea_id": "idea_365",
    "description": "将贝叶斯不确定性估计方法引入大规模Transformer文本摘要模型以提升摘要质量和可靠性。",
    "tech_stack": [
      "Transformer",
      "BART",
      "PEGASUS",
      "Monte Carlo Dropout",
      "贝叶斯推断",
      "变分贝叶斯",
      "BLEU方差指标"
    ],
    "input_type": "需要生成摘要的文本数据",
    "output_type": "包含不确定性估计的自动生成文本摘要",
    "source_paper_ids": [
      "ARR_2022_32"
    ],
    "pattern_ids": [
      "pattern_27"
    ]
  },
  {
    "idea_id": "idea_366",
    "description": "提出在多项选择机器阅读理解中同时检测答案不确定性和不可回答性的创新方法。",
    "tech_stack": [
      "机器阅读理解（MRC）",
      "不确定性度量",
      "不可回答性检测",
      "多项选择题",
      "分布外检测"
    ],
    "input_type": "包含问题、上下文段落和多项选择答案的机器阅读理解数据",
    "output_type": "系统对每个问题的答案选择或选择放弃作答（因不确定或不可回答）",
    "source_paper_ids": [
      "ARR_2022_330"
    ],
    "pattern_ids": [
      "pattern_33"
    ]
  },
  {
    "idea_id": "idea_367",
    "description": "首次构建了多参考多来源的中文语法纠错评测数据集，并用主流GEC模型进行基准测试。",
    "tech_stack": [
      "多参考多来源数据集构建",
      "Seq2Edit模型",
      "Seq2Seq模型",
      "预训练语言模型（PLM）",
      "StructBERT",
      "GECToR"
    ],
    "input_type": "包含语法错误的中文句子，来自多种文本来源",
    "output_type": "经过纠错的标准化中文句子及多参考标注",
    "source_paper_ids": [
      "ARR_2022_331"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_368",
    "description": "提出并构建了首个泰语嵌套命名实体识别（N-NER）数据集，并基于多种方法进行模型探索与比较。",
    "tech_stack": [
      "条件随机场（CRF）",
      "WangchanBERTa",
      "XLM-RoBERTa",
      "深度学习",
      "IOB标注方案",
      "层级实体识别"
    ],
    "input_type": "包含嵌套实体结构的泰语文本数据",
    "output_type": "带有层级嵌套实体标签的文本标注结果",
    "source_paper_ids": [
      "ARR_2022_333"
    ],
    "pattern_ids": [
      "pattern_7"
    ]
  },
  {
    "idea_id": "idea_369",
    "description": "提出基于变分分割的生成框架，显式建模对话回复的结构和内容风格。",
    "tech_stack": [
      "BART预训练模型",
      "变分推断",
      "分段式生成",
      "隐变量建模",
      "弱监督学习"
    ],
    "input_type": "对话上下文和相关背景知识文本",
    "output_type": "结构化、风格可控的对话回复文本",
    "source_paper_ids": [
      "ARR_2022_335"
    ],
    "pattern_ids": [
      "pattern_25"
    ]
  },
  {
    "idea_id": "idea_370",
    "description": "提出AMuLaP方法，实现无需人工干预的自动标签映射以提升少样本分类性能。",
    "tech_stack": [
      "预训练语言模型",
      "prompt-based learning",
      "自动标签映射",
      "统计方法",
      "few-shot learning"
    ],
    "input_type": "少样本训练集和预设的prompt模板",
    "output_type": "自动选择的标签映射和分类预测结果",
    "source_paper_ids": [
      "ARR_2022_336"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_371",
    "description": "提出了一种基于流适配器的无监督神经机器翻译框架，通过潜变量变换实现句子级语义对齐。",
    "tech_stack": [
      "流适配器（Normalizing Flows）",
      "句子级语义表示",
      "潜变量变换",
      "预训练词向量（MUSE, fastText）",
      "无监督神经机器翻译",
      "自编码器",
      "注意力机制"
    ],
    "input_type": "单语语料（源语言和目标语言的单语句子）",
    "output_type": "目标语言的翻译句子",
    "source_paper_ids": [
      "ARR_2022_338"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_372",
    "description": "提出了一种基于非自回归生成的无监督文本摘要方法，提升了推理速度和摘要质量。",
    "tech_stack": [
      "非自回归生成模型",
      "编辑式搜索",
      "Transformer",
      "动态规划长度控制",
      "无监督学习"
    ],
    "input_type": "长文本或句子，无需配对的摘要数据",
    "output_type": "简洁、流畅且符合长度约束的文本摘要",
    "source_paper_ids": [
      "ARR_2022_339"
    ],
    "pattern_ids": [
      "pattern_27"
    ]
  },
  {
    "idea_id": "idea_373",
    "description": "提出了一种结合多通道文档表示、标签特征学习和动态语义掩码注意力的自动MeSH索引方法。",
    "tech_stack": [
      "多通道文档表示",
      "标签特征学习",
      "动态语义掩码注意力",
      "极端多标签文本分类"
    ],
    "input_type": "大规模生物医学文献及其对应的MeSH标签集合",
    "output_type": "每篇文献对应的多个MeSH主题词标签",
    "source_paper_ids": [
      "ARR_2022_33"
    ],
    "pattern_ids": [
      "pattern_8"
    ]
  },
  {
    "idea_id": "idea_374",
    "description": "提出新型探针设计，揭示并利用BERT中句法信息的因果作用以提升任务表现。",
    "tech_stack": [
      "探针方法",
      "因果分析",
      "BERT模型",
      "反事实表示",
      "句法信息注入"
    ],
    "input_type": "预训练语言模型的嵌入表示及相关下游任务数据",
    "output_type": "模型对句法信息因果使用的评估结果及下游任务性能提升",
    "source_paper_ids": [
      "ARR_2022_340"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_375",
    "description": "提出一种基于实体聚焦的神经文本连贯性模型，将语言学理论与预训练语言模型结合以提升连贯性建模。",
    "tech_stack": [
      "实体表示",
      "预训练语言模型",
      "神经网络",
      "Centering理论",
      "句子编码",
      "前馈神经网络"
    ],
    "input_type": "需要评估连贯性的文本或文档",
    "output_type": "文本连贯性评分或标签",
    "source_paper_ids": [
      "ARR_2022_341"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_376",
    "description": "本论文创新性地研究了对话上下文对仇恨与反仇恨言论识别的影响，并构建了包含上下文的语料库。",
    "tech_stack": [
      "上下文建模",
      "仇恨言论检测",
      "反仇恨言论识别",
      "语料库构建",
      "自动化注释算法"
    ],
    "input_type": "包含对话上下文的用户评论对或对话片段",
    "output_type": "对评论的仇恨、反仇恨或中性标签",
    "source_paper_ids": [
      "ARR_2022_342"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_377",
    "description": "系统性评估并优化字符级神经机器翻译架构，提出高效解码器以提升性能。",
    "tech_stack": [
      "Transformer",
      "字符嵌入",
      "卷积神经网络",
      "局部自注意力",
      "Charformer",
      "元分析",
      "两步解码器"
    ],
    "input_type": "字符级分割的机器翻译输入序列",
    "output_type": "字符级神经机器翻译的译文输出及性能评估结果",
    "source_paper_ids": [
      "ARR_2022_343"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_378",
    "description": "系统性分析多语种预训练模型在不同语言间词性标注任务的跨语言迁移影响因素。",
    "tech_stack": [
      "多语种预训练语言模型",
      "跨语言微调",
      "词性标注",
      "混合效应回归分析",
      "ASJP词表LDND度量"
    ],
    "input_type": "多语言词性标注数据及语言特征变量",
    "output_type": "不同源-目标语言组合的词性标注准确率及影响因素分析",
    "source_paper_ids": [
      "ARR_2022_344"
    ],
    "pattern_ids": [
      "pattern_20"
    ]
  },
  {
    "idea_id": "idea_379",
    "description": "提出了结合等变对比学习和差异损失的DiffCSE方法，以提升无监督句子表示学习效果。",
    "tech_stack": [
      "对比学习",
      "等变对比学习",
      "dropout增强",
      "MLM词替换",
      "交叉熵损失",
      "句子嵌入"
    ],
    "input_type": "无标签的句子文本数据",
    "output_type": "高质量的通用句子表示（句子嵌入向量）",
    "source_paper_ids": [
      "ARR_2022_345"
    ],
    "pattern_ids": [
      "pattern_10"
    ]
  },
  {
    "idea_id": "idea_380",
    "description": "提出并分析长文本问答答案的句子角色结构，用于改进自动和人工评价方法。",
    "tech_stack": [
      "RoBERTa",
      "自动角色分类模型",
      "语篇结构分析",
      "数据注释"
    ],
    "input_type": "长文本问答数据，包括问题和多句答案（人工或机器生成）",
    "output_type": "每句答案的角色标签及答案整体语篇结构分析",
    "source_paper_ids": [
      "ARR_2022_346"
    ],
    "pattern_ids": [
      "pattern_30"
    ]
  },
  {
    "idea_id": "idea_381",
    "description": "提出基于能量的动态推理方法，将输入样本分配给大模型或轻量模型以兼顾效率与准确性。",
    "tech_stack": [
      "能量模型（Energy-Based Model, EBM）",
      "动态推理",
      "模型路由",
      "联合推理",
      "早退机制",
      "分布外检测（OOD Detection）"
    ],
    "input_type": "自然语言处理任务中的输入样本（如文本序列）",
    "output_type": "分类或预测结果，由大模型或轻量模型输出",
    "source_paper_ids": [
      "ARR_2022_347"
    ],
    "pattern_ids": [
      "pattern_5"
    ]
  },
  {
    "idea_id": "idea_382",
    "description": "利用单模态自监督预训练模型提升音视频语音识别任务的表现。",
    "tech_stack": [
      "自监督学习",
      "预训练模型",
      "MoCo v2",
      "ResNet",
      "LibriLight",
      "Baevski et al. 2020"
    ],
    "input_type": "音频和对齐的唇动视频数据",
    "output_type": "语音识别文本结果",
    "source_paper_ids": [
      "ARR_2022_348"
    ],
    "pattern_ids": [
      "pattern_9"
    ]
  },
  {
    "idea_id": "idea_383",
    "description": "提出并构建了CHAPTERBREAK数据集，通过章节断点后缀识别任务系统性评估LRLM对长距离依赖的理解能力。",
    "tech_stack": [
      "长文本语言模型（LRLM）",
      "稀疏注意力",
      "章节断点检测",
      "suffix identification",
      "BigBird",
      "Routing Transformer",
      "RoBERTa"
    ],
    "input_type": "包含长篇叙事文本（如小说）章节前缀和多个候选后缀的序列数据",
    "output_type": "模型对正确后缀的选择概率或分类结果",
    "source_paper_ids": [
      "ARR_2022_349"
    ],
    "pattern_ids": [
      "pattern_30"
    ]
  },
  {
    "idea_id": "idea_384",
    "description": "通过知识蒸馏将基于机器翻译的CLIR模型的能力迁移到端到端跨语言检索模型，无需依赖机器翻译。",
    "tech_stack": [
      "跨语言信息检索（CLIR）",
      "机器翻译（MT）",
      "知识蒸馏（KD）",
      "预训练多语言掩码语言模型（PLM）",
      "ColBERT",
      "XLM-RoBERTa"
    ],
    "input_type": "低资源语言的查询和仅包含高资源语言（如英语）文档的检索语料库",
    "output_type": "与输入查询语义相关的高资源语言文档排序结果",
    "source_paper_ids": [
      "ARR_2022_350"
    ],
    "pattern_ids": [
      "pattern_20"
    ]
  },
  {
    "idea_id": "idea_385",
    "description": "提出了一种优化的双向机器阅读理解方法，通过专属分类器和改进的分词、跨度匹配与概率生成提升三元组抽取性能。",
    "tech_stack": [
      "Bidirectional Machine Reading Comprehension (BMRC)",
      "Exclusive Classifiers",
      "Word Segmentation",
      "Span Matching",
      "Probability Generation"
    ],
    "input_type": "包含多个方面和观点的自然语言文本",
    "output_type": "包含方面、观点和情感极性的三元组列表",
    "source_paper_ids": [
      "ARR_2022_351"
    ],
    "pattern_ids": [
      "pattern_32"
    ]
  },
  {
    "idea_id": "idea_386",
    "description": "提出针对生成式预训练语言模型的低比特量化方法，通过对词嵌入和模块动态缩放提升压缩效果。",
    "tech_stack": [
      "低比特量化",
      "token-level对比蒸馏",
      "模块动态缩放",
      "Transformer",
      "生成式预训练语言模型"
    ],
    "input_type": "文本生成相关任务的数据，如语言建模、摘要生成、下一句预测等",
    "output_type": "压缩后的生成式预训练模型在各任务上的性能指标（如PPL、ROUGE分数）",
    "source_paper_ids": [
      "ARR_2022_352"
    ],
    "pattern_ids": [
      "pattern_5"
    ]
  },
  {
    "idea_id": "idea_387",
    "description": "将分类任务重构为生成任务，通过生成剩余特征以控制和消除结构性偏见，实现可控的无偏模型。",
    "tech_stack": [
      "生成模型",
      "贝叶斯分解",
      "结构性偏见分析",
      "o.o.d泛化评估"
    ],
    "input_type": "带有结构性偏见的自然语言处理数据集（如NLI任务中的前提和假设对）",
    "output_type": "对标签的无偏预测及偏见度量（如泛化差距和相关性）",
    "source_paper_ids": [
      "ARR_2022_353"
    ],
    "pattern_ids": [
      "pattern_19"
    ]
  },
  {
    "idea_id": "idea_388",
    "description": "提出STABLEMOE，通过两阶段训练和路由蒸馏，显著缓解MoE模型的路由波动问题。",
    "tech_stack": [
      "Mixture of Experts (MoE)",
      "Transformer",
      "路由蒸馏",
      "平衡损失",
      "Sigmoid门控机制",
      "学习式路由"
    ],
    "input_type": "序列化的文本输入或token序列",
    "output_type": "稳定的token到专家模块的分配结果及改进的模型表现",
    "source_paper_ids": [
      "ARR_2022_354"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_389",
    "description": "提出跨文档集群的虚假信息检测新任务，并通过知识图谱和异构图神经网络实现事件级和文档级检测。",
    "tech_stack": [
      "知识图谱（KG）",
      "事件抽取（IE）",
      "跨文档事件共指消解",
      "异构图神经网络（GNN）",
      "生成式数据增强"
    ],
    "input_type": "一组主题相关的新闻文档集群",
    "output_type": "文档级和事件级的虚假信息检测结果",
    "source_paper_ids": [
      "ARR_2022_355"
    ],
    "pattern_ids": [
      "pattern_6"
    ]
  },
  {
    "idea_id": "idea_390",
    "description": "分析人们在有争议问题上意见分歧的根源，强调人类价值观差异对决策和争议的影响。",
    "tech_stack": [
      "人类价值观理论",
      "社会科学分析",
      "形式论证方法"
    ],
    "input_type": "关于个人或群体在争议性问题上的观点及其背后的价值观数据",
    "output_type": "不同价值观优先级及其对意见分歧的解释",
    "source_paper_ids": [
      "ARR_2022_357"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_391",
    "description": "提出了一个集成深度学习模型TEAM，首次在单一框架下同时实现taxonomy的attach和merge操作。",
    "tech_stack": [
      "多任务学习",
      "深度学习",
      "回归模型",
      "分类模型"
    ],
    "input_type": "待扩展的概念及现有WordNet本体结构（包括定义、同义词等）",
    "output_type": "针对每个新概念的扩展操作类型（attach/merge/无操作）及候选锚点的排序",
    "source_paper_ids": [
      "ARR_2022_358"
    ],
    "pattern_ids": [
      "pattern_12"
    ]
  },
  {
    "idea_id": "idea_392",
    "description": "提出了SpeechT5统一模态预训练框架，实现语音和文本的跨模态序列到序列转换。",
    "tech_stack": [
      "Transformer encoder-decoder",
      "相对位置嵌入",
      "向量量化",
      "去噪序列到序列预训练",
      "vocoder",
      "modal-specific pre/post-nets"
    ],
    "input_type": "语音或文本数据（原始语音波形、文本字符序列）",
    "output_type": "语音或文本输出（语音特征、文本字符序列、最终语音波形）",
    "source_paper_ids": [
      "ARR_2022_359"
    ],
    "pattern_ids": [
      "pattern_9"
    ]
  },
  {
    "idea_id": "idea_393",
    "description": "提出将文本和语音数据统一转换为国际音标表征，实现多模态低资源语言的NLP模型训练。",
    "tech_stack": [
      "国际音标(IPA)转写",
      "Allosaurus通用音素识别器",
      "Epitran字形到音素转换",
      "BERT风格Transformer模型"
    ],
    "input_type": "低资源语言的文本和语音数据",
    "output_type": "基于音标表征的语言模型及下游NLP任务模型结果",
    "source_paper_ids": [
      "ARR_2022_35"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_394",
    "description": "利用深度学习模型自动识别和分类美式手语视频中的六类音系特征。",
    "tech_stack": [
      "深度学习",
      "计算机视觉",
      "预训练模型",
      "视频特征提取",
      "监督学习"
    ],
    "input_type": "标注有音系属性的美式手语视频数据",
    "output_type": "每个视频对应的六类音系特征分类结果",
    "source_paper_ids": [
      "ARR_2022_360"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_395",
    "description": "针对现代希伯来语开发和评估预训练语言模型，解决资源稀缺和形态复杂性问题。",
    "tech_stack": [
      "BERT",
      "RoBERTa",
      "预训练语言模型",
      "上下文词表示",
      "序列标注",
      "分类头",
      "BIOSE标签"
    ],
    "input_type": "现代希伯来语文本数据，包括句子级和标注实体的语料库",
    "output_type": "句子情感分类结果和命名实体识别标签",
    "source_paper_ids": [
      "ARR_2022_361"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_396",
    "description": "提出了在加性晚期融合多模态模型中为不同模态分配专属学习率（MSLR）的方法以提升训练效果。",
    "tech_stack": [
      "多模态融合",
      "加性晚期融合",
      "Transformer",
      "卷积神经网络（CNN）",
      "Adam优化器",
      "Modality-Specific Learning Rate (MSLR)"
    ],
    "input_type": "多模态数据（如文本和视觉图像）",
    "output_type": "融合后的多模态特征表示或下游任务的预测结果",
    "source_paper_ids": [
      "ARR_2022_363"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_397",
    "description": "提出并自动化实现对财报文本段落进行细粒度XBRL标签注释，并发布大规模标注数据集。",
    "tech_stack": [
      "spaCy NER",
      "BiLSTM",
      "BERT",
      "Conditional Random Fields (CRF)",
      "word2vec",
      "Bloom Embeddings",
      "Residual CNN"
    ],
    "input_type": "公开公司财报中的文本段落（text notes）及句子级别的原始文本数据",
    "output_type": "带有细粒度XBRL实体标签的词级注释结果",
    "source_paper_ids": [
      "ARR_2022_36"
    ],
    "pattern_ids": [
      "pattern_7"
    ]
  },
  {
    "idea_id": "idea_398",
    "description": "提出基于互信息的命名实体识别方法，通过最大化有用信息和最小化冗余信息缓解OOV问题。",
    "tech_stack": [
      "互信息优化",
      "信息瓶颈原理",
      "SpanNER架构",
      "深度学习",
      "上下文嵌入"
    ],
    "input_type": "包含命名实体的非结构化文本数据",
    "output_type": "文本中实体的位置及类别的识别结果",
    "source_paper_ids": [
      "ARR_2022_37"
    ],
    "pattern_ids": [
      "pattern_7"
    ]
  },
  {
    "idea_id": "idea_399",
    "description": "提出一种结合时间敏感知识图谱嵌入和预训练语言模型的时序知识图谱问答框架。",
    "tech_stack": [
      "时序知识图谱嵌入",
      "预训练语言模型",
      "辅助时间顺序学习任务",
      "邻域图提取",
      "联合训练",
      "时间敏感对比学习"
    ],
    "input_type": "包含隐式时间表达的自由文本时序问题和时序知识图谱数据",
    "output_type": "实体或时间戳作为答案",
    "source_paper_ids": [
      "ARR_2022_38"
    ],
    "pattern_ids": [
      "pattern_26"
    ]
  },
  {
    "idea_id": "idea_400",
    "description": "提出了一种基于自回归空白填充的通用预训练框架GLM，统一处理NLU和生成任务。",
    "tech_stack": [
      "自回归空白填充",
      "Transformer",
      "层归一化重排",
      "单线性输出层",
      "GeLU激活函数"
    ],
    "input_type": "包含任务描述的文本及随机空白片段",
    "output_type": "顺序重构空白片段的文本",
    "source_paper_ids": [
      "ARR_2022_39"
    ],
    "pattern_ids": [
      "pattern_23"
    ]
  },
  {
    "idea_id": "idea_401",
    "description": "提出并评估了一种用于探测语言模型视觉常识能力的分析数据集和方法，并通过知识蒸馏提升文本模型表现。",
    "tech_stack": [
      "软提示调优（soft prompt tuning）",
      "知识蒸馏（knowledge distillation）",
      "视觉-语言模型（vision-language models）",
      "频率分布分析",
      "零样本探测（zero-shot probing）"
    ],
    "input_type": "文本和图像数据，涵盖颜色、形状、材质、大小和视觉共现等视觉属性关系",
    "output_type": "模型对视觉属性分布的预测结果，包括准确率和与真实分布的相关性",
    "source_paper_ids": [
      "ARR_2022_3"
    ],
    "pattern_ids": [
      "pattern_22"
    ]
  },
  {
    "idea_id": "idea_402",
    "description": "提出了一种从开放域对话平滑过渡到任务型对话的数据构建新流程，实现无目标对话到任务完成的自然转化。",
    "tech_stack": [
      "DistilBERT",
      "T5-small",
      "BlenderBot-400M",
      "AdamW优化器",
      "Adafactor优化器",
      "top-K采样",
      "top-p采样"
    ],
    "input_type": "开放域社交对话文本，用户无特定目标的对话场景",
    "output_type": "包含从社交对话到任务完成自然过渡的多轮对话数据集",
    "source_paper_ids": [
      "ARR_2022_40"
    ],
    "pattern_ids": [
      "pattern_25"
    ]
  },
  {
    "idea_id": "idea_403",
    "description": "提出动态对抗式数据收集（DADC）方法，通过生成模型辅助众包标注，提升NLP数据集的泛化能力。",
    "tech_stack": [
      "生成式模型",
      "动态对抗式数据收集",
      "众包标注",
      "问题过滤策略"
    ],
    "input_type": "自然语言处理任务相关的原始文本或问题数据",
    "output_type": "经过生成模型和人工标注优化的高质量数据集",
    "source_paper_ids": [
      "ARR_2022_41"
    ],
    "pattern_ids": [
      "pattern_15"
    ]
  },
  {
    "idea_id": "idea_404",
    "description": "提出了一种自动化方法，将英文任务型对话数据集全球化，支持多语言和本地实体，兼顾代码切换现象。",
    "tech_stack": [
      "对话模板抽取",
      "自动翻译与后编辑",
      "本地本体构建",
      "实体自动替换",
      "mBERT",
      "Transformer-DST",
      "数据增强"
    ],
    "input_type": "英文任务型对话数据集及目标语言本地实体本体",
    "output_type": "包含本地化实体和多语言支持的任务型对话数据集",
    "source_paper_ids": [
      "ARR_2022_42"
    ],
    "pattern_ids": [
      "pattern_25"
    ]
  },
  {
    "idea_id": "idea_405",
    "description": "提出了一种基于对比学习和多级蒸馏的增量式医疗意图检测方法，有效应对新意图类别和遗忘问题。",
    "tech_stack": [
      "BERT",
      "增量学习",
      "对比学习",
      "多级蒸馏",
      "记忆回放",
      "软最大分类器",
      "交叉熵损失"
    ],
    "input_type": "包含查询文本及其意图标签的医疗问答数据，支持新类别持续加入。",
    "output_type": "针对每个输入查询的医疗意图类别分类结果。",
    "source_paper_ids": [
      "ARR_2022_43"
    ],
    "pattern_ids": [
      "pattern_16"
    ]
  },
  {
    "idea_id": "idea_406",
    "description": "提出在生成式开放域问答模型中集成指针网络以提升答案的事实一致性和准确性。",
    "tech_stack": [
      "开放域问答（ODQA）",
      "生成式模型",
      "FiD模型",
      "指针网络（Pointer Network）",
      "预训练编码器-解码器模型（如T5, BART）",
      "检索-阅读器框架"
    ],
    "input_type": "自然语言问题及其检索到的相关文本段落",
    "output_type": "针对输入问题生成的自然语言答案",
    "source_paper_ids": [
      "ARR_2022_45"
    ],
    "pattern_ids": [
      "pattern_21"
    ]
  },
  {
    "idea_id": "idea_407",
    "description": "提出跨文化NLP挑战与机遇的分析框架，并总结现有策略与未来方向。",
    "tech_stack": [
      "公平性优化",
      "群体鲁棒优化",
      "多语言建模",
      "数据选择与注释",
      "模型迁移"
    ],
    "input_type": "多语言和多文化的文本数据及相关属性",
    "output_type": "提升文化敏感性和公平性的NLP模型及分析框架",
    "source_paper_ids": [
      "ARR_2022_46"
    ],
    "pattern_ids": [
      "pattern_4"
    ]
  },
  {
    "idea_id": "idea_408",
    "description": "提出了一种结合标签嵌入和对比学习的新型有监督对比学习方法LaCon，用于提升分类任务表现。",
    "tech_stack": [
      "对比学习",
      "有监督对比学习",
      "标签嵌入",
      "数据增强",
      "标签中心对比损失",
      "实例中心对比损失",
      "正则化损失"
    ],
    "input_type": "多类别分类任务中的文本数据及其对应标签",
    "output_type": "判别性实例和标签的嵌入表示，用于提升分类性能",
    "source_paper_ids": [
      "ARR_2022_47"
    ],
    "pattern_ids": [
      "pattern_16"
    ]
  },
  {
    "idea_id": "idea_409",
    "description": "提出了一种基于情感词定位与动态修正的多模态情感分析模型，提升ASR错误环境下的情感识别鲁棒性。",
    "tech_stack": [
      "多模态信息融合",
      "情感词定位",
      "动态词嵌入修正",
      "ASR错误分析",
      "深度学习"
    ],
    "input_type": "包含文本（含ASR识别错误）、音频和视觉特征的多模态数据",
    "output_type": "情感极性或情感标签的预测结果",
    "source_paper_ids": [
      "ARR_2022_48"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_410",
    "description": "提出了将所有包含实体对的句子拼接为段落整体编码，并用关系感知注意力进行关系抽取的新基线模型PARE。",
    "tech_stack": [
      "BERT",
      "mBERT",
      "关系感知注意力",
      "段落级编码",
      "远程监督",
      "AUC评估"
    ],
    "input_type": "包含实体对(e1, e2)的句子集合（bag），即所有提及该实体对的句子",
    "output_type": "实体对之间的关系预测标签或概率（是否存在某种关系）",
    "source_paper_ids": [
      "ARR_2022_49"
    ],
    "pattern_ids": [
      "pattern_28"
    ]
  },
  {
    "idea_id": "idea_411",
    "description": "提出并系统研究了通过添加语义相似的对抗性推文（concatenation attack）攻击金融文本预测模型的新方法。",
    "tech_stack": [
      "深度学习语言模型",
      "对抗性攻击",
      "文本级对抗扰动",
      "语义相似性计算",
      "推文过滤与注入"
    ],
    "input_type": "社交媒体平台（如Twitter）上的原始与对抗性推文文本数据",
    "output_type": "金融预测模型的预测结果（如股票涨跌方向或情感分类）",
    "source_paper_ids": [
      "ARR_2022_4"
    ],
    "pattern_ids": [
      "pattern_19"
    ]
  },
  {
    "idea_id": "idea_412",
    "description": "提出多种半监督贝叶斯分词模型，利用已有语言材料提升极低资源语言的自动分割效果，助力语言学田野文献工作。",
    "tech_stack": [
      "贝叶斯非参数分词模型",
      "半监督学习",
      "Gibbs采样",
      "模拟退火",
      "Dirichlet过程",
      "Pitman-Yor过程"
    ],
    "input_type": "极低资源语言的未分词语音或正字法字符串及部分词表/分词信息",
    "output_type": "自动分割的有意义语言单元（如词或形态单位）",
    "source_paper_ids": [
      "ARR_2022_50"
    ],
    "pattern_ids": [
      "pattern_20"
    ]
  },
  {
    "idea_id": "idea_413",
    "description": "提出了一种基于自动词对齐的词级对比学习方法用于多对多神经机器翻译。",
    "tech_stack": [
      "多对多神经机器翻译",
      "自动词对齐",
      "词级对比学习",
      "BLEU评测"
    ],
    "input_type": "多语言平行语料及自动提取的词对齐信息",
    "output_type": "提升的多对多翻译质量（如BLEU分数）",
    "source_paper_ids": [
      "ARR_2022_51"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_414",
    "description": "提出针对列操作的Text-to-SQL领域泛化评测基准，并通过schema扩展与剪枝提升解析器的泛化能力。",
    "tech_stack": [
      "神经语义解析",
      "预训练语言模型",
      "schema expansion",
      "schema pruning",
      "合成数据集",
      "数据集重分区"
    ],
    "input_type": "自然语言问题及表格结构数据",
    "output_type": "可执行的SQL查询语句",
    "source_paper_ids": [
      "ARR_2022_52"
    ],
    "pattern_ids": [
      "pattern_2"
    ]
  },
  {
    "idea_id": "idea_415",
    "description": "提出将编码器-解码器模型分解为仅解码器语言模型以加速文档重排序推断。",
    "tech_stack": [
      "Transformer",
      "Encoder-Decoder架构",
      "Decoder-only语言模型",
      "多任务损失",
      "预计算记忆存储",
      "注意力机制"
    ],
    "input_type": "查询-文档对（query-document pair）",
    "output_type": "文档对相关性评分（如生成查询的概率）",
    "source_paper_ids": [
      "ARR_2022_53"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_416",
    "description": "提出并理论分析影响回译性能的合成数据质量与重要性权重，并提出平衡两者的新数据生成方法。",
    "tech_stack": [
      "神经机器翻译",
      "回译",
      "半监督学习",
      "理论下界推导",
      "启发式数据生成"
    ],
    "input_type": "单语语料与神经机器翻译模型",
    "output_type": "优化后的合成双语语料与提升的翻译性能",
    "source_paper_ids": [
      "ARR_2022_54"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_417",
    "description": "提出了一种基于掩码语言模型的双语对抗样本生成方法，用于改进神经机器翻译的对抗鲁棒性评估。",
    "tech_stack": [
      "神经机器翻译（NMT）",
      "掩码语言模型（MLM）",
      "Transformer",
      "Fairseq",
      "BLEU分数",
      "回译（Round-Trip Translation, RTT）"
    ],
    "input_type": "双语平行句对及其可扰动的源语言文本",
    "output_type": "生成的双语对抗样本及其对NMT模型鲁棒性的评估结果",
    "source_paper_ids": [
      "ARR_2022_55"
    ],
    "pattern_ids": [
      "pattern_19"
    ]
  },
  {
    "idea_id": "idea_418",
    "description": "提出UCTOPIC，无监督对比学习框架用于上下文感知短语表示和主题挖掘。",
    "tech_stack": [
      "无监督对比学习",
      "短语表示学习",
      "上下文感知嵌入",
      "数据增强",
      "主题建模"
    ],
    "input_type": "包含短语的文本语料库或文档集合",
    "output_type": "高质量短语嵌入和主题挖掘结果",
    "source_paper_ids": [
      "ARR_2022_56"
    ],
    "pattern_ids": [
      "pattern_10"
    ]
  },
  {
    "idea_id": "idea_419",
    "description": "提出ElitePLM框架，系统评估预训练语言模型的多维语言能力。",
    "tech_stack": [
      "Transformer",
      "预训练语言模型",
      "能力维度评估",
      "基准测试",
      "huggingface",
      "fairseq",
      "jiant"
    ],
    "input_type": "多种公开预训练语言模型及其在代表性NLP任务上的表现数据",
    "output_type": "各模型在记忆、理解、推理、写作四大能力维度上的定量评估结果",
    "source_paper_ids": [
      "ARR_2022_57"
    ],
    "pattern_ids": [
      "pattern_23"
    ]
  },
  {
    "idea_id": "idea_420",
    "description": "提出通过在主流任务型对话数据集上增加消歧回合，提升系统理解用户对数据库多结果选择的能力。",
    "tech_stack": [
      "任务型对话系统",
      "数据集增强",
      "GPT2微调",
      "消歧任务",
      "多轮对话建模"
    ],
    "input_type": "包含数据库检索结果歧义的任务型对话数据",
    "output_type": "能够理解并处理用户对多结果选择意图的对话系统响应",
    "source_paper_ids": [
      "ARR_2022_58"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_421",
    "description": "提出THE-X框架，实现基于同态加密的隐私保护Transformer推理服务。",
    "tech_stack": [
      "同态加密",
      "Transformer模型",
      "预训练语言模型",
      "差分隐私",
      "联邦学习"
    ],
    "input_type": "用户敏感文本数据（如医疗记录、搜索历史等）",
    "output_type": "加密推理结果，仅可由用户私钥解密",
    "source_paper_ids": [
      "ARR_2022_59"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_422",
    "description": "提出FRUIT任务，实现基于外部新知识对现有文本的忠实更新。",
    "tech_stack": [
      "序列到序列模型（T5）",
      "EDIT5编辑生成模型",
      "AdaFactor优化器",
      "TPU分布式训练"
    ],
    "input_type": "过时的维基百科文章及相关新信息（文本和表格）",
    "output_type": "与新信息一致并反映新事实的更新后文本",
    "source_paper_ids": [
      "ARR_2022_5"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_423",
    "description": "提出增强表格建模中数值推理能力的新方法，提升对表格中数值关系的理解和推断。",
    "tech_stack": [
      "表格预训练",
      "自监督学习",
      "Masked Language Model (MLM)",
      "单元格填空与错误检测",
      "表格-文本匹配与对齐"
    ],
    "input_type": "结构化或半结构化表格数据，包含数值型单元格及相关上下文",
    "output_type": "对表格中数值单元格的语义理解、数值关系推理结果或相关任务预测",
    "source_paper_ids": [
      "ARR_2022_62"
    ],
    "pattern_ids": [
      "pattern_2"
    ]
  },
  {
    "idea_id": "idea_424",
    "description": "提出了一种通过自然语言指令实现多任务并行和灵活学习的Plug-and-Play任务型对话系统（PPTOD）。",
    "tech_stack": [
      "预训练语言模型（PLM）",
      "端到端对话建模",
      "多任务学习",
      "自然语言指令（Prompt）",
      "in-context learning"
    ],
    "input_type": "包含对话上下文和任务特定自然语言指令的文本数据",
    "output_type": "对话状态、系统动作和自然语言响应等多任务生成结果",
    "source_paper_ids": [
      "ARR_2022_63"
    ],
    "pattern_ids": [
      "pattern_25"
    ]
  },
  {
    "idea_id": "idea_425",
    "description": "提出并构建了首个多领域中文谓词-论元结构数据集MuPAD，并基于多任务学习和BERT提升跨领域语义角色标注性能。",
    "tech_stack": [
      "多任务学习（MTL）",
      "BERT",
      "序列标注",
      "深度学习",
      "预训练语言模型"
    ],
    "input_type": "多领域中文句子及其谓词-论元结构标注数据",
    "output_type": "每个词的语义角色标签序列",
    "source_paper_ids": [
      "ARR_2022_65"
    ],
    "pattern_ids": [
      "pattern_17"
    ]
  },
  {
    "idea_id": "idea_426",
    "description": "通过融合岗位标签构建异构图，提升职位表示学习的准确性和鲁棒性。",
    "tech_stack": [
      "异构图建模",
      "网络嵌入",
      "标签增强",
      "节点表示学习"
    ],
    "input_type": "包含职位名称及其相关标签（责任、功能等）的招聘数据",
    "output_type": "低维度的职位表示向量",
    "source_paper_ids": [
      "ARR_2022_66"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_427",
    "description": "首次系统性评估并提出方法检测词义嵌入中的社会偏见，包括静态和上下文敏感的sense embedding。",
    "tech_stack": [
      "静态词义嵌入（LMMS, ARES）",
      "上下文敏感词义嵌入（SenseBERT）",
      "社会偏见评测基准扩展",
      "Sense-Sensitive Social Bias (SSSB) 数据集"
    ],
    "input_type": "多义词的词义嵌入向量及含有社会偏见的语料或句子",
    "output_type": "对词义嵌入中社会偏见的定量评估结果",
    "source_paper_ids": [
      "ARR_2022_67"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_428",
    "description": "系统性比较和分析八类预训练语言模型在多种探测任务中的语言能力表现，揭示模型差异来源。",
    "tech_stack": [
      "Transformer架构",
      "预训练语言模型",
      "探测任务(Probing Tasks)",
      "零样本评估(Zero-shot Evaluation)",
      "模型蒸馏"
    ],
    "input_type": "多种预训练语言模型及其在oLMpics和心理语言学探测任务上的输入数据",
    "output_type": "不同模型在各类语言能力探测任务上的表现结果和能力分析",
    "source_paper_ids": [
      "ARR_2022_69"
    ],
    "pattern_ids": [
      "pattern_23"
    ]
  },
  {
    "idea_id": "idea_429",
    "description": "提出并分析NLP领域的SQUARE ONE BIAS，呼吁多维度综合研究以克服单一创新偏向。",
    "tech_stack": [
      "问卷调查",
      "注释实验",
      "多维度贡献分析"
    ],
    "input_type": "NLP实验及相关论文的特征和维度数据",
    "output_type": "对NLP研究偏向的定性与定量分析结果",
    "source_paper_ids": [
      "ARR_2022_6"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_430",
    "description": "提出VALSE基准，用于评估预训练视觉与语言模型对多种语言现象的敏感性。",
    "tech_stack": [
      "预训练视觉与语言模型",
      "自动与人工验证",
      "NLI过滤",
      "Masked Language Modeling (MLM)",
      "语义推理",
      "困惑度计算"
    ],
    "input_type": "视觉输入（图片）和对应真实及篡改后的文本描述（caption foils）",
    "output_type": "模型区分真实描述与篡改描述的能力评估结果",
    "source_paper_ids": [
      "ARR_2022_70"
    ],
    "pattern_ids": [
      "pattern_22"
    ]
  },
  {
    "idea_id": "idea_431",
    "description": "提出CARETS测试套件，系统评估VQA模型在六大能力上的一致性与鲁棒性。",
    "tech_stack": [
      "VQA基准测试",
      "场景图填充",
      "模板生成",
      "自一致性度量",
      "综合准确率",
      "Faster R-CNN",
      "ResNeXt-152",
      "MMF库",
      "LXMERT模型"
    ],
    "input_type": "图像与自然语言问题对，包含系统生成的多样化变体",
    "output_type": "模型在各能力测试上的准确率、自一致性和综合准确率等评估指标",
    "source_paper_ids": [
      "ARR_2022_72"
    ],
    "pattern_ids": [
      "pattern_22"
    ]
  },
  {
    "idea_id": "idea_432",
    "description": "提出了一种数据驱动的自动为现代希伯来语无元音文本加注元音符号（点读）的新方法。",
    "tech_stack": [
      "数据驱动算法",
      "神经网络",
      "形态分析",
      "手工词典",
      "POS标注"
    ],
    "input_type": "现代希伯来语无元音（undotted）文本",
    "output_type": "自动加注元音符号（点读）的希伯来语文本",
    "source_paper_ids": [
      "ARR_2022_73"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_433",
    "description": "提出了一种基于鲁棒密度估计的新方法用于检测NLP中的多类型对抗样本。",
    "tech_stack": [
      "鲁棒密度估计（RDE）",
      "kPCA",
      "MCD",
      "最大似然估计（MLE）",
      "词频分析（FGWS）",
      "GPT-2困惑度（Perplexity）"
    ],
    "input_type": "文本数据，包括正常样本和多种攻击生成的对抗样本",
    "output_type": "对输入样本是否为对抗样本的检测结果",
    "source_paper_ids": [
      "ARR_2022_74"
    ],
    "pattern_ids": [
      "pattern_19"
    ]
  },
  {
    "idea_id": "idea_434",
    "description": "提出了一种通过语义-句法解耦的Siamese模型提升低资源语言多语种MRC边界检测精度的新方法。",
    "tech_stack": [
      "多语种预训练语言模型（PLM）",
      "Siamese语义解耦模块（S2DM）",
      "von Mises-Fisher分布",
      "高斯分布",
      "零样本跨语言迁移",
      "句法约束"
    ],
    "input_type": "多语种机器阅读理解（MRC）任务中的问题-段落对，包含源语言和目标语言的平行数据",
    "output_type": "目标语言中问题对应的精确答案片段（span）",
    "source_paper_ids": [
      "ARR_2022_75"
    ],
    "pattern_ids": [
      "pattern_20"
    ]
  },
  {
    "idea_id": "idea_435",
    "description": "系统性分析实体集合扩展方法在低资源用户生成文本中的泛化能力，并构建新基准。",
    "tech_stack": [
      "AutoPhrase",
      "SetExpan",
      "BERT",
      "CGExpan",
      "Hearst patterns",
      "语境嵌入",
      "语言模型探测"
    ],
    "input_type": "包含种子实体的用户生成文本语料（如评论），以及待扩展的实体集合任务",
    "output_type": "扩展后的实体集合，按与种子实体语义相关性排序",
    "source_paper_ids": [
      "ARR_2022_76"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_436",
    "description": "论文研究了图像领域对抗攻击检测方法在NLP任务中的可迁移性，并提出了针对NLP的检测方法。",
    "tech_stack": [
      "对抗攻击检测",
      "模型修改",
      "对抗训练",
      "输入扰动分析",
      "NLP任务专用检测算法"
    ],
    "input_type": "自然语言处理任务中的离散、序列化文本输入",
    "output_type": "是否为对抗攻击的检测结果",
    "source_paper_ids": [
      "ARR_2022_77"
    ],
    "pattern_ids": [
      "pattern_19"
    ]
  },
  {
    "idea_id": "idea_437",
    "description": "提出一种基于提示机制的无监督置信度估计方法，用于提升神经机器翻译模型的预测不确定性刻画。",
    "tech_stack": [
      "置信度估计",
      "无监督学习",
      "提示机制（Ask For Hints）",
      "置信度网络",
      "多层解码器隐藏状态",
      "置信度驱动标签平滑"
    ],
    "input_type": "神经机器翻译任务中的源语言文本及模型解码器隐藏状态",
    "output_type": "每个翻译预测的置信度估计值及置信度调整后的标签分布",
    "source_paper_ids": [
      "ARR_2022_78"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_438",
    "description": "提出跨模态共享离散嵌入空间及代码匹配目标，实现可解释的多模态表示学习。",
    "tech_stack": [
      "跨模态表示学习",
      "共享离散嵌入空间",
      "CrossModal Code Matching (CMCM)",
      "高层嵌入向量",
      "无监督学习"
    ],
    "input_type": "多模态数据对，如视频-文本、视频-音频、图像-音频",
    "output_type": "共享离散嵌入空间中的可解释嵌入向量及跨模态检索结果",
    "source_paper_ids": [
      "ARR_2022_79"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_439",
    "description": "提出了一种无需监督数据的预训练密集检索模型LaPraDoR，实现了高效且强泛化能力的零样本文本检索。",
    "tech_stack": [
      "密集向量检索",
      "无监督预训练",
      "对比学习",
      "Iterative Contrastive Learning (ICoL)",
      "ANN检索库（如FAISS）"
    ],
    "input_type": "查询文本和待检索文档集合",
    "output_type": "与查询相关的文档列表及其相关性得分",
    "source_paper_ids": [
      "ARR_2022_7"
    ],
    "pattern_ids": [
      "pattern_10"
    ]
  },
  {
    "idea_id": "idea_440",
    "description": "提出多任务学习框架同时评分作文整体分数和各作文特征分数，并分析其互助作用。",
    "tech_stack": [
      "多任务学习",
      "神经网络",
      "词嵌入",
      "均方误差损失函数",
      "五折交叉验证"
    ],
    "input_type": "包含多种作文特征的学生作文文本及评分任务",
    "output_type": "作文整体分数及各作文特征分数",
    "source_paper_ids": [
      "ARR_2022_80"
    ],
    "pattern_ids": [
      "pattern_13"
    ]
  },
  {
    "idea_id": "idea_441",
    "description": "提出了多模态对话状态跟踪（MM-DST）任务，并设计了结合视频和对话信息的神经网络架构VDTN。",
    "tech_stack": [
      "多模态对话状态跟踪（MM-DST）",
      "VideoDialogue Transformer Network (VDTN)",
      "对象级与片段级视频特征融合",
      "Markov决策过程解码策略"
    ],
    "input_type": "带有视频输入和多轮问答对的视觉基础对话数据",
    "output_type": "包含视觉对象及属性的多模态对话状态（slots和slot values）",
    "source_paper_ids": [
      "ARR_2022_81"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_442",
    "description": "提出并系统评估多种序列标注任务中NER数据不平衡的重采样方法。",
    "tech_stack": [
      "数据重采样",
      "条件随机场（CRF）",
      "数据增强",
      "Focal Loss",
      "Dice Loss",
      "预训练词嵌入"
    ],
    "input_type": "带有实体标签的文本序列数据",
    "output_type": "命名实体识别的标签序列",
    "source_paper_ids": [
      "ARR_2022_83"
    ],
    "pattern_ids": [
      "pattern_7"
    ]
  },
  {
    "idea_id": "idea_443",
    "description": "提出了一种去偏差的对比学习框架，通过噪声负样本生成和实例加权缓解负样本采样偏差，提升无监督句子表示学习效果。",
    "tech_stack": [
      "对比学习",
      "预训练语言模型（PLM）",
      "噪声负样本生成",
      "实例加权",
      "数据增强"
    ],
    "input_type": "无标签的句子文本数据，用于无监督句子表示学习",
    "output_type": "高质量的句子向量表示，可用于下游NLP任务",
    "source_paper_ids": [
      "ARR_2022_84"
    ],
    "pattern_ids": [
      "pattern_10"
    ]
  },
  {
    "idea_id": "idea_444",
    "description": "提出利用ICD代码同义词并通过多同义词匹配网络提升自动ICD编码的准确性。",
    "tech_stack": [
      "LSTM编码器",
      "多同义词注意力机制",
      "多头注意力",
      "Biaffine相似度",
      "多标签分类"
    ],
    "input_type": "电子病历（EMR）中的自由文本（如出院小结）",
    "output_type": "每个ICD代码的二元标签（是否分配该代码）",
    "source_paper_ids": [
      "ARR_2022_86"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_445",
    "description": "利用从半结构化表格自动生成的多样化推理训练数据提升语言模型的推理能力。",
    "tech_stack": [
      "预训练语言模型",
      "模板生成",
      "多任务训练",
      "自动数据生成",
      "错误驱动采样"
    ],
    "input_type": "半结构化表格及由模板生成的问题-上下文-答案三元组",
    "output_type": "具备多种推理能力的语言模型",
    "source_paper_ids": [
      "ARR_2022_87"
    ],
    "pattern_ids": [
      "pattern_2"
    ]
  },
  {
    "idea_id": "idea_446",
    "description": "提出了一种结构信息增强的语法控制释义生成模型，通过注意力机制提升释义多样性和结构兼容性。",
    "tech_stack": [
      "注意力机制",
      "结构信息增强",
      "语法控制生成",
      "句法分析树编码"
    ],
    "input_type": "原始句子及指定的句法结构信息",
    "output_type": "满足指定语法结构的多样化释义句",
    "source_paper_ids": [
      "ARR_2022_88"
    ],
    "pattern_ids": [
      "pattern_21"
    ]
  },
  {
    "idea_id": "idea_447",
    "description": "提出LITE多任务学习框架，通过多种辅助任务对待办事项文本进行统一表征学习。",
    "tech_stack": [
      "多任务学习（MTL）",
      "预训练语言模型",
      "上下文表示学习",
      "多头注意力",
      "辅助任务训练",
      "弱监督学习"
    ],
    "input_type": "待办事项文本及其列表名称",
    "output_type": "待办事项的实值向量表示",
    "source_paper_ids": [
      "ARR_2022_89"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_448",
    "description": "提出一种结合少量语义监督的自监督学习方法，用于自动学习语音的音素离散表示。",
    "tech_stack": [
      "自监督学习",
      "CPC模型",
      "VQ-VAE",
      "Dirichlet分布初始化",
      "EMA代码本更新",
      "Adam优化器"
    ],
    "input_type": "未标注或弱标注的连续语音信号及部分语义监督信息（如同词异词对）",
    "output_type": "离散化的音素级语音表示（音素inventory或音素序列）",
    "source_paper_ids": [
      "ARR_2022_8"
    ],
    "pattern_ids": [
      "pattern_9"
    ]
  },
  {
    "idea_id": "idea_449",
    "description": "系统性研究更强视觉模型（如ViT）及多种视觉特征在多模态机器翻译中的作用，并提出选择性注意力机制以细粒度关联图像与文本。",
    "tech_stack": [
      "Vision Transformer (ViT)",
      "选择性注意力机制",
      "对象检测特征",
      "图像描述特征",
      "多模态机器翻译",
      "ResNet-50"
    ],
    "input_type": "包含文本和对应图像的多模态翻译数据",
    "output_type": "翻译文本结果及对视觉信息贡献的细粒度分析",
    "source_paper_ids": [
      "ARR_2022_90"
    ],
    "pattern_ids": [
      "pattern_22"
    ]
  },
  {
    "idea_id": "idea_450",
    "description": "提出基于真人实时对话评估的开放域对话系统评价方法，避免参考答案依赖并提升评估可靠性。",
    "tech_stack": [
      "真人实时对话评估",
      "严格质量控制众包",
      "分数标准化",
      "自复制实验相关性分析"
    ],
    "input_type": "开放域对话模型与人类进行的实时对话数据",
    "output_type": "对话模型的标准化评分及排名",
    "source_paper_ids": [
      "ARR_2022_93"
    ],
    "pattern_ids": [
      "pattern_13"
    ]
  },
  {
    "idea_id": "idea_451",
    "description": "提出结合预训练语言模型的内外字符串表示，通过自举和自训练方法提升无监督句法分析性能。",
    "tech_stack": [
      "预训练语言模型",
      "内外字符串表示",
      "自举(seed bootstrapping)",
      "自训练(self-training)",
      "序列分类模型",
      "句法分析"
    ],
    "input_type": "未标注的自然语言句子及其可能的句法跨度(span)",
    "output_type": "每个句法跨度属于句法树的概率评分或标签",
    "source_paper_ids": [
      "ARR_2022_94"
    ],
    "pattern_ids": [
      "pattern_20"
    ]
  },
  {
    "idea_id": "idea_452",
    "description": "提出并构建了针对叙事理解、细分阅读理解子技能的高质量教育型问答数据集FairytaleQA。",
    "tech_stack": [
      "Rouge-L F1",
      "BART",
      "规则生成",
      "排序模型",
      "自动问答",
      "自动问题生成"
    ],
    "input_type": "叙事类童话故事文本及相关阅读理解子技能标签",
    "output_type": "细分子技能的问答对或自动生成的问题-答案对",
    "source_paper_ids": [
      "ARR_2022_95"
    ],
    "pattern_ids": [
      "pattern_15"
    ]
  },
  {
    "idea_id": "idea_453",
    "description": "提出了一种基于无标签样本预测分布的早停方法（BUS-stop），无需验证集即可有效防止过拟合。",
    "tech_stack": [
      "Early Stopping",
      "无监督学习",
      "预测概率分布",
      "类分布匹配",
      "梯度统计",
      "Local Intrinsic Dimensionality (LID)"
    ],
    "input_type": "带有少量标签样本和大量无标签样本的分类训练数据",
    "output_type": "模型训练的最优停止时刻（epoch）",
    "source_paper_ids": [
      "ARR_2022_96"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_454",
    "description": "提出了一种分层递归聚合生成（HRAG）模型，针对概念到文本的自然语言生成任务分阶段优化迁移学习效果。",
    "tech_stack": [
      "分层递归生成模型",
      "迁移学习",
      "预训练语言模型（PLMs）",
      "端到端神经网络",
      "少样本学习",
      "零样本学习"
    ],
    "input_type": "结构化的机器可读意义表示（Meaning Representation, MR）",
    "output_type": "描述输入语义内容的自然语言文本",
    "source_paper_ids": [
      "ARR_2022_97"
    ],
    "pattern_ids": [
      "pattern_5"
    ]
  },
  {
    "idea_id": "idea_455",
    "description": "提出GRS方法，将受控释义作为显式编辑操作引入无监督句子简化的迭代修订框架。",
    "tech_stack": [
      "Seq2Seq transformer",
      "lexically-constrained decoding",
      "complex component detector",
      "paraphrasing corpora"
    ],
    "input_type": "复杂句子文本，无需复杂-简化句对",
    "output_type": "简化后的句子及编辑操作序列",
    "source_paper_ids": [
      "ARR_2022_98"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_456",
    "description": "首次系统比较Transformer模型与人类眼动注意力及启发式阅读模型在任务型和自然阅读中的对齐程度。",
    "tech_stack": [
      "Transformer模型（BERT, RoBERTa, T5）",
      "Attention Flow",
      "E-Z Reader启发式模型",
      "眼动追踪数据分析",
      "输入减少实验",
      "词可预测性分析",
      "POS标签分析"
    ],
    "input_type": "任务型和自然英语阅读文本及对应的眼动追踪数据",
    "output_type": "模型注意力与人类眼动注意力的相关性分析结果及不同模型在任务分类上的表现",
    "source_paper_ids": [
      "ARR_2022_99"
    ],
    "pattern_ids": [
      "pattern_23"
    ]
  },
  {
    "idea_id": "idea_457",
    "description": "提出基于模型输入特征敏感性的累积预测敏感性指标，统一衡量和关联群体公平性、个体公平性及人类公平性感知。",
    "tech_stack": [
      "累积预测敏感性（Accumulated Prediction Sensitivity）",
      "特征归因（Feature Attribution）",
      "保护属性分类器（Protected Status Model）",
      "统计公平性指标（Statistical Parity）",
      "个体公平性指标（Individual Fairness）"
    ],
    "input_type": "包含保护属性（如性别、年龄等）和文本特征的数据集，适用于分类模型",
    "output_type": "模型预测对输入特征的敏感性度量值及其与公平性相关的评估结果",
    "source_paper_ids": [
      "ARR_2022_9"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_458",
    "description": "提出面向UGC的机器翻译鲁棒性现象级数据集PheMT",
    "tech_stack": [
      "神经机器翻译",
      "数据集构建",
      "鲁棒性评估"
    ],
    "input_type": "用户生成内容文本",
    "output_type": "现象级标注的翻译数据集与鲁棒性评测结果",
    "source_paper_ids": [
      "COLING_2020_0"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_459",
    "description": "提升指称表达生成模型对未见实体的泛化能力",
    "tech_stack": [
      "数据到文本生成",
      "指称表达生成",
      "泛化建模"
    ],
    "input_type": "结构化非语言数据及实体信息",
    "output_type": "针对实体的自然语言指称短语",
    "source_paper_ids": [
      "COLING_2020_10"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_460",
    "description": "提出基于锚点的自动文档摘要评价指标，提升评价准确性。",
    "tech_stack": [
      "锚点检测",
      "自动评价算法",
      "文本相似度计算"
    ],
    "input_type": "系统生成摘要与原始文档",
    "output_type": "摘要质量自动评分",
    "source_paper_ids": [
      "COLING_2020_11"
    ],
    "pattern_ids": [
      "pattern_13"
    ]
  },
  {
    "idea_id": "idea_461",
    "description": "探索个性化词嵌入对提升个体文本理解的价值",
    "tech_stack": [
      "词嵌入",
      "自然语言处理",
      "个性化建模"
    ],
    "input_type": "多作者或个人文本语料",
    "output_type": "针对个体优化的词嵌入表示",
    "source_paper_ids": [
      "COLING_2020_12"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_462",
    "description": "构建面向事件的社交媒体推文摘要数据集TWEETSUM",
    "tech_stack": [
      "数据集构建",
      "社交媒体分析",
      "事件驱动摘要"
    ],
    "input_type": "社交媒体推文及事件相关信息",
    "output_type": "事件导向的推文摘要数据集",
    "source_paper_ids": [
      "COLING_2020_13"
    ],
    "pattern_ids": [
      "pattern_27"
    ]
  },
  {
    "idea_id": "idea_463",
    "description": "提出用带理由的众包标注来衡量语义歧义和分歧信息。",
    "tech_stack": [
      "众包标注",
      "语义分析",
      "分歧建模"
    ],
    "input_type": "文本语义标注任务及标注者意见",
    "output_type": "包含分歧理由的多样化标注数据",
    "source_paper_ids": [
      "COLING_2020_14"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_464",
    "description": "融合多种异构句法知识提升语义角色标注性能",
    "tech_stack": [
      "深度学习",
      "句法分析",
      "特征融合"
    ],
    "input_type": "文本句子或语段",
    "output_type": "谓词-论元结构及语义角色标签",
    "source_paper_ids": [
      "COLING_2020_15"
    ],
    "pattern_ids": [
      "pattern_17"
    ]
  },
  {
    "idea_id": "idea_465",
    "description": "利用预训练语言模型提升多标签文本分类性能",
    "tech_stack": [
      "预训练语言模型",
      "多标签分类",
      "迁移学习"
    ],
    "input_type": "文本数据，含多个可能标签",
    "output_type": "每条文本对应的多个预测标签",
    "source_paper_ids": [
      "COLING_2020_16"
    ],
    "pattern_ids": [
      "pattern_8"
    ]
  },
  {
    "idea_id": "idea_466",
    "description": "构建并分析人类与模型物体命名行为的新数据集",
    "tech_stack": [
      "数据集收集",
      "语言分析",
      "模型对比"
    ],
    "input_type": "物体图片及命名数据",
    "output_type": "命名多样性分析与人机命名一致性评估",
    "source_paper_ids": [
      "COLING_2020_17"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_467",
    "description": "统一不同语义图库的结构以便比较和集成",
    "tech_stack": [
      "语义图表示",
      "结构归一化",
      "跨库映射算法"
    ],
    "input_type": "多种语义图库中的句子语义图",
    "output_type": "结构归一化后的统一语义图表示",
    "source_paper_ids": [
      "COLING_2020_18"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_468",
    "description": "提出二阶无监督神经依存句法分析方法，提升解析准确率",
    "tech_stack": [
      "神经网络",
      "无监督学习",
      "二阶依存关系建模"
    ],
    "input_type": "未标注的自然语言句子",
    "output_type": "依存句法树结构",
    "source_paper_ids": [
      "COLING_2020_19"
    ],
    "pattern_ids": [
      "pattern_17"
    ]
  },
  {
    "idea_id": "idea_469",
    "description": "跨段落分层记忆网络提升生成式评论问答效果",
    "tech_stack": [
      "分层记忆网络",
      "跨段落信息融合",
      "生成式问答模型"
    ],
    "input_type": "多段评论文本与问题",
    "output_type": "针对问题的生成式答案",
    "source_paper_ids": [
      "COLING_2020_1"
    ],
    "pattern_ids": [
      "pattern_25"
    ]
  },
  {
    "idea_id": "idea_470",
    "description": "根据语境中指称对象的显著性选择合适的指称表达特征集。",
    "tech_stack": [
      "语料库分析",
      "特征选择",
      "自然语言处理"
    ],
    "input_type": "文本语境与指称对象信息",
    "output_type": "生成的最优指称表达特征集",
    "source_paper_ids": [
      "COLING_2020_20"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_471",
    "description": "构建播客语音数据集，促进语音与语言技术研究",
    "tech_stack": [
      "自动语音识别",
      "语料库构建",
      "语音处理"
    ],
    "input_type": "播客音频数据",
    "output_type": "转录文本及相关语料库",
    "source_paper_ids": [
      "COLING_2020_21"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_472",
    "description": "提出人机对话中模态表达的双层解释框架",
    "tech_stack": [
      "语义解析",
      "模态推理",
      "人机交互建模"
    ],
    "input_type": "自然语言对话文本",
    "output_type": "模态表达的语义解释与环境对齐",
    "source_paper_ids": [
      "COLING_2020_22"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_473",
    "description": "预训练生物医学语言模型提升问答与信息检索效果",
    "tech_stack": [
      "BERT",
      "预训练模型",
      "自然语言处理"
    ],
    "input_type": "生物医学领域文本数据",
    "output_type": "高质量问答结果或相关文献检索结果",
    "source_paper_ids": [
      "COLING_2020_23"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_474",
    "description": "针对词级语义相似性任务对无监督预训练模型进行专门化优化",
    "tech_stack": [
      "无监督预训练模型",
      "BERT",
      "语言建模"
    ],
    "input_type": "大规模文本语料或词对",
    "output_type": "词对的语义相似度分数",
    "source_paper_ids": [
      "COLING_2020_24"
    ],
    "pattern_ids": [
      "pattern_23"
    ]
  },
  {
    "idea_id": "idea_475",
    "description": "提出问题特定奖励机制提升自动深度问题生成质量",
    "tech_stack": [
      "深度学习",
      "强化学习",
      "自然语言处理"
    ],
    "input_type": "文本或文档内容",
    "output_type": "针对输入内容生成的高质量深度问题",
    "source_paper_ids": [
      "COLING_2020_25"
    ],
    "pattern_ids": [
      "pattern_15"
    ]
  },
  {
    "idea_id": "idea_476",
    "description": "研究如何自动丰富和维护不同版本的WordNet词汇分类体系。",
    "tech_stack": [
      "本体构建",
      "分类体系比较",
      "自动化丰富算法"
    ],
    "input_type": "不同时间版本的WordNet词汇数据库",
    "output_type": "增强和更新后的WordNet分类体系",
    "source_paper_ids": [
      "COLING_2020_26"
    ],
    "pattern_ids": [
      "pattern_12"
    ]
  },
  {
    "idea_id": "idea_477",
    "description": "以赋权为导向，协作开发支持加拿大原住民语言的软件。",
    "tech_stack": [
      "自然语言处理",
      "协作开发平台",
      "定制化软件工具"
    ],
    "input_type": "原住民社区语言需求与相关语言数据",
    "output_type": "支持语言保存与复兴的软件工具",
    "source_paper_ids": [
      "COLING_2020_27"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_478",
    "description": "提出一种基于叙事学的角色识别简单方法",
    "tech_stack": [
      "叙事学理论",
      "文本分析",
      "角色识别算法"
    ],
    "input_type": "叙事文本或故事文本",
    "output_type": "文本中角色的识别与标注结果",
    "source_paper_ids": [
      "COLING_2020_28"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_479",
    "description": "分析并比较多种符号化语义表示的词汇语义内容及其差异。",
    "tech_stack": [
      "语义表示分析",
      "语料库比较",
      "符号方法"
    ],
    "input_type": "文本及其语义标注",
    "output_type": "语义内容差异分析与对比结果",
    "source_paper_ids": [
      "COLING_2020_29"
    ],
    "pattern_ids": [
      "pattern_3"
    ]
  },
  {
    "idea_id": "idea_480",
    "description": "从专业可比语料中选择数据以提升双语词典归纳效果",
    "tech_stack": [
      "可比语料处理",
      "数据选择算法",
      "双语词典归纳"
    ],
    "input_type": "专业领域的可比语料文本",
    "output_type": "高质量的双语词汇对列表",
    "source_paper_ids": [
      "COLING_2020_2"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_481",
    "description": "构建用于建模学生互评中同理心的语料库",
    "tech_stack": [
      "语料库构建",
      "自然语言处理",
      "同理心标注"
    ],
    "input_type": "学生撰写的互评文本",
    "output_type": "带有同理心标签的语料库",
    "source_paper_ids": [
      "COLING_2020_30"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_482",
    "description": "结合词嵌入与双语正字法嵌入提升词典归纳效果",
    "tech_stack": [
      "词嵌入",
      "双语正字法嵌入",
      "词典归纳算法"
    ],
    "input_type": "源语言词汇及少量种子词典",
    "output_type": "目标语言词汇翻译列表",
    "source_paper_ids": [
      "COLING_2020_31"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_483",
    "description": "将带噪声的长度约束融入Transformer的长度感知位置编码中",
    "tech_stack": [
      "Transformer",
      "长度感知位置编码",
      "神经机器翻译"
    ],
    "input_type": "源语言文本及长度约束",
    "output_type": "目标语言文本，满足长度约束",
    "source_paper_ids": [
      "COLING_2020_32"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_484",
    "description": "评估预训练Transformer模型在细粒度命名实体识别任务中的表现",
    "tech_stack": [
      "预训练Transformer模型",
      "BERT",
      "命名实体识别"
    ],
    "input_type": "包含命名实体的自然语言文本",
    "output_type": "细粒度类别的命名实体及其边界",
    "source_paper_ids": [
      "COLING_2020_33"
    ],
    "pattern_ids": [
      "pattern_7"
    ]
  },
  {
    "idea_id": "idea_485",
    "description": "利用词干数据实现形态学消歧，提升Kinyarwanda等语言的NLP处理能力。",
    "tech_stack": [
      "形态学分析",
      "词干提取",
      "消歧算法"
    ],
    "input_type": "词干化后的文本数据",
    "output_type": "消歧后的形态学分析结果",
    "source_paper_ids": [
      "COLING_2020_34"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_486",
    "description": "提出在转移系统中优化树结构表示以提升RST解析效果",
    "tech_stack": [
      "转移系统",
      "树结构表示",
      "RST解析"
    ],
    "input_type": "文本单元序列",
    "output_type": "文本的修辞结构树",
    "source_paper_ids": [
      "COLING_2020_35"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_487",
    "description": "利用深度学习NLP模型从财报电话中量化企业数字战略水平",
    "tech_stack": [
      "深度学习",
      "自然语言处理",
      "NLP模型"
    ],
    "input_type": "企业财报电话文本",
    "output_type": "企业数字战略测量结果",
    "source_paper_ids": [
      "COLING_2020_36"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_488",
    "description": "用实值逻辑框架分析和验证语言类型学普遍规律",
    "tech_stack": [
      "实值逻辑",
      "树库分析",
      "类型学数据建模"
    ],
    "input_type": "多语言句法树库数据",
    "output_type": "类型学普遍规律的逻辑表达与验证结果",
    "source_paper_ids": [
      "COLING_2020_37"
    ],
    "pattern_ids": [
      "pattern_3"
    ]
  },
  {
    "idea_id": "idea_489",
    "description": "低分区间自动摘要评价指标之间也存在显著分歧",
    "tech_stack": [
      "自动评价指标分析",
      "相关性统计",
      "元评价方法"
    ],
    "input_type": "摘要系统输出及人工标注",
    "output_type": "评价指标一致性与相关性分析结果",
    "source_paper_ids": [
      "COLING_2020_38"
    ],
    "pattern_ids": [
      "pattern_13"
    ]
  },
  {
    "idea_id": "idea_490",
    "description": "系统分析文本挖掘特征并提出全面分类法",
    "tech_stack": [
      "文本挖掘",
      "人工神经网络",
      "大数据分析"
    ],
    "input_type": "文本数据，如新闻、社交媒体内容等",
    "output_type": "文本特征分类体系或特征列表",
    "source_paper_ids": [
      "COLING_2020_39"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_491",
    "description": "探索并比较简单数据增强方法在命名实体识别中的效果",
    "tech_stack": [
      "数据增强",
      "命名实体识别",
      "深度学习"
    ],
    "input_type": "带标注的文本序列（如生物医药或材料科学领域）",
    "output_type": "每个词的实体类别标签序列",
    "source_paper_ids": [
      "COLING_2020_3"
    ],
    "pattern_ids": [
      "pattern_16"
    ]
  },
  {
    "idea_id": "idea_492",
    "description": "结合神经网络与人工特征实现自动作文评分",
    "tech_stack": [
      "神经网络",
      "手工特征提取",
      "自动评分模型"
    ],
    "input_type": "学生作文文本",
    "output_type": "作文分数或等级",
    "source_paper_ids": [
      "COLING_2020_40"
    ],
    "pattern_ids": [
      "pattern_13"
    ]
  },
  {
    "idea_id": "idea_493",
    "description": "利用义原知识实现端到端的中文词汇融合识别",
    "tech_stack": [
      "端到端神经网络",
      "义原知识融合",
      "中文词汇融合识别"
    ],
    "input_type": "中文文本序列",
    "output_type": "词汇融合识别结果（融合对及其关系）",
    "source_paper_ids": [
      "COLING_2020_41"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_494",
    "description": "利用异构图神经网络预测事件链中的下一个事件",
    "tech_stack": [
      "异构图神经网络",
      "事件链建模",
      "自然语言处理"
    ],
    "input_type": "事件链结构化表示或文本描述",
    "output_type": "下一个可能发生的事件预测",
    "source_paper_ids": [
      "COLING_2020_42"
    ],
    "pattern_ids": [
      "pattern_14"
    ]
  },
  {
    "idea_id": "idea_495",
    "description": "多层多视角学习提升神经机器翻译效果",
    "tech_stack": [
      "多层表示",
      "多视角学习",
      "神经机器翻译"
    ],
    "input_type": "源语言句子文本",
    "output_type": "目标语言句子文本",
    "source_paper_ids": [
      "COLING_2020_43"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_496",
    "description": "通过改进上下文词表示，实现依存句法分析的半监督领域自适应。",
    "tech_stack": [
      "半监督学习",
      "领域自适应",
      "上下文词表示（如BERT）"
    ],
    "input_type": "原始句子文本及部分有标签/无标签数据",
    "output_type": "依存句法树结构",
    "source_paper_ids": [
      "COLING_2020_44"
    ],
    "pattern_ids": [
      "pattern_17"
    ]
  },
  {
    "idea_id": "idea_497",
    "description": "结合定义信息构建混合的概念表示框架",
    "tech_stack": [
      "定义帧",
      "分布式表示",
      "本体知识融合"
    ],
    "input_type": "概念定义文本与本体结构",
    "output_type": "融合语义与分布式特征的概念表示",
    "source_paper_ids": [
      "COLING_2020_45"
    ],
    "pattern_ids": [
      "pattern_1"
    ]
  },
  {
    "idea_id": "idea_498",
    "description": "多任务易先依存句法分析，融合多种依存表示互补信息提升解析效果。",
    "tech_stack": [
      "多任务学习",
      "易先策略",
      "依存句法分析"
    ],
    "input_type": "阿拉伯语句子文本",
    "output_type": "多种依存关系结构",
    "source_paper_ids": [
      "COLING_2020_46"
    ],
    "pattern_ids": [
      "pattern_17"
    ]
  },
  {
    "idea_id": "idea_499",
    "description": "通过动态课程学习提升低资源神经机器翻译性能",
    "tech_stack": [
      "神经机器翻译",
      "动态课程学习",
      "低资源学习"
    ],
    "input_type": "双语平行语料（低资源）",
    "output_type": "高质量翻译文本",
    "source_paper_ids": [
      "COLING_2020_47"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_500",
    "description": "构建Kannada-English混合语料库并分析其情感预测方法",
    "tech_stack": [
      "语料库构建",
      "情感标注",
      "情感分类模型"
    ],
    "input_type": "Kannada-English混合推文文本数据",
    "output_type": "推文对应的情感类别标签",
    "source_paper_ids": [
      "COLING_2020_48"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_501",
    "description": "构建并标注媒体中针对弱势群体的居高临下语言数据集",
    "tech_stack": [
      "数据集构建",
      "文本标注",
      "语言分析"
    ],
    "input_type": "媒体文本数据",
    "output_type": "带有居高临下语言标注的数据集",
    "source_paper_ids": [
      "COLING_2020_49"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_502",
    "description": "利用逻辑引导的语义表示实现零样本关系分类",
    "tech_stack": [
      "逻辑推理",
      "语义表示学习",
      "零样本学习"
    ],
    "input_type": "包含实体对及上下文的文本数据",
    "output_type": "实体对之间的关系类别（包括未见过的关系）",
    "source_paper_ids": [
      "COLING_2020_4"
    ],
    "pattern_ids": [
      "pattern_28"
    ]
  },
  {
    "idea_id": "idea_503",
    "description": "结合图结构和主题词提升对话抽象摘要质量",
    "tech_stack": [
      "图结构建模",
      "主题词提取",
      "编码-解码框架"
    ],
    "input_type": "多轮对话文本",
    "output_type": "抽象性对话摘要",
    "source_paper_ids": [
      "COLING_2020_50"
    ],
    "pattern_ids": [
      "pattern_27"
    ]
  },
  {
    "idea_id": "idea_504",
    "description": "通过实体引导注意力和混淆感知训练提升小样本关系分类性能",
    "tech_stack": [
      "实体引导注意力机制",
      "混淆感知训练",
      "小样本学习"
    ],
    "input_type": "包含两个实体的句子及少量标注关系样本",
    "output_type": "实体对之间的关系类别",
    "source_paper_ids": [
      "COLING_2020_51"
    ],
    "pattern_ids": [
      "pattern_28"
    ]
  },
  {
    "idea_id": "idea_505",
    "description": "提出多词表达的词汇简化方法，提升文本易读性",
    "tech_stack": [
      "自然语言处理",
      "词汇替换算法",
      "语义保持模型"
    ],
    "input_type": "包含复杂词汇的自然语言文本",
    "output_type": "用更易懂多词表达替换后的简化文本",
    "source_paper_ids": [
      "COLING_2020_52"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_506",
    "description": "利用属性归纳偏置作为参考生成个性化产品评论",
    "tech_stack": [
      "归纳偏置建模",
      "属性表示学习",
      "文本生成"
    ],
    "input_type": "用户属性信息、产品属性信息",
    "output_type": "针对产品的个性化评论文本",
    "source_paper_ids": [
      "COLING_2020_53"
    ],
    "pattern_ids": [
      "pattern_21"
    ]
  },
  {
    "idea_id": "idea_507",
    "description": "提出通用模型以检测多种类型的网络辱骂语言。",
    "tech_stack": [
      "自然语言处理",
      "深度学习",
      "多任务学习"
    ],
    "input_type": "社交媒体文本或评论数据",
    "output_type": "文本是否包含辱骂语言的分类标签",
    "source_paper_ids": [
      "COLING_2020_54"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_508",
    "description": "提出评估文本内容是否适合儿童阅读的新方法",
    "tech_stack": [
      "自然语言处理",
      "文本可读性分析",
      "儿童语言能力建模"
    ],
    "input_type": "儿童可接触的网络文本内容",
    "output_type": "文本内容对儿童阅读理解能力的适宜性评估结果",
    "source_paper_ids": [
      "COLING_2020_55"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_509",
    "description": "提出可配置的对话系统自动评价指标，通过分解重构实现灵活评估。",
    "tech_stack": [
      "评价指标分解",
      "自动化评估方法",
      "对话系统性能比较"
    ],
    "input_type": "对话系统生成的回复及参考回复",
    "output_type": "可配置的评价分数或指标",
    "source_paper_ids": [
      "COLING_2020_56"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_510",
    "description": "系统性分析现有简单问答系统与数据集，推动通用问答发展",
    "tech_stack": [
      "知识库",
      "自然语言处理",
      "实证分析"
    ],
    "input_type": "自然语言事实型问题",
    "output_type": "知识库中实体-属性的答案",
    "source_paper_ids": [
      "COLING_2020_57"
    ],
    "pattern_ids": [
      "pattern_33"
    ]
  },
  {
    "idea_id": "idea_511",
    "description": "量化新闻稿中将相关性夸大为因果性的现象",
    "tech_stack": [
      "文本分析",
      "统计方法",
      "自然语言处理"
    ],
    "input_type": "新闻稿文本",
    "output_type": "相关性到因果性夸大的测量结果",
    "source_paper_ids": [
      "COLING_2020_58"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_512",
    "description": "针对中文生物医学专利进行命名实体识别以促进知识发现",
    "tech_stack": [
      "命名实体识别",
      "生物医学自然语言处理",
      "专利文本处理"
    ],
    "input_type": "中文生物医学专利文本",
    "output_type": "生物医学实体及其类别标注",
    "source_paper_ids": [
      "COLING_2020_59"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_513",
    "description": "利用WordNet路径信息提升神经网络对上位词关系的预测能力",
    "tech_stack": [
      "WordNet路径特征",
      "神经网络模型",
      "词嵌入"
    ],
    "input_type": "词对（待预测的词及其可能的上位词）",
    "output_type": "词对间的上位词关系预测结果",
    "source_paper_ids": [
      "COLING_2020_5"
    ],
    "pattern_ids": [
      "pattern_12"
    ]
  },
  {
    "idea_id": "idea_514",
    "description": "通过异构扭曲图的谱分解优化词向量的语义特化",
    "tech_stack": [
      "谱分解",
      "异构图建模",
      "词向量优化"
    ],
    "input_type": "分布式词向量与词关系图",
    "output_type": "语义特化后的词向量",
    "source_paper_ids": [
      "COLING_2020_60"
    ],
    "pattern_ids": [
      "pattern_1"
    ]
  },
  {
    "idea_id": "idea_515",
    "description": "构建并评测Kinyarwanda和Kirundi跨语言文本分类基准数据集",
    "tech_stack": [
      "深度学习",
      "跨语言文本分类",
      "数据集构建"
    ],
    "input_type": "Kinyarwanda和Kirundi语言的新闻文本",
    "output_type": "文本分类标签及基准评测结果",
    "source_paper_ids": [
      "COLING_2020_61"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_516",
    "description": "对AMR到英文生成系统进行人工评价，分析其效果与不足。",
    "tech_stack": [
      "AMR语义表示",
      "自然语言生成",
      "人工评价方法"
    ],
    "input_type": "AMR语义图（有向无环图表示句子语义）",
    "output_type": "英文自然语言句子",
    "source_paper_ids": [
      "COLING_2020_62"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_517",
    "description": "分析NMT模型在持续训练中灾难性遗忘现象及其影响。",
    "tech_stack": [
      "神经机器翻译",
      "持续学习",
      "灾难性遗忘分析"
    ],
    "input_type": "NMT模型及不同领域训练数据",
    "output_type": "模型性能变化及遗忘现象评估结果",
    "source_paper_ids": [
      "COLING_2020_63"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_518",
    "description": "提出大规模无监督图到文本生成数据集GenWiki，促进模型训练。",
    "tech_stack": [
      "知识图谱",
      "无监督学习",
      "深度文本生成"
    ],
    "input_type": "知识图谱结构数据",
    "output_type": "与图谱内容对应的自然语言文本",
    "source_paper_ids": [
      "COLING_2020_64"
    ],
    "pattern_ids": [
      "pattern_4"
    ]
  },
  {
    "idea_id": "idea_519",
    "description": "提出双动态记忆网络提升多轮任务型对话系统端到端性能",
    "tech_stack": [
      "动态记忆网络",
      "端到端学习",
      "多轮对话建模"
    ],
    "input_type": "用户多轮自然语言对话及相关任务信息",
    "output_type": "系统生成的任务型对话回复",
    "source_paper_ids": [
      "COLING_2020_65"
    ],
    "pattern_ids": [
      "pattern_25"
    ]
  },
  {
    "idea_id": "idea_520",
    "description": "无需先验知识的循环网络可自主学习语言结构。",
    "tech_stack": [
      "循环神经网络",
      "无先验学习",
      "好奇心驱动机制"
    ],
    "input_type": "原始语言数据序列",
    "output_type": "语言结构或语法规则表示",
    "source_paper_ids": [
      "COLING_2020_66"
    ],
    "pattern_ids": [
      "pattern_3"
    ]
  },
  {
    "idea_id": "idea_521",
    "description": "自动选择吸引读者注意力的文章拉引语（Pull Quote）",
    "tech_stack": [
      "自然语言处理",
      "文本分析",
      "机器学习"
    ],
    "input_type": "新闻或文章文本",
    "output_type": "高吸引力的拉引语文本片段",
    "source_paper_ids": [
      "COLING_2020_67"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_522",
    "description": "利用深度度量学习提升生物医学文献片段检索效果",
    "tech_stack": [
      "深度学习",
      "度量学习",
      "自然语言处理"
    ],
    "input_type": "医学文献片段及查询",
    "output_type": "相关性排序后的文献片段",
    "source_paper_ids": [
      "COLING_2020_68"
    ],
    "pattern_ids": [
      "pattern_10"
    ]
  },
  {
    "idea_id": "idea_523",
    "description": "利用无监督表征学习自动检测假新闻的立场倾向",
    "tech_stack": [
      "无监督学习",
      "表征学习",
      "假新闻检测"
    ],
    "input_type": "新闻文本或社交媒体内容",
    "output_type": "立场分类标签（如支持、反对、中立）",
    "source_paper_ids": [
      "COLING_2020_69"
    ],
    "pattern_ids": [
      "pattern_6"
    ]
  },
  {
    "idea_id": "idea_524",
    "description": "通过主动学习优化BERT在低资源NLP任务上的微调效果",
    "tech_stack": [
      "BERT",
      "主动学习",
      "迁移学习"
    ],
    "input_type": "少量标注文本数据",
    "output_type": "提升的NLP任务模型性能",
    "source_paper_ids": [
      "COLING_2020_6"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_525",
    "description": "无监督方法自动从文本交流中提取任务信息",
    "tech_stack": [
      "自然语言处理",
      "无监督学习",
      "文本分析"
    ],
    "input_type": "团队成员间的文本交流内容，如邮件、聊天记录",
    "output_type": "结构化的任务列表或任务描述",
    "source_paper_ids": [
      "COLING_2020_70"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_526",
    "description": "构建俄语词义历史变化数据集，助力语义演变研究",
    "tech_stack": [
      "语料库构建",
      "词义变化检测",
      "数据标注"
    ],
    "input_type": "俄语历史文本语料和词汇列表",
    "output_type": "标注有语义变化的俄语词汇数据集",
    "source_paper_ids": [
      "COLING_2020_71"
    ],
    "pattern_ids": [
      "pattern_1"
    ]
  },
  {
    "idea_id": "idea_527",
    "description": "构建阿拉伯方言低资源语音翻译评测数据集",
    "tech_stack": [
      "语音识别",
      "机器翻译",
      "数据集构建"
    ],
    "input_type": "阿拉伯方言语音数据",
    "output_type": "机器翻译评测数据集",
    "source_paper_ids": [
      "COLING_2020_72"
    ],
    "pattern_ids": [
      "pattern_24"
    ]
  },
  {
    "idea_id": "idea_528",
    "description": "利用层次图卷积网络结合方面类别，实现无需标注的细粒度情感分析。",
    "tech_stack": [
      "Hierarchical Graph Convolutional Network",
      "Aspect-Category Modeling",
      "Sentiment Analysis"
    ],
    "input_type": "包含评论文本及方面类别信息的数据",
    "output_type": "各方面类别的情感极性标签",
    "source_paper_ids": [
      "COLING_2020_73"
    ],
    "pattern_ids": [
      "pattern_32"
    ]
  },
  {
    "idea_id": "idea_529",
    "description": "利用Supersense标签构建轻量且可解释的上下文词嵌入模型",
    "tech_stack": [
      "Supersense标注",
      "词嵌入",
      "可解释性建模"
    ],
    "input_type": "文本序列（句子或段落）",
    "output_type": "带Supersense语义标签的词级上下文嵌入表示",
    "source_paper_ids": [
      "COLING_2020_74"
    ],
    "pattern_ids": [
      "pattern_12"
    ]
  },
  {
    "idea_id": "idea_530",
    "description": "通过用户语言特征建模提升虚假新闻检测效果",
    "tech_stack": [
      "自然语言处理",
      "用户表示学习",
      "深度学习"
    ],
    "input_type": "新闻文本及用户历史语言数据",
    "output_type": "新闻真假分类结果",
    "source_paper_ids": [
      "COLING_2020_75"
    ],
    "pattern_ids": [
      "pattern_6"
    ]
  },
  {
    "idea_id": "idea_531",
    "description": "分析不同体裁和媒介下指代策略的变化及其测量方法问题",
    "tech_stack": [
      "语料库分析",
      "指代链统计",
      "跨体裁比较"
    ],
    "input_type": "文本语料（不同体裁和媒介）",
    "output_type": "指代策略变化的定量与定性分析",
    "source_paper_ids": [
      "COLING_2020_76"
    ],
    "pattern_ids": [
      "pattern_32"
    ]
  },
  {
    "idea_id": "idea_532",
    "description": "通过引入噪声缓解序列到序列形态变化模型的曝光偏置，提高低资源条件下的泛化能力。",
    "tech_stack": [
      "序列到序列模型",
      "循环神经网络",
      "噪声注入"
    ],
    "input_type": "词干与形态句法描述对",
    "output_type": "词形变化后的单词",
    "source_paper_ids": [
      "COLING_2020_77"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_533",
    "description": "构建并标注英语口语转录语法错误数据集，推动口语语法错误检测研究",
    "tech_stack": [
      "语音转录",
      "语法错误标注",
      "自然语言处理"
    ],
    "input_type": "英语口语录音及其转录文本",
    "output_type": "转录文本中的语法错误检测与标注结果",
    "source_paper_ids": [
      "COLING_2020_78"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_534",
    "description": "利用弱标注数据实现多粒度中文分词，提高分词灵活性与适应性。",
    "tech_stack": [
      "多粒度分词建模",
      "弱监督学习",
      "深度神经网络"
    ],
    "input_type": "未分词或弱标注的中文句子",
    "output_type": "多粒度分词结果（多个分词方案）",
    "source_paper_ids": [
      "COLING_2020_79"
    ],
    "pattern_ids": [
      "pattern_34"
    ]
  },
  {
    "idea_id": "idea_535",
    "description": "提出医疗文本简化的智能自动补全工具，辅助人工编辑。",
    "tech_stack": [
      "自然语言处理",
      "文本自动补全",
      "人机协作界面"
    ],
    "input_type": "医疗原始文本",
    "output_type": "简化后的医疗文本建议",
    "source_paper_ids": [
      "COLING_2020_7"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_536",
    "description": "提出双解码器Transformer模型，联合实现语音识别和多语种语音翻译。",
    "tech_stack": [
      "Transformer",
      "双解码器架构",
      "端到端学习"
    ],
    "input_type": "语音音频（多语种）",
    "output_type": "源语言文本转录和目标语言文本翻译",
    "source_paper_ids": [
      "COLING_2020_80"
    ],
    "pattern_ids": [
      "pattern_9"
    ]
  },
  {
    "idea_id": "idea_537",
    "description": "结合子词表示和预训练语言模型提升泰语通用词性标注性能",
    "tech_stack": [
      "子词表示",
      "预训练语言模型",
      "神经网络"
    ],
    "input_type": "泰语文本序列",
    "output_type": "通用词性标签序列",
    "source_paper_ids": [
      "COLING_2020_81"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_538",
    "description": "融合多模态信息的神经网络提升隐喻检测准确率",
    "tech_stack": [
      "神经网络",
      "多模态融合",
      "隐喻识别"
    ],
    "input_type": "文本及其他模态数据（如图像、语音）",
    "output_type": "文本中隐喻的检测结果",
    "source_paper_ids": [
      "COLING_2020_82"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_539",
    "description": "构建多跳问答数据集以全面评估模型推理能力",
    "tech_stack": [
      "数据集构建",
      "多跳推理",
      "机器阅读理解"
    ],
    "input_type": "文本材料与问题",
    "output_type": "多跳推理问答数据集",
    "source_paper_ids": [
      "COLING_2020_83"
    ],
    "pattern_ids": [
      "pattern_33"
    ]
  },
  {
    "idea_id": "idea_540",
    "description": "自动生成标准测试多项选择题的干扰项",
    "tech_stack": [
      "自然语言处理",
      "机器学习",
      "文本生成"
    ],
    "input_type": "标准测试题目及正确答案文本",
    "output_type": "高质量多项选择题干扰项列表",
    "source_paper_ids": [
      "COLING_2020_84"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_541",
    "description": "通过对比模板提升机器翻译中的指代消解准确性",
    "tech_stack": [
      "对比学习",
      "指代消解分析",
      "模板方法"
    ],
    "input_type": "包含需翻译文本及指代信息的语料",
    "output_type": "指代消解优化后的目标语言翻译文本",
    "source_paper_ids": [
      "COLING_2020_85"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_542",
    "description": "利用眼动追踪数据预测葡萄牙语句子的可读性，比较多种学习方法。",
    "tech_stack": [
      "眼动追踪数据分析",
      "单任务学习",
      "多任务学习",
      "顺序迁移学习"
    ],
    "input_type": "葡萄牙语句子及对应眼动追踪数据",
    "output_type": "可读性预测评分或标签",
    "source_paper_ids": [
      "COLING_2020_86"
    ],
    "pattern_ids": []
  },
  {
    "idea_id": "idea_543",
    "description": "结合词内和词外特征提升蒙古语形态切分效果",
    "tech_stack": [
      "LSTM",
      "端到端分割方法",
      "特征融合"
    ],
    "input_type": "蒙古语单词或文本序列",
    "output_type": "分割后的蒙古语词素序列",
    "source_paper_ids": [
      "COLING_2020_8"
    ],
    "pattern_ids": [
      "pattern_34"
    ]
  },
  {
    "idea_id": "idea_544",
    "description": "通过视觉-文本对齐提升视觉对话中的图推理能力",
    "tech_stack": [
      "跨模态对齐",
      "图神经网络",
      "视觉对话建模"
    ],
    "input_type": "图像与多轮文本对话",
    "output_type": "对话回复或推理结果",
    "source_paper_ids": [
      "COLING_2020_9"
    ],
    "pattern_ids": [
      "pattern_22"
    ]
  }
]