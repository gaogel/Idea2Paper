[
  {
    "review_id": "853eea1c8518b5cc",
    "paper_id": "ACL_2017_67",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- The paper tackles an important issue, that is building ontologies or thesauri - The methods make sense and seem well chosen - Methods and setups are well detailed - It looks like the authors outperform the state-of-the-art approach (but see below for my concerns)",
    "weaknesses": "The main weaknesses for me are evaluation and overall presentation/writing.\n- The list of baselines is hard to understand. Some methods are really old and it doesn't seem justified to show them here (e.g., Mpttern).\n- Memb is apparently the previous state-of-the-art, but there is no mention to any reference.\n- While it looks like the method outperforms the previous best performing approach, the paper is not convincing enough. Especially, on the first dataset, the difference between the new system and the previous state-of-the-art one is pretty small.\n- The paper seriously lacks proofreading, and could not be published until this is fixed – for instance, I noted 11 errors in the first column of page 2.\n- The CilinE hierarchy is very shallow (5 levels only). However apparently, it has been used in the past by other authors. I would expect that the deeper the more difficult it is to branch new hyponym-hypernyms. This can explain the very high results obtained (even by previous studies)...",
    "comments": "The approach itself is not really original or novel, but it is applied to a problem that has not been addressed with deep learning yet. For this reason, I think this paper is interesting, but there are two main flaws. The first and easiest to fix is the presentation. There are many errors/typos that need to be corrected. I started listing them to help, but there are just too many of them. \nThe second issue is the evaluation, in my opinion. Technically, the performances are better, but it does not feel convincing as explained above. \nWhat is Memb, is it the method from Shwartz et al 2016, maybe? If not, what performance did this recent approach have? I think the authors need to reorganize the evaluation section, in order to properly list the baseline systems, clearly show the benefit of their approach and where the others fail. \nSignificance tests  also seem necessary given the slight improvement on one dataset.",
    "overall_score": "2",
    "confidence": "5"
  },
  {
    "review_id": "836146cdffe05dbd",
    "paper_id": "ACL_2017_67",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "* Knowledge lean, language-independent approach",
    "weaknesses": "* Peculiar task/setting   * Marginal improvement over W_Emb (Fu et al, 2014)   * Waste of space   * Language not always that clear",
    "comments": "It seems to me that this paper is quite similar to (Fu et al, 2014) and only adds marginal improvements. It contains quite a lot of redundancy (e.g. related work in  sec 1 and sec 2), uninformative figures (e.g. Figure 1 vs Figure 2), not so useful descriptions of MLP and RNN, etc. A short paper might have been a better fit.\nThe task looks somewhat idiosyncratic to me. It is only useful if you already have a method that gives you all and only the hypernyms of a given word. This seems to presuppose (Fu et al., 2013).  Figure 4: why are the first two stars connected by conjunction and the last two starts by disjunction?              Why is the output \"1\" (dark star) if the the three inputs are \"0\" (white stars)?\nSec 4.2, lines 587-589 appears to suggest that thresholds were tuned on the test data (?)  W_Emb is poorly explained (lines 650-652).\nSome parts of the text are puzzling. I can't make sense of the section titled \"Combined with Manually-Built Hierarchies\". Same for sec 4.4. What do the red and dashed lines mean?",
    "overall_score": "2",
    "confidence": "4"
  }
]