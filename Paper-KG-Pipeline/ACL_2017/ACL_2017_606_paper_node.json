{
  "paper_id": "ACL_2017_606",
  "title": "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with Weak Supervision",
  "conference": "ACL",
  "domain": {
    "research_object": "结合神经网络与符号推理的方法，提升语义解析在知识库上的表现。",
    "core_technique": "采用弱监督学习训练神经符号机器，实现自然语言到知识库查询的转换。",
    "application": "自动将自然语言问题解析为Freebase等知识库的结构化查询，实现智能问答。",
    "domains": [
      "人工智能",
      "自然语言处理"
    ]
  },
  "ideal": {
    "core_idea": "结合神经网络与符号推理，实现弱监督下的语义解析器学习",
    "tech_stack": [
      "神经网络",
      "符号推理",
      "弱监督学习"
    ],
    "input_type": "自然语言问题",
    "output_type": "可在Freebase执行的符号程序"
  },
  "skeleton": {
    "problem_framing": "引言部分通过回顾深度神经网络在监督学习任务中的成功，逐步引出在语义解析等需要将自然语言映射为可执行符号表示的任务中，弱监督训练仍面临挑战，明确聚焦于弱监督下的语义解析难题。",
    "gap_pattern": "作者指出现有方法多依赖于手工注释的程序，规避了程序执行中的不可微分操作，批评了当前方法在弱监督语义解析任务中的局限，强调了缺乏有效弱监督训练机制的研究空白。",
    "method_story": "方法部分以“程序员-计算机”类比引入，将自然语言转化为程序的过程，继而详细描述在标准seq2seq模型基础上的关键变量记忆扩展，层层递进地解释模型结构与创新点。",
    "experiments_story": "实验部分先阐明实验目的和数据集，随后报告NSM模型在弱监督下的性能提升，并与强监督方法进行对比，突出新方法的有效性，最后说明评价指标和实验细节，结构清晰有力支撑论点。"
  },
  "tricks": [
    {
      "name": "引用相关领域前沿工作",
      "type": "writing-level",
      "purpose": "展示研究背景与现有挑战",
      "location": "论文开头第一段",
      "description": "通过大量引用相关领域的经典和前沿文献，展示当前深度神经网络在不同任务中的表现及其局限性，为后续提出的方法铺垫背景。"
    },
    {
      "name": "明确问题难点",
      "type": "writing-level",
      "purpose": "突出研究意义和创新点",
      "location": "论文开头第一段",
      "description": "指出在弱监督下进行语义解析或程序归纳的难点，尤其是与非可微操作的交互，强调现有方法的不足。"
    },
    {
      "name": "对比现有方法的不足",
      "type": "writing-level",
      "purpose": "突出所提方法的优势",
      "location": "第一段后半部分",
      "description": "详细分析现有方法如依赖人工标注程序、可微分内存等的局限性，为新方法的必要性提供论据。"
    },
    {
      "name": "提出新方法的核心思想",
      "type": "writing-level",
      "purpose": "简明扼要地介绍创新点",
      "location": "第二段开头",
      "description": "用简洁的语言提出利用计算机的离散操作和内存的核心思想，并引出后续具体实现。"
    },
    {
      "name": "模块化模型结构描述",
      "type": "method-level",
      "purpose": "清晰分解模型架构，便于理解",
      "location": "第二段中部",
      "description": "将模型分解为“程序员”和“计算机”两部分，分别介绍其功能和相互关系，提高模型描述的条理性。"
    },
    {
      "name": "基于Seq2Seq模型并加入注意力机制",
      "type": "method-level",
      "purpose": "提高模型表达能力和可解释性",
      "location": "第二段后半部分",
      "description": "在标准的序列到序列模型基础上，采用注意力机制（dot-product attention），提升模型对输入信息的聚焦能力。"
    },
    {
      "name": "引入key-variable memory机制",
      "type": "method-level",
      "purpose": "增强模型对中间变量的表示和引用能力",
      "location": "第二段末尾",
      "description": "在decoder中扩展key-variable memory，允许模型学习表示和引用程序变量，实现更强的组合性。"
    },
    {
      "name": "详细公式化模型流程",
      "type": "method-level",
      "purpose": "增强方法的可复现性和严谨性",
      "location": "第二段中后部",
      "description": "对编码器和解码器的状态更新、输入输出嵌入、注意力机制等关键步骤进行公式化描述，便于他人实现。"
    },
    {
      "name": "动态限定输出token空间",
      "type": "method-level",
      "purpose": "提升生成程序的合法性和执行效率",
      "location": "第二段后半部分",
      "description": "在每一步解码时，softmax只作用于计算机提供的合法token集合，保证生成的程序可被执行。"
    },
    {
      "name": "强调可组合性（compositionality）",
      "type": "writing-level",
      "purpose": "突出模型对复杂结构的处理能力",
      "location": "第二段末尾",
      "description": "明确提出decoder需要学习表示和引用中间变量，强调模型对复杂程序结构的处理能力。"
    }
  ]
}