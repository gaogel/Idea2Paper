[
  {
    "review_id": "2efd08656e6ad1bd",
    "paper_id": "ACL_2017_636",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- Extensive experiments against various architectures (LSTM, LSTM + CRF)        - Novel architectural/training ideas (sharing blocks)",
    "weaknesses": "- Only applied to English NER--this is a big concern since the title of the paper seems to reference sequence-tagging directly.   - Section 4.1 could be clearer. For example, I presume there is padding to make sure the output resolution after each block is the same as the input resolution.  Might be good to mention this.   - I think an ablation study of number of layers vs perf might be interesting.\nRESPONSE TO AUTHOR REBUTTAL: Thank you very much for a thoughtful response. Given that the authors have agreed to make the content be more specific to NER as opposed to sequence-tagging, I have revised my score upward.",
    "comments": "",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "4977a9f37fcffb67",
    "paper_id": "ACL_2017_636",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The main strength promised by the paper is the speed advantage at the same accuracy level.",
    "weaknesses": "Presentation of the approach leaves a lot to be desired. Sections 3 and 4 need to be much clearer, from concept definition to explaining the architecture and parameterization. In particular Section 4.1 and the parameter tieing used need to be crystal clear, since that is one of the main contributions of the paper.\nMore experiments supporting the vast speed improvements promised need to be presented. The results in Table 2 are good but not great. A speed-up of 4-6X is nothing all that transformative.",
    "comments": "What exactly is \"Viterbi prediction\"? The term/concept is far from established; the reader could guess but there must be a better way to phrase it.\nReference Weiss et al., 2015 has a typo.",
    "overall_score": "3",
    "confidence": "4"
  }
]