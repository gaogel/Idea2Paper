{
  "paper_id": "ACL_2017_715",
  "title": "Reading Wikipedia to Answer Open-Domain Questions",
  "conference": "ACL",
  "domain": {
    "research_object": "利用维基百科作为知识库，回答开放域自然语言问题。",
    "core_technique": "结合文档检索与神经网络阅读理解方法，实现自动问答系统。",
    "application": "用于开放域问答系统、智能助手和信息检索服务。",
    "domains": [
      "自然语言处理",
      "信息检索"
    ]
  },
  "ideal": {
    "core_idea": "利用Wikipedia文本直接回答开放域事实性问题",
    "tech_stack": [
      "文档检索",
      "神经阅读理解模型",
      "端到端训练"
    ],
    "input_type": "自然语言问题",
    "output_type": "基于Wikipedia的简明答案"
  },
  "skeleton": {
    "problem_framing": "论文通过类比百科全书查找答案的日常场景，引入开放域事实性问答问题，强调以Wikipedia为唯一知识源的现实需求，突出其信息详实和不断更新的优势，为智能机器利用其知识奠定背景。",
    "gap_pattern": "作者批评现有知识库（如Freebase、DBPedia）虽易于机器处理，但覆盖面有限，难以满足开放域问答需求；而Wikipedia虽内容丰富且及时，却因面向人类设计，机器难以直接利用，凸显研究的必要性。",
    "method_story": "方法部分强调任务复杂性，因Wikipedia为人类而非机器设计，需结合多种技术进行问答。作者未直接展开具体方法，而是通过任务属性和知识源特点，铺垫后续方法选择的合理性。",
    "experiments_story": "实验部分首先介绍主流数据集SQuAD的构建方式及其局限性，随后提出采用多样化的开放域QA数据集（如CuratedTREC、WebQuestions），以保证评估的广泛性和代表性，体现实验设计的系统性和严谨性。"
  },
  "tricks": [
    {
      "name": "设定问题背景与挑战",
      "type": "writing-level",
      "purpose": "明确论文研究的问题背景和核心挑战，吸引读者关注研究意义",
      "location": "论文开头",
      "description": "通过对比Wikipedia与传统知识库（如Freebase、DBPedia），强调Wikipedia作为开放域问答知识源的独特性和挑战（信息量大、为人设计而非为机器设计），为后续方法论和实验设置奠定基础。"
    },
    {
      "name": "知识源选择与泛化能力说明",
      "type": "method-level",
      "purpose": "说明方法的通用性，减少对特定知识结构的依赖，提高可迁移性",
      "location": "方法介绍部分",
      "description": "将Wikipedia视为文章集合，而不依赖其内部图结构，强调方法可以应用于其他文档集合，提高系统的泛化能力。"
    },
    {
      "name": "任务分解：检索与阅读理解两步走",
      "type": "method-level",
      "purpose": "明确方法流程，将复杂任务分解为可操作的子任务，便于实现和优化",
      "location": "方法论描述",
      "description": "将开放域问答任务分解为“检索相关文档”和“在文档中定位答案”两步，分别应对大规模文档检索和机器阅读理解的挑战。"
    },
    {
      "name": "对比现有系统并定位创新点",
      "type": "writing-level",
      "purpose": "突出自身工作的创新性和区别于前人工作的地方",
      "location": "相关工作与方法介绍",
      "description": "通过与IBM DeepQA等系统对比，指出本方法仅使用Wikipedia单一知识源而非多源混合，强调研究的独特性和挑战。"
    },
    {
      "name": "多数据集实验设计",
      "type": "experiment-level",
      "purpose": "提升实验的全面性与结果的说服力，验证方法在不同任务和数据上的表现",
      "location": "实验设置部分",
      "description": "选择多个开放域问答数据集（SQuAD、CuratedTREC、WebQuestions、WikiMovies），覆盖不同构建方式和领域，避免单一数据集带来的偏差，增强实验结果的普适性。"
    },
    {
      "name": "数据集预处理与适配",
      "type": "method-level",
      "purpose": "确保不同来源的数据集能统一用于方法评估，提高比较的公平性",
      "location": "数据集描述部分",
      "description": "针对WebQuestions数据集，将答案从Freebase实体ID转换为文本形式，保证所有数据集均为文本问答对，便于统一处理和评估。"
    },
    {
      "name": "强调数据集构建方式对任务分布的影响",
      "type": "writing-level",
      "purpose": "提醒读者关注数据集的构建方式可能影响模型泛化与实际应用效果",
      "location": "数据集分析部分",
      "description": "指出SQuAD数据集的问答分布受人工出题流程影响，强调数据集构建方式对模型训练和评估的影响，提示后续需关注多样性和泛化能力。"
    },
    {
      "name": "引用并简述数据集来源",
      "type": "writing-level",
      "purpose": "增强论文的学术规范性和可追溯性，便于读者查阅原始资料",
      "location": "数据集描述部分",
      "description": "对每个数据集均给出文献引用并简要说明其构建过程和规模，为实验设计提供透明依据。"
    },
    {
      "name": "知识源与任务匹配分析",
      "type": "method-level",
      "purpose": "确保知识源能满足任务需求，提高实验的合理性",
      "location": "数据集与知识源配合说明",
      "description": "分析WikiMovies数据集，说明其原本基于Freebase，但可通过选择Wikipedia电影相关条目作为知识源，保证任务的知识可获得性和实验的可行性。"
    }
  ]
}