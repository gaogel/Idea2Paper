[
  {
    "review_id": "c8b05795ef1ddc6b",
    "paper_id": "ACL_2017_331",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Detailed guidelines and explicit illustrations.",
    "weaknesses": "The document-independent crowdsourcing annotation is unreliable.",
    "comments": "This work creates a new benchmark corpus for concept-map-based MDS. It is well organized and written clearly. The supplement materials are sufficient. I have two questions here. \n1)              Is it necessary to treat concept map extraction as a separate task? \nOn the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable. \n2)              How can you determine the importance of a concept independent of the documents? The definition of summarization is to reserve the main concepts of documents. Therefore, the importance of a concept highly depends on the documents. For example, in the given topic of coal mining accidents, assume there are two concepts: A) an instance of coal mining accidents and B) a cause of coal mining accidents. Then, if the document describes a series of coal mining accidents, A is more important than B. In comparison, if the document explores why coal mining accidents happen, B is more significant than A. Therefore, just given the topic and two concepts A&B, it is impossible to judge their relative importance.\nI appreciate the great effort spent by authors to build this dataset. However, this dataset is more like a knowledge graph based on common sense rather than summary.",
    "overall_score": "3",
    "confidence": "5"
  },
  {
    "review_id": "3941de5d2b14a0f2",
    "paper_id": "ACL_2017_331",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "This paper presents an approach to creating concept maps using crowdsourcing. \nThe general ideas are interesting and the main contribution lies in the collection of the dataset. As such, I imagine that the dataset will be a valuable resource for further research in this field. Clearly a lot of effort has gone into this work.",
    "weaknesses": "Overall I felt this paper a bit overstated in placed. As an example, the authors claim a new crowdsourcing scheme as one of their contributions. This claims is quite strong though and it reads more like the authors are applying best practice in crowdsourcing to their work. This isn’t a novel methods then, it’s rather a well thought and sound application of existing knowledge.\nSimilarly, the authors claim that they develop and present a new corpus. This seems true and I can see how a lot of effort was invested in its preparation, but then Section 4.1 reveals that actually this is based on an existing dataset.  This is more a criticism of the presentation than the work though.",
    "comments": "",
    "overall_score": "3",
    "confidence": "2"
  }
]