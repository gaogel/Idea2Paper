[
  {
    "review_id": "e2ec0be7eb025d1e",
    "paper_id": "ACL_2017_433",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "- They obtain good results and their experimental setup appears to be solid.\n - They perform many careful analyses and explore the influence on many parameters of their model.\n - They provide a small Singlish treebank annotated according to the Universal Dependencies v1.4 guidelines.\n - They propose very sound guidelines on how to analyze common Singlish constructions in UD.\n - Their method is linguistically informed and they nicely exploit similarity between standard English and the creole Singaporean English.\n - The paper presents methods for a low-resource language.\n - They are not just applying an existing English method to another language but instead present a method that can be potentially used for other closely related language pairs.\n - They use a well-motivated method for selecting the sentences to include in their treebank.\n - The paper is very well written and easy to read.",
    "weaknesses": "- The annotation quality seems to be rather poor. They performed double annotation of 100 sentences and their inter-annotator agreement is just 75.72% in terms of LAS. This makes it hard to assess how reliable the estimate of the LAS of their model is, and the LAS of their model is in fact slightly higher than the inter-annotator agreement.  UPDATE: Their rebuttal convincingly argued that the second annotator who just annotated the 100 examples to compute the IAA didn't follow the annotation guidelines for several common constructions. Once the second annotator fixed these issues, the IAA was reasonable, so I no longer consider this a real issue.",
    "comments": "I am a bit concerned about the apparently rather poor annotation quality of the data and how this might influence the results, but overall, I liked the paper a lot and I think this would be a good contribution to the conference.\n- Questions for the authors:  - Who annotated the sentences? You just mention that 100 sentences were annotated by one of the authors to compute inter=annotator agreement but you don't mention who annotated all the sentences.\n - Why was the inter-annotator agreement so low? In which cases was there disagreement? Did you subsequently discuss and fix the sentences for which there was disagreement?\n - Table A2: There seem to be a lot of discourse relations (almost as many as dobj relations) in your treebank. Is this just an artifact of the colloquial language or did you use \"discourse\" for things that are not considered \"discourse\" in other languages in UD?\n - Table A3: Are all of these discourse particles or discourse + imported vocab? If the latter, perhaps put them in separate tables, and glosses would be helpful.\n- Low-level comments:  - It would have been interesting if you had compared your approach to the one by Martinez et al. (2017, https://arxiv.org/pdf/1701.03163.pdf). Perhaps you should mention this paper in the reference section.\n - You use the word \"grammar\" in a slightly strange way. I think replacing \"grammar\" with syntactic constructions would make it clearer what you try to convey. ( e.g., line 90)  - Line 291: I don't think this can be regarded as a variant of it-extraposition. But I agree with the analysis in Figure 2, so perhaps just get rid of this sentence.\n - Line 152: I think the model by Dozat and Manning (2016) is no longer state-of-the art, so perhaps just replace it with \"very high performing model\" or something like that.\n - It would be helpful if you provided glosses in Figure 2.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "49e181ef1e612218",
    "paper_id": "ACL_2017_433",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Nice results, nice data set. Not so much work on Creole-like languages, especially English.",
    "weaknesses": "A global feeling of \"Deja-vu\", a lot of similar techniques have been applied to other domains, other ressource-low languages. Replace word embeddings by clusters and neural models by whatever was in fashion 5 years ago and we can find more or less the same applied to Urdu or out-of-domain parsing. I liked this paper though, but I would have appreciated the authors to highlight more their contributions and position their work better within the literature.",
    "comments": "This paper presents a set of experiments designed a) to show the effectiveness of a neural parser  in a scarce resource scenario and b) to introduce a new data set of Creole English (from Singapour, called Singlish). While this data set is relatively small (1200 annotated sentences, used with 80k unlabeled sentences for word embeddings induction), the authors manage to present respectable results via interesting approach even though using features from relatively close languages are not unknown from the parsing community (see all the line of work on parsing Urdu/Hindi, on Arabic dialect using MSA based parsers, and so on). \nAssuming we can see Singlish as an extreme of Out-of-domain English and given all the set of experiments, I wonder why the authors didn’t try the classical technique on domain-adaptation, namely training with UD_EN+90% of the Singlish within a 10 cross fold experiment ? just so we can have another interesting baseline (with and without word embeddings, with bi-lingual embeddings if enough parallel data is available). \nI think that paper is interesting but I really would have appreciated more positioning regarding all previous work in parsing low-ressources languages and extreme domain adaptation. A table presenting some results for Irish and other very small treebanks would be nice. \nAlso how come the IAA is so low regarding the labeled relations?\n***************************************** Note after reading the authors' answer ***************************************** Thanks for your clarifications (especially for redoing the IAA evaluation). I raised my recommendation to 4, I hope it'll get accepted.",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "1b9a1c09f3ce3a96",
    "paper_id": "ACL_2017_433",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Singlish is a low-resource language. The NLP community needs more data for low resource languages, and the dataset accompanying this paper is a useful contribution. There is also relatively little NLP research on creoles, and the potential of using transfer-learning to analyze creoles, and this paper makes a nice contribution in that area.\nThe experimental setup used by the authors is clear. They provide convincing evidence that incorporating knowledge from an English-trained parser into a Singlish parser outperforms both an English-only parser and a Singlish-only parser on the Singlish data. They also provide a good overview of the relevant differences between English and Singlish for the purposes of syntactic parser and a useful analysis of how different parsing models handle these Singlish-specific constructions.",
    "weaknesses": "There are three main issues I see with this paper: -  There is insufficient comparison to the UD annotation of non-English languages. Many of the constructions they bring up as specific to Singlish are also present in other UD languages, and the annotations should ideally be consistent between Singlish and these languages.\n-  I'd like to see an analysis on the impact of training data size. A central claim of this paper is that using English data can improve performance on a low-resource language like Singlish. How much more Singlish data would be needed before the English data became unnecessary?\n-  What happens if you train a single POS/dep parsing model on the concatenated UD Web and Singlish datasets? This is much simpler than incorporating neural stacking. The case for neural stacking is stronger if it can outperform this baseline.",
    "comments": "Line 073: “POS taggers and dependency parsers perform poorly on such Singlish texts based on our observations” - be more clear that you will quantify this later. As such, it seems a bit hand-wavy.\nLine 169: Comparison to neural network models for multi-lingual parsing. As far as I can tell, you don't directly try the approach of mapping Singlish and English word embeddings into the same embedding space.\nLine 212: Introduction of UD Eng. At this point, it is appropriate to point out that the Singlish data is also web data, so the domain matches UD Eng.\nLine 245: “All borrowed words are annotated according to their original meanings”. Does this mean they have the same POS as in  the language from which they were borrowed? Or the POS of their usage in Singlish?\nFigure 2: Standard English glosses would be very useful in understanding the constructions and checking the correctness of the UD relations used.\nLine 280: Topic prominence: You should compare with the “dislocated” label in UD. From the UD paper: “The dislocated relation captures preposed (topics) and postposed elements”. The syntax you are describing sounds similar to a topic-comment-style syntax; if it is different, then you should make it clear how.\nLine 294: “Second, noun phrases used to modify the predicate with the presence of a preposition is regarded as a “nsubj” (nominal subject).” \nHere, I need a gloss to determine if this analysis makes sense. If the phrase is really being used to modify the predicate, then this should not be nsubj. UD makes a distinction between core arguments (nsubj, dobj, etc) and modifiers. If this is a case of modification, then you should use one of the modification relations, not a core argument relation. Should clarify the language here.\nLine 308: “In UD-Eng standards, predicative “be” is the only verb used as a copula, which often depends on its complement to avoid copular head.” This is an explicit decision made in UD, to increase parallelism with non-copular languages (e.g., Singlish). You should call this out. I think the rest of the discussion of copula handling is not necessary.\nLine 322: “NP deletion: Noun-phrase (NP) deletion often results in null subjects or objects.” This is common in other languages (zero-anaphora in e.g. Spanish, Italian, Russian, Japanese… )Would be good to point this out, and also point to how this is dealt with in UD in those languages (I believe the same way you handle it).\nLing 330: Subj/verb inversion is common in interrogatives in other languages (“Fue Marta al supermercado/Did Marta go to the supermarket?”). Tag questions are present in English (though perhaps are not as frequent). You should make sure that your analysis is consistent with these languages.\nSec 3.3 Data Selection and Annotation: The way you chose the Singlish sentences, of course an English parser will do poorly (they are chosen to be disimilar to sentences an English parser has seen before). But do you have a sense of how a standard English parser does overall on Singlish, if it is not filtered this way? How common are sentences with out-of-vocabulary terms or the constructions you discussed in 3.2?\nA language will not necessarily capture unusual sentence structure, particularly around long-distance dependencies. Did you investigate whether this method did a good job of capturing sentences with the grammatical differences to English you discussed in Section 3.2?\nLine 415: “the inter-annotator agreement has an unlabeled attachment score (UAS) of 85.30% and a labeled attachment score (LAS) of 75.72%.”\n-  What’s the agreement on POS tags? Is this integrated with LAS?\n-  Note that in Silveira et al 2014, which produced UD-Eng, they measured 94% inter-annotator agreement on a per-token basis. Why the discrepancy?\nPOS tagging and dep parsing sections: For both POS-tagging and dep parsing, I’d like to see some analysis on the effect of training set size. E.g., how much more Singlish data would be needed to train a POS tagger/dep parser entirely on Singlish and get the same accuracy as the stacked model?\nWhat happens if you just concatenate the datasets? E.g., train a model on a hybrid dataset of EN and Singlish, and see what the result is?\nLine 681: typo: “pre-rained” should be “pre-trained” 742 “The neural stacking model leads to the biggest improvement over nearly all categories except for a slightly lower yet competitive performance on “NP Deletion” cases” --- seems that the English data strongly biases the parser to expect an explicit subj/obj. you could try deleting subj/obj from some English sentences to improve performance on this construction.",
    "overall_score": "4",
    "confidence": "3"
  }
]