[
  {
    "review_id": "06aaaa1eed2ebec2",
    "paper_id": "ACL_2017_145",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "Overall a very strong paper.",
    "weaknesses": "The comparison against similar approaches could be extended.",
    "comments": "The main focus of this paper is the introduction of a new model for learning multimodal word distributions formed from Gaussian mixtures for multiple word meanings. i. e. representing a word by a set of many Gaussian distributions. \nThe approach, extend the model introduced by Vilnis and McCallum (2014) which represented word as unimodal Gaussian distribution. By using a multimodal, the current approach attain the problem of polysemy.\nOverall, a very strong paper, well structured and clear. The experimentation is correct and the qualitative analysis made in table 1 shows results as expected from the approach.  Thereâ€™s not much that can be faulted and all my comments below are meant to help the paper gain additional clarity.  Some comments:  _ It may be interesting to include a brief explanation of the differences between the approach from Tian et al. 2014 and the current one. Both split single word representation into multiple prototypes by using a mixture model.  _ There are some missing citations that could me mentioned in related work as : Efficient Non-parametric Estimation of Multiple Embeddings per Word in Vector Space Neelakantan, A., Shankar. J. Passos, A., McCallum. EMNLP 2014 Do Multi-Sense Embeddings Improve Natural Language Understanding? Li and Jurafsky, EMNLP 2015 Topical Word Embeddings. Liu Y., Liu Z., Chua T.,Sun M. AAAI 2015 _ Also, the inclusion of the result from those approaches in tables 3 and 4 could be interesting.  _ A question to the authors: What do you attribute the loss of performance of w2gm against w2g in the analysis of SWCS?\nI have read the response.",
    "overall_score": "4",
    "confidence": "5"
  },
  {
    "review_id": "878c1121f1c33c69",
    "paper_id": "ACL_2017_145",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The problem is clearly motivated and defined. Gaussian mixtures are much more expressive than deterministic vector representations. It can potentially capture different word meanings by its modes, along with probability mass and uncertainty around those modes. This work represents an important contribution to word embedding.  This work propose a max-margin learning objective with closed-form similarity measurement for efficient training.\nThis paper is mostly well written.",
    "weaknesses": "See below for some questions.",
    "comments": "In the Gaussian mixture models, the number of gaussian components (k) is usually an important parameter. In the experiments of this paper, k is set to 2. What is your criteria to select k? Does the increase of k hurt the performance of this model? What does the learned distribution look like for a word that only has one popular meaning?\nI notice that you use the spherical case in all the experiments (the covariance matrix reduces to a single number). Is this purely for computation efficiency? \nI wonder what's the performance of using a general diagonal covariance matrix. \nSince in this more general case, the gaussian mixture defines different degrees of uncertainty along different directions in the semantic space, which seems more interesting.\nMinor comments: Table 4 is not referred to in the text. \nIn reference, Luong et al. lacks the publication year.\nI have read the response.",
    "overall_score": "4",
    "confidence": "3"
  }
]