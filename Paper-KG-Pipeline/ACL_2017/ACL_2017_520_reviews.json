[
  {
    "review_id": "13ee009b4c865120",
    "paper_id": "ACL_2017_520",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "",
    "weaknesses": "",
    "comments": "This paper proposes a method for generating datasets of pictures from simple building blocks, as well as corresponding logical forms and language descriptions. \nThe goal seems to be to have a method where the complexity of pictures and corresponding desciptions can be controlled and parametrized.   - The biggest downside seems to be that the maximally achievable complexity is very limited, and way below the complexity typically faced with image-captioning and other multimodal tasks. \n - The relative simplicity is also a big difference to the referenced bAbI tasks (which cover the whole qualitative spectrum of easy-to-hard reasoning tasks), whereas in the proposed method a (qualitatively) easy image reconition task can only be quantitatively made harder, by increasing the number of objects, noise etc in unnatural ways. \n - This is also reflected in the experimental section. Whenever the experimental performance results are not satisfying, these cases seem like basic over/underfitting issues that may easily be tackled by restricting/extending the capacity of the networks or using more data. It is hard for me to spot any other qualitative insight. \n - In the introduction it is stated that the \"goal is not too achieve optimal performance\" but to find out whether \"architectures are able to successfully demonstrate the desired understanding\" - there is a fundamental contradiction here, in that the proposed task on the one side is meant to provide a measure as to whether architectures demontrate \"understanding\", on the other hand the score is not supposed to be taken as meaningful/seriously.\nGeneral comments: The general approach should be made more tangible earlier (i.e. in the introction rather than in section 3)",
    "overall_score": "2",
    "confidence": "4"
  },
  {
    "review_id": "df347c25522ec4d4",
    "paper_id": "ACL_2017_520",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The authors introduce a new software package called ShapeWorld for automatically generating data for image captioning problems. The microworld used to generate the image captions is simple enough to make the data being generated and errors by a model readily interpretable. However, the authors demonstrate that configurations of the packages produce data that is challenging enough to serve as a good benchmark for ongoing research.",
    "weaknesses": "The primary weakness of this paper is that it does look a bit like a demo paper. The authors do provides experiments that evaluate a reasonable baseline image captioning system on the data generated by ShapeWorld. However, similar experiments are included in demo papers.\nThe paper includes a hyperlink to the software package on github that presumably unmasks the authors of the paper.",
    "comments": "Scientific progress often involves some something analogous to vygotsky's zone of proximal development, whereby progress can be made more quickly if research focuses on problems with just the right level of difficulty (e.g., the use of tidigits for speech recognition research in the early 90s). This paper is exciting since it offers a simple microworld that is easy for researchers to completely comprehend but that also is just difficult enough for existing models.\nThe strengths of the work are multiplied by the fact that the software is opensource, is readily available on github and generates the data in a format that can be easily used with models built using modern deep learning libraries (e.g., TensorFlow).\nThe methods used by the software package to generate the artificial data are clearly explained. It is also great that the authors did experiments with different configurations of their software and a baseline image caption model in order to demonstrate the strengths and weakness of existing techniques.  My only real concern with this paper is whether the community would be better served by placing it in the demo section. Publishing it in the non-demo long paper track might cause confusion as well as be unfair to authors who correctly submitted similar papers to the ACL demo track.",
    "overall_score": "2",
    "confidence": "5"
  }
]