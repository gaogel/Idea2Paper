[
  {
    "review_id": "1c5f7e7cbe5d82c5",
    "paper_id": "ACL_2017_150",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "The authors present a novel adaptation of encoder-decoder neural MT using an approach that starts and ends with characters, but in between works with representations of morphemes and characters.  The authors release both their code as well as their final learned models for fr-en, cs-en, and en-cs. This is helpful in validating their work, as well as for others looking to replicate and extends this work.\nThe system reported appears to produce translation results of reasonable quality even after the first training epoch, with continued progress in future epochs.\nThe system appears to learn reasonable morphological tokenizations, and appears able to handle previously unseen words (even nonce words) by implicitly backing off to morphemes.",
    "weaknesses": "In the paper, the authors do not explicitly state which WMT test and dev sets their results are reported on. This is problematic for readers wishing to compare the reported results to existing work (for example, the results at matrix.statmt.org). The only way this reviewer found to get this information was to look in the README of the code supplement, which indicates that the test set was newstest2015 and the dev test was newstest2013. This should have been explicitly described in the paper.\nThe instructions given in the software README are OK, but not great. The training and testing sections each could be enhanced with explicit examples of how to run the respective commands. The software itself should respond to a --help flag, which it currently does not.\nThe paper describes a 6-level architecture, but the diagram in Figure 2 appears to show fewer than 6 layers. What's going on? The caption should be more explicit, and if this figure is not showing all of the layers, then there should be a figure somewhere (even if it's in an appendix) showing all of the layers.\nThe results show comparison to other character-based neural systems, but do not show state-of-the-art results for other types of MT system. WMT (and matrix.statmt.org) has reported results for other systems on these datasets, and it appears that the state-of-the-art is much higher than any of the results reported in this paper. That should be acknowledged, and ideally should be discussed.\nThere are a handful of minor English disfluencies, misspellings, and minor LaTeX issues, such as reverse quotation marks. These should be corrected.",
    "comments": "Paper is a nice contribution to the existing literature on character-based neural MT.",
    "overall_score": "4",
    "confidence": "4"
  },
  {
    "review_id": "b5e71cd06094cfb3",
    "paper_id": "ACL_2017_150",
    "reviewer": null,
    "paper_summary": "",
    "strengths": "In general, the paper is well structured and clear. It is possible to follow most of the explanation, the ideas presented are original and the results obtained are quite interesting.",
    "weaknesses": "I have some doubts about the interpretation of the results. In addition, I think that some of the claims regarding the capability of the method proposed to learn morphology are not propperly backed by scientific evidence.",
    "comments": "This paper explores a complex architecture for character-level neural machine translation (NMT). The proposed architecture extends a classical encoder-decoder architecture by adding a new deep word-encoding layer capable of encoding the character-level input into sub-word representations of the source-language sentence. In the same way, a deep word-decoding layer is added to the output to transform the target-language sub-word representations into a character sequence as the final output of the NMT system. The objective of such architecture is to take advantage of the benefits of character-level NMT (reduction of the size of the vocabulary and flexibility to deal with unseen words) and, at the same time, improving the performance of the whole system by using an intermediate representation of sub-words to reduce the size of the input sequence of characters. In addition, the authors claim that their deep word-encoding model is able to learn morphology better than other state-of-the-art approaches.\nI have some concerns regarding the evaluation. The authors compare their approach to other state-of-the-art systems taking into account two parameters: training time and BLEU score. However, I do not clearly see the advantage of the model proposed (DCNMT) in front of other approaches such as bpe2char. The difference between both approaches as regards BLEU score is very small (0.04 in Cs-En and 0.1 in En-Cs) and it is hard to say if one of them is outperforming the other one without statistical significance information: has statistical significance been evaluated? As regards the training time, it is worth mentioning that the bpe2char for Cs-En takes 8 days less than DCNMT. For En-Cs training time is not provided (why not?) and for En-Fr bpe2char is not evaluated. I think that a more complete comparison with this system should be carried out to prove the advantages of the model proposed.\nMy second concern is on the 5.2 Section, where authors start claiming that they investigated about the ability of their system to learn morphology. However, the section only contains a examples and some comments on them. Even though these examples are very well chosen and explained in a very didactic way, it is worth noting that no experiments or formal evaluation seem to have been carried out to support the claims of the authors. I would definitely encourage authors to extend this very interesting part of the paper that could even become a different paper itself. On the other hand, this Section does not seem to be a critical point of the paper, so for the current work I may suggest just to move this section to an appendix and soften some of the claims done regarding the capabilities of the system to learn morphology.\nOther comments, doubts and suggestions:  - There are many acronyms that are used but are not defined (such as LSTM, HGRU, CNN or PCA) or which are defined after starting to use them (such as RNN or BPE). Even though some of these acronyms are well known in the field of deep learning, I would encourage the authors to defined them to improve clearness.\n - The concept of energy is mentioned for the first time in Section 3.1. Even though the explanation provided is enough at that point, it would be nice to refresh the idea of energy in Section 5.2 (where it is used several times) and providing some hints about how to interpret it: a high energy on a character would be indicating that the current morpheme should be split at that point? In addition, the concept of peak (in Figure 5) is not described.\n - When the acronym BPE is defined, capital letters are used, but then, for the rest of mentions it is lower cased; is there a reason for this?\n - I am not sure if it is necessary to say that no monolingual corpus is used in Section 4.1.\n - It seems that there is something wrong with Figure 4a since the colours for the energy values are not shown for every character.\n - In Table 1, the results for model (3) (Chung et al. 2016) for Cs-En were not taken from the papers, since they are not reported. If the authors computed these results by themselves (as it seems) they should mention it.\n - I would not say that French is morphologically poor, but rather that it is not that rich as Slavic languages such as Czech.\n - Why a link is provided for WMT'15 training corpora but not for WMT'14?\n - Several references are incomplete Typos:   - \"..is the bilingual, parallel corpora provided...\" -> \"..are the bilingual, parallel corpora provided...\"   - \"Luong and Manning (2016) uses\" -> \"Luong and Manning (2016) use\"   - \"HGRU (It is\" -> \"HGRU (it is\"   - \"coveres\" -> \"covers\"   - \"both consists of two-layer RNN, each has 1024\" -> \"both consist of two-layer RNN, each have 1024\"   - \"the only difference between CNMT and DCNMT is CNMT\" -> \"the only difference between CNMT and DCNMT is that CNMT\"",
    "overall_score": "3",
    "confidence": "4"
  }
]