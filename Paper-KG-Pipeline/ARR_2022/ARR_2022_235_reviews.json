[
  {
    "review_id": "81680af5af05010e",
    "paper_id": "ARR_2022_235",
    "reviewer": null,
    "paper_summary": "This paper presents an analysis of the representations obtained from the CLIP language encoder and compares them to representations from the GPT-2 language model. The paper makes the following main contributions:  1) It studies the isotropy of the language encoder of the CLIP model and compares it to that of the GPT-2 language model and finds that:     - CLIP language representations are more isotropic than GPT-2 ones. \n2) It evaluates contextualized word and sentence embeddings obtained from the CLIP language encoder to those obtained from GPT-2 and finds:      - CLIP contextualized word embeddings (CWEs) outperform GPT-2 CEWs on 6 world-level evaluation tasks. \n     - CLIP sentence embeddings outperform GPT-2 sentence embeddings on the STS-benchmark. ",
    "strengths": "- The paper is very well written and easy to follow.\n- The experimental setup is straight-forward and closely follows that of related work.\n- The results are highly interesting and of great relevance for the community. Mostly because:      - The high anisotropy of contextualized word representations derived from GPT-2 or BERT-like models is a puzzeling finding (1) and this paper provides evidence that high anisotropy might be a result of the training objective and not the architecture. This opens up interesting directions for future work. \n    - While the original CLIP paper shows strong results on the visual modality, the text modality is left mostly unexplored. This paper fills that gap and demonstrates the applicability and usefulness of CLIP language representations.  (1) Ethayarajh (2019) https://aclanthology.org/D19-1006/ ",
    "weaknesses": "- The authors use the [EOS] token representation to construct sentence embeddings from GPT-2. This could be a weak baseline. I would recommend to use the average across all tokens in a sequence as another pooling strategy to obtain sentence embeddings. ",
    "comments": "- I would recommend to clarify notation when it comes to GPT-2. There's the GPT-2 architecture and the GPT-2 pre-trained language model. Saying that CLIP uses GPT-2 as it's language model (line 84) might lead readers to assume it uses the pre-trained GPT-2. Which is not the case. It will be helpful if the authors are more explicit and consistent when referring to the CLIP language encoder.\n- In lines 507-514 authors talk about how CWEs lose mutual information with the the input word and cite Voita et al (2019). Additionally, they argue how this might not be the case for the CLIP [EOS] token. I'm not sure if the finding of Voita et al (2019) applies to the CLIP representations as no language modeling objective is used during CLIP training. So mutual information with the input might be higher for all tokens, not just the [EOS] token. This is something that could be checked. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]