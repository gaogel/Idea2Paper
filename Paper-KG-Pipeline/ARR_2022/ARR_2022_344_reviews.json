[
  {
    "review_id": "9be4cd67158be4fa",
    "paper_id": "ARR_2022_344",
    "reviewer": "Ramy Eskander",
    "paper_summary": "The paper explores zero-short cross-lingual transfer (through fine-tuning) for the task of part-of-speech tagging, as a case-study. The authors evaluate the effect of several factors such as the LDND distance between the source and target languages, matching language families, matching writing systems and the pre-training of the source and target languages. In addition, the paper proposes insights on the selection of the source language when transferring to a target language of little or no annotated data. Moreover, the authors promise the release of 65 fine-tuned language models. The paper shows that pre-training on the target language and the LDND distance between the source and target languages have the biggest impact on the cross-lingual performance. They also show that English is not necessarily the optimal source language as usually perceived by the community. ",
    "strengths": "- The paper investigates an interesting question, especially with the recent increasing interest  in multilingual NLP. While the paper focuses on part-of-speech tagging as a case-study, the conclusions might be generalizable to other cross-lingual tasks such as named-entity recognition and dependency parsing.\n- The paper proposes an in-depth analysis, both quantitative and qualitative, that studies several factors that impact the cross-lingual performance. ",
    "weaknesses": "- The claims in the paper are too strong for cross-lingual transfer, as there are other cross-lingual techniques that are out of the scope of the paper such as annotation projection. Instead, it should be explicitly mentioned that the findings only apply to zero-shot model transfer through fine-tuning.\n- There is a strong bias towards Indo-European languages, and thus it's hard to generalize the findings on what a good source language is.\n- There is no discussion about the impact of word order, as expected from the introduction, other than in Table 1. ",
    "comments": "- Make it clear that the paper focuses on zero-shot model transfer through fine-tuning instead of generalizing the findings on cross-lingual transfer.\n- It would be valuable to investigate the reason behind the high impact of the LDND distance.\n- I would recommend removing singletons from Figure 4.\n- Add statistical significance tests in Section 5.\n- Cite Pires et al., 2019 in the 2nd paragraph in the introduction. ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "82ec5736c2f53f33",
    "paper_id": "ARR_2022_344",
    "reviewer": null,
    "paper_summary": "This work provides a comprehensive empirical analysis on cross-lingual transfer for POS tagging over 100 languages (on UD v2.8). The pre-trained XLM-RoBERTa model is taken as the backbone, fine-tuned on one source language with a large enough training set (65 source languages) and then tested on target languages (105 languages). The results show that there can be various factors that influence the effectiveness of cross-lingual transfer, like pre-training inclusion, language families, etc. Further analyses and discussions over the influencing factors are also included. ",
    "strengths": "This work provides a comprehensive exploration for cross-lingual transfer for POS tagging, with abundant experimental results.\nCertain influencing factors for transfer effectiveness are analyzed, with further qualitative discussions on best source language/language-pair, which may provide some useful information for future work and relevant applications. ",
    "weaknesses": "One of my concerns is that though it's nice to have results over such a large number of language pairs, only looking at the overall trend might not be enough to reveal certain interesting patterns. There can be too many influencing factors, some of which are explored in this work, while some others might also be important, such as training data size, quality of annotation, etc. Especially, when looking at the source languages, it seems that a large portion of them belong to Indo-European, which may make the overall results biased. Maybe it would be nice to explore a small subset of languages from different language families.\nIt seems that \"pretraining-inclusion\" is a major predictor, which may be not surprising. Nevertheless, there might be a hidden factor inside: if a certain language is not included in the pre-training, the vocabulary of the pre-training model might not even have items for the words in this language, which might be split into strange pieces by the subword segmenter. In addition, source-target vocabulary overlapping may be another important factor to look at. I think there should be more exploration and analysis in these aspects. ",
    "comments": "There is a related work [1] performing similar experiments and analyses as in this work, but with more tasks and ranking features (though not using pre-trained models). It would be helpful to check it.\nThe prediction of POS tagger mostly depends on the lexicons themselves. It might be interesting to explore simpler models with static (cross-lingual) word embeddings, especially for the languages that are not included in the pre-training of XLM-R. [1] (Lin et al., 2019) Choosing Transfer Languages for Cross-Lingual Learning. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]