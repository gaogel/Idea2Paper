[
  {
    "review_id": "850e20b46914cc63",
    "paper_id": "ARR_2022_208",
    "reviewer": null,
    "paper_summary": "The authors present a method of calibrating learned multiclass classification models during training, that is, improving model calibration (in other words, pushing accuracy-versus-model-confidence graphs towards the identity line---well-calibrated models have confidence perfectly reflecting prediction accuracy).\nThe proposed method is done during training, rather than being a post-hoc model which recalibrates a learned model to maximize performance on a held-out calibration set. This has the benefit of allowing the model to use the whole train set (i.e. not requiring a held-out calibration set), in addition to ideally leveraging the entire train set to do calibration. In this sense it follows a small number of published methods.\nOverall the core writeup of the method (sec 3) was difficult to follow. The core design decisions of the method were difficult to find the motivation of. I believe the decisions pivot around an increased sample-efficiency claim (line 316), but this claim was not stated precisely (the free variable $\\epsilon$ bounds \"calibration error,\" which I do not believe is defined), and the claim does not have a proof, so the relationship between the system setup decisions and sample efficiency claims is not at all clear.\nGenerally, the calibrated systems' plots do not seem obviously notably superior to the baseline calibrations (Fig 2, Fig 1(top)), though the methods do indeed achieve superior ECE (expected calibration error) performance across some of the tasks (xSLUE, a suite of multiclass classification NLP tasks, Table 1) and, somewhat surprisingly, superior top-level accuracy/F1 (table 1).\nGiven the importance of proper calibration to so many applications of NLP, I suspect these methods would be of interest to practitioners in the field even if they do not provide an across-the-board calibration improvement, but it is not clear from the writeup when they work and when they do not.  That is, this writeup does not really address the question \"when should a practitioner use this very complex method of calibration, rather than the much conceptually simpler post-hoc Platt scaling, given that the curves in Fig 2 and Fig 1 seem to hint that the calibration benefits of this method seem to be very noisy.\" Is there a property of the dataset or predictor where we can expect this method to really help?\nGenerally, I think with notable revisions this paper would be of interest to practitioners, but it is somewhat difficult to follow the method exactly and know how we should expect it to affect performance on a given novel task. ",
    "strengths": "- The authors present a novel calibration technique in multiclass classification setups that leverages the entire train set (rather than a held-out calibration set). This method learns both a Platt-scaling transform (an affine transform on logits, basically), in addition to iteratively adaptively binning the train set based on this learned transform, I believe so that we can calculate the ECE without a held-out set.\n- The proposed methods perform superior to baselines with respect to calibration error on more tasks than they perform worse.\n- The proposed modifications improve against the baseline MLE systems in terms of accuracy/F1 across many tasks; that is, the calibration term appears to act as an effective model regularizer in some setups. ",
    "weaknesses": "- The technical description of the method (sec 3) was quite difficult to follow. The central methodological decisions (adaptive binning, discretization thereafter by doing what appears to be essentially isotonic regression) were difficult to find the motivation of. I believe these decisions center around being able to make a sample efficiency claim (line 317), but I'm not sure why this method gives that sample efficiency property, and no proof of the claim is presented. More concrete questions/concerns are given in \"comments\" below, written as I read the presentation.\n- There are a few properties of the evaluation/results which are a bit confusing and call conclusions into question. For example:   - PosPS (post-hoc Platt Scaling) is lower than MLE in accuracy on a few tasks---If I understand correctly, this should not be true in principle, since the max predictions are supposed to be invariant. Is this just model retrain noise from starting off at different initial model params? Or do I misunderstand? \n  - The reliability diagrams (Fig 1, top), that is the accuracy vs confidence plots, certainly don't make the proposed methods look better (a perfect system will have the identify function here), and in fact exhibit some pretty notable pathologies (high-accuracy spikes in low confidence regimes, e.g.). Is this a property of dataset pathologies or does it reflect variance/unpredictability in the proposed methods? \n  - In the ECE-versus-number-of-bins plots in Figure 2, the two calibrated systems (PosCal and the proposed) all have ECEs (calibration performance) very close to the MLE for most values. This hints at the fact that the methods may be very sensitive to bin size and often provide no actual expected calibration improvement, is that right? How was this bin-size hyperparam selected? Was its selected value chosen from the held-out dev set without looking at test at all?\n- Difficult to see how this can extend to the structured prediction encoder/decoder setups used quite often across NLP. Does this only work for relatively few-class multiclass classification? This is not a fatal flaw. ",
    "comments": "- \"These observations prove the efficacy of our method in maintaining a perfect balance between model performance and model uncertainity-a testimony of an ideal calibrator\" this is a pretty over-the-top claim! \" Perfect balance?\" This doesn seem like isn't an \"ideal calibrator,\" it has nontrivial ECE still, right? I'd scale this paragraph's claims back to things that are empirically supportable - tab 2 is somewhat confusing. Can you turn P1 and P2 into one confusion matrix per row and present it that way? These different columns are all just very different sort of things, strange that they're presented next to each other - What is Fig 2's dataset? the average across all the tasks? Just one task?\n- The definition of perfect calibration in the info is perhaps a bit confusing as-is (namely, $P$ has to be a joint over the covariates $x$ and the predictions $f$ right so you need some sort of metric over both, is that right? Or perhaps you just require $f$ to be Borel-measurable and deterministic or something. ( Unrelatedly, I also realize that if $f$ is itself nondeterministic, then you probably need an expectation operator right?). Anyways, it might be helpful to add a short sentence to the text here explaining this eq in words, no need to be too precise.\n- Is the output of step (3) differentiable? Do you need subgradients or something to get the loss? I guess this is why it's helpful to know exactly what $\\beta$ is.\n- A nit but i might use something other than \"distance\" to describe $d$ in line 231, since it's not in general a valid distance metric, maybe \"calibration mismatch\" or something.\n- Also maybe change $q$ to $\\hat q$ in the eq on line 232? Since the thing you're minimizing isn't the true value $q$, which we don't have access to even in principle.\n- I'm a bit perplexed by the discrepancy between the matrix $Q$, which is essentially a function of the predictor network, and the $q$ in 232, which is not. Does $Q$ as given suffice to allow us to calculate arbitrary distance functionals between $p$ and $q$, as described in line 231?\n- You haven't defined calibration error but instantiate it via $\\epsilon$ in line 254. It's abs(p - q) to use the terminology of line (232), is that right? ECE is usually given with the expectation randomness integrating over the simplex $D_K$ right, do we have to do that here? Not sure if this is essential to get all the details precise here but probably it would be good to define what \"calibration error\" is at the least.\n- Don't think you define what the Platt-scaling set $G$ is ranged over by the argmin in step 1.\n- What is the set $\\beta$ in line 311?\n- It's not clear to me why step (3) of the algorithm should be necessary at all. It seems like this wouldn't change the calibration but allows us to estimate the ECE? Is that right? Or does this adaptive binning + discretization (essentially isotonic regression, right?) actually affect calibration in expectation?\n- Line (7) of alg 1's pseucode is referenced in the text but the fig doesn't have line numbers, is it possible to add them \"the achieved reduction in ECE as compared to all baselines is significant\" what does this mean? Paired t-test? p < 0.05? Either describe the test or remove this significance claim - do you have a proof of the sample-efficiency claim in line 317? would be good to put this into an appendix - Is it possible to compare to MCDropout? It would be really nice to see that comparison, but this isn't crucial. ",
    "overall_score": "2.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]