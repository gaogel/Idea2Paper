[
  {
    "review_id": "f50c6edd6f7d33a8",
    "paper_id": "ARR_2022_141",
    "reviewer": "Malte Ostendorff",
    "paper_summary": "The paper presents a sentence-level matching method for fine-grain document similarity of research papers (named ASPIRE). The method relies on co-citation sentences as learning signal to train a multi-vector model. The key difference to related work is the ability of determining paper’s similarity not only on a single aspect but multiple ones, whereby the set of aspects is not predefined.  The method uses a combination of a SciBERT encoder, contrastive learning, multi-instance learning, and optimal transport. In experiments with four datasets all baselines are outperformed. Besides the promising experimental results, one strength of presented method is its scalable inference that could easily enable future applications based on the method. ",
    "strengths": "- ASPIRE outperforms the strong baselines (e.g., SPECTER).\n- A series of relevant ablations are presented. \n-Even though the paper presents a niche topic, the approach is important and could be also applied on other domains apart from scientific documents. \n-The multi vector approach makes aspect similarity feasible for production environments due to scalable inference (with ANN). ",
    "weaknesses": "- The approach description (§ 3) is partially difficult to follow and should be revised. The additional page of the camera-ready version should be used to extend the approach description (rather than adding more experiments).\n- CSFCube results are not reported with the same metrics as in the original publication making a comparison harder than needed. ",
    "comments": "- The standard deviation from the Appendix could be added to Table 1 at least for one metric. There should be enough horizontal space.\n- Notation of BERT_\\theta and BERT_\\epsilon is confusing. Explicitly calling them the co-citation sentence encoder and the paper encoder could make it clearer.  - How are negative sampled for BERT_\\epsilon?\nAdditional relevant literature: - Luu, K., Wu, X., Koncel-Kedziorski, R., Lo, K., Cachola, I., & Smith, N.A. (2021). Explaining Relationships Between Scientific Documents. ACL/IJCNLP.\n- Malte Ostendorff, Terry Ruas, Till Blume, Bela Gipp, Georg Rehm. Aspect-based Document Similarity for Research Papers. COLING 2020.\nTypos: -\tLine 259: “cotation” -\tLine 285: Missing “.” ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "30aa157cc690ecbf",
    "paper_id": "ARR_2022_141",
    "reviewer": "Jingang Wang",
    "paper_summary": "This paper tries to use co-citations in full-text papers to learn an aspect matching model with fine-grained aspect representation of paper abstracts in sentence level for fine-grained retrieval, and it also achieves to compute document-level distance with a multi-vector document similarity model for abstract level retrieval. ",
    "strengths": "Advantages of this paper are as follows. \n• Co-citations used in this paper seems to be a new data source for fine-grained retrieval task. \n• The proposed ASPIRE model uses co-citing sentences as textual supervision signals to train fine-grained retrieval models. \n• This paper gets good performance across several datasets. ",
    "weaknesses": "Weaknesses of the paper: • The mix of different approaches and tasks leads to a confusion for readers. \n• The logic flow of the paper needs to be improved. \n• Not sure why the proposed model work for the experimental datasets, since those datasets do not have such textual supervision as co-citing sentences. \n• There are no comparison and contrast between proposed approaches and baselines. ",
    "comments": "I strongly suggest to improve the narrative of the paper. ",
    "overall_score": "2.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]