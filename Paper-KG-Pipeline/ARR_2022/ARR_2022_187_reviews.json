[
  {
    "review_id": "6695a2d16cadbaad",
    "paper_id": "ARR_2022_187",
    "reviewer": "Yilun Zhu",
    "paper_summary": "This paper proposes a neural pairwise ranking model to evaluate readability assessment. The model outperforms existing methods on two tasks - cross-corpus ranking and zero shot, cross-lingual ranking - based on ranking metrics. The paper also publish a new bilingual (English-French) readability dataset for cross-lingual evaluation. ",
    "strengths": "1. The pairwise ranking method shows better robustness on various monolingual dataset and out-domain cross-lingual dataset than previous methods. \n2. The paper creates a new cross-lingual dataset for readability assessment. ",
    "weaknesses": "1. The paper does not clearly introduce the dataset. For example, what is average length of each text in NewsEla? If the text is long, the paper should also mention how the text is encoded by BERT when it exceeds the maximum length. \n2. As the paper already pointed out, the nature of pairwise model restricts the inference for downstream applications, so that the contribution of this paper is not very strong. ",
    "comments": "- L400: citation for \"for evaluating ranking or information-retrieval tasks in literature\"?\n- L502: Is NPRM (0.999, 0.995, 0.990, 0.948) better than regBERT (0.999, 0.997, 0.994, 0.977)?\n- L613: \"while\" -> \"\" ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "d60ff492ac6bf5bd",
    "paper_id": "ARR_2022_187",
    "reviewer": "Matthew Mulholland",
    "paper_summary": "This paper explores the topic of automatic readability assessment, specifically reframing the task as one of pairwise ranking. The authors experiment with neural and other baseline models. ",
    "strengths": "- A number of data-sets are used, some for testing only - Cross-corpus and cross-lingual experiments are done to show generalizability - A number of baseline systems are used, such as traditional classification- and regression-based systems (where possible) for comparison - A data-set is being released along with this work, which may lead to more development opportunities in the future ",
    "weaknesses": "- For the regression-based systems, I see that OLS was used while for the classification-based systems, SVM was used. Why not use SVR for the former? It would seem to be a closer comparison and it might prove to be better baseline against NPRM, etc.\n- For the cross-linguistic analyses, multilingual BERT is used. I would assume French and Spanish are included here, so claiming that this does well cross-linguistically assumes that too, right? ",
    "comments": "- Are \"slugs\" restricted to being only in either test/train for the cross-validation experiments? I'm a little unclear on if this would be necessary or not. It might be good to address whether this is a concern.\n- \"In this background\" - This phrase is used a couple of times and it's unclear what is meant. Consider rephrasing, e.g. \"With this background\", \"Given this background\", etc.\n- 2 Related Work - \"result in\" --> \"results in\" - 4.4 Evaluation - \"w\" --> \"we\"? It's unclear what is meant.\n- Limitations of NPRM - \"while\" --> \"why\" (or just get rid of \"while\") ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "da389b316edeba40",
    "paper_id": "ARR_2022_187",
    "reviewer": "Qinlan Shen",
    "paper_summary": "This paper proposes to examine the problem of Automatic Readability Assessment (ARA) as a neural pairwise ranking task, with the goal of addressing multiple transferability issues with classification-based ARA.  The paper compares a pairwise neural ranking model with existing non-ranking ARA methods and explores the monolingual and cross-lingual transferability of their ARA ranking model.  From a technical contributions standpoint, the paper mostly provides benchmarking results for ranking vs. other ARA paradigms, rather than a new model. ",
    "strengths": "The paper is thorough in what ARA experiments are run across different paradigms (classification, regression, ranking) and transferability settings (monolingual/cross-lingual transfer).  The paper also provides a well-motivated argument for the benefits of using a ranking-based paradigm for readability assessment, especially for settings involving transfer. ",
    "weaknesses": "The paper lacks significance tests in their comparisons between models, which may draw some of the conclusions in the paper into question.  The results themselves could be presented more clearly with a little more reflection on why certain methods seem to work better than others.  Most of my concerns are covered in more detail in Comments, Suggestions, and Typos. ",
    "comments": "Lines 207-208: Isn't this just mathematically equivalent to taking Binary Cross Entropy loss over the sigmoid?\nLines 225-232: One limitation of the current ranking based approach is that while it's easier to compare across different datasets with potentially different label scales, running comparisons over multiple instances may be less efficient.  For example, if we want to get an idea of the absolute readability for a new document compared to a set of S other documents that have \"already been scored\", we would have to re-run the NPRM over the S documents in your comparison basis.  Have you thought of methods of aggregating or scaling the NPRM score to get around this drawback?  Or are there particular applications you can see where this comparison problem as not being as much of an efficiency issue.\nSection 5.3: One suggestion I had to make things a little easier to follow in this section is to add a table comparing the performance of the best models for each paradigm (classification, regression, pairwise-ranking) on the ranking metrics.  Currently, it's a little hard to follow the written results here because the reader has to check between multiple tables to make the comparison.\nTables 1-6: I would recommend bolding the best performance numbers in these tables.\nResults: Overall, did you run any significance tests when making comparisons between two different models?  Some of the numbers, such as the pairwise ranking evaluations and the comparisons between classification, regression, and ranking on the ranking metrics are quite close.  I would recommend adding a note in the text whenever you make a comparison between two models as to whether the difference in results is significant.\nLine 559-561: Do you think the drop in performance for Vikidia-Fr is because of the domain difference between Newsela and Vikidia? ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "0a5b410ba117cc9d",
    "paper_id": "ARR_2022_187",
    "reviewer": null,
    "paper_summary": "The paper proposes a new pairwise ranking approach for the task of automatic readability assessment (ARA). The proposed approach is compared against other modeling paradigms used in the ARA literature, namely classification and regression across 3 languages (English, French and Spanish). The experiments show that the approach works competitively in monolingual settings and gets strong results in cross-corpus and zero shot cross-lingual settings. The paper also releases a new parallel bilingual ARA dataset for future research. ",
    "strengths": "1. Clear motivation for need of such a technique with cross-corpus compatibility being a realistic desired property for ARA approaches.\n2. Well defined research questions, adequately answered through experiments.  3. Thorough experimentation, spanning 3 languages and different modeling approaches.\n4. New dataset (and the first of its kind) for a relatively understudied task of ARA. ",
    "weaknesses": "1. Not clear if the contribution of the paper are sufficient for a long *ACL paper. By tightening the writing and removing unnecessary details, I suspect the paper will make a nice short paper, but in its current form, the paper lacks sufficient novelty.  2. The writing is difficult to follow in many places and can be simplified. ",
    "comments": "1. Line 360-367 are occupying too much space than needed.  2. It was not clear to me that Vikidia is the new dataset that was introduced by the paper until I read the last section :)   3. Too many metrics used for evaluation. While I commend the paper’s thoroughness by using different metrics for evaluation, I believe in this case the multiple metrics create more confusion than clarity in understanding the results. I recommend using the strictest metric (such as RA) because it will clearly highlight the differences in performance. Also consider marking the best results in each column/row using boldface text.  4. I suspect that other evaluation metrics NDCG, SRRR, KTCC are unable to resolve the differences between NPRM and the baselines in some cases. For e.g., Based on the extremely large values (>0.99) for all approaches in Table 4, I doubt the difference between NPRM’s 0.995 and Glove+SVMRank 0.992 for Avg. SRR on NewsEla-EN is statistically significant.  5. I did not understand the utility of presenting results in Table 2 and Table 3. Why not simplify the presentation by selecting the best regression based and classification based approaches for each evaluation dataset and compare them against NPRM in Table 4 itself?  6. From my understanding, RA is the strictest evaluation metric, and NPRM performs worse on RA when compared to the baselines (Table 4) where simpler approaches fare better.  7. I appreciate the paper foreseeing the limitations of the proposed NPRM approach. However, I find the discussion of the first limitation somewhat incomplete and ending abruptly. The last sentence has the tone of “despite the weaknesses, NPRM is useful'' but it does not flesh out why it’s useful.  8. I found ln616-632 excessively detailed for a conclusion paragraph. Maybe simply state that better metrics are needed for ARA evaluation? Such detailed discussion is better suited for Sec 4.4 9. Why was a classification based model not used for the zero shot experiments in Table 5 and Table 6? These results in my opinion are the strongest aspect of the paper, and should be as thorough as the rest of the results.  10. Line 559: “lower performance on Vikidia-Fr compared to Newsela-Es …” – Why? These are different languages after all, so isn’t the performance difference in-comparable? ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]