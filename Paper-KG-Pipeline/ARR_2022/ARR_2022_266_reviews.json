[
  {
    "review_id": "770026b8f19c8c72",
    "paper_id": "ARR_2022_266",
    "reviewer": null,
    "paper_summary": "This work proposes to frame the task of controlled text generation as sampling from a combination of black-box models that are responsible for various components of interest such as fluency, the controlled attribute, and relevance to the prompt. Pre-trained black-box experts provide scores for each of these desired components that are linearly combined to form an \"energy-based\" model, from which samples can be drawn without needing any task-specific training. Experiments are conducted on several tasks such as controllable debiasing, sentiment and formality transfer and prompted generation, and results show that this method outperforms task-specific baselines. ",
    "strengths": "1. Paper is well-written and easy to follow. \n2. The idea of having a modular controlled text generation model that uses blackbox components is interesting and novel. The approach can be adapted to any desired components or metrics by using the appropriate pre-trained expert models, and does not need any additional task-specific training. \n3. Several experiments are conducted to show that their model outperforms task-specific baselines on multiple tasks. Ablation results are also shown which help assess the contributions of each component. \n4. Human evaluations are conducted to demonstrate the superior quality of the generated text. Some limitations of the model are also briefly discussed. ",
    "weaknesses": "1. One of the main drawbacks of this approach is that presumably the different component black-box experts of the controlled text generation have to be manually selected and the weighted linear combination has to be fine-tuned for each task. It is also not discussed if the inference time is significantly affected by this approach. \n2. For the sentiment transfer task, the model with the higher Hamming distance coefficient is considered to be the best model based on the BertScore with respect to the source, which essentially measures how much deviation has been introduced. It appears however that the model with the higher Discriminator coefficient is better, in terms of perplexity and the internal/external classifiers. Given that the Hamming distance in the reference is much higher, it may not be necessary to absolutely reduce the number of changes made, if it serves the overall purpose of the text generation to make more changes. This is somewhat true for the formality transfer task as well. \n3. In Table 3, for the formality transfer task, the method sees a decline in performance for the ->Informal task. While the improvement in the ->Formal task is probably a decent tradeoff, this issue is not addressed at all. \n4. Percentage preference through majority voting is reported for the human evaluation. More robust correlation/agreement metrics such as Cohen's Kappa should be reported for reliability. ",
    "comments": "- BertScore and BLEURT are inconsistently typeset through the paper (alternatively as Bertscore or Bleurt). It would be better to maintain consistency.\n- Line 244 in Section 2.3 refers to $E_{gen}$ and $E_{rev}$ which have not been previously introduced. It is not easy to deduce what they mean since they are not explained until the next section. Some re-writing for clarity might help here.  - Line 182: discirminate: discriminate - Line 203: This penalization token -> This penalizes token - Line 254: describe -> described - Line 376: Dathathri et al. (2020) -> (Dathathri et al, 2020) - Line 434: Ma et al citation missing year - Line 449: describedd -> described - Line 449: in the text -> in a text - Line 520: prodduct -> product - Table 3 BertScore(sc) -> BertScore (src) - Line 573: which use for -> which are used for  - Line 631: similar, approaches -> similar approaches ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work."
  },
  {
    "review_id": "148e732f32b1256c",
    "paper_id": "ARR_2022_266",
    "reviewer": "Nishant Subramani",
    "paper_summary": "The authors of this work propose Mix and Match LM. Their method uses global-scores for controllable text generation by drawing samples from an energy-based model. Here, the values of the energies come from blackbox models that correspond to attributes of generation (fluency, faithfulness to conditioning, etc.). They show empirical gains relative to some other baseline methods. ",
    "strengths": "- The paper is written well and much of the method is quite accessible. I feel like readers would have a decent grasp of the method and approach after a single read.\n- The method is simple.\n- Demonstrate performance improvements against methods that are more computationally prohibitive or necessitate fine-tuning.\n- Quite modular and hypothetically flexible to add in other aspects - more experimentation would need to be done to see how truly flexible to add stuff in. ",
    "weaknesses": "For the same problem and tasks, there's a lot of other prior work that are not energy-based modeling (two are) that could be compared to. After reading, I'm unsure how well this method would compare to those and what situations this approach would be better for. Many of those combine some blackbox methods in different ways. A discussion of these works and comparisons would really strengthen the paper greatly. Some of the work I'm talking about are: DExperts: Decoding-time controlled text generation with experts and anti-experts (Liu et al. 2021) Plug and Play Autoencoders for Conditional Text Generation (Mai et al 2020) Sentence Bottleneck Autoencoders from Transformer Language Models (Montero et al. 2021) A distributional approach to controlled text generation (Khalifa et al. 2020) GeDi (Kruase et al. 2020) DAPT (Gururangan et al. 2020) CTRL (Keskar et al. 2019) FUDGE (Yang and Klein 2021) You do cite a few of these, but comparisons to them would really strengthen the work.\nThese papers just came out, around or after the ARR deadline, but could be interesting too:  - Controlling Conditional Language Models with Distributional Policy Gradients (Korbak et al. 2021) - Bag-of-Vectors Autoencoders for Unsupervised Conditional Text Generation (Mai and Henderson 2021) Many of these methods compare favorably against PPLM, so they could be stronger baselines against which your work could be compared to.\nAnother aspect that could be improved is the experimentation around the trade-offs between accuracy and BLEU score. Table 6 and 7 start this. It'd be informative to see some of the baselines varied slightly on a plot similar to Mai et al. 2020 or Montero et al 2021 for this task where self-BLEU/ref-BLEU is on one axis and accuracy via the classifier is on the other and things are plotted (top right being better).\nA task such as toxicity via RealToxicityPrompts could be quite interesting and offer a test-bed for comparison against some of the other prior work.\nI'd be interested in seeing how this method extends to other models, i.e. an auto-regressive model such as GPT-2. Is it simply extendible? How well would it do?\nThis is a paper on controllable text generation and a method for this. A discussion of the potential broader impact is missing. It has potentially wide-ranging downstream impact, so including one to highlight potential harms and effects is important. ",
    "comments": "Misc comments: - line 35: cite GPT3 - line 46: cite GPT2 - Figure 1: The caption seems oddly placed being on the right-hand-side (presumably this was to fit the figure easily for review, but may not strictly be okay so something to check, I'm not sure).\n- line 331: what discriminator did you use? Did you use the same one for the PPLM setup for evaluation?\nAnother flavor of related work to help motivate the approach (no fine-tuning, no optimization needed), could be around prompting, steering LMs without fine-tuning, etc.\nItâ€™s Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners (Shick and Schutze 2020) Few-Shot Text Generation with Pattern-Exploiting Training (Shick and Schutze 2020) Eliciting Knowledge from Language Models Using Automatically Generated Prompts (Shin et al. 2020) Can Unconditional Language Models Recover Arbitrary Sentences? ( Subramani et al. 2019) ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]