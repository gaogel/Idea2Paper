[
  {
    "review_id": "ed6a448153d21c5e",
    "paper_id": "ARR_2022_159",
    "reviewer": null,
    "paper_summary": "This paper dealt with a interchange intervention training with a distillation technique. They propose to train a student model to become a causal abstraction of a teacher model, which is faithful with a simpler causal structure. Their experimental results show that the proposed method improved some tasks compared with a standard distillation method. ",
    "strengths": "+ Distillation method with a causal abstraction is novel approach. \n+ They experimented the proposed method with various types of tasks. ",
    "weaknesses": "+ They did not show the detailed method (of distillation training procedure) in the paper. They should include them in main text not in appendix. \n+ They should explain why the proposed model can deal with a causal abstraction. At least they should show some samples and analyze them. \n+ They should compare among the proposed methods and analyze these results. ",
    "comments": "+ Performance of each teacher model should be shown in Table 1. \n+ They can use the following paper: Knowledge Distillation from Internal Representations [Aguilar, AAAI20] ",
    "overall_score": "2.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]