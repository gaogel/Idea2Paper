[
  {
    "review_id": "5b5ef3b14ef11f5d",
    "paper_id": "ARR_2022_164",
    "reviewer": null,
    "paper_summary": "This paper is about determining the syntactic ability of two Dutch variants of transformer based language model BERT: BERTje and RobBERT. The authors use a Multiple Context Free Grammar (MCFG) formalism to model two patterns of Dutch syntax: control verb nesting and verb raising. These rule-based grammatical models are used to generate a test set which is limited by a bound on recursion depth and populated from a lexicon. For evaluation, each verb occurrence garners a prediction of which referential noun phrase is selected by it, and the resulting accuracy is reported. The authors show results that demonstrate drastically worse performance as recursive depth and number of noun phrases increase, and conclude that the models have not properly learned the underlying syntax of the linguistic phenomena they describe; ie discontinuous constituents/cross-serial dependencies. ",
    "strengths": "As someone unfamiliar with Dutch and with this area of research, I felt this paper did an excellent job of motivating their reasoning for their research and of describing the ways that Dutch syntax is different from English. Figures and examples were clear and well-done. The article was clearly and concisely written, and appears to be a valuable contribution that adds counter-evidence to claims about how much syntax BERT-based models actually “know”. Authors are careful not to exaggerate the consequences of their findings and make suggestions for how this work could be expanded with other languages or other tasks. ",
    "weaknesses": "I was unable to get the provided code to work. I tried both on my Macbook and on a Linux-based computing cluster. To be fair, I did not try for very long (< 15 minutes), and I also did not have access to a GPU so I tried to run it on a CPU. It’s possible that was the problem, but it wasn’t stated that that was a requirement. It seems that if I understood the instructions in the readme properly, there were a few __init__ files missing. However, even after changing those, I ran into a number of other errors. The readme was also a bit sparse, ie “Play around with the results as you see fit”. I commend the authors for including the code and data with the submission, but I would have liked to see a script included already (i.e. not just a snippet in the readme) along with a brief description of any dependencies required beyond the requirements.txt and what one might expect when running the script.\nAnother weakness I felt, was a lack of description of previous/related work. They mention in the very beginning that \"Assessing the ability of large-scale language models to automatically acquire aspects of linguistic theory has become a prominent theme in the literature ever since the inception of BERT\", but didn't reference other work to provide similar counter evidence to the consensus they referenced in Rogers et al. (2020). As someone not familiar with this area of research, maybe there is not so much to cite here, but if that is the case, I feel it should be mentioned why there is no related work section. ",
    "comments": "Citation should be formatted like so: \"The consensus points to BERT-like models having some capacity for syntactic understanding (Rogers et al., 2020).\" ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "1 = Not my area, or paper is very hard to understand. My evaluation is just an educated guess."
  }
]