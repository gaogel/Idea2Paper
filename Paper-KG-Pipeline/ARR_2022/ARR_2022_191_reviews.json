[
  {
    "review_id": "be6cec72e91dff75",
    "paper_id": "ARR_2022_191",
    "reviewer": null,
    "paper_summary": "The paper tackles the challenge of long documents/dialogs summarizations. \nIt presents a simple but efficient approach \"split-then-summarize\" using a greedy algorithm for choosing the good partition.\nThe experiments in the second part of the paper show a real improvement using this generic algorithm (agnostic to the used model)  over the naive baseline. ",
    "strengths": "- The challenge that this paper tends to solve is a painful problem and the proposed solution is simple to implement and yields to good results.\n- The paper is well written and easy to follow and understand.\n- The set of experiments proving the efficiency of the method is good. ( not optimal) ",
    "weaknesses": "- In the related work, you cite several approaches - It would be fair to compare the SummN results with the implementation of the reported approaches. \nComparing SummN to the naive Baseline (Bart) is obviously better, but what is the benefit of SummN over the previous solutions ? ",
    "comments": "In the Target segmentation, it is not clear, what can we do when the number of sentences is lower than K ? \nIn section 5.6, could you please provide the Kappa ? ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "1aabeed5142e70ba",
    "paper_id": "ARR_2022_191",
    "reviewer": null,
    "paper_summary": "The paper presents a framework for abstractive summarization of long documents that repeatedly segments text, summarizes each segment and then feeds the concatenated summaries as an input to the next iteration. When the input is below a predefined number of tokens, the final summary is generated. This design has the advantages of being able to use powerful pretrained models (e.g. BART) that are designed for shorter text, and avoiding expensive attention computation over long spans. ",
    "strengths": "- The approach is simple, powerful and flexible - a good idea for applying powerful pretrained models to longer text without truncation.\n- A very comprehensive evaluation is provided with a good selection of datasets and both automatic and human evaluation which show the model’s clear advantage over baselines.\n- The paper is well-written and easy to understand. ",
    "weaknesses": "- **Inefficient design of target segmentation**: If I understand correctly, all input segments are assigned a target segment. This probably causes unnecessary summarization of irrelevant input segments - if the text is massive and the gold summary is very short, why not ignore most of the input segments and only select sufficient input segments to cover the targets? Also, this allows targets to be duplicated over different input segments.  - **Missing analysis of model behavior**: It would be useful and interesting to know what exactly is happening at each stage, e.g. how much text is produced at each stage, and how noisy or detailed intermediate summaries look like.\n- **Missing analysis of empirical running time** ",
    "comments": "**Suggestions** - Line 249: “Huge time costs” sounds very informal, maybe change to “considerable running time” or similar - Please indicate if f-score, precision or recall is used when ROUGE is mentioned - Follow-up on weakness 1): ignoring input segments could be implemented by assigning “empty” targets, e.g. using a single special token that when predicted would trigger a segment to be discarded in the next step.\n**Questions** - Table 9 in Appendix: what does “keyword emerged in gold summary” mean?\n- “Separate model for each stage” is mentioned, but I am still not sure whether this means 2 (fine and coarse) or N models?\n- Does the number of input segments at each coarse stage always stay the same as it was in stage 1, before reaching the fine-grained stage? ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "3b15e747f73612a9",
    "paper_id": "ARR_2022_191",
    "reviewer": null,
    "paper_summary": "This work approaches the task of long text summarization with a multi-stage framework, dealing with the input length limitation of modern Transformer-based models. In this work, the summarization is performed in N coarse-grained stages and one fine-grained final stage. The coarse stage aims to not only generate the coarse summary but also segment the input text. The coarse stage performs multiple times to gradually reduce the input length specific to the input length limitation of the backbone model. The training instances for the coarse stage are prepared with a greedy algorithm for aligning the source segments and the target segments. BART is adopted as the backbone summarization model. The proposed method was evaluated on both dialogue and written data. A total of five datasets are involved in the experiments. Experimental results show the proposed methodology outperforms baseline models in most cases. ",
    "strengths": "- The work addresses a very practical and important issue in abstractive summarization. The length limitation of modern Transformer-based models forms a major barrier to long text summarization, while longer the document/dialogue, the higher the demand for summarization.  - The proposed method is model-agnostic where a variety of models can be chosen as the backbone summarizer. Table 6 shows all the three backbone models benefit from the multi-stage strategy. ",
    "weaknesses": "- The computation cost can be an issue for the multi-stage generation. A measurement of the runtime can be supplied for comprehension. ",
    "comments": "- I am curious about the number of total predictions to be made for a single input. As shown in Table 1, the number of N + 1 is 3 for the GovReport dataset. The total number of summarization to be performed is N + 1 or N^2 + 1? ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]