[
  {
    "review_id": "308a3dae511df165",
    "paper_id": "ARR_2022_17",
    "reviewer": null,
    "paper_summary": "The paper introduces a set of the speech tasks called SUPERB-SG that can be used as an addition to the currently available SUPERB set for benchmarking speech-based semi-supervised learning models. SUPERB-SG can be used to evaluate the generalization and semantic capabilities of the models by including the tasks that concentrate both on content understanding abilities of the models (speech translation and ASR) and signal quality properties of the audio (speech separation and speech enhancement), as well as includes out-of-domain ASR tasks that allows to test the capabilities of ASR generalization. The paper provides statistics and evaluation results on the proposed benchmark using a big variety of pertaining approaches, while in their analysis includes cross-task similarities and the generalization ability of the approaches tested. ",
    "strengths": "The usage of the new benchmark is well motivated as it provides more in-depth evaluation of speech models on tasks that are more diverse and include tasks that look at very different aspects of voice signal. The proposed set of tasks is well chosen and can provide a useful extension of SUPERB benchmark set which will allow more generalizable conclusions. The findings suggest that there is no single model that works for all of the tasks, which show additional evidence of the usefulness of the diverse tasks used in the proposed benchmarking set. The paper includes a big variety of the baselines (pretraining approaches) tested that make the empirical evidence of the usefulness of the proposed benchmarking dataset. ",
    "weaknesses": "The authors have no mention of releasing the code for preprocessing of the datasets and evaluation. Since the main contribution of the paper is standardizing evaluation for speech-based semi-supervised models across multiple dimensions, releasing such code would be crucial in order to make a strong contribution to the community. Otherwise, this paper would not be as useful and would be hard to compare against. Also, including more results on the original SUPERB dataset will make it easier to put the results on the proposed tasks into a perspective. ",
    "comments": "No ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "6267cc0fb878ff34",
    "paper_id": "ARR_2022_17",
    "reviewer": null,
    "paper_summary": "This paper is an extension of the SUPERB benchmark for assessing pre-trained upstream speech models acquired through Self-supervised learning. The authors argue that SUPERB is limited to 10 tasks and that 5 new tasks, here AST, Out-of-Domain ASR, Voice Conversion, Speech separation and Speech enhancement would permit to better evaluate the semantic and generative capability of such SSL models. The authors compare the same SSL models used in the SUPERB paper on the 5 tasks. All SSL models were used as-is without fine-tuning on the task (frozen). Overall results show some superiority of Wav2vec and Hubert models on AST and OOD ASR tasks, while VC SE and SS tasks lead to less obvious superiority of models. The authors then performed experiments with varying data training for some SSL models or downstream tasks and showed that the ranking of ssl models stay consistent which should prove the robustness of these new evaluation tasks. ",
    "strengths": "- Setting up benchmarks is highly important to assess SSL speech models - The proposed tasks provide a complementary view to the 10 already existing task of SUPERB  - The authors have shown that the proposed tasks leads to consistent results according to the upstream and downstream training data - Although the authors did not trained the SSL models, this stay an impressive and useful amount of work, that according to what I understood from the text, should be made available to the community (when?) ",
    "weaknesses": "- If the 5 new tasks are indeed assessing different properties than the 10 superb tasks do, they are not all well justified. AST is tested on only one couple of language (EN->De) while OODASR on 3 (+spontaneous). Regarding SE and SS, given that SSL models are trained only on clean read speech it is not properly justified why such tasks are relevant.  - The 5 tasks are indeed an interesting addition to SUPERB but : AST is not new since it has already been used in SSL benchmarks (cf. LeBenchmark). A more realistic OODASR would arguably be out-of-domain data (cf. Hsu et al 2021)  rather than out of language data for which XLSR and XLS-R are far more adequate. Finally, the SE and SS tasks are difficult to justify and probably far less relevant for an ACL venue than more speech oriented events.   - Such contribution is really useful to assess to which extent models can improve downstream tasks but it does not reveal why they do it. At the end we don't learn much about the models.  - Finally, it is rare that models are used without fine-tuning so it is unclear why such fine-tuning was not considered in the study (adaptability is not an interesting feature?) ",
    "comments": "Most of my comments are given above.  Details : some references have no venue -> please correct this.     misc{hsu2021robust,       title={Robust wav2vec 2.0: Analyzing Domain Shift in Self-Supervised Pre-Training},        author={Wei-Ning Hsu and Anuroop Sriram and Alexei Baevski and Tatiana Likhomanenko and Qiantong Xu and Vineel Pratap and Jacob Kahn and Ann Lee and Ronan Collobert and Gabriel Synnaeve and Michael Auli},       year={2021},       eprint={2104.01027},       archivePrefix={arXiv},       primaryClass={cs. SD} } ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]