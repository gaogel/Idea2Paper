[
  {
    "review_id": "7e09b61c2285ad6b",
    "paper_id": "ARR_2022_274",
    "reviewer": null,
    "paper_summary": "Masked language models suffer from 2 big problems:  Overthinking - too much processing may hinder results for some instances. \nLong inference times - due to high rate of queries and amount of computation needed. \nTo address these issues, adaptive inference has been suggested. In adaptive inference, classification layers are added to intermediate layers of the model. During inference, the model may choose to perform early exit, and make a prediction on a specific sample  without utilizing all of it’s processing power (i.e. all transformer blocks). \nWhether or not the model makes an early prediction is based on hyperparameters and architecture specific details. \nUsing this approach helps deal with both issues at once. It prevents some samples from being over-processed, and allows the model to make a prediction before overfitting, and also speeds up inference, as many samples do not reach the final layer for a prediction.\nIn this work, the authors suggest a new way of deciding whether to perform early exit. This is a combination of 2 previous approaches which exploit the classifier’s output (as a confidence measure) and the agreement between previous classifiers. Each method has its advantages and drawbacks: Exiting based on a confidence threshold is dependent on a single layer’s output, making it prone to noisy decisions. However it is easy to control the model’s speed-accuracy tradeoff  in real time  by adjusting the confidence threshold. Exiting based on classifier consensus is not as easily adjustable, but provides more stable results. By combining the two methods, the writers utilize the advantages of each method while avoiding their flaws. \n  They define some threshold which determines whether a classifier is confident of it’s prediction. Additionally, they define a patience counter which counts how many consecutive classification layers have been confident in their prediction (based on the previous definition). The proposed model will perform an early exit based on layer K’s prediction if layer K is confident, and the patient counter is above a predetermined limit. \nThis allows for robustness while still providing an easy real time control of the speed-accuracy tradeoff, and achieves SOTA results on most of the GLUE benchmark tasks. ",
    "strengths": "This paper’s topic is practical and can benefit both research and industrial usages.  Progressing on previous works, utilizing their strengths and overcoming drawbacks.\nImprovements to SOTA maintained through various tasks and benchmarks.  Interesting ablation studies, strengthening the main claim of the paper.  applicable to other DL tasks. ",
    "weaknesses": "The paper is missing some implementation details. How many tests were run? Are the results an average of a few runs? If so, what is the STD of these results? How many epochs of training?  A similar but not identical issue: There is no mention of the computational budget\\cost in this article.  A natural question to ask is how PCEE-BERT compares to a model that requires both patience and confidence, for which the patience would also be defined as consistent prediction (Similar to PABEE). It makes me wonder why this was not tried.  It is claimed that the improvement in results (compared to PABEE) stems from the fact that PCEE-BERT requires some measurement of confidence in order to perform an early exit. To be convinced of this being the main explanation, I would have liked to see the two models compared in a “budgeted exiting” scheme and getting comparable results. If you can not perform such experiments for some reason, maybe address this in a “future work” section.\nWhat is the cost of using the PCEE-BERT? How does it affect training time? how many more parameters are added to the model? ",
    "comments": "Training the BERT base model with multiple intermediate classification layers may impede the performance of each separate layer (a Bert model with only 3 layers, will perform better than the 3rd layer on a full model trained with multiple classification layers). This is because the optimizer is focused on improving a single classifier’s loss (instead of many). For this reason, I would like to see the PCEE-BERT compared to smaller models (instead of a large model with budgeted exit).  Using only the speedup ratio for diagnosing the behavior of the model is lacking important information. Which layers are the ones actually being used? A histogram of the number of examples classified by each layer may reveal interesting evidence on the model’s performance. May also allow better comparison between models.  Some of the results in the paper suggest the “overthinking problem” may not exist. First, I fail to see how figure 1 demonstrates the problem at all. What I understand from this plot is that the MCC (which is a way of measuring the model's success) gets higher with the number of layers, and the entropy (which is an inverse measurement of how sure the model is of it's prediction) gets lower with the number of layers. So it seems to me that  the more layers the model has, the better it performs on both measurements, although it is possible that I am missing something. If the overthinking problem did exist, I would expect the mcc to have a maximum at an intermediate layer. Second, I would expect the performance of the model to slightly increase when the speedup ratio is low (but not 0). It seems from the results that the best performance is still achieved by the largest model. Hence, It seems that using the largest possible model would still yield the best results (as opposed to what is claimed in the overthinking problem).\nI am not a vision expert, but it seems to me that the ratio between added parameters of a classification layer and convolutional layers is much less significant than that of a simple FC layer and a transformer. Maybe the problem is actually non-existent in vision tasks?  It seems that adjusting PABEE through the patience parameter still provides a useful range for the S/R tradeoff. Can this not be done in real time? Am I missing something?  I fail to see how the ablation study of cross layer ensemble is relevant to this paper. You need to further explain why you are comparing PCEE-BERT to an ensemble model, and what insight did you gain from the results. In addition, the exact exit strategy of the model you are comparing PCEE-BERT to is unclear. Is it similar to a budgeted exit where you simply take the ensemble instead of the desired intermediate layer? or is it a PCEE-BERT in which when a decision to exit has been made, the ensemble is used instead of the intermediate layer?  Why is the loss structured that way? Why is it normalized? and why is it weighted? Is training layer 12 more important than training layer 4?  l2 remove “the”  l6 more widely = wider? forbids = blocks, prevents, prohibits, impedes? \nl14 enough numbers of consecutive = enough consecutive? \nl24 make achieve l29 have become or became l33 remove “and” l39 remove comma, add “and” l58 have no doubt (need a comma or without have) l65 much often (add more)  l81 drop “the” l89 process = handle, deal with? \nl135 exit’s = exit layer’s ? \nl158 block instead of black l159 ** same as l14 **  l189 ablation l331 missing “not”?  l24 make achieve l58 have no doubt (need a comma or without have) l65 much often l158 black->block l159 number of, you already said it once, I understand what you mean, but it is not the numbers that you want enough of. Maybe just drop it or rephrase. ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "3406e6c5f3945b41",
    "paper_id": "ARR_2022_274",
    "reviewer": null,
    "paper_summary": "The authors present a method of adaptively truncating a BERT forward pass (or any deep-net with an iteratively-modified hidden vector a-la self-attention) at inference time in a classification setting; this method in theory allows a practitioner to trade off accuracy for latency: early-stopping takes fewer FLOPs but does slightly worse on average.\nThe model is, like the baselines compared to, adaptive, in the sense of allowing different test examples to use more or less computation as needed. The way it works is by training a per-layer classifier during finetuning for each layer; each hidden layer can be used to directly predict the label. Once a sufficient number of layers yield sufficiently low-entropy predictions, computation can be terminated (with both of these \"sufficiencies\" in the previous sentence corresponding to a different inference-time hyperparameter).\nThe classification tasks on GLUE are analyzed under this setting; the setup does a bit better than the competing adaptive-computation methods in the regime of heavily-truncated computations (only going through about 25% of the BERT's layers at test-time), and does comparably or marginally better, in general, than the other adaptive methods at less aggressive speedups.\nThe authors also demonstrate that the method is applicable to a ResNet trained on CIFAR to do vision classification.\nGenerally, the performance gains are not unambiguous and the settings in which this method is strongly preferred to other comparable systems could be clarified. I believe the comparison would be greatly strengthened by comparing directly to the more obvious way of reducing inference-time computation (namely, simply using a smaller fixed-sized model, comparable in FLOPs or wall-clock throughput to the adaptive large model).  I suspect the body of researchers interested directly in studying the performance-computation tradeoff of problems may find the work of interest---this method seems to consistently outperform the other comparable systems across a few of the tasks probed. ",
    "strengths": "- Comparison of this method to a number of directly comparable published adaptive computation methods on a number of baselines (GLUE tasks and a basic CIFAR-10/100 vision setup).\n- Method outperforms competing methods in very-restricted-computation regime (that is, when very aggressively early-stopping inference forward-passes).\n- Method does comparably or better than a few comparable systems for a few tasks across different accuracy/performance tradeoffs.\n- The method is intuitive and the presentation was generally clear (except a few smaller points of clarification I outline below in \"suggestions\"). ",
    "weaknesses": "- Not totally clear exactly when you'd want this scheme over the various other related early-stopping-adaptive-inference BERT setups. It seems like this setup generally works about the same as (or marginally better than) the others on many of the tasks (Table 2), and better in the regime of very-truncated computations (only doing a forward pass through 25% of the network on average). Could be clarified a bit if there are other clear differences in behavior beyond doing better in the very-aggressive-early-stopping regime.\n- The main comparison I'd really like to see is a comparison to smaller fixed-size models, ideally of different sizes. If the point of the early-exiting program is to save computation, then the most straightforward to reduce computation is to try a smaller model (simpler than a large model that does adaptive inference-time things which are not present during learning). It's possible that just using fixed-size smaller BERTs will do better in the performance-versus-latency tradeoff. I don't think this is a rejection-worthy shortcoming but I think it would really improve the strength of the claims in the paper.\n- I'm concerned by the speedup metric (this is not a rejection-level concern---I'm just arguing that they're using a proxy measurement without providing proof that it stands as a linearly-related proxy). The central results are task-specific-performance versus relative speedup afforded by the early stopping model. However, the speedup used as the x axis of the graphs is essentially the average percentage of layers the model goes through---that is, for a 50-layer BERT, if examples stop on average at layer 25, this will have a speedup ratio of 0.5. The authors assert \"according to our experiments, it is proportional to actual wall-clock runtime\". I think it's very important to show this, particularly because in the world of GPU usage and batching, there's often a weak coupling and non-linear relationship between the number of FLOPs required total (which is what this is) and the actual runtime (which depends on GPU utilization, which depends on the shape of the calculation, which depends on batching), and in particular this can vary from task-to-task. So this is a proxy measurement of efficiency (the \"efficiency\" we would actually care about in any setting is throughput or wall-clock time, rather than the number of flops required). The authors claim they observed a proportionality between the two, but the extent to which one actually stands as a proxy for the other (in particular, as a linearly-related proxy) seems very important to explicitly quantify and present. ",
    "comments": "- There's two task-specific hyperparams to set to parameterize early stopping, aren't there, the $\\tau$ param which binarizes the \"is this a low-entropy-enough\" decision, and the number of layers after which stopping occurs if the entropy's sufficiently low, I believe. These assumedly interact and each of them contribute to the speedup. How was this two-parameter decision turned into a single-parameter speedup? That is, the paper/system is motivated by there being a single knob to turn to control accuracy/computation tradeoffs, but this is two knobs, I think. Could be clarified in the \"experiments\" writeup.\n- Generally, I found the motivations somewhat contrived (the part about keeping up with high demand for flu queries via adaptively scaling back latency during flu season, for example, or sec 3.1 saying that the fact that there's a discontinuity in the PABEE latency-accuracy tradeoff curve, etc). It's not at all clear that \"the accuracy on this GLUE task must be exactly 60%\", as given in 3.1 as a desideratum limiting industrial applicability of other techniques, ever makes sense. I thing it really suffices to say \"we are investigating the accuracy/latency tradeoff, motivated by the fact that some applications will require lower latency\" and leave it at that? I would expect most readers would recognize this to be one of the central tradeoffs across the entire field and acknowledge that it's useful as an object of study.\n- Just to be clear -- the different early-existing classifiers $f^{(i)}()$, for $i=1,\\ldots,m$ each get the training label during fine-tuning right (that is to say, each of them is trained to predict the exact same label as the others, conditioned on a different intermediate vector)? This could be stated more explicitly in subsecs 2.2 and 2.2.1 (I may have just missed it).\n- What are the relative weights $m$ in 2.2.1? are they hyperparams? 2.2.2 seems to describe different schemes for using weights $m$ during inference, but aren't they needed during training? Are they fixed the same across the different setups?\n- hard to read table 2 because of its size. maybe if there's room in next revision or appendix turn each of these cols into a small fig like fig 3? ",
    "overall_score": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]