[
  {
    "review_id": "555b9b4626e46bff",
    "paper_id": "ARR_2022_352",
    "reviewer": "Daniel F Campos",
    "paper_summary": "The authors compress generalized models using established quantization methods. Finding these fail, they evaluate the failure and to rectify it introduce a novel quantization framework that offers improvements leaps and bounds better than prior methods. Their token-level contrastive distillation approach is elegant, simple, and easy to comprehend. ",
    "strengths": "The authors introduce a well-researched, effective, and scalable method for quantizing generalized models. They implement competitive baselines,  evaluate the failure cases. Using the discovery that quantization produces homogenous embeddings they create their contrastive token distillation method which is shown to improve the generator model performance heavily. ",
    "weaknesses": "The only weakness I see in this paper is a lack of description of the incurred cost for token-level conservative loss. Without being able to see the associated code and run the experiments it is difficult to understand what negative impact using token level contrastive learning has. How much slower does it make the training? How does it affect GPU memory usage? ",
    "comments": "It would be beneficial to the community to release the software and associated models along with exporting the models to an engine that can take advantage of lower precision for increased inference speed. ",
    "overall_score": "4.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "cb1661f19db1f1d4",
    "paper_id": "ARR_2022_352",
    "reviewer": "Parminder Bhatia",
    "paper_summary": "Paper proposes quantization of generative PLMs coming from GPT and BART family. To overcome the challenges of homogenous word representations with quantized model which can impact conditional models such as GPT, authors present two modifications 1./ Word level contrastive distillation loss. 2./ Module aware quantization that depends on the magnitude of the layer to incorporate more adaptability.  Empirical results on various tasks show that proposed method outperforms the state of-the-art compression methods on generative  PLMs across next utterance prediction and summarization problems. With comparable performance with the full-precision models, paper present 14.4× and 13.4× compression rates on GPT-2 and BART, respectively. ",
    "strengths": "Paper proposes quantization of generative PLMs coming from GPT and BART family. To overcome the challenges of homogenous word representations with quantized model which can impact conditional models such as GPT, authors present two modifications 1./ Word level contrastive distillation loss. 2./ Module aware quantization that depends on the magnitude of the layer to incorporate more adaptability.  Lot of earlier work focusses on BERT based model. Thorough analysis and proposal to mitigate gaps for generative models makes compelling case for future research where the problem is harder than the BERT counter-part and opens avenues for generative tasks by quantizing large PLMs.\nEmpirical results on various tasks show that proposed method outperforms the state of-the-art compression methods on generative  PLMs across next utterance prediction and summarization problems. With comparable performance with the full-precision models, paper present 14.4× and 13.4× compression rates on GPT-2 and BART, respectively. Thorough ablation studies for the two proposed directions. ",
    "weaknesses": "Lot of interesting ideas presented int he paper. It would be useful to incorporate latency aspects of the proposed methods. Additionally, it seems that GPT is used for utterance and BART for summarization but it will be good to add results for both the models across the tasks to see their impact. ",
    "comments": "There is lot of content and tables. Perhaps moving Related work higher might be useful for readers. Additionally adding results for both QuantGPT and QuantBART for both the tasks for completeness. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work."
  }
]