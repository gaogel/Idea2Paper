[
  {
    "review_id": "94148813f03ca637",
    "paper_id": "ARR_2022_302",
    "reviewer": null,
    "paper_summary": "The paper presents experiments where mBERT models along with appropriate task-specific layers on top are fine-tuned on multiple languages simultaneously. Moreover, active learning is used across all training languages based on low confidence criteria.  The tasks addressed are tagging, NER, and dependency parsing. experiments are done on English, Spanish, Japanese, German and Dutch.\nThe paper seems to presume that zero-shot transfer of individual monolingual models has been the only method of choice when tackling multilingual tasks. However, there is a sizable body of work that aims solely on producing a single model for various languages, seeking to reduce the number of parameters while retaining performance. Here are some examples: https://proceedings.neurips.cc/paper/2021/file/473803f0f2ebd77d83ee60daaa61f381-Paper.pdf https://arxiv.org/pdf/1602.01595.pdf ",
    "strengths": "A fair comparison between various mono and multilingual setups and active learning regimen. ",
    "weaknesses": "No comparison with past works that produce higher quality multilingual models.  In fact, no comparison with past work is made. ",
    "comments": "The paper needs more innovation and comparison with past work.\nAL in ML setting could be a line to pursue further. ",
    "overall_score": "2.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "92b7e51465728e0c",
    "paper_id": "ARR_2022_302",
    "reviewer": null,
    "paper_summary": "This paper proposes a strategy of joint learning across multiple languages using a single model. \nExperimental results on a diverse set of tasks with multiple languages under limited annotation budgets show that the proposed method outperforms the models that are trained on one language and employed with zero-shot transer for inference and the models that are trained on target datasets where the annotation budget are equally divided across languages. \nThe paper also demonstates that active learning provides additional benefits. ",
    "strengths": "- The propose method outperforms two alternative approaches in diverse tasks with several languages.\n- The paper presents the effectiveness of active learning on a single model for multiple languages. ",
    "weaknesses": "The paper lacks clarity for the details of the experimental settings and results.\n- There is a lack of information about how to give annotations to selected data. If annotations are given by an oracle or human annotators, it must be mentioned in the paper.\n- When not applying active learning, what are different between seed and following rounds? Does the setup without AL mean applying random sampling for annotation?\n- Is annotation budget equally divided to each round (b_t/{number of rounds} for each round) or does every round have b_t budget? Also, does a training set linearly grow as the round progresses or are annotations acquired in the previous round discarded and no longer used in the following rounds?\n- It is hard to understand the results shown on Figure 1 and 2. Figure 1 contains 14 legends but only 13 lines are shown. Some lines look very similar and not distinct (e.g. MMA and MM[100%]). What do the types of lines (solid, dotted, dashed) mean?\n- It is hard to understand Figure 3. Do the bars indicate difference of tokens and the lines indicate performance? ",
    "comments": "- Footnote 1 (Page 7): For -> for ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]