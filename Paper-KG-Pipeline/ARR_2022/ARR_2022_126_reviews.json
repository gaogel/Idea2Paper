[
  {
    "review_id": "0717df21c948fc36",
    "paper_id": "ARR_2022_126",
    "reviewer": null,
    "paper_summary": "This paper introduces a dataset intended for structured multi-document summarisation, based on annotated meta-reviews from recent ICLR conferences along with the corresponding reviews and ratings of the papers. The data is thoroughly described and analysed, and a series of experiments are carried out comparing a range of baselines to a system based on Transformers. The paper presents the results from a particular configuration, while the appendix gives more details of the parameter space and other settings that were explored. A human evaluation was also carried out, which indicated that the proposed method produced output that was rated more highly than the baseline systems. The authors intend to release the data and code when the paper is published. ",
    "strengths": "The main strength of this paper is the quality of the dataset, which is of a good size and appears to have high-quality, reliable annotations. This dataset, together with the code for the baseline and Transformer-based systems, can provide an extremely useful resource for other researchers working in this and similar areas.\nThe paper itself is well written throughout, and the inclusion of the appendix giving details of the other settings that were explored is a useful extra. ",
    "weaknesses": "The \"Related Work\" section is quite brief: it cites a number of papers, but mostly only in a list of references with very little context. It would help to give a more thorough comparison, particularly to the most similar approaches such as the Bhatia et al. (2020) paper which also addressed meta-reviews. ",
    "comments": "See above for my main comments.\nThe font in Table 7 in particular is EXTREMELY small, particularly with the highlighting which makes it even harder to read. Similar issues also apply to the axes of the graph in Figure 5. ",
    "overall_score": "4.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "e6cb5ef05444c6a3",
    "paper_id": "ARR_2022_126",
    "reviewer": "Hyunwoo Kim",
    "paper_summary": "The authors collect the meta reviews of ICLR and label the sentences with 9 categories of sentence types: abstract, strength, weakness, rating summary, AC disagreement, rebuttal process, suggestion, decision, and misc.. With these sentence labels, the authors introduce a task of controlled generation focused on the macro structure of the review. They show some control methods to begin with this task. ",
    "strengths": "- The meta review dataset looks very interesting, and many researchers (including me) will be interested in playing with models trained on the dataset. ",
    "weaknesses": "- The experiment section is not informative to the readers, because it is stating obvious results such as Transformer models being more “capable of capturing content-specific information”.\n- The case study section is also not very informative. The authors evaluate whether the model assigns bigger attention weights to the appropriate control token when generating relevant sentences. However, they only show a single sample as an illustration. Such a single sample illustration is very vulnerable to cherry-picking and biases. Moreover, interpreting bigger attention weights as being more important is somewhat controversial in the field. It would be more helpful to the readers if the authors show diverse quantitative analyses.\n- The authors show how different control sequences affect the generated outputs with only 3 examples, which is extremely limited to see whether the model can be controlled. ",
    "comments": "- It will be interesting to drop the control sequences from the gold labels (or replace them with negative ones) one after one and see how the automatic metric changes. This will show whether the control tokens really do contribute to the model’s performance. ",
    "overall_score": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "0bf79665ef5fe215",
    "paper_id": "ARR_2022_126",
    "reviewer": null,
    "paper_summary": "This paper proposes a Meta-Review Dataset of annotated ICLR reviews and a new task of controlled generation with passage macro structures. The annotation is done by attributing each sentence to one of the 9 categories (abstract, strength, weakness, ... ). It also builds some baselines for this task. ",
    "strengths": "- proposed a task of controlled text generation based on passage macro structure using sentence labels - collected and annotate a meta review dataset - good analysis about the properties of the dataset ",
    "weaknesses": "- The type of annotation that labels each sentence into 1 of 9 category is very specific to this task and does not transfer to broader control text generation problem. It has some value as a niche research benchmark but lacks broader impact on research/application ",
    "comments": "Referring BART as Transfermers in table 5 is misleading, could just write BART to indicate it is pretrained. ",
    "overall_score": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]