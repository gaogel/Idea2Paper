[
  {
    "review_id": "c84a94af6f02cdbc",
    "paper_id": "ARR_2022_328",
    "reviewer": null,
    "paper_summary": "This paper releases a dataset of punctuated transcripts of spontaneous speech. The speech is automatically generated from instructional livestreams but the punctuation is added by manual annotation.  The main novelty of this work is the dataset itself, which includes punctuation on spontaneous speech because models trained on prepared speech do not perform as well.  Finally, the authors demonstrate that performance on downstream tasks improves with better punctuation annotations. ",
    "strengths": "- New punctuation dataset on spontaneous speech is useful for cascade systems where the ASR system does not output punctuation, if the original audio doesn't exist or where computing resources are limited and transcripts exist.\n- ",
    "weaknesses": "1. Punctuation only includes four marks (.,!?), but others are more common (specifically \":). Why not include these? ( non-scientific source: http://www.viviancook.uk/Punctuation/PunctFigs.htm).\n2. Table 4, if I understand the experiment correctly, seems unfair. You're comparing sentence segmentation from models trained on mostly punctuated text to your model, which expects unpunctuated text. A fairer comparison would be to run the original text through a pretrained punctuation restoration model. ",
    "comments": "What is the point of the taxonomy when you have four punctuation marks? ~L130 The annotator disagreement might be interesting for the different punctuation marks. I expect high agreement for question marks but much lower for commas. ",
    "overall_score": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "155fec04885f4a7a",
    "paper_id": "ARR_2022_328",
    "reviewer": "Hrishikesh Terdalkar",
    "paper_summary": "The main contribution of the paper is a human-annotated dataset (BehancePR) for punctuation restoration in livestreaming video transcripts. Punctuation restoration is the process of restoring text structures such as sentences, phrases etc by inserting four types of punctuation marks (period, comma, question mark and exclamation mark) into non-punctuated text. The paper also illustrates that several standard NLP toolkits underperform on the sentence-boundary detection task on non-punctuated transcripts. ",
    "strengths": "The paper highlights some interesting points why existing models might be unsuitable for task of punctuation restoration on video transcripts. \nAlthough it is unclear how effective the presented dataset would be, it has a potential for being useful in further research in this area. \nThe trained model seems to use one additional punctuation mark than the norm in the area. ",
    "weaknesses": "It is unclear if the additional punctuation mark of exclamation mark adds any significant theoretical difficulty to the problem apart from the lack of trained models. In fact, it would play a role in poor performance of models that have not been trained on dataset without that punctuation mark, which doesn't truly highlight the flaw in these models. The drawn conclusion that this is due to the \"different nature of data\" thus does not hold much value. \nFurther, in the field of PR, it seems like there is plenty of scope for creation of synthetic datasets by simply removing punctuation marks from arbitray English texts, movie transcripts and so on. Comparative study of such synthetic datasets, and an analysis of why human-annotated sets might be of more value, if at all, should be performed. ",
    "comments": "Creation of synthetic datasets on large scale, by removing punctuation marks seems possible. This option should be explored. \nModels trained on such synthetic datasets can also be compared on the human-annotated datasets. These might serve as a better baseline than models trained on data without the same set of punctuation marks. ",
    "overall_score": "2 = Revisions Needed: This paper has some merit, but also significant flaws, and needs work before it would be of interest to the community.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "2f3ae15fda7644fb",
    "paper_id": "ARR_2022_328",
    "reviewer": null,
    "paper_summary": "The paper presents a dedicated annotated corpus for the punctuation restoration task. The dataset annotated 4 types of punctuations, i.e. comma, period, question, and exclamation for live streaming video transcripts. The use of live streaming video transcripts poses new and interesting challenges compared to existing datasets in which  punctuation restoration is included as a sub-task of automatic speech recognition. Some of the challenges include the presence of more speakers, less formal and more spontaneous speech, and presence of frequent emotional sentences that need to be marked by exclamation marks. The dataset is annotated by experienced annotators. The authors have evaluated the state-of-the-art punctuation restoration models (neural based BiLSTM and graphical based CRF) and found that the performance of the PR models on their data set is far below compared to existing datasets revealing its challenging nature. Adding data augmentation brought slight changes in the performance. When these models were tested for domain adaptation i.e. training on existing datasets and testing on proposed dataset it was found that performance of all the models degrade significantly. They have also evaluated the performance of existing NLP toolkits for the task sentence splitting on this dataset and found that they perform very poorly suggesting the importance of this kind of training data. ",
    "strengths": "The paper presents a punctuation restoration dataset from the domain of livestreaming videos. Livestreaming videos are one of the main modern mediums of communication and an increasing number of livestreaming videos are becoming a potential knowledge base.  The paper describes many distinctive characteristics of livestreaming video transcripts that are essential to study for the task of punctuation restoration as well as for the field of automatic speech recognition.\nThe paper shows how the state-of-art model performs poorly on this dataset raising the need to develop models which can deal with the practical challenges posed by the dataset.\nThe paper also demonstrates that the current models performance degrades when the domain is changed from formal to informal domains even when they are almost monologual. Further study of this problem is very important.\nThis paper also reveals how existing NLP toolkits fail in the simple task of sentence boundary identification without the availability of proper punctuation. This suggests the need for the inclusion of punctuation restoration models, which can handle the type of challenges described in the paper, in these toolkits.\nThe dataset will be made public which will help the progress of the field. ",
    "weaknesses": "None ",
    "comments": "The authors may include some discussion on inter-annotator disagreements.\n8 annotators were used but only the roles of 6 annotators are described. Was there any hierarchical annotation role?\nLivestreaming videos tend to have more word eros. What is the impact of word errors?\nIn domain adaptation, if the models are trained on the proposed dataset and tested on the existing dataset, does it perform better than the other way around? ",
    "overall_score": "4.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "d513753e38dcfe4c",
    "paper_id": "ARR_2022_328",
    "reviewer": null,
    "paper_summary": "The paper produces a dataset and baseline systems for punctuation restoration in live-streaming videos. The motivation is that prior datasets for punctuation restoration are on scripted or non spontaneous speech like TED talks and meetings. ",
    "strengths": "The dataset is potentially useful, and the baseline experiments as well as comparisons to existing libraries are well carried out. ",
    "weaknesses": "The dataset and problem seems somewhat niche to me. Does it matter that the existing datasets are not fully spontaneous speech? I see from the experiments that the TED model doesn't generalize well to Behance, but is that because of the nature of the conversations or the domain (design)? I worry that this dataset, coming from such a narrow domain, will only be helpful for building models for that domain, which isn't particularly useful for wider applications of NLP.  At the very least, I would like to see some experiments using models trained with the Behance dataset on other domains as test sets. It would also be helpful to have a deep dive into the existing datasets to help the reader understand how exactly the Behance dataset differs (summary statistics as well as illustrative qualitative examples). It may also be good to add in more diverse sources of spontaneous speech to go beyond the design domain, such as YouTube videos or Spotify podcasts.\nThe agreement score of 0.59 seems low for punctuation restoration. How does it compare to previous annotation efforts on the other datasets. The paper says \"Toward the end, the annotators are allowed to discuss to make the final version of the dataset\" [line 154] but no details are given on how the annotators come to an agreement. Are there any patterns (tricky decisions, ASR issues, etc) in the disagreements? These would all be helpful to know. ",
    "comments": "See the weaknesses section for suggestions. ",
    "overall_score": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]