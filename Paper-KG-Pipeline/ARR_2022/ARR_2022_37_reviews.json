[
  {
    "review_id": "736b12f6224f77a1",
    "paper_id": "ARR_2022_37",
    "reviewer": null,
    "paper_summary": "This work presents a novel NER learning framework to address the poor performances issue on out of vocabulary (OOV) entity recognition; and, more in general, to help NER systems to generalize better and avoid \"mention\" memorization. To do so, the authors propose an approach based on two mutual information objectives: 1) generalizing the information maximization and 2) superfluous information minimization. ",
    "strengths": "1. The approach seems to work very well on the datasets used by the authors and to achieve a new state of the art in the OOV entity recognition task. \n2. Both the ablation and the interpretability analysis seem to back the authors' claims and give a complete view of the proposed framework. \n3. I liked the very comprehensive and well-documented choice of the comparison systems. \n4. Even if I am not an expert in information-theoretic approaches, their approach seemed intuitive and explained very well with meaningful examples. ",
    "weaknesses": "1. The authors provide the performance improvement of the proposed approach on the OOV dataset but do not show how the performances on the NER datasets change (if they). \n2. I am not an expert in the field, so I don't know other span-based approaches other than the ones the authors cite, but it seems very expensive in terms of speed classifying all the possible spans in possibly long input sentences. ",
    "comments": "line 128: BN has never been defined. \nline 216: subdividing -> subdivide? \nline 420: the the -> the line 430: MOreover -> Moreover line 436: 5Our -> 5 Our There are other typos that I did not mention, like parentheses starting right after the word without any space, more in general, the paper needs a good session of typos polishing. ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "f228a370d578a045",
    "paper_id": "ARR_2022_37",
    "reviewer": null,
    "paper_summary": "The paper proposes an approach with an information-theoretic solution for the out-of-vocabulary (OOV) entity recognition problem. Specifically, the proposed approach contains two mutual information based training objectives: i) Information maximization between context and entity surface forms. 2) Superfluous information minimization, which entails minimizing task-non-specific information. Both these objectives are modeled after the information bottleneck principle in information theory science.\nThe paper leverages several experimental datasets and baselines and in the end demonstrates the robustness and effectiveness of the proposed solution. ",
    "strengths": "1) The additional of the information theoretic approach within the current state-of-the-art for NER extraction i.e. SpanNER architecture is not just creative but an elegant engineering solution. It seems to extend the state-of-the-art incrementally, while showing a nice understanding of addressing precisely some problems or information leakages that can help boost NER for OOV NER.\n2) The solution proposed tackles a relevant advancement of the NER problem in the NLP community specifically OOV NER.\n3) From this paper, one can gain new insights into the OOV NER problem in terms of both the problem itself and the solution.  4) One can also gain a surveyistic birds-eye-view on the state-of-the-art systems and available datasets for the task.\n5) The experiments while they could have been better structured/organized (see my comments in “Summary of Weaknesses”) are still informative, and offer sound and meaningful observations. ",
    "weaknesses": "1) I would have liked to have seen a better application domain motivation for why the OOV problem in NER is so important.  2) The paper has the following phrase explaining the problem that it wishes to address: “entail preventing the model from rote memorizing the entity names or exploiting biased cues via eliminating entity name information.” However, majority of the paper and indeed the method itself seems to then emphasis the following specific problem, i.e. “eliminating task-specific nuisances.” I think maybe the former line should then either be removed or somewhere it state whether it is a hypothesis of the authors that “eliminating task-specific nuisances” indeed means “preventing the model from rote memorizing the entity names or exploiting biased cues via eliminating entity name information.” Perhaps the authors would need another whole paper to analyze what “eliminating task-specific nuisances” w.r.t. the proposed approach precisely means. So I would not say this as a weakness of this paper. However, is the phrase “preventing the model from rote memorizing the entity names or exploiting biased cues via eliminating entity name information” is used, it would help the reader if a connection is made to the actual solution which is mostly referred to as “eliminating task-specific nuisances.”\n3) I also wonder if the authors would be willing to rephrase “nuisances” in “eliminating task-specific nuisances?” Maybe as “noise?”\n4) Table 4. Curious why the ablation results are not shown on the CONLL 2003 dataset even though the method seems to have a maximum impact on it? Further, the CONLL 2003 – OOV data in particular seems quite relevant in this experimental setting. Can the authors provide these results? Otherwise the empirical analysis would seem incomplete to me by missing a seeming very relevant detail.  5) Not a weakness of this work as such. But maybe as a suggestion for future work. Lin et al [1] described four categories of artificially simulating OOV scenarios in NER. An empirical analysis with the method proposed in this work over their dataset or at least a dataset using their OOV synthesis methodology would be very interesting to have particularly on this problem of OOV NER.\n6) The authors did not mention MINER code will be publicly released if the paper were accepted.\n---------------- References ---------------- [1] Lin, Hongyu, et al. \"A Rigorous Study on Named Entity Recognition: Can Fine-tuning Pretrained Model Lead to the Promised Land?.\" Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020. ",
    "comments": "Table 1 in this paper corresponds exactly to Table 3 Vanilla Baseline in Lin et al. [1]. However, Lin et al. mention using the ACE 2005 dataset (paragraph 1 in section 2.1 in Lin et al. [1]) and this paper says it uses the CONLL 2003 dataset. Two concerns here: 1) the dataset used in this paper must be clarified – how do the authors get the exact same results?; 2) if indeed the experimental datasets are the same, I am not sure if it is okay to replicate the exact same table across papers.\n-------------------------------- Comments on Structure -------------------------------- Line 194: Should it be section 3.4? The subsection organization seemed confusing.\n--------- Typos --------- The paper has several typos and grammar errors more toward the latter half of the paper. I list a few of the typos here. But this paper if accepted needs better proofreading.\nLine 83: “cus via eliminating” -> “cues via eliminating” Line 123: “shotcut learning problem” -> “shortcut learning problem” Line 217: “we can subdividing” -> “we can subdivide” Line 334: “static results” -> statistic?\nLine 420: “the the span classification” ---------------- References ---------------- [1] Lin, Hongyu, et al. \"A Rigorous Study on Named Entity Recognition: Can Fine-tuning Pretrained Model Lead to the Promised Land?.\" Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]