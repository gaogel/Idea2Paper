[
  {
    "review_id": "6d9d8c0ef5b9d579",
    "paper_id": "ARR_2022_229",
    "reviewer": null,
    "paper_summary": "This paper proposes three new tasks based on a corpus of historical texts. Moreover, it prepares and releases such a corpus of historical tests (Chronicling America), data that can be used as pre-training for language models. The data collection and preparation are described in the paper, as well as the data used for the proposed challenges/tasks. After the three tasks are introduced, the paper investigates the performance of baselines systems, mainly based on language models. Everything is to be released with the paper, providing a useful data resource for future work. ",
    "strengths": "-The paper provides a useful resource in the form of a very large corpus of historical texts, carefully cleaned and prepare. \n-The paper proposed a suite of challenging machine learning-based tasks that can be useful to evaluate models in the domain of historical text and time-aware tasks. ",
    "weaknesses": "-The paper lacks a clear structure and at points it is a bit hard to follow the thread (see below for more detailed comments and suggestions). This being said, the goals are clearly described. \n-The evaluation is a bit poor in which there is no error analysis or clear insights (what these results tell us, what is lacking, etc.) and the choice of baselines and experimental setting not fully described. \n-The proposed tasks lack, in my opinion, a strong motivation. There is a short paragraph describing the motivation of each task (which is good) but perhaps not entirely convincing why the tasks are necessary.\nNote: many of these weaknesses can be addressed in a subsequent submission, and I would be happy to reassess them if a resubmission is made. ",
    "comments": "In the following I propose a few suggestions that I would personally think that would improve the paper.\n-Reduce list of contributions in the introduction -Add a table with relevant statistics (e.g. number of documents per time period, etc.) about the collected corpus. I think this would give a better overview of the (massive) nature of the resource. \n-Move Section 3 (essentially related work) before Section 2 or before conclusion not to break the thread of the paper. \nSections 2 and 4 (and even 5) could be merged as they are all data-related - this would also give a bit of structure to the paper, as now it has a concatenation of highly-related sections. \n-Add examples from the dataset and especially from the datasets on which the three proposed tasks are based on. \n-In Section 7.4 the statistics can be presented in a table in a more summarised manner. \n-Section 8 can be renamed “Results” or something similar? Also, it could be integrated with the previous one. I find it a bit odd to name a section “Baselines” and show the results, but this may be a personal opinion. \n-I was a bit confused by Section 9. It indicates that some text has been filtered after all the results were presented. These texts were filtered for the whole corpus? If so, I believe it is better to move this section to the data preparation sections, as it is an integral part of the whole preparation. \n-I would add a bit more of a discussion on the nature of the data (fully open, consent, limitations for research/commercial purposes, etc.) and how it will be shared. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "2f05764671764519",
    "paper_id": "ARR_2022_229",
    "reviewer": null,
    "paper_summary": "This paper presents a challenge that covers three tasks on historical text, that suit neural language models well: temporal classification, place classification and prediction of missing words. \nTo complement the previous, a benchmark is prepared with data for training, validation and testing, and a RoBERTa model is trained and used as a baseline. \nBesides describing the challenge, the data and the baseline, the paper has a short discussion about discriminatory language in the data and how an existing model can be used for filtering abusive texts. ",
    "strengths": "New challenge that might be useful for evaluating the \"pre-train and fine-tune\" scenario nowadays common in NLP, with additional interest for studying the evolution of language; tasks are well-enough motivated; a procedure to make the challenges future-proof; the release of a new public corpus and a benchmark. ",
    "weaknesses": "Despite the motivation and presented statistics, the paper could transmit a better overview of the proposed challenge. As it is, I find it hard to evaluate its merits. Among other improvements, examples of the data for each task and its annotation would help.\nThe reason for balancing the test sets regarding the year of publication, not the training, is not clear.\nAfter the abstract and introduction, I was expecting more about the presence of hate speech in the data, but it does not go further than reporting the % of documents filtered for being tagged as abusive by an existing model. ",
    "comments": "Although, at a high level, tasks are well-motivated, what makes the authors think that other researchers will adhere to a challenge in this format?\nWhy using RoBERTa as the baseline? What were the alternatives?\nNot clear how the perplexity is applied in the evaluation of the RetroGap challenge. This measure should be further explained for self-containedness.\nCould a model trained for the RetroGap task contribute to better OCR?\nTypos: consists in -> consists of (several times until section 5) pdf2dvju -> pdf2djvu described in Section ) avoid contractions like didn't and can't ",
    "overall_score": "2.5 ",
    "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work."
  }
]