[
  {
    "review_id": "21d02d860d3c41a7",
    "paper_id": "ARR_2022_95",
    "reviewer": null,
    "paper_summary": "This paper presents a new dataset called FairytaleQA, which consists of 10,580 expert-annotated questions from 278 children-friendly stories. One key feature of this dataset is that each question is annotated with the specific reading comprehension skill required to answer it. The authors run several versions of BART-based QA models to show that the model cannot obtain human performance. Additionally, they use FairytaleQA for training question generation models and show that it can be used to ask high-quality reading comprehension questions. ",
    "strengths": "- This paper introduces a reasonable-sized dataset for QA in narratives. The dataset (if available) would be a useful resource for the NLP community.  - A good review of existing datasets that focused on stories and narratives. It covers all the related datasets that I am aware of.  - The categorization of reading comprehension skills in Section 3.2 is useful.  - The paper is fairly easy to follow. ",
    "weaknesses": "- One major concern is that the contribution of this paper to the QA and NLP community seems mediocre. To me, the difference between FairytaleQA and other reading comprehension datasets on stories and narratives is not obvious. Although the additional annotations about reading skills could be counted as one highlight, this categorization is still coarse-grained to me since it seems nothing more than question types (I assume the categorization might be easily inferred from the question surface form, e.g., \"How did ... feel?\" belongs to the \"Feeling\" category).  - Some details of the annotation process are still vague to me. In Line 342, it claims that the \"texts were broken down into small sections based on their semantic content by our annotators.\" The annotation details for this part seems not revealed. For the question annotation, how many annotators work for each article? What is the detailed annotation guideline? How many questions are required for each article? How the annotators are paid? How to avoid annotation bias?\n- Another worrying fact is that: from Table 5, we see the performance gap between the QA model and the human is not very big. The BART model fine-tuned on FairytaleQA is 0.533 Rouge and 0.536 F1, which is already around 82% of the human performance (0.651 / 0.644). This suggests that FairytaleQA might not be challenging enough. The possibility is that, with some additional engineering efforts on the QA model, FairytaleQA becomes yet another QA dataset that gets solved within a matter of weeks that does not truly evaluate the phenomena it sets out to. ",
    "comments": "Minor Suggestions:  - I think explicit and implicit questions might not be a good naming. Factual detail questions and inductive questions might be more clear?  Putting together all the strengths and weaknesses, I suggest this paper would benefit from another round of revision to address the above concerns before publishing. Otherwise, it might become yet another incremental reading comprehension dataset that might be addressed within a couple of months. ",
    "overall_score": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]