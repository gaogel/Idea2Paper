[
  {
    "review_id": "4db1f42565b77cdf",
    "paper_id": "ARR_2022_47",
    "reviewer": "Vikas Bahirwani",
    "paper_summary": "The paper presents an approach to supervised contrastive learning for NLU - classification task by anchoring CL losses using label embeddings. The overall CL loss comprises of three losses. First, the (Multihead) Instance-centered contrastive loss that aims to bring the instance embedding closer to the label embedding by anchoring on the instance and its label as positive example and instance-other labels as negative examples. Second, the label centered contrastive loss which uses labels to mine positive and negative labels from a batch. And finally, to promote better uniformity across learned embeddings - label embeddings are also regularized.  The authors conduct experiments on standard benchmark NLP tasks showing that their approach produces better/competitive results to baselines. They also show the effectiveness of their approach in Few Shot and Data imbalance settings.  Ablation study provides the importance of each of the component / architectural choices. ",
    "strengths": "1. Well written, well reasoned, easy to follow how model / architecture / experiments are setup up. \n2. Extensive experimentation / ablation with statistical significant testing. \n3. Effective use of label semantics in Supervised Contrastive Learning. \n4. The work seems well researched given their knowledge of CL field which is showcased not only in related work section but also the mention of alignment/uniformity, preventing degradation in CL models, regularizing label embeddings etc. From a CL modeling perspective - the authors have crossed all the \"t\"s dotted all the \"i\"s. \n5. Results show improvements / competitiveness across standard benchmarks and their technique is easy to implement / replicate. ",
    "weaknesses": "1. While the paper details a lot of \"How\"s - the \"Why\"s are missing from their design choices. E.g. why do they think the label semantics arent explored enough (line 70-72), what motivated the use of multi-head instance CL loss (135-137), why the need for a 3 layer projection head g (line 187-194).  Research papers should address the why if they wish the community to learn and extend their lessons.\n2. The author mention uniformity / alignment which is win in and off itself but they do not present any results based on these metrics (e.g. in https://arxiv.org/pdf/2104.08821.pdf)  3. Why do the authors use regularization for label embeddings but not for instance embeddings? Given that the goal is improving uniformity - they should have at least considered using (spread out) regularization for instance embeddings too (ref: eq 6 in https://arxiv.org/pdf/1708.06320.pdf) ",
    "comments": "1. The claim they make in lines 225-228 are partially true. The equation aids in aligning instance with label embeddings but it does not contain any elements that encourages different instances to disperse. Such dispersion is only encouraged by considering hinge losses or using spread out regularization (e.g. by equation 6 in this paper). As such the claim of uniformity should be removed from lines 225-228.  2. Line 172 should read \"The input of LaCon contains two parts consisting of the text and all the labels for the task) ",
    "overall_score": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",
    "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work."
  },
  {
    "review_id": "be029c5b5b401b32",
    "paper_id": "ARR_2022_47",
    "reviewer": null,
    "paper_summary": "This paper proposes a novel supervised contrastive learning method, LaCon, which makes sufficient use of label information by three defined contrastive loss: instance-centered contrastive loss, label-centered contrastive loss, and label embedding regularizer loss. Experiments on eight datasets show that this method is competitive against other baselines. Extensive experiments on few-shot and imbalanced data also demonstrate that this method has a good capability. Visualization of label and instance representation also shows the reason for different performances. ",
    "strengths": "- This paper proposed a good supervised contrastive learning method. Especially as described in Sec. 4.1-4.2, using it in few-shot learning and data imbalance setting can improve the performance than using traditional methods.\n- Sufficient experiments are conducted. To validate the effectiveness of this new method on natural language understanding, it experiments on 8 datasets. Moreover, different settings including few-shot and imbalanced data are taken into account. Details about experiments are demonstrated in Appendix.\n- Good presentation. Figure 1 shows the core information of this paper, three contrastive losses, which can help readers to understand the design. Figure 4 shows the different representations of CE and LaCon, and it can be used to explain the reason from a perspective of representation learning. ",
    "weaknesses": "- The improvement is not a large margin as shown in Table 2. Although the average score of LaCon-vanilla is better than others mostly but the difference ranges from -0.6 to 3.6, which is a small number. Moreover, the improvements on YelpRev, DBPedia, QNLI, and QQP are always less than 1. These four datasets are also not considered in the ablation study.\n- Code is unavailable. It's hard to reproduce the results.\n- Writing should be improved and double-checked. ",
    "comments": "Please refer to the comments above.\nGrammar errors:  - Line 90, “constrastive learning” -> “contrastive learning” - Figure 1 (a), there are two ‘ICL loss 2’s?\n- line 237, “an clipped” -> “a clipped” - line 499-500 “a more strict experiments” ? ",
    "overall_score": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "278244beef48eec4",
    "paper_id": "ARR_2022_47",
    "reviewer": null,
    "paper_summary": "This paper proposes LaCon, a new supervised contrastive learning approach, which is based on learning label embeddings jointly. Specifically, three losses are proposed: 1) multi-head instance-centered contrastive loss, 2) label-centered contrastive loss, and 3) label embedding regularization loss. The authors shows that combining all three losses improves the fine-tuning results of BERT. ",
    "strengths": "- LaCon achieves strong results in 8 datasets and outperforms several prior methods.\n- Table 3 demonstrates the gain of each component of the proposed method. The authors show that all three proposed losses help.\n- It is good to see that the authors provide the mean and standard deviation out of 10 runs in their experiments and also bold the ones with p-value < 0.05.\n- Overall, the writing is clear and easy to follow. I like how the authors visualize the method in Figure 1. ",
    "weaknesses": "- It is surprising to see that unlike conventional NCE or InfoNCE, the authors omit the positive term in the denominator in equations (3), (4), and (5). In NCE or InfoNCE, adding the positive term in the denominator is intuitive since it is optimizing the log probability of selecting the positive example. However, the authors do not explain the reason behind their counter-intuitive change nor provide an ablation study on this modification. I would like to see the justification of this change and experimental results that support it.\n- In subsection 2.6, the authors introduce LaCon with label fusion which requires additional computation compared to LaCon-vanilla and CE baseline. It would be better if the authors can quantify the computation overhead by providing the inference time of the methods.\n- The Instance-centered Contrastive Loss (ICL) is not very novel given that it is just a different view of the softmax classifier. By treating the weights of the last linear layer as the embedding vectors of each class, optimizing the cross-entropy is almost identical to ICL (except that the vectors should be L2 normalized which is basically applying weight normalization to the last linear layer and normalize the features right before the last layer). ",
    "comments": "- In line 211, \"we modify the InfoNCE (Gutmann and Hyvärinen, 2010) to calculate the loss\", here the authors cite the wrong paper. Gutmann and Hyvärinen (2010) propose NCE, but InfoNCE is proposed by Oord et al. (2018). The proposed loss is similar to both NCE and InfoNCE, so changing either the citation or name of the method is fine. The same error also appears in line 94.\n- The proposed method lies in the metric learning field; however, the authors do mention the related metric learning work in their section 5 (related work). For example, the contrastive loss was first proposed by Chopra et al. (2005) and triplet loss (Weinberger et al., 2005; Schroff et al., 2015) is also quite relevant to this paper.\n- It would be great if we can see that the proposed method works for other pretrained models besides BERT as well.\nReferences - Oord, Aäron van den et al. “Representation Learning with Contrastive Predictive Coding.” ArXiv abs/1807.03748 (2018): n. pag.\n- Chopra, Sumit, Raia Hadsell, and Yann LeCun. \" Learning a similarity metric discriminatively, with application to face verification.\" 2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'05). Vol. 1. IEEE, 2005.\n- Weinberger, Kilian Q., John Blitzer, and Lawrence Saul. \" Distance metric learning for large margin nearest neighbor classification.\" Advances in neural information processing systems 18 (2005).\n- Schroff, Florian, Dmitry Kalenichenko, and James Philbin. \" Facenet: A unified embedding for face recognition and clustering.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2015. ",
    "overall_score": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]