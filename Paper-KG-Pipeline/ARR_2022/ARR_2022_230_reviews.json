[
  {
    "review_id": "db8aa6185bc49182",
    "paper_id": "ARR_2022_230",
    "reviewer": "Liang Yao",
    "paper_summary": "This manuscript summarizes representative shallow and deep learning text classification methods with extensive experimental comparison. The authors categorize text classification literature into BOW-based models, Graph-based Models and Sequence models. They then compare these models on five benchmark datasets and find that an efficient wide MLP can achieve competitive results and should be considered as a strong baseline for text classification tasks. The work is interesting and can make much impact on practitioners. ",
    "strengths": "The paper is technically correct and the finding is interesting. \nThe results have practical impacts. ",
    "weaknesses": "The experiments can be better conducted. Firstly, only 5 small datasets are used, large data sets [1] could be considered. Secondly, the performance of CNN and linear SVM (e.g., LIBLINEAR) should be compared. Lastly, parameter counts and the total runtime of graph-based models are missing.\nThe proposed Wide MLP is not novel.\n[1] Zhang, X., Zhao, J. and LeCun, Y., 2015. Character-level convolutional networks for text classification.  Advances in neural information processing systems, 28, pp.649-657. ",
    "comments": "Page 6, line 542, compact --> complex ",
    "overall_score": "3.5 ",
    "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work."
  },
  {
    "review_id": "b4e81da505ebef5a",
    "paper_id": "ARR_2022_230",
    "reviewer": null,
    "paper_summary": "A comprehensive set of experiments are conducted which suggests that baseline wide MLPs with bag-of-words (BoW) input can perform just as well as recently popular graph-based models, such as TextGCN and HeteGCN, in topical text categorization. In addition to wide MLP, sequential BERT and DistilBERT models are fine-tuned, which overall yield state-of-the-art results on 5 well-known text categorization datasets. Parameter counts of the models as well as runtime are also compared, showing the effectiveness of wide MLP with BoW models. ",
    "strengths": "- A comprehensive comparison of models on text categorization datasets demonstrating the effectiveness of a relatively simple baseline model - Consideration of model size, runtime performance in addition to accuracy metrics - Clear description, detailed explanations, acknowledgement of potential limitations, good insights ",
    "weaknesses": "- Limited to multi-class topical text categorization, which is while important, is ultimately just one task - Fair amount of repetition, which is sometimes helpful but can also be a bit distracting (e.g., the difference between inductive learning and transductive learning is explained a couple of times, same with how they used BERT vs. DistilBERT) ",
    "comments": "- Can parameter counts/training time for graph-based models be provided?\n- less classes -> fewer - Minaee et al. (2021) can be cited: Deep Learning–based Text Classification: A Comprehensive Review - Dataset characteristics discussed  (line 359-376) can be combined with Table 2.\n- Missing period (line 224).\n- Sentence line 315-319 is ungrammatical. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]