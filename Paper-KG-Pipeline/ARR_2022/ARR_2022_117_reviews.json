[
  {
    "review_id": "0c8881f95c9a0f6e",
    "paper_id": "ARR_2022_117",
    "reviewer": null,
    "paper_summary": "This work compares whole-word-masking and character-level masking for Chinese masked language modelling. Two probing tasks (insertion & replacement) adapted from grammatical error correction datasets are used to test models trained with the two masking schemes. The results presented show that when single characters needs to be inserted or replaced, the models trained via character-level masking are better. When more than one character is to be replaced/inserted models trained with whole-word-masking show better performance. On sentence-level tasks, there is little difference between the two. ",
    "strengths": "Work presents a targeted investigation of a masking in for Chinese Language model training. Their findings and the models they train could be useful for other researchers in the field. ",
    "weaknesses": "Authors claim to be presenting work 'diagnosing the language understanding ability of Chinese BERT models', but the probing tasks introduced can hardly be described as testing understanding. Indeed, grammatical error correction relies only tangentially on semantic 'understanding'. The results presented in the appendix for the sentence level tasks are actually a better proxy for language understanding capacity, but for those, it doesn't seem to matter if we mask characters or words. ",
    "comments": "line 083: use colon instead of period. ",
    "overall_score": "2.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "939ce6eb49e6445c",
    "paper_id": "ARR_2022_117",
    "reviewer": "Artur Kulmizev",
    "paper_summary": "This paper introduces two grammatical error correction probing tasks for     Chinese masked language models: one for replacing erroneous characters or     words, and another for inserting the correct character or word in order to     make a sentence grammatical. The authors employ these tasks to investigate     models trained via different masking strategies: character-based masking,     word-based masking or both. They find that, for spans of length 1, their     character-masked models perform best for both task, while for spans of     length 2-4, the combined character and word model generally fares better. \n    The authors also experiment with fine-tuning existing pretrained RoBERTA     models, and demonstrate that these models fare worse on both tasks. ",
    "strengths": "This paper articulates an interesting distinction between character and     word-level masking for Chinese masked language modeling, and investigates     the implications of such modeling decisions in a practical way. Generally,     it is concise and easy to follow. The illustrations are very informative and     provide a good overview of the tasks. The examples are likewise very useful for non-Chinese speakers. \n    Naturally, the dataset and models the authors create can be useful for     future research. ",
    "weaknesses": "A study focusing on grammatical error correction should include an error     analysis that attempts to characterize the types of a mistakes the model     makes. The authors likely had to go forego this step due to space     constraints, but without it the insights produced here are generally     limited.  Furthermore, the authors do not include any sort of context for what kind of     performance the models are expected to achieve for these tasks in favorable     circumstances, e.g. in a supervised learning regime. Without this     information, it is difficult to ascertain if the out-of-the-box models'     probing performance is at all good, or if it pales in comparison to simple     supervised baselines. I imagine this would be easy to achieve by fine-tuning     a model on some splits of the dataset, or training another architecture for     the task, e.g. a BiLSTM.  Lastly, it would be useful to see how their models fare on downstream tasks,     how they compare with other Chinese-language MLMs like RoBERTa, and what     implications this might hold for Chinese MLM design. Indeed, the authors     provide this information in the appendix, but nothing is shown in the main     body of the paper, and the discussion is limited to a sentence. ",
    "comments": "- How is P@10 calculated for spans of length 2+? This should be mentioned.\n- Is the [MASK] token applied to the omitted characters in the insertion task?\nLine 249: Michelle, not Micheal ",
    "overall_score": "2.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]