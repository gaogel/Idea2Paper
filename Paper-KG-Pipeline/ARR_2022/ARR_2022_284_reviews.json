[
  {
    "review_id": "57be2198126878a1",
    "paper_id": "ARR_2022_284",
    "reviewer": "Sina Zarrie√ü",
    "paper_summary": "The paper presents a new dataset for image retrieval in Language and Vision research, and some first results computed with state-of-the-art models in L&V (CLIP and Vilbert). The idea of the data collection is to present human annotators with a set of images, out which one is the target that needs to be described. The image sets are sampled from existing resources of static images, and videos. The data analysis in the paper shows that the datasets contain interesting and challenging phenomena for language-and-vision reasoning. The experiments with CLIP and Vilbert show that existing models are clearly not up to human performance on this task. ",
    "strengths": "This paper is an interesting, relevant, well-executed, and well-written resource paper. The dataset is well-motivated, the collection is described clearly and the tested models and baselines are perfectly reasonable. The data set will be of great use to the community and foster further research in challenging language phenomena in the area of L&V. ",
    "weaknesses": "The only thing I can complain about is the rather general framing of the dataset in the introduction. To me, it seems that \"contextual descriptions\" is a bit too vague and generic as a term for describing the contribution. The contexts explored in this work are very specific distractor-like types of contexts where an image appears in combination with extremely similar images. This is close to the classical referring expression setting that investigates descriptions of objects in the context of similar distractor objects. The resulting expressions may be called discriminative descriptions/unambiguous references or something along these lines. ",
    "comments": "My suggestion would be to mention the connection to referring expressions somewhere in the paper and to think about refining the intro/notion of context a bit.\nYu, Licheng, et al. \"Modeling context in referring expressions.\" European Conference on Computer Vision. Springer, Cham, 2016. \nMao, Junhua, et al. \"Generation and comprehension of unambiguous object descriptions.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "41de6104b78ec0e5",
    "paper_id": "ARR_2022_284",
    "reviewer": "Jack Hessel",
    "paper_summary": "The authors propose a multiple choice task posed over 10 images. These images are quite similar (either frames from the same video, or static images selected to be similar). Given a fine-grained visual description that applies to only one of the images, the task is to predict the correct one. Compared to human accuracy which hovers at ~90% for humans, the CLIP and ViLBERT get only roughly 30% accuracy. ",
    "strengths": "The task is well-posed and well-motivated: it is essentially a harder version of NLVR2. While NLVR2 is not saturated (SoTA = high 80s, human = high 90s), there's no reason why both of these tasks can't be considered in parallel. The paper is well-written and easy to follow. The experimental sections clearly illustrate the importance of selecting negative examples that are semantically similar during training. The filtration scheme that the authors use to construct their corpus ensures that only pairs humans (generally) agree upon make it into the final train/val/test sets. The error analyses show that questions with more similar images are harder, and shorter sentences are easier. ",
    "weaknesses": "In my view, there aren't too many shortcomings of this work. One could make the argument that this task is inspired pretty directly by NLVR2, and doesn't really add much beyond that beyond just being a harder version of that corpus. However... That's okay! Not every V+L task needs to test for something completely different than all others before it. One could also make the argument that the language itself is fairly artificial in the sense that these are not the sort of descriptions that would appear in any use case directly. But: the value as a benchmark is nonetheless clear.\nI had a few small technical/presentation suggestions: - It would be nice in table 5 could present the human results   directly, instead of having the reader need to flip back to compare - It would be nice if figure 4 could include human accuracy on these   subsets. ",
    "comments": "Overall, the work straightforwardly builds upon prior efforts like NLVR2 and represents a fair and interesting test for vision+language models. Clearly, given the large gap between human agreement and model performance, there's a long way to go with this task. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]