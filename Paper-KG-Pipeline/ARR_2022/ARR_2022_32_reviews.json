[
  {
    "review_id": "e226035b46a2b228",
    "paper_id": "ARR_2022_32",
    "reviewer": null,
    "paper_summary": "The paper studies the benefits of introducing a Bayesian perspective to abstractive summarization. The authors run the MC dropout method on two pre-trained summarization models, sampling different summarization texts according to specific dropout filters. They use BLEUVarN as a metric of uncertainty for a possible summarization, showing the variations across the summary samples. The authors conduct experiments on three datasets on the correlation between the uncertainty and summarization performances, and show that the performance of the summarization can slightly improve by selecting the \"median\" summary across the pool of sampled ones. ",
    "strengths": "- To the extent of my knowledge, it is the first work that study model uncertainty (in the particular form of the variability of generated summaries) in abstractive summarization.  - The paper provides an analyses on three collections, showing the (cor)relations between the metric of summarization uncertainty (or in fact summarization variability) and ROUGH. They observe that in general the higher the uncertainty score of a summary, the lower its ROUGH score.\n- The work shows that the performance of summarization can be slightly improved by selecting the summary that lays in the \"centroid\" of the pool of generated summaries. ",
    "weaknesses": "My main concerns are the lack of novelty and proper comparison with a previous study.\n- As correctly mentioned in the paper, the work of Xu et al. is not based on MC dropout. However, that work still provide a metric of uncertainty over a generated summary. In fact, the metric of Xu et al. (namely the entropy of the generation distributions) comes with no or little extra computational costs, while the MC dropout of 10 or 20 introduces considerably large feedforward overheads. I believe the method of Xu et al. can be compared against in the experiments of 5.1. This can let the reader know whether the extra cost of MC dropout method comes with considerable benefits.\n- There is no specific novelty in the method. The observation regarding the correlation between uncertainty and performance is in fact an expected one, and has already observed in several previous studies (also in the context of language generation), like: Not All Relevance Scores are Equal: Efficient Uncertainty and Calibration Modeling for Deep Retrieval Models Daniel Cohen, Bhaskar Mitra, Oleg Lesota, Navid Rekabsaz, Carsten Eickhoff In proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021)- - The reported improvement is marginal, while achieved with the large overhead of MC sampling. My guess is that the improvement is only due to the effect of ensembling, inherent in MC droupout. ",
    "comments": "As mentioned above: - I believe the method of Xu et al. can be compared against in the experiments of 5.1. This can let the reader know whether the extra cost of MC dropout method comes with considerable benefits.\n- More evidences regarding the performance improvement, showing that it is not only due to the effect of ensembling.\n- Studying more efficient and recent Bayesian approaches, such as: Agustinus Kristiadi, Matthias Hein, and Philipp Hennig. 2020. Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks. In Proceedings of the 37th International Conference on Machine Learning (Proceedings of Machine Learning Research, Vol. 119), Hal Daum√© III and Aarti Singh (Eds.). PMLR ",
    "overall_score": "2.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "d5d309540d05687f",
    "paper_id": "ARR_2022_32",
    "reviewer": "Reinald Kim Amplayo",
    "paper_summary": "The paper proposes to use Monte Carlo dropout to introduce the notion of uncertainty in text summarization models, which is basically performing multiple forward passes with different dropout masks. By sampling multiple summaries from the models, the authors also propose the use of a \"predictive mean\" which is basically selecting the summary with the lowest disagreement, as the output summary. This method improves the performance of both BART and PEGASUS models. ",
    "strengths": "1. Trustworthy in abstractive text summarization is a very important topic. The method proposed in the paper would help evaluate whether or not to use and deploy summarization systems, and to introduce an \"unanswerability\" function when the system is unsure about the summary that it produced.\n2. The proposed method also improves the performance, which is an added benefit to introduce uncertainty. ",
    "weaknesses": "1. Since the focus of the paper is model trustworthiness, it would be nice to have more analyses and less (or no) focus on the model performance. That is, it would be nice to have more model comparisons (aside from BART and PEGASUS and including state-of-the-art models), more datasets (different domains, multi-document vs single-document, etc.), and most importantly factuality evaluations.\n2. The novelty of the proposed method is also very limited. This work is basically an application of the MC dropout to text summarization, and I find that there isn't a technical difficulty to apply such in text generation models. This also relates to the first weakness -- since the novelty is limited, readers would expect more analyses with regards to trustworthiness in summarization systems. Also, as stated in the paper, [1] already have used MC dropout in MT, which is also a text generation problem similar to text summarization. In fact, Section 3.2 is inspired from [1] and is only extended very marginally.\n3. The paper mentions [2] as related work that focused on uncertainty for summarization. However, no comparison is made between this and the proposed method on the effectiveness to measure uncertainty. Moreover, Section 5.1 and Figure 1 do not evaluate the proposed method's effectiveness to measure uncertainty. There should be a baseline method, such as [2], and a metric that measure such effectiveness. Instead, Figure 1 just shows if the summarization systems are more certain than the other.\n4. Finally, in Section 3.3, the \"predictive mean\", which is to select the summary with the lowest disagreement, seems to be very simple. Simple is not bad, but I think there can be smarter ways to ensemble multiple predictions from a summarization system. For example, the method used in [3] is one possible way. There should have been an exploration (or at least an ablation) to compare different possible techniques to do \"predictive mean\", especially since selecting a summary is far from taking a mean.\n[1] Wat zei je? Detecting Out-of-Distribution Translations with Variational Transformers. BDL 2019 [2] Understanding Neural Abstractive Summarization Models via Uncertainty. EMNLP 2020.\n[3] A Character-Level Decoder without Explicit Segmentation for Neural Machine Translation. ACL 2016. ",
    "comments": "Please mention which system/s are used to generate the summaries in Table 3 and 4. Also, it is interesting to see that summaries in Table 3 are very different from each other, while the same cannot be said for Table 4. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]