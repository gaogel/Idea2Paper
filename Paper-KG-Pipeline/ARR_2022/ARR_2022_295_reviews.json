[
  {
    "review_id": "534dd4d039e9277f",
    "paper_id": "ARR_2022_295",
    "reviewer": null,
    "paper_summary": "This paper provides a large new dataset written in Hanja, an old Korean language not fully understood yet by modern Korean speakers. This dataset comprises three datasets: AJD, DRS, DRRI (DRRI was proposed in this work). The authors investigate the performance of two pretrained language models (PLMs) based on BERT on this data by first continuing the pretraining of the PLMs and later finetuning them to perform a series of supervised tasks. \nThe authors considered two such models: mBERT and AnchiBERT (pretrained on ancient Chinese data). As for the tasks, the authors propose extracting labels from the dataset structure and realizing supervised learning for king prediction (era prediction), topic classification, NER, and summary retrieval. \nThe paper provides a thoughtful set of experiments evaluating the performance of these models with and without finetuning on Hanja data, including zero-shot experiments on DRRI. Later, the authors also investigate the impact of time and historical events using the best performant model (finetuned AnchiBERT). ",
    "strengths": "- The paper provides a large new dataset for a low-resource language (Hanja) - The paper does an excellent job at using state-of-the-art approaches for investigating linguistic structures of the datasets, including the impact of name entities and document ages on classification - The presentation of the results is simple and yet comprehensive ",
    "weaknesses": "- The paper would be easy to follow with an English-proofreading even though the overall idea is still understandable.\n- The new proposed dataset, DRRI, could have been explored more in the paper.\n- It is not clear how named entities were extracted from the datasets. ",
    "comments": "An English-proofreading would significantly improve the readability of the paper. ",
    "overall_score": "3.5 ",
    "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work."
  },
  {
    "review_id": "1bfcb4def70d12c5",
    "paper_id": "ARR_2022_295",
    "reviewer": null,
    "paper_summary": "This paper presents a 'HUE' dataset and pre-trained Hanja language models, which aim to help to analyze the Korean historical documents written in Hanja, an extinct language based on Chinese characters.\nAs a great amount of the ancient Korean documents were written in Hanja, that is hard to understand now. The author(s) thus provide a  dataset comprising three corpora of records written in Hanja during the Joseon dynasty; four models for four tasks (King prediction, topic classification, named entity recognition, summary retrieval); as well as a language model pretrained on the Hanja documents.\nThey then conduct experiments on HUE corpora with the LM model, taking BERT as the baseline. \nOverall, models pretrained on time-specific specific corpus data achieve the best performance in these four tasks. By providing additional information as input, the extended experiments show further improvement on KP, TC and NER tasks. Finally, a zero-shot experiment is conducted to demonstrate the effectiveness of the models on information extraction from unseen data. ",
    "strengths": "The proposed HUE would be the first resource for the processing and understanding of ancient Korean texts in Hanja. In light of combining with advanced language models, this approach has a lot of potential in computational classical languages and scripts. ",
    "weaknesses": "I understand that Section 6 tries to getting more factors involved in the interpretation of the modal's capability. However, the target theses seem reasonable (to get a quick yes answer) yet require much more pages. They are two of the big questions in digital humanities, and I don't see any convincing arguments/reviews on the extended analysis that contributes. ",
    "comments": "In general, I think this is a great pioneering work and could be fed into a much more interesting resources in the future. \nIt would be better to introduce more about the background for readers who are not familiar with (ancient) Korean. For instance,  why is Hanja an extinct 'language' instead of a 'script'? If ancient Korean only wrote Hanja, why do we call it a 'language'? \nIn addition, Table 4 would need more explanation which might really help classical humanities studies, e.g., why naive AnchiBERT only outperforms others on NER? etc. ",
    "overall_score": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",
    "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work."
  }
]