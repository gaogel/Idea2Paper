[
  {
    "review_id": "e775acc875915a0d",
    "paper_id": "ARR_2022_273",
    "reviewer": null,
    "paper_summary": "This paper proposed a method to elicit knowledge from language models and used the generated knowledge to enhance commonsense reasoning tasks’ performance. Specifically, they designed templates for each task they consider and prompted GPT3 to produce relevant knowledge for each question. The generated knowledge is then appended to the question individually and sent to the inference model (T5 11B or its finetuned version). They show that the additional knowledge improves the performance for both zero-shot and finetuned models. ",
    "strengths": "1. The proposed method is simple and straightforward to implement, yet it achieves performance gain compared to several baselines and got SOTA results on 3 tasks. \n2. The analysis showed that the model gets better performance with more knowledge (up to 20) and the improvements on smaller inference models are even larger. The human evaluation also suggests that the generated knowledge correlated well with human judgment in terms of helpfulness. ",
    "weaknesses": "1. Since this paper is proposing a knowledge prompting method, the only real comparison is with self-talk method, however in table 3, only CSQA results are reported (because self-talk templates are only available for CSQA).  Thus advantage over other knowledge prompting methods is less well supported. Also, I think the results of self-talk+UnifiedQA should be included. \n2. Intuitively, I’m not surprised that GPT3 can produce helpful knowledge for these tasks, however it’s unclear whether the proposed method would still be effective when using a smaller LM (as the knowledge source)? ",
    "comments": "It would be great to address the weaknesses above. ",
    "overall_score": "3.5 ",
    "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work."
  }
]