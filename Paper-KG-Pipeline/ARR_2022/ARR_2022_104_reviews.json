[
  {
    "review_id": "bd9e1f80c81f4619",
    "paper_id": "ARR_2022_104",
    "reviewer": null,
    "paper_summary": "The paper suggests an alternative to attention that achieves similar results but with less electricity used. ",
    "strengths": "The results seem convincing. The work seems novel and the contribution and writing clear. ",
    "weaknesses": "The method helps in (theoretical?) energy consumption, but does not improve speed as GPUs do not benefit from this change. ",
    "comments": "fixed from last version ",
    "overall_score": "4.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "76fefdb3e81da2f7",
    "paper_id": "ARR_2022_104",
    "reviewer": "Hongfei Xu",
    "paper_summary": "This paper presents 2 approaches to removing multiplication computation in the popular Transformer attention: 1) binary quantization of attention inputs, and 2) the use of L1 norm to compare between queries and keys. Experiments show that their approach can lead to lower energy consumption in the attention module with some losses in performance. For its weaknesses: 1. it utilizes quantization technology but does not compare with the other quantization approaches. \n2. there is still a softmax computation left in the presented approach. \n3. energy consumption is only estimated within the attention module, even though the reduction in a Transformer block is added (17%), the reduction of the full model (with the classifier) is not reported. I have concerns that the overall reduction may not be sufficiently significant. \n4. L1 norm between vectors of 2 matrices needs a high memory cost than the matrix multiplication. \n5. experiments were performed on 3 language pairs with the base setting, I wonder whether the approach can perform well in challenging settings (Transformer Big and deep Transformers). \n6. the maximum performance loss (- 0.78 BLEU) might be a significant loss. ",
    "strengths": "This paper presents 2 approaches to removing multiplication computation in the popular Transformer attention: 1) binary quantization of attention inputs, and 2) the use of L1 norm to compare between queries and keys. Experiments show significantly lower energy consumption with few losses in performance. ",
    "weaknesses": "1. it utilizes quantization technology but does not compare with the other quantization approaches. \n2. there is still a softmax computation left in the presented approach. \n3. energy consumption is only estimated within the attention module, even though the reduction in a Transformer block is added (17%), the reduction of the full model (with the classifier) is not reported. I have concerns that the overall reduction may not be sufficiently significant. \n4. L1 norm between vectors of 2 matrices needs a high memory cost than the matrix multiplication. \n5. experiments were performed on 3 language pairs with the base setting, I wonder whether the approach can perform well in challenging settings (Transformer Big and deep Transformers). \n6. the maximum performance loss (- 0.78 BLEU) might be a significant loss. ",
    "comments": "It's recommended to report the overall energy reduction of the full model, conduct experiments on more language pairs and with the big setting on a few of them. ",
    "overall_score": "2.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]