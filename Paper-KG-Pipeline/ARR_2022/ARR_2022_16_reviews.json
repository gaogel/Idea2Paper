[
  {
    "review_id": "c06af5ec8047eafe",
    "paper_id": "ARR_2022_16",
    "reviewer": "Divyansh Kaushik",
    "paper_summary": "This paper addresses training models to be robust to adversarial perturbations for sentiment analysis tasks. The authors first identify two limitations that exists with prior works: (i) adversarial perturbations for text often require a large number of search steps; and (ii) while expanding the search space for adversarial perturbations to augment the training set may improve robustness of the trained model to such perturbations, it could also hurt model performance on the clean data. To address this issue, the paper introduces friendly adversarial data augmentation (FADA) and geometry-aware adversarial training (GAT). The key idea behind FADA is to create adversarial examples in a way that takes them closer to the decision boundary but does not push them over it (as that would harm performance on original test set). Following creation of such adversarial examples in one go (rather than the dynamic approach adopted in prior work), the authors optimize the model using GAT, making it computationally efficient. Experiments on IMDb and SST-2 datasets show that adversarially trained models using GAT outperforms several strong baselines against five attack methods (TextFooler, TextBugger, BAE, PSO, and FastGA). The authors also present several ablation studies to identify the effect of each component in GAT, finding positive contribution from each. Additional analysis on step size, number of steps, training epochs, etc. is also presented in the paper. Overall, I think it is a very good paper that merits acceptance to an ACL venue. ",
    "strengths": "- The paper is well motivated and well written (except some minor issues I've laid below). It provides enough background to a first time reader and is very easy to follow. With motivation from the toy example, going into the experiments on SST-2 and IMDb datasets, the results are very interesting and provide good evidence in support of the efficacy of the proposed method. I do think the paper could be restructured in a way that highlights the contributions better than they have been in the current version.\n- The authors complement the experiments with a thorough analysis to determine the effect of several choices made in constructing adversarial augmentations. The ablation studies also provide a good insight into the effectiveness of GAT.\n- There are limitations of the proposed method and the paper, but to the authors' credit they offer a discussion of some of these limitations in the paper itself (thus warranting a mention in Strengths from my perspective). ",
    "weaknesses": "- There seems to be a slight disconnect between the motivation and the actual experiments. While the toy setup used to motivate the paper shows dramatic performance decline on clean data, that drop isn't as dramatic on the actual dataset as shown in the experiments.\n- The experiments are limited to only sentiment analysis as a task. I would have liked to see some \"more difficult\" tasks as well, such as NLI.\n- I don't understand why the results on RoBERTa and DeBERTa are restricted to only SST-2. I would have liked to see RoBERTa or DeBERTa be the default model for experiments on both SST-2 and IMDb over BERT as we know from prior work that these two methods perform better than BERT.\n- The paper lacks any error analysis to see cases where prior methods fail but this method works well and vice versa. Aside from a practical utility offered by this method, I'm not sure if I am gaining any insights from the paper. I'd like to see how the accuracy sees significant shifts by just changing one word, some qualitative analysis (or examples to highlight this) would have supported the argument better. Right now, it does not pop up clearly to the reader. ",
    "comments": "Comments: - In Section 4.2, why does sentence length influence percentage of perturbed words? The fact that its percentage of the tokens in a sentence and not an absolute number should remove the dependence on sentence length, no?\n- Please specify the maximum sentence length kept for BERT/RoBERTa/DeBERTa in Section 4.4 - Lines 458-460: I don't think that we can conclude anything based on experiments on one dataset.\n------------------------------------------ Suggestions: - Please include an error analysis as described above. Furthermore, it would be nice to also see some examples of friendly adversarial data, before and after they cross the decision boundary.\n- Provide a greater discussion of why almost all defense techniques work similarly well against BAE on SST-2, and how a vanilla DeBERTa base outperforms adversarially trained methods against BAE on SST-2.\n---------------------------------------- Typos and other presentation comments: - General comment: Specify upfront that when you say robustness, you mean robustness to adversarial perturbations and when you say accuracy, you mean accuracy on the original held out test set.\n- Lines 25, 26: \"outperform humans on many natural language processing (NLP) tasks\". Please do not use such hyperbole. These methods are great but iid performance on one dataset for a task does not suggest that we've outperformed expert humans on that task.\n- Line 28: \"However, recent studies...\" Which recent studies? Provide citations.\n- Lines 337, 338: Footnotes should appear after the period. The only time a footnote should appear before a punctuation (if there is a punctuation right after where you want to place a footnote) is if the punctuation is a dash. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "83415e9e4c2f38b9",
    "paper_id": "ARR_2022_16",
    "reviewer": "Ning Shi",
    "paper_summary": "The paper aims to investigate the robustness of pre-trained language models from a geometry-aware perspective. The authors argue that adversarial examples across the decision boundary, in fact, are harmful to models' performance, which is supported by a controlled experiment. These findings motivate the introduction of a so-called geometry-aware adversarial training (GAT) equipped with proposed \"friendly\" adversarial data augmentation (FADA). Evaluated on SST-2 and IMDb, language models fine-tuned with this method show stronger robustness. A thoughtful ablation study follows to analyze the impact of some related factors (e.g., search steps, step size, training epochs). ",
    "strengths": "1). Borrowing the concept of geometry, it is an interesting research topic to analyze the quality of generated adversarial examples against the decision boundary. 2). It is a nice try to introduce the motivation of this work by an experiment on a toy set at the very beginning. 3). To promote the robustness of pre-trained language models, GAT with FADA is proposed and achieves better defense performance on SST-2 and IMDb against three wide-used attacks. ",
    "weaknesses": "1). Although the hypothesis is quite interesting, it is not well verified by the designed experiment. As pointed out in Section 3.1, models in conventional methods are trained on the original training set in addition to the generated adversarial examples. In contrast, the base model is trained on the adversarial set only. It is better to compare the model trained on the original dataset with that trained on the mixture so as to highlight the impact of the augmented adversarial examples. Since this experiment serves as the motivation throughout this work, it is critical to make it more convincing. 2). The statement (title or scope) needs more extensive experiments to support. The overall goal of this work is to improve the robustness of pre-trained language models, while RoBERT and DeBERTa are only examined on SST-2, and large versions are not touched. Evaluation on only SST-2 and IMDb ( two classification tasks) seems weak. 3). It is encouraged to introduce the motivation by a lightweight experiment, but the Introduction is not the appropriate place for it. It is said \"achieved with ADA, FADA, and the original training set\" in the caption of Figure 1, but I can only see two pairs of learning curves. FADA should be defined formally in Section 3.1. The experiments conducted in line 207-216 is unclear without proper reference to either tables or figures. Is it the same as the one in the Introduction? I also try to find the definition of evaluation metrics in Section 4, but finally, they are in the caption of Table 2. I believe huge efforts have been made in this work, but the paper structure needs reorganization to underline the motivation and then highlight the contributions with detailed settings. ",
    "comments": "Also see the comments above. \n+ An appendix is recommended to involve more details regarding experimental setup (e.g., hyper-parameters exploration) and case studies (e.g., “friendly” examples). \n+ Please avoid using words like \"friendly\" and FGM in the Abstract without further definition. \n+ Line 021 – less steps + Line 228 – Take … ",
    "overall_score": "2.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]