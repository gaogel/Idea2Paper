[
  {
    "review_id": "67aa42fd3fff5135",
    "paper_id": "ARR_2022_270",
    "reviewer": "Kevin Jesse",
    "paper_summary": "This paper focuses on the privacy concerns of large language models specifically the identification of speaker attributes from the hidden states of GPT-2. This work attempts to demonstrate that privacy concerns exist by a simple attacker network that identifies persona attributes and proposes defense objectives that reduce the exploitation of the model's hidden state. With learning objectives that minimize mutual information and reduce over-learning, this work demonstrates that defense objectives satisfactorily defend against the proposed attacker network.\nProblem Large language models are ubiquitous in application and particularly useful when constructing chatbots. Figure 1 shows that the learned representations of utterance can manifest sensitive information about the persona. The authors also show that with no access to the model, just the final sequence hidden state, simple attacker architectures (MLP) can infer personas from utterances with 37.59% accuracy over 4332 personas. In order to address this, the authors propose two additional learning objectives with an adversarial attacker network. From the perspective of social chatbots, no existing chatbot dataset for private persona information exists. The authors contribute a derivative dataset from PersonaChat that aligns private attributes to the respective personas.\nTheir contributions are as follows - Analyzing persona identification and understanding as a privacy risk - Defensive training loss - Experiments that validate the defense and shows no informativity loss i.e downstream capabilities.\nNote, the defense is across the utterance embeddings which are inherently richer than output token sequences. A defense on sequence embeddings, i.e contextual embeddings in GPT, should constitute effective defense at the token sequence level.\nThe authors use a pre-trained version of GPT-2, DialogGPT, as the initial set of model weights and fine-tune DialogGPT with the PersonaChat dataset. The authors fine-tune their model with three combined losses: Language modeling CE loss, KL loss over the persona distribution predicted by the attacker, and a MI adversarial loss that learns to maximize the attackers classification accuracy which in turn forces the model to incur a defense loss. The KL loss seems to flatten the over-learning problem in CE fine-tuning and the MI loss makes the network private by leveraging a MLP to remove statistical co-occurences in the embeddings related to personas. The overall loss is computed as a weighted sum.\nExperiments The authors gather a persona aligned utterance dataset from Dialogue NLI and PersonaChat. They train a 2 layer MLP attacker model using the sequence embedding as input. The metrics used to evaluate their model is Bayesian Privacy, Bert-score, Top-k, BLEU, PPL, and Distinct.\nTables Table 2 illustrates that traditional LM has a glaring over-learning issue that captures the personas of speakers. This reaffirms the authors motivations. Best guess and random are anticipated to be quite low. The KL portion of the loss in LM+KL variants show that the accuracy is decreased by the regularization effect of the KL loss which is quantified by the max-ratio score (the ratio of most predicted to all predictions). We see that MI adversarial loss forces the model to change output so that the MLP cannot guess persona attributes (reduces accuracy), but does so by making the optimal performance similar to the best guess in that the max ratio jumps significantly and predominately predicts a few persona attributes (my favorite color is blue). The LM+MI variants are likely sufficiently private from my understanding, but reveals attributes/personas that are not relevant; this is of concern in smaller persona datasets. The authors point this out which is very important i.e can fixing privacy open other threat surfaces for other persona applications. This problem emphasizes why KL loss is particularly useful. The authors show that the best model is one that achieves low accuracy on the person prediction task and achieves, secondly, the lowest max-ratio score.\nTable 3 Shows that the scores across a variety of metrics are relatively unchanged for the defensed models to the original LM. We see an increase in PPL indicating that the CE encoding requires more bits but with no change to performance metrics. The PPL increase was explained by the authors as possibly stemming from the noisy nature of the KL loss with respect to the persona predictors task; the attacker model does not want a uniform model. The authors additionally use BERTScore to evaluate the similarity in embedding space, BLEU to check similarity of unigram and bigrams, and Dist to check the distinct unigram and bigrams.  Table 4 shows that the performance across unseen persona labels. This is an important evaluation because the training defense mechanism is significantly more useful if it does not require all personas present for training. We see that performance is pretty similar even when the defender is not allowed to see some of the personas. The subset of personas that are unseen are still significantly below the original language model. From my intuition, this means that the defense objective robustly defends against the attacker model regardless of persona. This invites a conversation about increasingly capable attackers.\nFinally the authors conclude with a qualitative probe of persona predictions and visually demonstrate the output we see in high max-ratio scores; the repetition of the a particular persona/attribute \"my favorite color is blue\". This is intriguing because it shows that the attacking model has focused on some features that lead to an (sub)-optimal result, namely, \"my favorite color is blue\" persona/attribute. This result shows that the attacking model is no longer capable of probing the embeddings; however a more complex model might still be able to represent the GPT-2 parameterized defensed embeddings.\nIn summary it appears that the defense strategies work with little affect to the evaluated metrics. Various downstream applications of the defensed embeddings with compatible performance to traditional LM embeddings would concretely prove that there is little utility degradation, thus expanding on the evaluated metrics; this is largely out of the scope of the paper but would solidify its results. Additionally, expanding the attacker model to more complex neural networks is a worthwhile practical application for the proposed loss functions. ",
    "strengths": "- Privacy oriented dataset with personas - Interesting application of KL loss and MI adversarial losses - Experiments that demonstrate success of loss objectives The paper demonstrates a useful methodology for defending against NN-based persona attacks. The empirical results demonstrates that the proposed approach works for that particular neural network and specifically was not dependent on personas existing in training data. ",
    "weaknesses": "This paper does not have any glaring concerns in my opinion.\nWeaknesses - It is not clear if the defense success is depending on the neural architecture specifically the solutions the network is capable of representing. Are alternative networks, capable of representing higher order functions, still capable of exploiting the defensed embedding? In other words, is the robustness of your approach dependent on the representational capacity of attacker?\n- The approach was applied to a sequence level representation defined in the end of text token in GPT-2. Is it conceivable that the attacker could still attack the individual token level representations? A defense of these token representations would necessitate larger changes to the underlying DialogGPT model. ",
    "comments": "Missing reference for \"First there is no existing data that can be used to quantify how much private information is revealed by a LM\". Maybe rephrase as, \"no standalone dataset to evaluate privacy of training data in a LM\"? \nCarlini, Nicholas, et al. \"Extracting training data from large language models.\" 30th USENIX Security Symposium (USENIX Security 21). 2021.\nExplain Distinct-1 and Distinct-2. It is a key column in your evaluation but is not explained at all. Just say Distinct-1 and Distinct-2 are the number of distinct unigrams and bigrams divided by total number of words.\nIs this an error? \n\"Here the defender owns 6,654 conversations with personal labels ranging from 3 to 7 while the adversary holds 3,753 dialogues with persona labels ranging from 0 to 7.\" \nShould this be \"the adversary holds 3,753 dialogues with persona labels ranging from 0 to 3.\"?\nKL loss equation The dropped constant C in the log is different from the mean reduction 1/C sum 0-(C-1). Due to the notation, the reduction where one C term (the constant in KL) is removed is unclear. Maybe change the mean reduction to another letter such as M or N. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "046eedcedda44a56",
    "paper_id": "ARR_2022_270",
    "reviewer": "John Ortega",
    "paper_summary": "This paper focuses on language-model based chatbots and how to reveal the owners, or personas, of each utterance and effectively defend against attacks that do this. It claims that for 35.9% of the cases in the experiment it is able to properly defend per a specific language model and Kullback-Leibler divergence. It does this by using preventing GPT-2 overlearning and in its results use metrics that are more common to the NLP industry. ",
    "strengths": "The paper is well written and time is taken to explain all of the topics well. Also, the foundation of attacks and then their defenses is formulated using mathematical conclusions that are sound. Details about each attack tried and the metrics that are applied are statistically proven and show that the authors have spent a lot of time in preparing it. ",
    "weaknesses": "The paper makes a few claims about accuracy and best guesses which are understandably not so common. One of the main metrics it leans on in its abstract is the achievement of 37.59% accuracy for defense. While this is done in a scientific way, the authors do not consider other avenues for testing that make more sense such as random experiments based on other work. ",
    "comments": "034 - \"Unfortunately, large 035 language models tend to memorize training data\", cite or explain more... 042 - \"LM-based...\", how do you know this?\n045 - This may be better attached to the previous sentence.\n045 - Figure 1 does not seem to show that the attacks were successfully able to pick the persona consistently. It shows 5 out of 14, but the truth is not given in the figure.\n083 - this paragraph is really good, the explanation helps understand what is done later in the experiments 089 - \"KL loss\", please cite == Problem Formulation = 158 - \"Casual\", describe what casual means with language instead of math please.  160 - The LM shown is the negative log of the probability, while that could be a typical training phase, how do you assume it is the \"casual\" one?\n173 - \"The goal\", why did you choose this goal, is there evidence that by correctly choosing a persona you are able to get more personal information?\n196 - \"Machine Learning as a Service\" --> machine learning as a service (MLaaS) 196 - \"can be directly..\", are you sure? Please cite the evidence where this has happened.\n== Defense Learning Strategies == 204 - \"simple LM training\", what is this, cite?\n206 -LM = LMs?\n207 - \"to avoid...\", sentence no longer needed 213 - 216 - The statement makes sense but do you have something to show that?\n220 - \"intuition...\", this is good == KL Loss == - The KL divergence is introduced here but mentioned before. Also, please cite.\n== MI Loss == 281 - 299 The paper takes on the idea of a game gotten from other work. This is okay but it probably should have been mentioned as the main premise for the work instead of a finding.\n== Experiment == --> Experiments?\n319 - experimental setting --> experimental settings == Experimental Setting == --> Experimental Settings?\n- Table 1 --> Good 363 - PPL, this is not defined anywhere.\n371 - Table 2, from random 0 to 37.59 is a stretch. Also, the fact that there is \"no\" knowledge is probably not a fair comparison. Not sure whether to leave it or not but \"best guesses\" makes more sense.\n384 - 52x --> 52 times? This number does not seem to be a valid measurement. The method of collection and distribution assumptions are questionable.\n== Ablation Study == - Good comparisons ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "4d39b07f0ccc1213",
    "paper_id": "ARR_2022_270",
    "reviewer": "Won Ik Cho",
    "paper_summary": "This paper tackles the privacy concerns of modern social chatbots by investigating the privacy leakage of language model-based chatbots. The authors further show that the speakers' personas can be inferred with a simple neural network, finally proposing a practical defense objective to protect 'persona leakage' from hidden states.\nIn detail, the authors formulate the attacking causal language models as an adversary which owns an external annotated dialog dataset inferring speakers' personas from utterances' embeddings. The persona inference attack is viewed as a supervised classification task, and the defense learning strategies to prevent it include loss functions such as Kullback-Leibler divergence and mutual information. Loss functions utilized in the training phase are aggregated with weight factors to make up the overall loss.\nIn experiment GPT2-based DialoGPT pretrained upon Reddit comment chains, PersonaChat is used in fine-tuning, about 10K dialogs and 98K personas. The authors state evaluation metrics for both privacy and perplexity, showing that the proposed training strategy can fool the attacker while preserving the naturalness of the answer. ",
    "strengths": "- This paper attacks an important and up-to-date issue of privacy leakage and suggests a new objective for defending network, using publicly available dialog-based PLM and PersonaChat dataset.\n- The paper adequately formulates the problem, which is a bit obscure if stated in natural language, and suggests the evaluation scheme to check the effect of the proposed training objective. ",
    "weaknesses": "- The structure of the whole paper is not grasped at a glance due to the presentation being less clear. ",
    "comments": "Suggestions - This reviewer thinks that the indicator for adversary should be 'it' rather than he (line 374, 448).\n- In the usage of cases 'LM+KL+MI', 'LM+MI', and 'LM+KL', LM is commonly placed in, which means the authors can abbreviate it by using only KL and MI. It would make the whole passage more readable.\n- In Section 4, it would be better if some paragraphs are split, and the meaning of results would be better presented if organized into a single table with the interpretation.\nTypos - Please correct citation format to inline, such as in line 262.\n- There are many usages of 'casual' language modeling. Think this is a typo. ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]