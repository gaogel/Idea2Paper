[
  {
    "review_id": "759b93caae756ac8",
    "paper_id": "ARR_2022_300",
    "reviewer": "Yue Yu",
    "paper_summary": "This paper suggests a solution to tackle the practical challenges of using active learning (AL) for 2 NLP tasks. Specifically, the authors (1) propose to use a distilled version of a PLM during data collection and then leverage a larger model to annotate unlabeled examples for training on the (presumably more complex) successor model.  (2) propose UPS to reduce candidates for labeling. ",
    "strengths": "1. The paper introduces an important problem: the inefficiency of data acquisition for AL and proposed a useful method. \n2. The author conducts experiments on different tasks/datasets. \n3. The paper is clear and easy to follow. \n4. I appreciate the authors for the efforts of adding extra experiments and writing the response letter. ",
    "weaknesses": "1. In the previous rounds, the reviewer qXqD mentioned that there are no more explanations for the claim `PLASM can generalize to models that are not aligned with DistilBERT/BERT'.  The authors added experiments on XLNet, which partially solves this issue. But I expect the authors to give a deeper analysis for it. Also, it is better for authors to include textCNN/LSTM (1 of them is enough) for text classification, which can show whether non-pretrained models can be used under PLASM.\n2. The authors seems to leverage some existing techniques for AL methods, thus the technical novelty is not very high. ",
    "comments": "- What's the time complexity of Tracin operation?\n- It would be even better to evaluate more datasets for text classification (e.g. SST-2 or TREC-6).  - In PLASM, the authors propose to use pseudo labeling to mitigate the ASM problem. Maybe I do not understand correctly, but using pseudo-labeling for AL is not a new concept. The are many works that leverage this technique for AL. Some relevant papers includes: (1) Semi-supervised active learning for sequence labeling; (2) Rethinking deep active learning: Using unlabeled data at model training; (3) ATM:  An Active self-training framework for label-efficient text classification.  The author could probably elaborate more on the details of pseudo-labeling. ",
    "overall_score": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]