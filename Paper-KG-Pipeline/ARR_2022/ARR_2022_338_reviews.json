[
  {
    "review_id": "d8eba0a50dbf0d51",
    "paper_id": "ARR_2022_338",
    "reviewer": null,
    "paper_summary": "This work proposes to explicitly model sentence-level representations of both the source and target side of unsupervised machine translation. The authors utilize normalizing flows to model the sentence representations in a flexible space as transformed from a (shared between languages) simple base distribution. At translation time the invertibility of normalizing flows can be used to map between sentence representations in different languages. In experiments the authors test the methods' viability on many language pairs and show competitive performance across the board. ",
    "strengths": "- The proposed method seems sound and novel.\n- The authors run extensive experiments on unsupervised machine translation and show moderate improvements across the board. Applying the method on top of XLM seems to result in good improvements over existing techniques, except for MASS.\n- The paper is mostly well-written except for one crucial point mentioned below in the weaknesses. ",
    "weaknesses": "- The unsupervised translation tasks are all quite superficial, taking existing datasets of similar languages (e.g. En-De Multi30k, En-Fr WMT) and editing them to an unsupervised MT corpus.\n- Improvements on Multi30k are quite small (< 1 BLEU) and reported over single runs and measuring BLEU scores alone. It would be good to report averages over multiple runs and report some more modern metrics as well like COMET or BLEURT.\n- It is initially quite unclear from the writing where the sentence-level representations come from. As they are explicitly modeled, they need supervision from somewhere. The constant comparison to latent variable models and calling these sentence representations latent codes does not add to the clarity of the paper. I hope this will be improved in a revision of the paper. ",
    "comments": "Some typos: - 001: \"The latent variables\" -> \"Latent variables\" - 154: \"efficiently to compute\" -> \"efficient to compute\" - 299: \"We denote the encoder and decoder for encoding and generating source-language sentences as the source encoder and decoder\" - unclear - 403: \"langauge\" -> \"language\" ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]