[
  {
    "review_id": "dd23042b3a19ead6",
    "paper_id": "ARR_2022_1",
    "reviewer": null,
    "paper_summary": "The authors propose Prix-LM, a unified multilingual representation model that can capture, propagate and enrich knowledge in and from multilingual KBs. Prix-LM is trained via a casual LM objective, utilizing monolingual knowledge triples and cross-lingual links. It embeds knowledge from the KB in different languages into a shared representation space, which benefits transferring complementary knowledge between languages. Comprehensive experiments demonstrate the effectiveness and robustness of Prix-LM for automatic KB construction in multilingual setups. ",
    "strengths": "1. The authors leverage two types of knowledge, monolingual triples and cross-lingual links, extracted from existing multilingual KBs, and tune a multilingual language encoder XLM-R via a causal language modeling objective. Prix-LM integrates useful multilingual and KB-based factual knowledge into a single model. \n2. The authors evaluate Prix-LM model on four different tasks essential for automatic KB construction, covering both high-resource and low-resource languages. The main results across all tasks indicate that Prix-LM brings consistent and substantial gains over various state-of-the-art methods, demonstrating its effectiveness. ",
    "weaknesses": "The languages in this paper are some rich-resourced languages indeed, the authors need to test Prix-LM model on some low-resourced languages. ",
    "comments": "See Weaknesses ",
    "overall_score": "3.5 ",
    "confidence": "1 = Not my area, or paper is very hard to understand. My evaluation is just an educated guess."
  },
  {
    "review_id": "51bbd4cb34fd8f1b",
    "paper_id": "ARR_2022_1",
    "reviewer": "Yunmo Chen",
    "paper_summary": "The paper proposes a pre-training strategy that leverages on the multilingual knowledge base to learn representations for entities and relations among them. The pre-trained language models can thus serve knowledge base construction tasks such as link prediction (LP), (cross-lingual) entity linking (XEL), bilingual lexicon induction (BLI), and prompt-based knowledge probing (LM-KP).  For inference, the authors propose two types of inferences; one as autoregressive inference that makes the LM generate objects to complete KB tuplets using constrained beam search, and the other as similarity-based inference that generates similarity-comparable embeddings using a contrastive learning method (Mirror-BERT).  They evaluated the proposed approach on the aforementioned downstream tasks. For the LP task, they evaluated on the DBpedia and achieved the best performance on Hits@1 across all languages compared to baseline models (TransE, ComplEx, and RotatE). For the XEL task, they evaluated on the UMLS and outperformed baselines  (XLM-R and mBERT, both with contrastive learning) in most languages. For the BIL task, they evaluated on the VecMap and outperformed baselines (XLM-R + Mirror). For the LM-KP task, they evaluated on the mLAMA benchmark.  Based on the experimental results, they conclude that the proposed approach can leverage pre-training on multilingual knowledge bases to learn better representations for downstream knowledge base construction tasks.\nOther comments are addressed below respectively. ",
    "strengths": "Based on the experiments, the paper demonstrates that using the surface form of knowledge tuples with special control tokens to pre-train language models can enable the LMs to learn better representations for entities and their relations. Several analyses in the paper might benefit the community for tackling pre-training scheme designs, such as how representations would differ w.r.t. different stages during the training (sec 3.6). ",
    "weaknesses": "- Using original encoders as baselines might not be sufficient. In most experiments, the paper only compares with the original XLM-R or mBERT trained without any knowledge base information. It is unclear whether such encoders being fine-tuned towards the KB tasks would actually perform comparable to the proposed approach. I would like to see experiments like just fine tuning the encoders with the same dataset but the MLM objective in their original pretraining and comparing with them. Such baselines can leverage on input sequences as simple as `<s>X_s X_p X_o </s>` where one of them is masked w.r.t. MLM training.\n- The design of input formats is intuitive and lacks justifications. Although the input formats for monolingual and cross-lingual links are designed to be consistent, it is hard to tell why the design would be chosen. As the major contribution of the paper, justifying the design choice matters. In other words, it would be better to see some comparisons over some variants, say something like `<s>[S]X_s[S][P]X_p[P][O]X_o[O]</s>` as wrapping tokens in the input sequence has been widely used in the community. ",
    "comments": "- The abstract part is lengthy so some background and comparisons with prior work can be elaborated in the introduction and related work. Otherwise, they shift perspective of the abstract, making it hard for the audience to catch the main novelties and contributions.\n- In line 122, triples denoted as $(e_1, r, e_2)$ would clearly show its tuple-like structure instead of sets.\n- In sec 3.2, the authors argue that the Prix-LM (All) model consistently outperforms the single model, hence the ability of leveraging multilingual information. Given the training data sizes differ a lot, I would like to see an ablation that the model is trained on a mix of multilingual data with the same overall dataset size as the monolingual. Otherwise, it is hard to justify whether the performance gain is from the large dataset or from the multilingual training. ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "eb76be8eb039c9d2",
    "paper_id": "ARR_2022_1",
    "reviewer": null,
    "paper_summary": "This paper proposes a unified representation model Prix-LM for multilingual knowledge base (KB) construction and completion. Specifically, they leverage monolingual triples and cross-lingual links from existing multilingual KBs DBpedia, and formulate them as the autoregressive language modeling training objective via starting from XLM-Râ€™s pretrained model. They conduct experiments on four tasks including Link Prediction (LP), Knowledge probing from LMs (LM-KP), Cross-lingual entity linking (XEL), and Bilingual lexicon induction (BLI). The results demonstrate the effectiveness of the proposed approach. ",
    "strengths": "1. They propose a novel approach Prix-LM that can be insightful to the community about how to integrate structural knowledge from multilingual KBs into the pretrained language model. \n2. They conduct comprehensive experiments on four different tasks and 17 diverse languages with significant performance gains which demonstrate the effectiveness of their approach. ",
    "weaknesses": "Though this paper has conducted comprehensive experiments on knowledge related tasks, it would be even stronger if they demonstrate there also exists improvement on the multilingual knowledge-intensive benchmark, like KILT. ",
    "comments": "N/A ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]