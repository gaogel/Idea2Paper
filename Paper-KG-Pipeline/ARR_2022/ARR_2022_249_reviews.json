[
  {
    "review_id": "5cc904d7b7e8a539",
    "paper_id": "ARR_2022_249",
    "reviewer": null,
    "paper_summary": "This paper presents a study on leveraging crosslingual pretrained language models for zero-shot event-argument extraction (EAE). The main idea proposed in this paper is to devise event-argument templates which are language agnostic and act as a bridge across across languages. Event argument extraction is then reduced to a slot filling task where the difference between languages lies in the different arguments. The model uses cross-lingual embeddings and treats slot filling as a sequence generation task. ",
    "strengths": "- Good empirical results (modulo experimental caveat mentioned in my comments below) - Simple model, works for many languages.\n- Template-based approach could be useful for other tasks (e.g., IE, or even cross-lingual generation) - the paper is well-written and includes several experiments, across languages, and experimental conditions and also abalation studies and interesting discussion about the results ",
    "weaknesses": "The authors have addressed all my comments on the previous version of their paper! No obvious weaknesses. ",
    "comments": "One suggestion is to mention in a footnote why it is difficult to present back-translation baselines (like you explain in your letter). ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "07b8d0d3e1080fd2",
    "paper_id": "ARR_2022_249",
    "reviewer": null,
    "paper_summary": "This paper proposed to format the event argument extraction task as a generation task and designed language-agnostic templates to fine-tune multilingual seq2seq language models for zero-shot cross lingual transfer. The experimental results show its superiority over traditional classification models. Overall the idea is clear and simple with strong motivations. The analysis introduced its advantages as well as its current limitations. This work is a well-done paper. ",
    "strengths": "1. The problem formulation for zero-shot cross-lingual event extraction is new and clear. \n2. The results show the strong potential of generative models for information extraction. \n3. The analysis is detailed and informative for future work. ",
    "weaknesses": "1.The novelty is a bit limited for the small focused task. ",
    "comments": "In the information extraction area, recently a lot of work has been explored to replace the traditional classification methods with the generative models. It is not novel enough to directly adapt to another task. For the major contribution of this work,  the special design of inputs and outputs could well capture the characteristics of the task. Fine-tuning multilingual LMs enables good generalization for zero-shot cross-lingual transfer. If the language-agnostic templates could be applied to other structure prediction tasks with good improvements, the contribution could be larger. Currently this paper is okay but not novel enough. It would be better to discuss the potential of language-agnostic templates (html tag). ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "a28caf18ebbeb3bf",
    "paper_id": "ARR_2022_249",
    "reviewer": "Aman Madaan",
    "paper_summary": "*Note: I reviewed this paper in an earlier ARR cycle. There are no changes in the updated version that warrant a change in my score or the review. I’ve updated a summary of weaknesses to reflect the updates, and have listed a few suggestions on grammar.*\nThis work presents a method (X-GEAR) for zero-shot, cross-lingual event argument extraction. X-GEAR takes as input i) a passage, ii) a trigger word (a predicate, e.g., \"killed\"), and iii) a template indicating the desired roles (e.g., <Victim>NONE</Victim>). The output is the template filled with event arguments extracted from the passage (e.g., NONE might be replaced with civilian). X-GEAR is built using the standard Seq2seq framework with a copy mechanism, where the input is composed of the triplet (passage, template, trigger word) flattened as a sequence, and the output is the template filled with desired roles.\nThe method relies on recent advances in large, multilingual pre-trained language models (PTLM) such as MT5, which have been shown to perform robust cross-lingual reasoning. The key insight of the method is to use language-agnostic special tokens (e.g., <Victim>) for the template. Fine-tuning on the source language helps learn meaningful representations for templates, which allows their approach to work across target languages supported by the PTLM. ",
    "strengths": "- The paper presents a simple but intuitive method for solving an important problem. The simplicity of the proposed method is a significant strength of this work. As the authors note, existing systems that perform structured extraction often rely on a pipeline of sub-modules. X-GEAR replaces that with a simple Seq2seq framework which is considerably easier to use and maintain.\n- The proposed method is clearly defined, the experiments are thorough and show considerable gains over the baselines.\n- The analysis provides several insights into the strengths and weaknesses of the proposed approach. ",
    "weaknesses": "The authors have addressed some of the weaknesses highlighted in the previous review. However, it would be great if the weakness of the proposed approach is also highlighted in the future version. Specifically, the method is not *truly* zero-shot as it can only work in cases where a PLTM for the target languages is available. I believe that this is an important point and should be highlighted in conclusion or related work. ",
    "comments": "- L100 “Zero-shot cross-lingual learning *is an” - L104: Various structured prediction tasks have *been studied, - The footnote markers should be placed after the punctuation mark (e.g., L557). ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]