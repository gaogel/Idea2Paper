[
  {
    "review_id": "c9c950ac880f0d3f",
    "paper_id": "ARR_2022_324",
    "reviewer": null,
    "paper_summary": "Authors propose augmenting the Knowledge Grounded Conversation (KGC) models by incorporating personal memory to produce persona-consistent responses in given context. Grounding responses in knowledge and personal memory makes those responses knowledgeable and personalized which is desirable.\nAuthors follow the standard approach of knowledge/memory selection, followed by response generation. In this paper they focus on knowledge/memory selection. They posit that knowledge selection is dependent on memory selection and memory is selected based on the context. They introduce two latent mappings: 1. Forward mapping from personal memory to external knowledge and 2. Inverse mapping of knowledge to personal memory. The authors present a variational inference based method to learn these mappings without access to explicit supervision. They set up a dual learning problem where the forward and inverse mappings are additionally informed by the produced response and learn from each other.\nAuthors also collected an appropriate corpus for this task which will be released to the community. They have conducted automatic and human evals and compared the proposed approach to existing baselines in both tasks (i.e., personalized dialogue response generation and knowledge grounded response generation). The proposed approach is shown to be significantly better than the baselines. Overall evaluation is well conducted and has meaningful comparison although a more intuitive explanation of differences (specifically compared to KnowledGPT + M) would be helpful.\nAuthors have also presented ablation studies that prove the value of each component. It’s interesting to see that modeling knowledge selection as a dependent process on memory selection results in significantly improved results. Although, the same results question the utility of dual learning and it should be discussed in more detail. ",
    "strengths": "Authors show that the proposed approach for knowledge/memory selection can be trained without explicit supervision by employing a dual learning method based on variational inference.\nThe proposed knowledge/memory selection indeed leads to better response generation and establishes a new SOTA in personalized KGC as evaluated by automatic and human evals.\nAuthors also promise to release the corpus they collected further establishing this task as benchmark in personalized KGC. This should be valuable to the research community. ",
    "weaknesses": "The paper presentation could benefit from a rewriting that focuses on introducing the problem of personalized KGC and motivating it. \nAuthors should also discuss why it’s not enough to treat personal memory as just some additional knowledge candidates.\nThe corpus creation described in section 4.1 does not give enough details to understand exactly how knowledge and memory candidates are created. Authors should also give examples of the data that was collected.\nThe distillation process of closing the loop in the dual learning setup should be discussed in more detail. E.g., where is equation 17 used in Algorithm 1? ",
    "comments": "There are many sentences that begin with “And …”. Just remove the ‘and’ and rewrite. ( e.g., line 92, 98).\nLine 149 and line 163, remove repetition.\nLine 173, cases Line 205, “Auxiliary task” This is later called a dual task in line 336. Are these the same?\nOverall in the manuscript for random variables and equations it will be better to avoid brevity and be explicit with all variables being conditioned on. ( e.g., line 321-325). Also in line 216 the variable should be conditioned on the sampled $\\widetilde{Z}^p$. Also in figure 2, It should be $q(Z^p | C, R)$ without $Z^k$, right? Please verify all variables and carefully review it all. Please don't leave this as an exercise for the user.\nLine 248-268, Standard BERT description could be avoided to make space for discussing points more specific to your contribution.\nWhy is the pseudo knowledge label named {$P\\bar}? Is this accurate?\nPlease explain the dataset collection with examples of how knowledge and memory is compiled. ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "a4d17e177b9cc956",
    "paper_id": "ARR_2022_324",
    "reviewer": null,
    "paper_summary": "This paper describes a novel technique for improving knowledge-grounded conversational systems by incorporating *personal memory* into the process.  Personal memory, defined abstractly as the influence of previous personal experience, preference, and values when interpreting dialogue context and selecting follow-up dialogue, is represented in this work as user-specific utterance histories under multiple types of context.  The paper's key contributions include: - A new dataset, scraped from Reddit, providing user-specific personal memory to function as a training and testbed for this task - The first approach for introducing personal memory into knowledge-grounded conversational systems The approach, a variational method that models interdependencies between persona and knowledge using latent variables and employs dual learning to jointly optimize the relationship between dialogue context, personal memory, and knowledge, achieves strong performance on the new dataset relative to competitive baselines under both automated and human evaluation paradigms. ",
    "strengths": "The paper is for the most part clearly written, and tackles an interesting topic that is likely to be of interest to the conversational AI community.  Key strengths include: - The proposed methods are described thoroughly.  They seem to be sound and well-suited for the intended purposes.\n- A strong evaluation is conducted, comparing the proposed model to a suite of competitive baselines including a Generative Profile Memory Network (Zhang et al., 2018), Transformer Memory Network (Dinan et al., 2019), transfer learning-based Transformer model (Wolf et al., 2019), Sequential Knowledge Transformer (Kim et al., 2020), KnowledGPT (Zhao et al., 2020), a custom variation of KnowledGPT designed to incorporate personal memory, a transmiter-receiver based framework (Liu et al., 2020), and a recently published persona-based baseline (Song et al., 2021).\n- Further analyses are conducted to establish the individual contributions of model components and additional hyperparameters. ",
    "weaknesses": "Despite its strengths, there are also several areas that offer opportunities for improvement in future revisions: - In general, the writing quality seems weaker in the latter part of the paper (from the experimental results onward).  Revising this section could benefit the overall clarity of findings and subsequent takeaway points.\n- Some aspects of the dataset are unclear.  For instance: (a) following ACL ethics guidelines, was approval granted by an external institutional review board to collect and publish this data?; ( b) what other requirements are there for data release (e.g., will a public link be provided, or will the data be available upon request from the authors)?; ( c) how many annotators labeled each sample (and how many annotators were there in total)?; and (d) were all samples in English?\n- Some of the presented results seem very close, particularly in the ablation study.  It is unclear whether the results in Table 5, for example, are statistically significant; if not, it may be good to temper the claims made based on these findings. ",
    "comments": "Two typos that could be addressed: - Line 278: The word *the* is repeated - Line 321-323: The text seems to indicate that $p_{\\theta}(Z^{p})$ is shorthand for $p_{\\theta}(Z^{p})$.  Should the second instance of this be something else? ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "e98fb117fe41ef1a",
    "paper_id": "ARR_2022_324",
    "reviewer": null,
    "paper_summary": "This paper works on the problem of personalization in knowledge grounded conversation (KGC). To develop a benchmark, the authors collected a new KGC dataset based on Reddit containing personalized information (e.g. user profile and dialogue history). The authors propose a probabilistic model for utterance generation conditioned on both personalized profile (personal memory) and personal knowledge. Dual learning is employed to better learn the unconstrained relation between personal memory $Z^m$ and knowledge $Z^k$ , and variational method is proposed to approximately marginalize out $Z^m$ and $Z^k$ during inference. The results with automatic evaluation show promising improvement and human evaluation also validates this. Finally, various ablation studies are conducted to reveal the contribution of each model component. ",
    "strengths": "- The problem of personalization in KGC is a relatively overlooked yet important problem. The authors developed a promising method and benchmark for this new challenge.\n- The idea of incorporating dual learning to link personalized sources (e.g. personal memory and knowledge) is very interesting and convincing. I’d like to see follow-up works comparing the ideas against this paper’s.\n- The improvement in automatic evaluation is significant (though not fully reliable, as the author’s acknowledge in line 522). Human evaluation also corroborates the proposed model’s superiority, though the improvement becomes less significant. ",
    "weaknesses": "- The paper is generally well-written and easy to follow, but the definition of personal memory was quite ambiguous and not fully defined. For instance, does this concept include false beliefs (incorrect knowledge), subjective opinions (unsupported knowledge) or inferential knowledge? What would be the unit of personal memory in the context of visually grounded dialogues (line 134)? How can we extend the idea to inter-personal knowledge, i.e. common ground?\n- I understand the space is limited, but I think more information/explanation on the collected dataset should be added (e.g. data collecting procedure and reviewing process). ",
    "comments": "- In lines 198-220, explanation of $\\phi$, $\\psi$ and $\\pi$ is not clear. Can they be better explained or incorporated in Figure 2?\n- In Figure 2, should the distilled distribution of $Z^p$ not be conditioned on $Z^k$? In the text, $q_\\phi (Z^p | C, R)$ is not conditioned on $Z^k$ (lines 199, 207) - Typo: “the the” in line 278 - For Table 3, did you also evaluate against human answers (e.g. original response)? If available, it may be better to be incorporated.\n- What exactly is personal memory? How is this defined, esp. in other domains? I’d like to see more discussion on this in the updated paper. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]