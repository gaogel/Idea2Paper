[
  {
    "review_id": "bf6b83670f486149",
    "paper_id": "ARR_2022_67",
    "reviewer": "Yupei Du",
    "paper_summary": "This paper investigates social biases in sense embedding. \nFirst, by either manually (WEAT) or automatically (WAT) assigning sense ids, this paper extends the previous datasets for examining word embedding biases to sense embedding.   Second, this paper proposes a new dataset for testing sense embedding biases (SSSB), by creating templates and filling them with target/attribute words. \nThird, using these datasets, this paper performs a set of experiments to test the social biases of sense embeddings. The results show that different senses of the same word bear different levels of biases. Moreover, even when the word embedding shows low biases, the biases of sense embeddings can be large. \nGenerally, this paper introduces an important but not well-studied research question (i.e. to what extent are sense embeddings biased?), and proposes a framework for testing this (extends existing datasets and creates a new one). However, some choices of the experimental setup can be improved and some of the claims made are not convincing from the existing results. ",
    "strengths": "1. This paper introduces an important yet under-studied research question --- to measure the social biases in sense embeddings. This is very important, because understanding the senses of words is a fundamental problem in NLU, and sense embedding plays a vital role in this direction. \n2. This paper extends the existing datasets (WEAT and WAT) for testing social biases of (static and contextualized) word embedding to sense embeddings, by assigning sense ids to words, either manually (WEAT) or automatically (WAT). The assigning procedure is solid and can thus benefit the following studies.   3. This paper creates a new dataset (SSSB) to examine the biases of sense embeddings. It is very beneficial (as in 2), and most cases of the dataset are of good quality. \n4. Using these datasets, this paper examines the biases of sense embeddings and compares them with word embedding biases. This paper finds that likewise word embeddings, sense embeddings contain social biases as well. Moreover, this paper observes that different senses of the same word are of different bias levels, and they are also different with word embedding biases. ",
    "weaknesses": "1. Some claims in the paper lack enough groundings. For instance, in lines 246-249, \"This difference in the composition of bias types explains why the bias score of BERT is higher in CrowS-Pairs, while the same is higher for SenseBERT in StereoSet.\" This claim will be justified if the authors can provide the specific bias scores and numbers of examples of each bias type, but I didn't find the corresponding part for analyzing this. Also, this paper mentions several times the intuition \"occupations and not actions associated with those occupations are related to gender, hence can encode social biases\" (lines 595-597). However, I don't really agree. Take \"engineer\" as an instance, in the Merriam-webster dictionary, the first meaning of \"engineer\" as a verb (https://www.merriam-webster.com/dictionary/engineer#:~:text=engineered%3B%20engineering%3B%20engineers,craft%20engineer%20a%20business%20deal) is \"to lay out, construct, or manage as an engineer\". I think it is very much biased towards the male gender as well, according to social conventions. \n2. Some analyses can be more detailed. For example, in \"language/nationality\", the data includes Japanese, Chinese, English, Arabic,  German... (~20 different types). Biases towards different languages/nationalities are different. I was wondering whether there would be some interesting observations comparing them. \n3. The definition of \"bias\" is debatable. In SSSB, a language that is \"difficult to learn/understand/write\" is considered to be stereotypical, and \"easy to learn/understand/write\" is anti-stereotypical. In daily conversations, I think it is not widely considered as \"biased\". Also, I don't really understand the meaning of \"<xxx language is hash>\". Do you mean \"harsh\" by \"hash\"? If so, I think the conclusions derived from this dataset are less trustworthy. \n4. Writing can be improved. For example, even though I read very carefully, I am not sure I fully follow the method in lines 203-210. It will be nice if you can provide some examples for s(_i) and a(_j) 5. A question: why would you use Equation 7 to derive word embeddings? From your results, I assume the sense embeddings are not normalized. This will bring an issue: the embedding will be dominated by the sense with a larger length. To make the experiments more rigorous, I think it would be nice to also use pre-trained static word embeddings (e.g. skip-gram) and normalize the embedding. ",
    "comments": "1. I think it would be more clear if you can introduce sense embeddings a bit before introducing the bias measuring procedures. \n2. A number of typos exist. Mostly, it doesn't influence the reading. However, sometimes it affects my understanding of the paper (e.g., again \"occupations and not actions associated with those occupations are related to gender\" (lines 595-596, and -> but, if I understand it correctly)). Another round of proofreading may be needed. ",
    "overall_score": "2.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "1c70e3101984616e",
    "paper_id": "ARR_2022_67",
    "reviewer": "Denis Newman-Griffis",
    "paper_summary": "This paper tackles the problem of measuring social biases in sense embeddings. It presents the new Sense-Sensitive Social Bias (SSSB) dataset and extends existing bias evaluation datasets (WEAT and WAT) to be compatible with word sense-level evaluation. It presents new evaluation approaches for measuring social biases in static and contextualized sense embeddings, and reports on experimental analysis of state-of-the-art sense embedding models under these evaluation schemes. The findings demonstrate that sense embeddings do not conform to the behavior of word embeddings with respect to social biases, and that sense embeddings often show higher degrees of bias under the presented evaluations. ",
    "strengths": "This is a strong paper. The text is well written and clearly structured, with a logical flow that makes it easy to follow the arguments. Related work is presented in appropriate detail and the paper is clearly situated with respect to existing work in computational analysis of social biases within the ACL community. The experiments are thorough and well-motivated, and the methodology is clear and reproducible. he analysis is well described and appropriate, and includes a thoughtful discussion of differences in observed behavior between word and sense embeddings on the various datasets evaluated. The three categories included in SSSB are well-motivated, and the occupation/action distinction is an especially clever choice. The tables and figures are (largely) appropriate and informative. ",
    "weaknesses": "The main weakness of this paper is that it is not sufficiently grounded in the sociolinguistic side of bias measurement, and therefore makes some significant claims and design decisions that are not entirely appropriate.\nThe proposed task and the experimental design are predicated on the assumption that word senses as semantically disjoint with respect to social biases, but this is not really true (which is actually nodded to in the paper's discussion section). References to nationality and language can often be used metonymically to imply one another (particularly for voicing negative opinions), and colour and race are most certainly strongly correlated in visual design and storytelling (eg the use of dark colours, especially black, for evil in Western European storytelling, which is less common in African stories). Occupations and associated actions, while not used metonymically, may still be correlated: it is likely higher surprisal to refer to a woman \"engineering a solution\" than a man, for example. ( It is worth noting that the maximum pairwise similarity approach used in WAT calculation runs counter to this assumption of disjoint senses.)\nThere is also an important distinction between observability of a bias (e.g., through surveying/prompting native speakers) and measuring it using computational tools. Social biases may be observable without being measured by current approaches. These nuances are important to discuss throughout the paper: social bias assessment is not a computational task alone, but must incorporate human judgment and be framed through specific human viewpoints with acknowledged limitations.\nThere are several further issues with the precision of the language used in the paper that are important to address when discussing a sensitive topic such as social bias measurement.\n- The term \"bias\" itself needs some context. Biases may be positive or neutral (eg preferring to complete \"they sat at the computer to write ___\" with \"code\" vs \"a book\" when trained on different data), as well as negative. As such, it is not clear what it means to have an \"unbiased\" model, or when that is desirable. A clearer definition of what kinds of bias are being assessed, what their impact is, and what mitigation of these biases might look like will help set the context better.\n- The term \"stereotype\" is not used correctly. A stereotype is a commonly-held association between one group and some attribute or behavior; although most stereotypes involve negative attributes or disfavored behaviors, a stereotype is not inherently negative. The labels of \"stereotype/anti-stereotype\" are therefore not appropriate; positive/negative would be better. ( An anti-stereotype would be something that contradicts the stereotypical attribute or behavior.) Stereotypes are also socially-derived based on common beliefs/associations; for example, \"Japanese people are stupid\" is not a commonly-held belief, so this is not a stereotype.\n- The terms \"pleasant\" and \"unpleasant\" are used, but are not clearly defined and feel subjective enough to mask the import of the difference being discussed. \" Positive\" and \"negative\" are the standard words used to discuss emotional valence of this type; these should either be used or the use of alternatives should be justified and clearly defined.\n- Race and ethnicity are distinct constructs; the evaluation described in Section 4.2 is comparing race senses with colour senses, not ethnicity. ( Ethnicity is related more to heritage and origin, while race is a social group that one can be assigned to by others or identify with voluntarily.)\n- Referring to nationality groups as \"disadvantaged groups\" (Line 286) doesn't quite work; while the racial identities that nationalities are correlated with may be disadvantaged, non-US nationalities themselves are not disadvantaged. ( Eg Scandinavian nationalities are very unlikely to be considered disadvantaged.)\nIt is not clear what the impact could be of measuring bias in sense embeddings or of debiasing sense embeddings. These models are much more rarely used than word embeddings, and in much more specialized settings.\nOther issues with the presentation: - Section 3 is difficult to follow. It is not clear how (X and Y) and (A and B) are related, or what a high or low score on the s and w functions means.\n- Figures 2 and 3 are unreadable.\n- P-value calculation is discussed in Section 3, but no p-values are reported in the experimental results. ",
    "comments": "- The inclusion of the verb sense of \"carpenter\" in Section 4.3 is a little questionable; this is a very rarely-used sense. The other occupation/action words are commonly used and could reasonably be measured, but very few texts (for embedding training) will use carpenter as a verb, and many native speakers will not recognize it as correct.\n- There is growing consensus to capitalize racial identifiers (certainly \"Black\", increasingly \"White\").\n- Lines 442-445: This actually does not accurately simulate the word-level embedding case, as it assigns equal weight to all senses. In practice, word senses are more likely to be Zipf-distributed, so a word-level embedding model will be exposed to much more training data for some senses than for others.\n- It is not clear how the categories in WEAT (Table 2) are associated with the social biases this paper is framed around.\n- It would be helpful to mark the extended WEAT/WAT contributed by this paper using a modified label (eg WEAT*), in Table 2 and in text.\n- The Ethical Statement is well written, but should be extended with some more concrete discussion of challenges not addressed in the dataset (eg gender beyond the binary, races other than Black, other sources of social bias).\n- It would be helpful to have a listing of nationalities/racial identities/occupations included in the dataset (along with the adjectives used) as part of the paper, such as in supplementary tables.\n- It would be interesting to see what human behavior is for these prompts/comparisons... - Dev et al 2019 and Nadeem et al 2020 references are missing publication information.\nTypos - Figure 1, graph titles \"embembeddings\" - Line 051: extra \"is\" - Line 122: missing \"are\" - Line 603: \"bises\" -> \"biases\" - Line 610: missing \"being\" - Spaces not needed between section symbol and number - Spaces after non-terminal periods (\"vs.\", \"cf.\") should be escaped to avoid spacing issues ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]