[
  {
    "review_id": "29b4b4ffced31f17",
    "paper_id": "ARR_2022_315",
    "reviewer": "Yawei Sun",
    "paper_summary": "In this paper, the author proposes a deep-learning based inductive logic reasoning method. The method firstly extracts query-related (candidate-related) information. Then, the method conducts logic reasoning among the filtered information by inducing feasible rules that entail the target relation. The reasoning process is accomplished via attentive memories with novel differentiable logic operators. ",
    "strengths": "1. Combining deep learning with ILP is interesting. The authors introduce a smooth connection between deep representation learning with logic reasoning by associating distributed representations with discrete logic predicates and their probabilistic evaluations. \n2. The research described in the paper seems technically sound and correct. ",
    "weaknesses": "1. The experiments are not convincing. For example, the authors use the development dataset to evaluate the results. Test data is not publicly available. The reason is insufficient. In addition, four different query relations is small. It is hard to convince the effectiveness of DILR. \n2. This work is not clearly written. For example, the second paragraph of section 5.1 should be rewritten and divided into more than one paragraph. ",
    "comments": "1. What is DNNs, in Section 1 Introduction ? \n2. The rule is \"if A is in B and B is part of country C, then A is in country C\". How to solve the example by DILR? Specifically, how to process the example by Hierarchical Attentive Reader and Multi-hop Reasoner ? \n3. BERT is a strong baseline model from Table 1. Do you explain why DILR-BERT is stronger than BERT in detail? ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "1d7c93153cb5e578",
    "paper_id": "ARR_2022_315",
    "reviewer": null,
    "paper_summary": "***This is from the reviewer `2guV` of the previous version.***\nI have read the responses from the authors to these weaknesses and I think most of them are well addressed in the revised version. Thus, I have raised my overall score. \n----- This paper presents a multi-hop reasoning method to reading comprehension with structured queries in the format of triplets (subject, relation, ?) --- predicting the object of a triplet via reading multiple documents. The key idea of the proposed method is to combine neural token representations and inductive logic programming (ILP) so that the reasoning chains are more interpretable to humans. The proposed method DILR is an end-to-end model, consisting of a hierarchical attentive reader module and a multi-hop logic reasoner, where the former continually produces a query&context-aware representation of entities, and the latter use them for softly selecting atoms to form logic clauses. A highlight of such a neural-symbolic method is that the final loss can smoothly be back-propagated to the trainable modules. ",
    "strengths": "- The proposed method avoids using pre-extracted NER results and limited rule templates. Instead, the module learns to select the atoms end-to-end and does improve the performance.\n- The logic rules and attentions are naturally interpretable and can be used for understanding the reasoning process of the model. ",
    "weaknesses": "- The evaluation part lacks the results of recent pre-trained LMs' results such as RoBERTa and UnifiedQA, etc, which thus makes the results and conclusions less convincing. It is unclear if the performance gain from DILR is still there if with a more powerful pre-trained LM. And also, it seems that the DILR-BERT doesn't fine-tune the BERT parameters? From the description, I feel like the BERT here is only used as an encoder to get contextualized representation but is not trainable. Please correct me if I missed anything here.\n- The model is limited to the triplet-format questions, which is limited to a pre-defined list of relations and may not generalize to other MRC datasets and other relations.\n- There are a few missing baseline methods such as DrKIT (Bhuwan Dhingra et al. ICLR 2020), which is also a differentiable multi-hop reasoning framework, focusing on entity mentions. Please also look at the datasets and baseline methods this paper used for evaluation.\n- There is no qualitative analysis of the used logic clauses for case studies. It's unclear whether the logic clauses induced by the DILR are really meaningful to reason about the answers.\n---  I have read the responses from the authors to these weaknesses and I think most of them are well addressed in the revised version. Thus, I have raised my overall score. ",
    "comments": "N/A ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "35b5754794282296",
    "paper_id": "ARR_2022_315",
    "reviewer": null,
    "paper_summary": "The authors present a neuro-symbolic approach to answer multi-hop questions by combining neural representations with differentiable ILP reasoning. They show the effectiveness of their approach on multiple-choice structured queries (Wikihop and Medhop dataset). The improvements are noteworthy, albeit on a narrow task. The authors have done a good job responding to previous reviews and making appropriate changes where possible. However, I think the system description is still very involved and make it hard to judge the technical contributions of their system. ",
    "strengths": "- A novel approach that combines deep-learning with logic(ILP)  - An attention-based approach to produce representations that do not rely on NER - Improved scores on WikiHop and MedHop subsets compared to prior work - Ablations show clear benefit of each part of their system ",
    "weaknesses": "- Complicated system with a narrow scope (but that seems to be the current issue with other neuro-ILP work too)   - Complex attention mechanisms that are hard to interpret - The Complexity vs Accuracy tradeoff may not be worth it (e.g. BERT is a much much simpler model with a marginal drop in accuracy). The interpretability of the soft rules could be a potential benefit of the DILR approach, but it is still limited (no interpretation on relations, mostly attention-based entity chains) ",
    "comments": "- #54: Min et al 2019 is a decomposition-based approach not a Neural Module Network - #55: Doesn't your attention-based approach also implicitly encodes relevant contexts into unnamed relations?\n- #277: What is i and j here refer to? It is only introduced later on the next page. It would help if every term is clearly defined. I would even be okay if the equations are only spelled out in the appendix with the main paper only using some meaningful function names. E.g. B_ij = subj_context_attn(S, C) - It would help a lot in terms of understanding if the indices are not shared. E.g if \"i\" refers to a token in the subject, don't use it to also refer to the index in the answer candidate later. This will make the reader's life much easier since []_{ij} would then always refer to the attention between the subject and context. We don't have to carefully look around each equation to understand the meaning of each term.\n- In 4.2, please think about what are the key ideas that you want the reader to know vs what are the key details needed for reproducibility. Try to move the latter to an appendix.  - The description of the BERT model is unclear. You get the representations for the query-subject OR the candidate? Shouldn't you be getting a representation for query-subject-candidate-context for each candidate and context? Or do you get multiple vector representations and concatenate them? \n  - Are all the models re-trained on the same subset? ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]