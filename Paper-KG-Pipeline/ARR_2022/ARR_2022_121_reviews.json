[
  {
    "review_id": "ac12f092fedee748",
    "paper_id": "ARR_2022_121",
    "reviewer": null,
    "paper_summary": "This paper introduces a novel model based on pre-trained Los (BERT family) for the task of Automated Essay Scoring (AES). The key feature of this model is that it computes representations at multiple scales (token, segment and document level). Another contribution is use of multiple loss functions (margin ranking and similarity) and transfer learning to improve over their baseline model. ",
    "strengths": "1. Their approach uses hierarchical representations which is intuitive. \n2. The use of multiple loss functions also makes sense (but not completely novel). \n3. They utilise transfer learning on out of domain data which is useful addition. ",
    "weaknesses": "1. The use of BERT architecture for this task has already been explored in R^2 BERT [Yang et al. 2020]. Existing work [1] explores use of contextualised representations of multiple sentences for document classification, so the idea of multi-scale representation may not be completely novel.\n2. For document-level representation, the model uses only first 510 tokens. Ironically, the authors claim to get most improvement on prompt 1,2,8 for ASAP dataset (all of which have >510 tokens) 3. Some statements in the discussion are misleading. in [518-521], the authors mention \"We further re-implement BERT^2 proposed by (Yang et al., 2020), and the performance is not so strong as the published state-of-the art like models 9, 10 and 12.\". The BERT^2 model is same as model 10. If the authors are trying to claim that their own implementation of BERT^2 is not as well-performing as the published result, there could be some implementation issues not taken care of resulting in sub-par performance.\n4. For the loss functions, the authors should either give references to existing work which proposes these losses (for any task) or explicitly indicate that it is their contribution.  [1] Lu, Jinghui, et al. \"A Sentence-Level Hierarchical BERT Model for Document Classification with Limited Labelled Data.\" International Conference on Discovery Science. Springer, Cham, 2021. ",
    "comments": "1. The citation of R^2 BERT in table 2 is incorrect. \n2. The work R-DROP has been mentioned in introduction without giving any context (it is a dropout strategy) 3. The equations for attention pool are incomplete (terms under the summation missing). It makes them hard to understand. \n4. The grammar in the paper writing could be improved. ",
    "overall_score": "2.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "42ab122400706c4e",
    "paper_id": "ARR_2022_121",
    "reviewer": null,
    "paper_summary": "The paper describes an approach to automatically grade essays using pre-trained language models such as BERT and Longformer. The paper also describes an approach to grade out of domain essays (i.e. essays written in response to another prompt). ",
    "strengths": "1. Using multiple pre-trained language models (BERT and Longformer) as well as more than 1 dataset (ASAP Automatic Essay Grading dataset and Common Literacy Prize dataset).\n2. Analysis is quite clear and complete. ",
    "weaknesses": "1. The writing needs to be improved. Structurally, there should be a \"Related Work\" section which would inform the reader that this is where prior research has been done, as well as what differentiates the current work with earlier work. A clear separation between the \"Introduction\" and \"Related Work\" sections would certainly improve the readability of the paper.\n2. The paper does not compare the results with some of the earlier research work from 2020. While the authors have explained their reasons for not doing so in the author response along the lines of \"Those systems are not state-of-the-art\", they have compared the results to a number of earlier systems with worse performances (Eg. Taghipour and Ng (2016)). ",
    "comments": "Comments: 1. Please keep a separate \"Related Work\" section. Currently \"Introduction\" section of the paper reads as 2-3 paragraphs of introduction, followed by 3 bullet points of related work and again a lot of introduction. I would suggest that you shift those 3 bullet points (\"Traditional AES\", \"Deep Neural AES\" and \"Pre-training AES\") to the Related work section.\n2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.\n3. While the out of domain experiment is pre-trained on other prompts, it is still fine-tuned during training on the target prompt essays.  Typos: 1. In Table #2, Row 10, the reference for R2BERT is Yang et al. (2020), not Yang et al. (2019).\nMissing References: 1. Panitan Muangkammuen and Fumiyo Fukumoto. \" Multi-task Learning for Automated Essay Scoring with Sentiment Analysis\". 2020. In Proceedings of the AACL-IJCNLP 2020 Student Research Workshop.\n2. Sandeep Mathias, Rudra Murthy, Diptesh Kanojia, Abhijit Mishra, Pushpak Bhattacharyya. 2020. Happy Are Those Who Grade without Seeing: A Multi-Task Learning Approach to Grade Essays Using Gaze Behaviour. In Proceedings of the 2020 AACL-IJCNLP Main Conference. ",
    "overall_score": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",
    "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work."
  }
]