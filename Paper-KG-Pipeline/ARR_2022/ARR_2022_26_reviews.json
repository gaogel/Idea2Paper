[
  {
    "review_id": "93f2dbcf1adfb648",
    "paper_id": "ARR_2022_26",
    "reviewer": null,
    "paper_summary": "The authors introduce a new annotated event detection dataset that is focused on annotation suicidal events. \nThe annotation focuses on the following event types: suicide-related actions, thoughts/ideation, risk factors related to life, relationship, health, and other, in addition to protective factors related to positive activities such as taking medication. \nThe author apply state-of-the-art event detection models on their dataset, where the performance is poorer in comparison to more established event detection domains. As a result, the authors are calling the community to build models that can improve the performance. ",
    "strengths": "- The developed dataset is interesting and the task is very important to NLPers who work on mental health. I see its impact going beyond the event detection task to other tasks in the same domain that are related to understanding what kind of triggers affect suicidality. This, hopefully, would yield better performance of the suicide risk assessment and other mental-health related classifiers.  - The fact that the annotators are mental health domain experts makes of a more reliable data.  - I find the discussion of the challenges of annotations very valuable and it resonates with many previous research where cases of someone talking about a friend trying to committing suicide should be differentiated than the person who is posting attempting suicide.  - The baseline models that the authors use are strong and recent.  - Thorough reproducibility check list (e.g model parameters, annotation examples) ",
    "weaknesses": "- Although it is not uncommon to see the \"OTHER\" label in annotation schema as a candidate label when the annotators cannot find a better mapping with the rest of the labels, I think OTHER label makes the annotations weaker. The annotators would default to it in many cases and as a result the number of annotations for that label become much higher in comparison to the other labels (in your case 15343 for OTHER vs. 21635 for the rest of the labels). This would generate problems such as data imbalance, potential noisy labels where the model might over predict the OTHER label.  - The call to the community by the authors for better performance models is a valid one, but we cannot eliminate putting some burden of the poor performance on the dataset itself. This might be due to embedded annotations' issues that can be simply a result of the domain itself. Did the authors do a thorough analysis to confirm that hypothesis?\n- I think it is very important to see the performance of the models on each label separately. For instance, a valuable analysis would be to compare the performance per label with the annotation challenges. ",
    "comments": "- Please check the weaknesses section for suggestions on how to improve the work. For instance, I would emphasize again that the readers need to see the performance on each event type to better understand the challenges and how to improve the models. \nFor RF-HEALTH event type, you mention that it is about mentions that directly affect the subject's health. This can be ambiguous, for instance you might have a transitive relationship between life events and health, thus the event types are not exclusive (as you mention this was part of your annotation design). Could you please elaborate more on that? \nIs your dataset intersected with CLPsych 2019 dataset (Zirikly et al. 2019)? It would be very interesting and valuable if we have the ED annotations on that dataset to further push the performance of the risk assessment models with the use of the triggers.   Typos: Preposition is missing after models on line 276 ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "00b1d8296a836fe8",
    "paper_id": "ARR_2022_26",
    "reviewer": null,
    "paper_summary": "The work focuses on the Event Detection task to identify event trigger words of suicide-related events in public posts of discussion forums. ",
    "strengths": "Authors introduce a new dataset for ED (SuicideED) that features seven suicidal event types to comprehensively capture suicide actions and ideation, and general risk and protective factors. ",
    "weaknesses": "Limited awareness of the literature. ",
    "comments": "The manuscript is centered on a very interesting and timely topic, which is also quite relevant to the themes of ARR. Organization of the paper is good and the proposed method is quite novel. The length of the manuscript is about right.\nThe manuscript, however, does not link well with recent literature on AI for mental health, e.g., see sentic computing for patient centered applications. Also, latest trends in mental disorder detection with attentive relation networks are missing. Finally, check Ji et al.’s recent review of suicidal ideation detection.\nThe manuscript presents some bad English constructions, grammar mistakes, and misuse of articles: a professional language editing service (e.g., the ones offered by IEEE, Elsevier, or Springer) is strongly recommended in order to sufficiently improve the paper's presentation quality for meeting the high standards of ARR.\nFinally, double-check both definition and usage of acronyms: every acronym, e.g., CNN, should be defined only once (at the first occurrence) and always used afterwards (except for abstract and section titles). Also, it is not recommendable to generate acronyms for multiword expressions that are shorter than 3 words, e.g., ED (unless they are universally recognized, e.g., AI). ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work."
  },
  {
    "review_id": "42e1a1ffb6697cfc",
    "paper_id": "ARR_2022_26",
    "reviewer": "Ali Hürriyetoğlu",
    "paper_summary": "The authors construct a corpus on suicide events. The token level annotations and event types are invaluable. This corpus enable quantification of the performance of the state-of-the-art modeling approaches. The performance scores show that we still need to work a lot on this task. ",
    "strengths": "The inter-annotator agreement and the use of Reddit data, which is user-generated text, make this work valuable. The token level annotations and event types are invaluable. ",
    "weaknesses": "The paper should contain discussion of some design decisions: 1- How did the documents are filtered? Is it a random sample or keyword based filtering? What did you do for the documents that do not contain any suicide related information? \n2- \"with more than 50 words are kept to increase\" -> What do the excluded documents contain? What is the ratio of the posts excluded in respect of this decision? \n3- \"Finally, we further fine tune the pre-trained BERT model over unlabeled Reddit posts (i.e., about 40K posts) using masked language modeling (Devlin et al., 2019).\" - > how did you select the 40k docs? The document selection matters: Caselli, T., Mutlu, O., Basile, A., & Hürriyetoğlu, A. (2021, August). PROTEST-ER: Retraining BERT for Protest Event Extraction. ... ",
    "comments": "1- The ratio of RF-Other is too high (40%) in the corpus in comparison to other event types. I think this requires some elaboration. \n2- The text mention 2300 docs annotated. However, table 1 row #documents contains more documents 2214+130+109 3- How is the disagreements were resolved for the 20% that was co-annotated. \n4- The reddit corpus could be compared to other event detection datasets that utilize user-generated text. Comparing this corpus to MAVEN and CycSecED is not fair. \n5- Do you use sentences or whole posts as the unit to be annotated?\nEnglish language: \"pubic posts\" -> public posts \"a ED model must\" -> \"an ED model must\" ",
    "overall_score": "2.5 ",
    "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work."
  }
]