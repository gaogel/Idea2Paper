[
  {
    "review_id": "8d7f381bb67845a1",
    "paper_id": "ARR_2022_107",
    "reviewer": null,
    "paper_summary": "This paper proposes a latent-variable model based on a variational probabilistic modeling framework for conditional compute in multi-domain and multilingual machine translation. They introduce latent variables which are learned during model training. These latent variables decide which sub-network of the model is selected for each task / language pair. \nThe results on multi-domain machine translation show promise compared to vanilla Transformer and Adapter baselines. But most of the improvements can be captured by a heuristic multi-task group dropout baseline, and there is a small improvement (0.5 bleu) over this baseline with the variational approach proposed. \nThe paper includes some interesting analysis of the models as well. On the other hand, some one important ablations on whether group dropout helps vs a baseline with group size=1 is missing and the improvements on multilingual translation are not significant enough. Overall, this leaves me unsure whether the added complexity of this approach is worth it over the simpler baselines (vanilla Transformer or heuristic multi-task group dropout). ",
    "strengths": "1. This work proposes a technique to learn which subnetworks to use for which task / language pair based on learning from the data. \n2. The proposed probabilistic framework is mathematically sound and explained clearly and completely. \n3. There are experiments compared with decent baselines and multiple benchmarks and there has been a good attempt to evaluate the new technique well. \n4. The analysis showing that accuracy of Adapter baselines declines on a domain when that domain is divided into two pseudo-domains, where as the accuracy on this proposed framework remains the same motivates the reason of using the proposed technique very well 5. A nice advantage of this approach is that it doesn’t need additional parameters to the added to the baseline Transformer model. There are some additional hyper-parameters to tune correctly per setup though. \n6. There is also analysis showing whether the learned subnetworks for related domains/languages overlap significantly. \n7. [ Updated] A nice baseline - heuristic multi-task group dropout has been added in the revised version of the paper that helps disentangle the impact of the variational approach. ",
    "weaknesses": "1. While the authors mention leaving this to future work, the study of how many groups to divide each layer into is missing and how sensitive is model performance to the choice of number of groups is missing → how does the proposed group dropout compare to just a regular dropout with group size = 1; I think this is important because the paper proposes “Latent Group Dropout” and whether group dropout is needed or not is a key question that is not experimentally verified. \n2. “ heuristic multi-task group dropout” (HMGD) captures most of the improvements between Transformer vs proposed LaMGD with a 0.5 BLEU difference between HMGD and LaMGD - It is worth making a case that the simpler approach HMGD is also a win 3. It is unclear if there was an attempt to tune the sizes and dropout for the adapter modules in the adapters baseline 4. Table 5 - very few pairs (7 out of 32) show significant gains over the Transformer baseline - it is actually unclear how much language specific model capacity helps on this TED benchmark at all. Even the Adapters baseline is very similar to the Transformer baseline. ",
    "comments": "1. Regarding “Finally, we set the weight of the entropy term to 0.0001 in the training loss in every experiments.” - ideally, you should refer back to the exact term in the exact equation described in the previous sections 2. I don’t understand what “0^nd” means in section 3.1.4  3. Boldface in Table 3 doesn’t always refer to the best BLEU per column ",
    "overall_score": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "a0926a0cf87a2038",
    "paper_id": "ARR_2022_107",
    "reviewer": "Zi-Yi Dou",
    "paper_summary": "This paper proposes a routing strategy for multitask machine translation. For a given language or domain, only a fixed amount of nodes is activated at each layer and the model learns which nodes should be activated. They experiment on both multilingual and multidomain settings, and the experimental results demonstrate that they can achieve comparable and sometimes marginally better performance than adapter-based methods with fewer parameters. ",
    "strengths": "1. The general idea is intuitive and makes sense, and the paper proposes a technically sound way of implementing it. \n2. The empirical results are good as they can achieve comparable performance with an adapter-based method with fewer parameters. \n3. The paper is well-organized. ",
    "weaknesses": "Previous comments:  1. There is little analysis of their methods. Sensitivity analysis of their hyper-parameters ($n_d$, $k$, $\\tau$), some qualitative analysis are required. \n2. Requiring partitioning nodes into groups and a fixed number of nodes to be activated at each layer makes their method less flexible. \n3. More baselines should be included (e.g. Li et al., (2020), Gong et al., (2021a, b) as mentioned in the paper).\nAfter reading the response: The added sensitivity analysis - Thanks for adding the sensitivity analysis on $k$! However, more analyses are still required as in my previous comment (e.g. the effect of $n_d$, the use of the annealing strategy for $\\tau$, analysis of the translation outputs, ...).\nThe flexibility of their approach - The method requires a fixed number of nodes to be activated at each layer for all the tasks. This is inflexible because intuitively, harder tasks require more nodes to process while simpler tasks may only need a few nodes. However, their method cannot take this into account, and thus the method is not very flexible.\nThe added baseline - It'd be better to compare with published baselines (e.g. [1, 2]). The baselines share similar claims as the author(s) mentioned in the rebuttal. I agree that this paper doesn't need to obtain state-of-the-art performance, however, the comparisons are necessary so that other researchers can know which methods to use in practice. Therefore, comparisons are required even if the results are not favorable to their method.\n[1] Lin et al., Learning Language-Specific Sub-network for Multilingual Machine Translation, ACL 2021. \n[2] Li et al., Deep transformers with latent depth, NeurIPS 2020. ",
    "comments": "The added significance tests - The added significance tests are questionable. First, many of the results (e.g. most of the results in Table 5) are not significantly better than the Transformer baseline. Second, it is better to compare with the best baseline model (e.g. the HMGD Transformer in Table 3) rather than the vanilla Transformer baseline. Third, in Table 3 the author(s) claim that the average BLEU score is significantly better than the baseline using compare-mt, but more details are required as compare-mt does not support comparing average numbers. ",
    "overall_score": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]