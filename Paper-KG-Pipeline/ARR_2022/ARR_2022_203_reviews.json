[
  {
    "review_id": "74422589ed375a73",
    "paper_id": "ARR_2022_203",
    "reviewer": "Bo Zheng",
    "paper_summary": "The paper proposes an efficient and flexible BERT-based multi-task framework. The framework first trains a single-task model for each task using partial fine-tuning, where the bottom layers of BERT are frozen. Then the task-specific layers in each single-task model are compressed using knowledge distillation. The compressed layers are finally merged into one multi-task model, while the frozen layers can be shared across tasks. Experiments show that the framework can reduce up to 2/3 of the model parameters while hardly hurting the model performance. ",
    "strengths": "1. The paper is well-written, and the method is easy to follow. \n2. In addition to the reduced number of model parameters, the inference speed also increases. ",
    "weaknesses": "1. Missing comparison with other parameter-efficient fine-tuning methods, e.g., adapter-based methods. \n2. The training cost is much larger than conventional fine-tuning or other parameter-efficient fine-tuning methods. ",
    "comments": "See above. ",
    "overall_score": "2.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "f9d2c04ce99e3997",
    "paper_id": "ARR_2022_203",
    "reviewer": "Zhiqing Sun",
    "paper_summary": "The paper proposed a flexible framework for multi-task BERT serving. Specifically, instead of training a single multi-task BERT or training several independent fine-tuned BERT models for different tasks, the proposed framework only partially fine-tune the top layers in the BERT model, which allows these sub-task models to share the parameters in the bottm layers. The paper further proposes to compress the fine-tuned topmost layers into less layers to reduce model parameters. On the GLUE benchmark, the proposed method outperfroms other smaller BERT variants with a better accuracy-model_size trade-off. Overall, the review believes this is a strong short paper. ",
    "strengths": "1. The writing of the paper is clear and the motivation is clear. The proposed method (i.e., combining partial weight sharing and compression) is novel.\n2. The proposed method can compress BERT-base by 60% with only 0.3 performance degeneration on GLUE tasks. The empirical results are quite good. ",
    "weaknesses": "1. For flexible multi-task, the paper proposes to initialize the student from the bottommost Ns layers of the teacher. Besides, the paper proposes to determine the number of task-specific layers for each task based on the marginal benefit. These two design choices seem to be a bit arbitrary. Have the authors tried other strategies such as 1) initialize the student like in PKD-BERT 2) determine the number of layers by total budgets (for example, fine-tune students with different number of layers and set an hard overhead budget for all sub-tasks, like the current 34.3%, and do a brute-force search to fine the optimal student layer configuration)? ",
    "comments": "See Weaknesses ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "3076e22c80c84ecd",
    "paper_id": "ARR_2022_203",
    "reviewer": null,
    "paper_summary": "The paper deals with multi-task learning. The authors find that having separate networks to learn separate tasks would lead to good performance, but requires a large memory. Using MT-DNN would save memory, but the results are not satisfying. The authors thus propose an approach that saves memory and at the same time achieves good results on GLUE.\nFigure 1 gives a good summary. First, train task-specific models (but for each model, only finetune the top n layers). Second, do knowledge distillation, so that we can compress the n layers into a smaller number of layers. Third, merge all the models together as shown in Figure 1(c). The GLUE performance is comparable to full fine-tuning (i.e., tuning k models separately where k is the number of tasks), and the proposed approach saves 2/3 of memory. ",
    "strengths": "- Writing is clear.\n- Many baseline experiments are performed, including DistillBERT, BERT-of-Theseus, MT-DNN, and many others.  - The observation about MT-DNN’s degradation on QQP (line 237) is interesting. This observation reminds me of the intermediate training empirical paper, where in Table 1 they find that QQP is a very different task compared to others: https://arxiv.org/pdf/2005.00628.pdf ",
    "weaknesses": "Other models - Another simple baseline is to have two separate models: one for tasks that lead to “task interference” like QQP, another for other tasks. I wonder if this baseline will perform better than the authors’ approaches (both in terms of memory and performance). There could be other ways of clustering the tasks.  - Adapter is a widely used framework that performs well for MTL. Although not directly connected to the authors' approach, I think that the authors should at least discuss it a bit more in the paper. Given the adaptor framework, would the authors’ approaches perform better? Are the authors’ approaches complementary?  Experimental details - Given that this is an empirical paper, I believe that much more detailed descriptions on hyperparameters (for example, on each task) are necessary.  More tasks - Relatively minor: Efficient BERT related papers recently often report SQuAD results as well, given that it’s a different format (span selection instead of multiple choice) and the skillset may be different from GLUE. Do authors think that their approach would adapt to SQuAD?  Motivation - Relatively minor: If there is a much larger number of tasks, the authors’ approach may not be as efficient as shown in table 2 (i.e., two thirds of memory), given that the authors’ approach is still O(k) where k is the number of task. ",
    "comments": "Abstract: “overhead” -> it’ll be great to elaborate what overhead you’re referring to (especially because this is the abstract).\nMinor: commas should follow “i.e.” Line 136: “We find that as long as the student model is properly initialized, the vanilla KD can be as performant as those more sophisticated methods” <- it'll be great if the authors can elaborate. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]