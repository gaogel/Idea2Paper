[
  {
    "review_id": "e720868dc986e40c",
    "paper_id": "ARR_2022_86",
    "reviewer": null,
    "paper_summary": "This paper proposes to use the synonyms of the ICD codes to enrich the code representations. A multiple synonyms matching network (MSMN) is proposed to encode multiple synonyms and improve the task of automatic ICD coding. Experimental results show that the proposed method improves the performance over the baseline methods, especially when evaluated using AUC.  The idea of using synonyms of the ICD codes is simple and effective. The paper is well-written and concise. The experimental results show promising improvement over previous work. I like that the authors include discussion of varying m and different scoring functions in the paper to make it more complete. Overall, I think this paper introduces a simple but effective method to improve automatic ICD coding and conducts experiments to verify its effectiveness. I think the material presented is well-suited for a short paper. ",
    "strengths": "- The idea of using synonyms of the ICD codes is simple.\n- The paper is well-written and concise.\n- The experimental results show promising improvement over previous work. ",
    "weaknesses": "I have some questions regarding the results, see Questions below ",
    "comments": "- The proposed method obtains larger gains on the top-50 setting. My intuition was that the top codes would be benefited less since we have enough training data for those codes. Why do you think this happens?\n- For the full setting, AUC and precision improved a lot, but not F1. Do you have any observation or explanation for this? ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "d81677a8c643c5a3",
    "paper_id": "ARR_2022_86",
    "reviewer": null,
    "paper_summary": "This paper addresses a text classification task: assigning ICD-9-CM codes to a clinical text.  This is a difficult task because only short spans of the input text are relevant to a given target code, hence the use of attention mechanisms in recent work.  In the present work, multiple synonyms are used in the attention mechanism in order to attend to all descriptions of a code.  Furthermore, a biaffine transformation is learnt to help compare synonym representations to the representation of the input text for a given code. \nThat combination of using multiple synonyms in the attention mechanism and a biaffine transformation in the architecture is novel to the best of my knowledge.  On the MIMIC-III ICD9 coding dataset (full and top-50 codes), both sub-methods are shown to improve results over using a single term per code or learning independent code representations as in previous work. \nThe paper finally explores the space of synonym representations. ",
    "strengths": "- The introduction of multiple synonyms and of the biaffine transformation in the architecture is motivated.\n- Each are shown to bring improvements.\n- Improves over previous state of the art, represented by five published systems.\n- Short exploration of the resulting synonym representation space. ",
    "weaknesses": "- Experiments are performed only once, with no test of multiple random seeds.  Performing experiments N times and reporting mean and standard deviation is good practice.\n- Tests should be performed to check whether the observed differences are significant. ",
    "comments": "- The basic principle of using multiple synonyms of ICD terms is quite common in earlier work: using the presence of ICD in the UMLS makes it straightforward to obtain more terms for the same ICD code, and terms can then be used to help concept extraction and normalization from text.  In a way, it is therefore surprising that recent work using deep learning methods does not use this simple resource. \n  - Note that in the related task of entity linking, for instance BioSyn (Sung et al., ACL 2020) uses multiple synonyms for each concept.\n- How did you sample the synonyms when not all of them are selected: fully randomly? \n  - what if less synonyms are available than the required number $m$?  Does this happen in the performed experiments?\n- More synonyms probably increase the memory and time requirements: can you say more about the complexity of the method relative to the number of synonyms?\n- \"Deep learning methods usually treat this task as a multi-label classification problem\": this is the case more generally of supervised machine learning methods, and is not specific to deep learning methods.\n- \"A d-layer bi-directional LSTM layer with output size h /is followed by/ word embeddings to obtain text hidden representations H.\" -> I assume that the LSTM takes word embeddings as input instead?\n- \"We notice that the macro F1 has large variance in MIMIC-III full setting\": please explain what you mean by variance here.  This does not seem to be visible in Table 1.\n@inproceedings{Sung:ACL2020,     title = \"Biomedical Entity Representations with Synonym Marginalization\",     author = \"Sung, Mujeen  and       Jeon, Hwisang  and       Lee, Jinhyuk  and       Kang, Jaewoo\",     booktitle = \"Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics\",     month = jul,     year = 2020,     address = \"Online\",     publisher = \"Association for Computational Linguistics\",     url = \"https://aclanthology.org/2020.acl-main.335\",     doi = \"10.18653/v1/2020.acl-main.335\",     pages = \"3641--3650\" } ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]