[
  {
    "review_id": "9366c6f19b029f72",
    "paper_id": "ARR_2022_171",
    "reviewer": null,
    "paper_summary": "The paper has not changed materially from the previous version. Please refer to my previous detailed summary.  The new version addresses a few weaknesses I had pointed out previously, such as to include important results that were initially deferred to the appendix and to drop a misleading comparison. It also adds more comparisons to BitFit in table 2. I do appreciate that these changes improve the clarity of the paper, however, the present version still lacks an in-depth comparison to other related work on parameter efficient models as criticized in my previous review. Likewise, experimentation on only GLUE provides an inherently limited picture on the performance of the proposed approach and can draw an overly positive conclusion (refer to Figure 2 in [1] from the previous review). ** I am increasing my score due to improved clarity to 3, but underscore that a more in-depth comparison on other datasets and with other parameter-efficient approaches is still missing.** Currently, the paper could be interesting to a narrow audience that is knowledgeable in the area, i.e., being able to assess the proposed solutions amid the limited experimental setup.\n[1] He et al. (ICLR 2022) \"Towards a Unified View of Parameter-Efficient Transfer Learning.\" https://arxiv.org/pdf/2110.04366.pdf ",
    "strengths": "The paper has not changed materially. Please refer to previous summary. ",
    "weaknesses": "A few weaknesses have been addressed, especially as to the lack of information and to remove misleading information. Some major points of criticism, however still stand: More comparisons would be necessary to get a better sense of whether AdapterBias performs universally well. This concerns both datasets and models/methods.\n1) Experimentation on only the GLUE datasets is limited in that it often draws an overly positive picture. Please refer to [1] from the summary above and other references from my prior review. This raises the question in which setups the proposed approach would be usable. \n2) Various baselines are missing. A comparison to other adapter architectures would be reasonable and a few other approaches such as LoRA [2], prefix tuning [3], parallel adapter [4], and Compacter [5].\n[1] He et al. (ICLR 2022) \"Towards a Unified View of Parameter-Efficient Transfer Learning.\" https://arxiv.org/pdf/2110.04366.pdf [2] Hu et al. (ArXiv 2021). \" LoRA: Low-rank adaptation of large language models.\" https://arxiv.org/abs/2106.09685 [3] Li et al. (ACL 2021). \" Prefix-tuning: Optimizing continuous prompts for generation.\" https://arxiv.org/abs/2101.00190 [4] Zhu et al. (ArXiv 2021). \" Serial or Parallel? Plug-able Adapter for multilingual machine translation.\" https://arxiv.org/abs/2104.08154v1 [5] Mahabadi et al. (NeurIPS 2021). \" Compacter: Efficient Low-Rank Hypercomplex Adapter Layers.\" https://arxiv.org/pdf/2106.04647.pdf ",
    "comments": "no further comments ",
    "overall_score": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]