[
  {
    "review_id": "e1272fb808dce19b",
    "paper_id": "ARR_2022_318",
    "reviewer": null,
    "paper_summary": "This paper proposes a new pre-trained variational encoder-decoder dialog model which has continuous latent variables to deal with the one-to-many mapping problem in dialogue response generation. \nThis paper conducts empirical experiments on 3 datasets to show their proposed model has better performances both on relevance and diversity than previous state-of-the-art dialog systems. \nThey also conducted additional analysis to show the impact of latent variable sizes, different decoding strategies and position embeddings for their proposed model. \nHowever, in the model evaluation part, the paper does not show how their model can generate diverse responses given the same context. ",
    "strengths": "1. This paper proposes a new variational encoder-decoder dialog model for open-domain dialogue generation tasks, which shows better performance in terms of relevancy and diversity than previous state-of-the-art models. The new model leverages several techniques to better alleviate the one-to-many mapping problem in dialogue response generation, including conditional variational autoencoder, n-stream self-attention, memory scheme, masked language modeling, free bits and bag-of-words loss for reducing KL-vanishing, and position embeddings for Transformers. \n2. This paper conducts empirical experiments on three benchmark datasets to validate the effectiveness of the proposed model. The proposed model achieves better performances than previous baselines (e.g. PLATO) in both automatic metrics (e.g. BLEU-1/2, Distinct-1/2) and human evaluation (fluency, coherence, informativeness, overall). \n3. This paper analyzes the effect of latent variable sizes, sizes of K in top-k sampling and different combinations of position embeddings, which are helpful empirical observations for training Transformer-based variational encoder-decoders. ",
    "weaknesses": "1. This paper lacks a discussion on its novelty, e.g., why continuous latent variables are better than discrete latent variables for solving the one-to-many mapping problem. Though empirically the proposed DialogVED performs better than PLATO, DialogVED incorporates more other techniques (e.g. n-stream self-attention, memory scheme, masked language modeling, free bits) than PLATO. It is not very clear to me why we should choose those techniques to train a continuous variational encoder-decoder. Maybe some ablation studies on the training objective and model architecture can help explain it. \n2. The model evaluation does not show the model's superiority in solving the one-to-many mapping problem in open-domain dialogue response generation. Distinct-1/2 are corpus-level metrics and are not showing the diversity of generated responses given the same context. To verify their claim, the authors may consider using self-bleu and the ratio of unique generated sentences to better evaluate the diversity. ",
    "comments": "1. It is unclear to me what's the input to the prior network. Fig 1 suggests the [CLS] token is at the end of the context, but line234 says the [CLS] token is at the beginning of the context. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]