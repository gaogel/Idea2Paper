{
  "paper_id": "ARR_2022_229",
  "title": "Challenging America: Modeling language in longer time scales",
  "conference": "ARR",
  "domain": {
    "research_object": "论文主要研究文本数据，特别关注在较长时间尺度下的语言建模问题，涉及对语言随时间演变的建模与分析。",
    "core_technique": "论文可能采用或改进了长时序文本建模相关的技术方法，如时间感知的语言模型、长期依赖建模技术，可能包括但不限于基于Transformer的架构或其他序列建模方法。",
    "application": "研究成果可应用于历史文本分析、社会语言演变研究、长期趋势预测、新闻或社交媒体内容分析等场景。",
    "domains": [
      "自然语言处理",
      "时序建模",
      "计算社会科学"
    ]
  },
  "ideal": {
    "core_idea": "构建并公开了首个大规模、细粒度时间标注的历史英文文本语料库及NLP基准，用于训练和评估历史语言模型。",
    "tech_stack": [
      "Transformer架构",
      "预训练-微调范式",
      "时间感知语言模型",
      "OCR处理",
      "大规模文本语料构建"
    ],
    "input_type": "带有精确日期标注的大规模历史英文报纸文本数据",
    "output_type": "可用于预训练和评估的历史语言模型及相关NLP基准测试结果"
  },
  "skeleton": {
    "problem_framing": "论文首先从当前NLP主流方法的实际应用痛点出发，指出大规模预训练语言模型（如GPT-2、RoBERTa、T5）及其在标准基准（如GLUE、SuperGLUE）上的评测已成为常态。随后，作者引入数据污染（data contamination）问题，强调了训练集与测试集分离的重要性，并进一步指出数字信息的时间轴扩展（新数据不断产生，历史数据不断被数字化）。通过强调历史文献的数字化和公开，作者自然引出在历史文本上构建和评估语言模型的需求，明确提出该领域缺乏合适的基准和数据集，进而引出本文的研究主题和贡献。",
    "gap_pattern": "论文批评现有方法主要采用‘现有方法忽视了X’和‘现有资源存在Y局限’的逻辑。具体表现为：1）现有NLP基准和模型主要关注现代文本，缺乏对历史文本的系统性建模和评测；2）Google Ngram Viewer等历史语料资源仅提供粗粒度的n-gram统计，且数据异质且不可完全公开，缺乏高质量、细粒度（如日级别时间戳）的历史语料；3）近期的时间感知语言模型（如Temporal T5、TempoBERT）只关注现代文本且时间分辨率较低。通过这些批评，作者突出自身工作的创新点和必要性。",
    "method_story": "方法部分采用‘先整体后局部’的叙述策略。首先整体介绍了大规模历史语料的构建过程，包括数据来源、数据规模、数据清洗（去除垃圾和噪声）、时间标注（精确到日）等核心特征。随后，方法部分会进一步细化，介绍如何基于该语料进行预训练，以及如何设计下游任务和基准测试，逐步展开每个模块的具体实现细节。",
    "experiments_story": "实验部分的叙述策略以‘主实验+多角度分析’为主。首先会在新构建的历史文本基准上进行主实验，验证预训练模型在历史文本上的表现。其次，可能包含与现有模型（如现代语料预训练模型）的对比实验，突出新方法的优势。此外，实验还可能涉及不同时间分辨率、不同任务（如文本分类、时间预测等）下的性能分析，展示方法的全面性和有效性。"
  },
  "tricks": [
    {
      "name": "权威引用建立背景",
      "type": "writing-level",
      "purpose": "通过引用主流模型和基准测试，建立研究背景和方法的权威性",
      "location": "introduction",
      "description": "作者引用GPT-2、RoBERTa、T5等主流模型及GLUE、SuperGLUE等基准，说明当前NLP主流做法，并为后文创新点埋下伏笔。"
    },
    {
      "name": "问题引入与需求铺垫",
      "type": "writing-level",
      "purpose": "引导读者关注数据时间维度和历史文本建模的需求，突出研究意义",
      "location": "introduction",
      "description": "通过讨论数字信息的时间扩展（前向新数据与后向历史数据），强调历史文本建模的重要性和未被满足的需求。"
    },
    {
      "name": "现有工作局限性对比",
      "type": "writing-level",
      "purpose": "通过对比现有数据集和方法，突出自身工作的创新性和必要性",
      "location": "introduction",
      "description": "指出Google Ngram Viewer等现有历史语料的不足（如数据不可用、时间粒度粗、异质性强），为自己的方法提供合理性。"
    },
    {
      "name": "贡献点分条列举",
      "type": "writing-level",
      "purpose": "清晰、结构化地突出论文创新点和主要贡献，增强说服力",
      "location": "introduction",
      "description": "以条列形式明确列出论文的主要贡献，包括语料规模、质量、时间分辨率和公开性等。"
    },
    {
      "name": "数据规模对标主流模型",
      "type": "method-level",
      "purpose": "通过与主流模型训练数据规模对比，证明语料库的充分性和实用性",
      "location": "introduction",
      "description": "强调语料库大小（201GB）与GPT-2、RoBERTa、T5等主流模型训练数据规模相当，突出其实用价值。"
    },
    {
      "name": "细粒度时间标注强调",
      "type": "method-level",
      "purpose": "突出数据集的独特性和创新性，强调时间维度的精细化",
      "location": "introduction",
      "description": "强调语料库按天标注时间，提供比以往更细粒度的时间信息，为时间敏感语言建模提供新可能。"
    },
    {
      "name": "公开可复现承诺",
      "type": "writing-level",
      "purpose": "提升工作透明度和学术影响力，增强可信度",
      "location": "introduction",
      "description": "明确承诺将整个语料库公开，便于社区复现和后续研究。"
    },
    {
      "name": "与最新相关工作对比",
      "type": "writing-level",
      "purpose": "通过对比最新时间感知模型，突出自身方法的时间跨度和分辨率优势",
      "location": "introduction",
      "description": "对比Temporal T5、TempoBERT等仅处理现代文本和年粒度数据的方法，突出本工作在时间跨度和时间分辨率上的突破。"
    },
    {
      "name": "逐步递进的叙事结构",
      "type": "writing-level",
      "purpose": "通过逻辑递进引导读者理解问题、现有不足、创新方案和贡献",
      "location": "introduction",
      "description": "从主流方法讲起，逐步引出历史文本的特殊需求，再对比现有方法不足，最后提出创新方案和贡献。"
    }
  ]
}