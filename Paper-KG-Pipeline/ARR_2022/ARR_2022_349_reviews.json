[
  {
    "review_id": "35d102a8beb922f7",
    "paper_id": "ARR_2022_349",
    "reviewer": "Zekun Wang",
    "paper_summary": "This paper introduces a suffix identification dataset ChapterBreak to evaluate the long-range understanding ability of long-range Transformer language models. This dataset contains complex types of chapter transitions that require long-range context understanding. The authors evaluate previous long-range language models on the challenge dataset, such as Big-Bird and Routing Transformer. They observe all models perform poorly compared to a segment-based RoBERT model trained on the suffix identification task. The authors assume the accuracy of suffix identification is a more useful metric than PPL for long-range context understanding evaluation. ",
    "strengths": "1. A new task (suffix identification) is proposed for evaluating the long-range context understanding ability of language models. \n2. Some previous typical models are evaluated on the new dataset, which can facilitate future works. ",
    "weaknesses": "1. The previous LRLMs perform poorly on suffix identification compared to the RoBERTa-based model, which is trained with this task. The LRLMs may have better results when also trained with this task. The authors need to show their task is more useful than vanilla language modeling task on more downstream tasks, such as QA or summarization. ",
    "comments": "(1) The authors claim that the ChapterBreak (suffix identification) is more chanllenge on long-range understanding tasks. However, in order to show the effectiveness of the suffix identification task task, the long-range Transformer model pretrained with the task need to have powerful performance on the downstream understanding tasks than vanilla language modeling. The authors can give more results about that, following previous works (Big-Bird or Longformer)? ",
    "overall_score": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "3d2846d4c1ce6510",
    "paper_id": "ARR_2022_349",
    "reviewer": null,
    "paper_summary": "This paper introduces ChapterBreak, a new dataset built for evaluating the capacity to model long range dependencies in text. The dataset is built using two sources of data, the PG-19 dataset (Rae et al., 2020) and a fanfiction online archive (Archive of Our Own, AO3). The entire dataset is comprised of around 7 thousand examples, and is meant only for validation (all evaluations are \"out-of-distribution\"). The authors experiment with multiple baselines models, showing that existing models perform poorly on the task. A segment-level model trained directly for this task substantially outperforms existing models including Local Transformer, Routing Transformer, Big Bird, GPT-2 and GPT-3. The authors perform some analyses of their dataset and ablations, including the distribution of types of chapter transitions and other types of discourse boundaries. The dataset will be made publicly available. ",
    "strengths": "Overall, this is a solid paper that presents a useful resource for exploring the ability of current systems to handle long term dependencies. This research direction is fertile and growing, so I believe this resource will be useful to many.\nThe experiments in the paper provide useful information about the capabilities of existing models, showing that there is much headroom for progress.\nThe paper is clear and well written. ",
    "weaknesses": "Measuring human performance on this task would add great value to the paper. This should be feasible considering the size of the dataset.\nWhile the authors evaluate on 5 existing models (3 designed for long-range tasks), a wider coverage of the existing models and their performance on this task would greatly strengthen the paper. Some examples include Linformer, Reformer, Synthesizer, Performer, etc. ",
    "comments": "- ",
    "overall_score": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]