[
  {
    "review_id": "46fa0e8ff8319c78",
    "paper_id": "ARR_2022_120",
    "reviewer": null,
    "paper_summary": "This paper introduces LexGLUE, a new benchmark for language understanding for English legal texts. \nThe authors claim it is important to have a benchmark for this, as NLU is more and more used for applications in legal domains, but it is not so clear what works and what not. \nWith the easier access to datasets and tools for legal NLP, hope is also to introduce more interdisciplinary researchers to legal NLP and  speed up the adoption and transparent evaluation of new legal NLP methods. \nThe authors collect 7 datasets for legal NLP (Multi-label classification, Multi class classification, multiple choice QA). \nThey use a SVM baseline and different pretrained transformers on this task to get an initial performance picture ",
    "strengths": "- Authors chose only publicly available datasets - Datasets have a variety of sizes and sub domains - Baseline evaluation with many different models - Nice ethics statement ",
    "weaknesses": "- Only English, it is understandable and explained in the paper why that is but still sad - Only 2.5 different tasks (Multi-label classification, Multi class classification, multiple choice QA) - Datasets are not new, but just aggregated from other people ",
    "comments": "It would be nice to hedge a bit more the NLU claim, as the tasks in this benchmark are mostly classification. NLU for me is more sematnic tasks NLI/QA/semantic parsing, relation extraction, ... ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "21412b2c556d3d50",
    "paper_id": "ARR_2022_120",
    "reviewer": null,
    "paper_summary": "The authors present LexGLUE, a benchmark for legal language understanding. Their goal is that in releasing this benchmark, it will be easier to measure progress on legal language understanding, which currently has relatively fragmented resources and evaluations. They also release several baselines on the benchmark they release. ",
    "strengths": "This is a well written paper that's motivation is quite clear. \nThe datasets/benchmarks gathered should be useful to researchers working on legal language understanding. ",
    "weaknesses": "None of the datasets presented are actually new, rather these are just new splits and a packaging of existing resources into a benchmark. \nThe tasks require input texts of drastically different lengths, which a single model might not be very good at. \nThe baselines are a little bit old and arbitrary (no encoder-decoder models and only one type of hierarchical modeling). ",
    "comments": "LexGLUE feels like kind of an odd name for a legal focused dataset 2 Line 231-232: s/access/assess 3 It feels odd to try to make only have english a desirable property of the benchmark, instead of a limitation 3.2  what does it mean articles cannot be violated? \nare these cases normally split chronologically? While this ensures a lack of overlap, it seems possible that each court produces a slightly different distribution of decisions. Is this type of out of distribution evaluation intended?\n4.2 Why are all of the models tested encoder-only? Why no encoder-decoder models like T5? \nBERT-like models etc. not that good at storing sentence encoding representations in the CLS token (see e.g., Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders (Liu et al 2021) or Condenser: a Pre-training Architecture for Dense Retrieval (Gao & Callan 2021), so difficult to conclude these baselines are particularly strong. In general, hierarchical modeling seems to be its own task area, where both different pretraining techniques might produce better sentence encodings and different methods of hierarchical modeling may produce better overall results. \nThis baseline seems to combine tasks of various different lengths – do you expect the same models to be good at both? Is it even important for one model to be good at both?\n5.2: feels a bit hard to say that your result clearly favors the legal models -- 13 total results, 6 of which the legal models do the best Why is there no “overall” score reported, like there is with GLUE? ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work."
  },
  {
    "review_id": "da6008c2bf458c66",
    "paper_id": "ARR_2022_120",
    "reviewer": "Bashar Alhafni",
    "paper_summary": "The authors introduce LexGLUE, a benchmark dataset to evaluate the performance of NLP models in legal NLU tasks. LexGLUE is based on seven English publicly available legal NLP datasets, selected using criteria largely from SuperGLUE. The datasets span text classification (6 datasets) and legal question answering (1 dataset) tasks. The authors introduced several simplifications to the datasets to make the benchmark more standardized and easily accessible. ",
    "strengths": "1) The paper is very clear and well written. \n2) I believe that the legal NLP community will definitely benefit from a standardized and easily accessible benchmark to help facilitate and encourage research on legal NLP. \n3) LexGLUE could also encourage future work to push towards generalized models that could handle various legal NLP tasks with limited annotated data. ",
    "weaknesses": "1) Although the authors provide a very clear description of the various tasks and datasets that are included in LexGLUE, it would be helpful for the readers to see examples of what the input(s)/output(s) are for each task as part of the Appendix. ",
    "comments": "See weaknesses. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]