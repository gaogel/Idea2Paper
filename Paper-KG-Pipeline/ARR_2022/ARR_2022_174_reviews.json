[
  {
    "review_id": "9246e12b2a95f616",
    "paper_id": "ARR_2022_174",
    "reviewer": null,
    "paper_summary": "This paper proposes VHED, 1) a dataset comprising human judgments over story pairs from the VIST dataset, and 2) Vrank, a reference-free metric for VIST. To motivate the new metric contribution, this work provides rich insights regarding the evaluation protocols being used in VIST. \nExperiments demonstrate Vrank superior capacity to align with human judgments when compared with existing automatic metrics. ",
    "strengths": "1. The contributed dataset is valuable for the field and can help researchers working on the task of visual storytelling.\n2. The conducted analysis on the annotated story pairs brings relevant insights to this task, such as the fact that in some cases, machine-generated stories are better than references. Such observations not only deliver a strong motivation for the work, but also contribute directly to better understanding the task at its evaluation protocols.\n3. The Appendix provides extra experiments that complement the main experimental section.\n4. Paper manuscript is well written and structured. ",
    "weaknesses": "1. Authors could have investigated Vrank predictions, towards understanding where the model makes mistakes. Assuming that Vrank would be adopted to evaluate VIST models, it is crucial to know its limitations. Additionally, this could serve as guidance to collect extra annotations to mitigate those mistakes.  2. Vrank does not account for the actual images. This is somehow reflected in the correlation results of Vrank for Obj and Event error types. This seems to be a weakness of the proposed approach since by definition, it is trying to imitate human judgment, but without access to the same information that annotators had to decide.  In the Appendix, \"Model Design\" section, authors state that tried vision and language models but do not provide details. What was actually tried? Did it improve in detecting Obj and Event error types?\n3. Given the insights gathered by the authors, I would expect a critical discussion regarding the inclusion of Vrank in VIST evaluation protocols. Namely, provided that Vrank also has error, the best possible automatic protocol would include only Vrank or be complemented with other metrics? ",
    "comments": "In overall, the paper is well written, but there are some minor typos so I would suggest proofreading the manuscript. Some of the typos/grammar issues found: - Line 130-131: check grammar - Line 248: \"... and is ...\" - Line 400-401: check grammar - Line 426: \"autometric\" - Line 556-557: check grammar ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "229c437e49835ceb",
    "paper_id": "ARR_2022_174",
    "reviewer": "Alessandro Suglia",
    "paper_summary": "This paper describes a novel dataset for visual storytelling evaluation. The authors collect VHED, a dataset containing semantic quality scores for several human and machine-generated stories. Thanks to their data collection, they are able to collect very fine-grained evaluation data for 13,875 story pairs. An error analysis of the collected dataset is performed which sheds some light on the properties of the stories. First of all, human references are not always the best reference: 38% of the machine-generated stories are actually better than human-generated ones. Additionally, 79.8% of machine-generated stories have at least one error (according to their detailed classification).   Using such a dataset, they develop VRank as a reference-free model for story generation evaluation. This is a model finetuned on the VHED dataset starting from the SIMCSE pretrained model. The model receives in input the two stories (separated by [SEP]) and generates a score to predict the ranking gap. The model is then finetuned by minimising the MSE loss. The results of the evaluation show that Vrank has better performance than model-based and n-gram based evaluation methods. Another interesting part of their analysis is that they use their model for evaluating text-only stories as well showing very promising results. ",
    "strengths": "1. They collect a very fine-grained and high-quality dataset for story generation evaluation consisting of both human and machine-generated stories. \n2. They complete an error analysis that provides important insights into the errors made by humans and machines. \n3. They developed Vrank, a novel reference-free evaluation model for story generation evaluation: it has very good performance on both visual and textual stories. \n4. The paper is very well written and easy to follow. The contributions are clearly described and all the experimental results support the main paper idea. ",
    "weaknesses": "1. The related work section, especially the part related to \"VIST-Human Evaluation\", should report more details about the difference between their approach and previous state of the art 2. It would be beneficial if the authors could release both their data as well as pretrained model in order to make sure that other researchers can reuse their evaluation and potentially build upon their work ",
    "comments": "1. Typo Line 516 ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "ce67bb03e40a4cb5",
    "paper_id": "ARR_2022_174",
    "reviewer": "Jack Hessel",
    "paper_summary": "The authors propose a new learned metric for ranking the quality of generated visual stories. Their model/metric, VRANK, is trained directly on pairwise human judgments. Compared to metrics traditionally used for caption generation evaluation like METEOR and ROUGE-L, the authors new metric performs better. They also evaluate on a non-visual corpus, and demonstrate that their model also works to evaluate in that case. ",
    "strengths": "I liked this work! I suspect that some controversy because the author's metric is i) model-based; and ii) trained directly on human ratings, but the authors make a compelling case that our current metrics simply are not working for this setting. In some sense, the results are not surprising: of course the learned metric does better in the evaluation setting that it was trained in. But, the fact that it also does well in a unimodal setting suggests that the model has not overfit to particular spurious factors in the corpus, which is promising. Overall --- I think the authors make a convincing case for their model-based metric. And, the fine-grained evaluations in table 5 provide some perspective on where the metric could be improved, e.g., accounting more for grammatically and irrelevant nouns. ",
    "weaknesses": "Of course, training on human judgments directly is not a panacea. There's always the concern that whichever pairwise comparisons were originally collected won't generalize to future corpora, e.g., if the models improve in the future, will the particular model generations that Vrank was trained on become \"stale\"? \nThat being said, because the current evaluation metrics are essentially performing at random for the story generation setting, even a stale Vrank couldn't possibly be worse.\nThe author's contribution is in visual story generation evaluation; the task itself is somewhat niche, and so, while I think folks who evaluate on this dataset/setup should use this dataset, I suspect that the work's works impact will mostly be focused on that specific community.\nOne small technical concern: For Table 4, The BLEU-4 score is suspiciously low, especially in comparison to SacreBLEU, which also computes BLEU-4. My suspicion is that BLEU-4 is often zero for some stories, which leads to many ties of 0 vs 0. I think that ties should be broken randomly, or that the authors should mention this is the cause (if indeed my suspicions are correct). ",
    "comments": "Overall: the authors make a compelling case that current generation evaluation metrics perform poorly for ranking the quality of visual story generation. While a somewhat niche task, their model clearly performs better than the other metrics, and given that the other metrics are essentially performing at random, the normal critiques of \"staleness\"/\"neural nets are uninterpretable as metrics\" that could apply in this setting hold less weight. ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]