[
  {
    "review_id": "8e0619c3528a101f",
    "paper_id": "ARR_2022_3",
    "reviewer": null,
    "paper_summary": "This paper probes language-only and language-and-vision models for their knowledge about the visual properties of concrete concepts (i.e., the common colors associated with 'dog', 'banana', etc). \nThe main question is shaped around the issue of reporting bias: do vision & language models (trained on visually grounded text & images) suffer less from reporting bias than text-only models (trained on wikipedia etc)? \nThe probing experiments conclude that visual grounding can alleviate some of the reporting bias found in pretrained text-only models. ",
    "strengths": "- Reporting bias is an important problem to try to quantify, given NLP is   assuming that wikipedia trained models are good representations of the world. \n  The experiments in this paper are a reasonable first step towards this goal.\n- Well written, easy to follow for the most part. ( It is hard to structure this   many experiments and models well.)\n- Thorough analyses and breakdowns across attribute types and distributions;    this made the results much more interesting and useful. ",
    "weaknesses": "1, The paper conflates two separate questions: a, Does the visual information in multimodal models guard against reporting bias, and b, Does more-visually grounded text suffer less from reporting bias than less-visually grounded text? \nBy only comparing pretrained Bert vs Oscar, the experiments cannot separate questions, and is in danger of drawing unmerited conclustions (E.g. L504 'concludes that ... images+text outperform text-only models': but how well does text-only Oscar perform here?). \nDisentangling them would require training a BERT model on Oscar's text data (maybe the model alluded to in preliminary experiments, L354?). \nI think the 'distilled' model is close to this, but the discussion around this model is unclear.\n2, Oscar (and distilled Bert) are trained on COCO, which part of Visual Genome. \nThere is therefore a 'training on test' problem here, which makes it difficult to separate the effect of adding the visual modality vs matching the test distribution much better than wikipedia. In fairness, Section 5.3 and Table 5 compare VG-trained Oscar with (probably) VG-free CLIP and don't see any large differences between the two, indicating that it's not the VG-in-training that is driving the differences between Bert and the multimodal models (and so more likely to be genre/domain differences, or the actual visual inputs, see point  1).\n3, The paper is missing a discussion of a lot of early related work that used e.g. McRae norms; e.g. Silberer et al (2013) and references therein.  It would also be useful to check whether the McRae norms agreed with the extracted VG properties. \nThe McRae dataset is obviously much smaller than the extracted VG dataset in this paper, but perhaps models do better (or show different patterns) for the more salient concepts included in McRae. ",
    "comments": "The term \"common sense\" is (IMO) currently overused and underdefined in NLP. The discussion on L113-120, about \"visual commonsense\" vs \"seteroytpic visual attributes\" etc is helpfully explicit about the context of the term in this paper; I would still encourage reconsidering whether \"visual commonsense\" is the best terminology for this paper.\nDo all the models use the same tokenisation? The 'mask first token' methodology (Footnote 4) would lead to better MLM results for models that use finer-grained/shorter subwords, if tokenisation is not controlled for.\nWhy does soft prompt tuning have such a huge effect on Color (~20-30) points but much less for the other attribute types?\nFigure 2/451: I think the Vokenization model has the second highest correlation gap between wikipedia and coda, after BERT - if this is true (I didn't do the math), does this imply vokenisation doesn't work?\nIn Table 3, Column Color, Bert_L has the same values for Tune=N/Y. Is this a typo or a coincidence?\nA.2 Table 9 is with or without tuning? \nIn general the figures in the appendix could use more explanation (e.g. what are the color indices in Figures 8,9; what are the subjects in Figures 6,7). It would be helpful to show high/low Corr subjs (Table 11) for Single/Multi/Any subjects separately.\nSection 4.2 (Elictation methods) includes paragraphs on soft promt tuning and knowledge distilation, which is confusing. ",
    "overall_score": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "bebacb425efb7bf0",
    "paper_id": "ARR_2022_3",
    "reviewer": "Da Yin",
    "paper_summary": "This paper centers on commonsense knowledge probing and the issue of reporting bias. Authors construct a new dataset to probe the commonsense embedded in unimodal and multimodal language models with prompt tuning methods and discuss the performance discrepancy between the two groups of models. Besides, authors attempt to evaluate the correlation between the commonsense in language models and pre-training data such as Wikipedia and VG. They observe that although multimodal models are better storing visual commonsense, they are still subject to reporting bias. Also, knowledge distillation is able to preserve the commonsense when the models are compressed. ",
    "strengths": "The experiments are pretty comprehensive and the topic of the paper (reporting bias) is an intriguing and challenging issue for commonsense reasoning capacity of language models. ",
    "weaknesses": "The writing could be further improved, including the structure organization and detailed concept clarification. It's also unclear why V&L models are able to store visual commonsense effectively (Is it really from the images or just from the captions?). ",
    "comments": "1. It's better to add more vision-and-language models including VisualBERT and ViLBERT, since they are also pre-trained with MLM objective. \n2. It seems not reasonable to me for the formula in Adjective Projection part. If we want to estimate whether the object is large or small, we need to first calculate the similarity between \"large\" and the object, then \"small\" and the object, respectively. Then we should calculate the difference between the two similarity instead of using the similarity between the object embedding and the difference between \"large\" and \"small\" embeddings. \n3. It's not crystal clear to me about what the main topic of the paper is: Is that about reporting bias analysis, or do we mainly focus on probing visual commonsense? Although I saw authors trying to build relationship between the two topics. But for me, the two parts still seem a bit independent from each other. I may suggest that the work can focus on reporting bias *on top of* commonsense probing, i.e., commonsense probing can be just treated as an analysis tool to help you study the key reporting bias issues. \n4. Paper title is a bit overclaimed and also does not cover one of the main topics reporting bias. ",
    "overall_score": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]