[
  {
    "review_id": "1209e8c920cbbdbd",
    "paper_id": "ARR_2022_98",
    "reviewer": null,
    "paper_summary": "The paper is about an unsupervised approach to sentence simplification that integrates paraphrasing into an iterative text simplification system. The authors start with an iterative framework in which an input sentence is revised using two edit operations: delete and paraphrase. For deletion, the system uses the constituency tree of the input sentence to obtain all constituents from different depths of the parse tree, and new candidate sentences are created by removing each of these phrases from the input sentence. For paraphrasing, the system uses lexically-constrained decoding, by identifying \"complex components\" in the input - which, in turn, is done by training a text complexity classifier whose attention head matrices in the second layer are used to perform the complex word identification. Candidate sentences generated by these edit operations are then filtered using a score function on the basis of grammaticality, simplicity, and semantic similarity. ",
    "strengths": "1. The approach is interesting, simple, and easy to understand - combining edit-operation-based and paraphrasing-based techniques into an iterative refinement framework. It is a hybrid between seq2seq generative modeling and explicit edit-based modeling and tries to leverage the strengths of these different groups of modeling techniques. \n2. Unsupervised system - different components (models and datasets) can be replaced, which makes the system less dependent on pairwise-annotated datasets. \n3. Interpretability and controllability - these are quite impactful characteristics of the system. ",
    "weaknesses": "1. Human evaluations were not performed. Given the weaknesses of SARI (Vásquez-Rodríguez et al. 2021) and FKGL (Tanprasert and Kauchak, 2021), the lack of human evaluations severely limits the potential impact of the results, combined with the variability in the results on different datasets. \n2. While the authors explain the need to include text generation models into the framework of (Kumar et al., 2020), it is not clear as to why only the delete operation was retained from the framework, which used multiple edit operations (reordering, deletion, lexical simplification, etc.). Further, it is not clear how including those other operations will affect the quality and performance of the system. \n3. ( minor) It is unclear how the authors arrived at the different components of the \"scoring function,\" nor is it clear how they arrived at the different threshold values/ranges. \n4. Finally, one might wonder that the performance gains on Newsela are due to a domain effect, given that the system was explicitly tuned for deletion operations (that abound in Newsela) and that performance is much lower on the ASSET test corpus. It is unclear how the system would generalize to new datasets with varying levels of complexity, and peripheral content. ",
    "comments": "1. Is there any reason why 'Gold Reference' was not reported for Newsela? It makes it hard to assess the performance of the existing system.   2. Similarly, is there a reason why the effect of linguistic acceptability was not analyzed (Table 3 and Section 4.6)? \n3. It will be nice to see some examples of the system on actual texts (vs. other components & models). \n4. What were the final thresholds that were used for the results? It will also be good for reproducibility if the authors can share the full set of hyperparameters as well. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work."
  }
]