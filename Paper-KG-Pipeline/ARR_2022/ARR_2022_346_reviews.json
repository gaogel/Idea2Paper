[
  {
    "review_id": "64aa79fd5ff9c7a1",
    "paper_id": "ARR_2022_346",
    "reviewer": null,
    "paper_summary": "This paper studies the discourse structure of long-form answers by analyzing two datasets: ELI5 and Natural Questions (NQ). The authors define six sentence-level discourse roles for long-form answers and ask human annotators to label a total of 1.3K examples. The authors further conduct experiments on two tasks based on the annotated corpus: automatic discourse analysis and summarizing long-form answers. ",
    "strengths": "- To my belief, this is the first work that analyzes the discourse structure of long-form answers. It calls our attention that better understanding discourse structure might benefit long-form QA.  - The data annotation process is described in detail and inter-annotator agreements are fully revealed.  - It did some initial studies on how machine-generated answers and human-written answers differ in discourse structure.  - The paper is fairly easy to follow. ",
    "weaknesses": "However, the work leaves a few things to be desired, let me order them from most to least important.\n- I am not clear about the goals of the paper. The authors argue that a better understanding of the discourse structure of long-form answers can benefit the long-form QA systems or the evaluation of long-form answers. However, this end goal is not covered in the paper. The authors did not perform further experiments to show whether a long-form QA system can benefit from that discourse-level information to generate better answers.  - The six sentence-level discourse roles defined by the authors are still coarse-grained to me. It is also not surprising that most long-form answers are consist of answer summary, auxiliary information, and examples. I did not find many motivating insights from the analysis that could potentially benefit the future work of long-form QA.  - Section 5 is good to me. It analyzes the weaknesses of current long-form QA systems in discourse structure. However, the authors only analyzed 52 examples. Such a small sample size may not be enough to make a statistically significant observation. ",
    "comments": "Putting together all the strengths and weaknesses I believe the authors proposed a new angle to study long-form QA and the NLP community will benefit from the annotated corpus. However, there are some problems that do not allow me to accept the current version. I think there are two directions the authors can work on to improve the paper:  - Conduct more in-depth or fine-grained analysis on the discourse structure of long-form answers, which hopefully would provide more insightful observations that can benefit the long-form QA community. For example, analyzing how the discourse structure of long-form answers differs in different domains and in different question types.  - Make this paper a closed loop. That is to show the additional discourse structure information can benefit the long-form QA system in terms of either improving its performance or facilitating the evaluation of long-form answers. ",
    "overall_score": "2.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]