[
  {
    "review_id": "74644b8c9ecdd5fa",
    "paper_id": "ARR_2022_202",
    "reviewer": null,
    "paper_summary": "This paper uses document-level co-occurence of events to learn event representations, using contrastive learning with multiple positive and negatives examples and prototype-base clustering to group the semantically related events. Empirical results evaluating performance on several event similarity tasks showed the proposed approach outperforming baseline models consistently. ",
    "strengths": "- Fairly clearly written and cited, indicating similarity and building on previous work as well as departures.  - Clearly outlined contribution combining several pieces from existing literature in multiple domains - Strong empirical evaluation and results ",
    "weaknesses": "- Motivation at the beginning is clear but then gets somewhat lost transitioning to presenting SWCC. Can you elaborate on \"seen as a superset of current defined explicit discourse relations, as most existing automatic methods extract event relations from documents or sentences\" and tie it more closely with what is presented later.\n- Underspecified experimental setup, especially data. It's in the appendix, but it's pretty important for the flow of the paper to have a better idea of what / how many events are - Not quite clear how semantically related events are defined and prototype for cluster defined ",
    "comments": "Several minor grammatical issues throughout ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "0a077695fd70410b",
    "paper_id": "ARR_2022_202",
    "reviewer": "I-Ta Lee",
    "paper_summary": "In this paper, the authors propose SWCC to improve event representation learning. The method mainly modifies the objective function, which can be broken down into a modified InfoNCE objective, a prototype-based clustering objective, and a masked language model objective. The experiment includes two intrinsic evaluations, based on event embedding similarity and one extrinsic evaluation, MCNC. The result shows that the proposed method outperforms baselines on all the evaluations. The ablation and visualization studies also identify the importance of each component (objective function). ",
    "strengths": "1. The experiment results show strong performance of the proposed method. \n2. To the best of my knowledge, this is the first work that leverages prototype-based clustering in event representation learning. ",
    "weaknesses": "1. The write-up has many typos and some formulas/explanations are confusing. \n2. The technical innovation of the proposed method is limited. The proposed objective function is basically a combination of two related works with tiny changes. \n3. Reproductivity is not ideal, as some essential parts are not addressed in the paper, such as training data. \n4. More strong baselines should be included/discussed in the experiments. ",
    "comments": "Comments 1. Line 18: it’s better to specify the exact tasks rather than stating several tasks in the abstract 2. Line 69: “current” -> “currently” 3. Line 112: margin-based loss can use one positive with MULTIPLE negatives. \n4. Figure 1: the relation types are not explicitly modeled in this work so the figure is kind of confusing. \n5. Eq. 1: what is z_p 6. Eq. 2: how do you convert BERT token embeddings into event embeddings? Concatenation? Any pooling? \n7. Line 236-237: “conciser” -> “consider” 8. Line 209-211: I can understand what you mean but this part should be re-written. For example, “given an anchor event, we generate 3 positive samples with different dropout masks.” \n9. Table 1: needs to include a random baseline to show the task difficulty 10. Experiments: what training data you use for pre-training? \n11. Table 1: do the baselines and the proposed method use the same training data for pre-training? How did you get the results for the baselines? Did you have your own implementation, or directly use their released embeddings/code? \n12. Line 450: L_{pc} or L_{cp}| in Eq. 7? \n13. Table 2: what’s the difference between “SWCC w/o Prototype-based Clustering” and “BERT(InfoNCE)”? \n14. Table 3: MCNC should have many strong baselines that are not compared here, such as the baselines in [1]. Can you justify the reason? \n15. Can you provide an analysis on the impact of the number of augmented samples (e.g., z_{a1}, z_{a2}) here? ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]