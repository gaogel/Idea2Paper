[
  {
    "review_id": "1a43bd325a6abdae",
    "paper_id": "ARR_2022_216",
    "reviewer": "Yunfang Wu",
    "paper_summary": "This paper claims the Distinct’s bias that tends to pose higher penalties over longer sequences, and then fixes the bias by calculating the expectation of distinct tokens of a random text with the same length, and divide the origin Distinct value by it. They provide theoretical evidence to the formula, and do experiments on the dialog generation task to prove that the newDistinct correlates better with human evaluations. ",
    "strengths": "1. As the paper mentioned, the idea is inspired by psychological linguistics, making it more convincing. \n2. This paper gives math analysis and derivation of the formula, making it more solid. \n3. The experiments prove the efficiency of the new Distinct. ",
    "weaknesses": "1. In the results in Table 2, there is a large score gap between the newdistinct and original distinct for the system of AdaLab, please give more explanations. \n2. In Page 7, line 517~523, C can’t be so large that the limitation method can get a good enough result. Perhaps you need more accurate math to show that when C is not so big, for example, when C is only 5x or 10x of V, the derivative of NewDistinct is still bigger than the original Distinct. Besides, the conclusion “the bigger C is, the slower the original Distinct increases” is also right for NewDistinct. ",
    "comments": "This paper is well written and is elegant. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]