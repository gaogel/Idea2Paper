[
  {
    "review_id": "364a28a87cf32839",
    "paper_id": "ARR_2022_112",
    "reviewer": null,
    "paper_summary": "The paper introduces SkillSpan, the first open-source, expert-annotated dataset for skill extraction, which is essentially sequential labeling of skill and knowledge in job postings. Authors experimented with BERT-based models with various tweaks and found that pretraining language models on job postings yields better performance. ",
    "strengths": "- Commercial motivation for automatic skill extraction is understandable and open-sourcing an expert-annotated dataset thus will be beneficial for the community.\n- This is a very solid paper with thorough description on the annotation and experiment processes.\n- The paper is well written and easy to follow. ",
    "weaknesses": "- The paper does not discuss much about linguistic aspect of the dataset. While their procedures are thoroughly described, analyses are quite limited in that they do not reveal much about linguistic challenges in the dataset as compared to, for example, information extraction. The benefit of pretraining on the target domain seems to be the only consistent finding in their paper and I believe this argument holds in majority of datasets.\n-  Relating to the first point, authors should describe more about the traits of the experts and justify why annotation must be carried out by the experts, outside its commercial values. Were the experts linguistic experts or domain experts? Was annotation any different from what non-experts would do? Did it introduce any linguistic challenges?\n- The thorough description of the processes is definitely a strength, it might be easier to follow if the authors moved some of the details to appendix. ",
    "comments": "L23: I was not able to find in the paper that single-task models consistently outperformed multi-task models. Could you elaborate a bit more about this?\nTable 1: It would be easier if you explain about \"Type of Skills\" in the caption. It might also be worth denoting number of sentences for your dataset, as Jia et al (2018) looks larger than SkillSpan at a first glance.\nSection 3: This section can be improved to better explain which of \"skill\", \"knowledge\" and \"attitude\" correspond to \"hard\" and \"soft\" knowledge. Soft skills are referred as attitudes (L180) but this work seems to only consider \"skill\" and \"knowledge\" (L181-183 and Figure 1)? This contradicts with the claim that SkillSpan incorporates both hard and soft knowledge.\nL403: I don't think it is the field's norm to call it multi-task learning when it is merely solving two sequential labeling problems at a same time.\nL527: Is there any justification as to why you suspected that domain-adaptive pre-raining lead to longer spans?\nL543: What is continuous pretraining?\nL543: \"Pre-training\" and \"pretraining\" are not spelled consistently. ",
    "overall_score": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "e0111eb824f3bc89",
    "paper_id": "ARR_2022_112",
    "reviewer": "Deblin Bagchi",
    "paper_summary": "This paper presents a new dataset on skill extraction and also explains the annotation guidelines of the dataset. They also provide baselines for the dataset. ",
    "strengths": "The strengths are as follows: 1) Very well written paper. \n2) Novel dataset with helpful guidelines that are made public which are very rare 3) Baseline model has been provided as well. ",
    "weaknesses": "No weaknesses as such found. This is a resubmission and the authors have dealt with the earlier questions effectively. ",
    "comments": "Pretty impressive paper. ",
    "overall_score": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]