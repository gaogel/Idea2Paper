[
  {
    "review_id": "0e4cddf9f9f10241",
    "paper_id": "ARR_2022_144",
    "reviewer": null,
    "paper_summary": "This paper investigates the effects of moral framing on the persuasiveness of arguments relating to controversial topics. The work is comprised of three main parts: development of a technique for mining argumentative statements which correspond to a given moral position from text; integration with IBM Project Debater’s narrative generation API to generate arguments pro/con a given topic which match a given moral position; and, a detailed user-study assessing the impact of morally-framed arguments on audiences of different political ideologies.\nFor mining of morally framed statements, a BERT classifier based on data collected automatically using distant supervision was used. This approach is clearly explained and produces good results, with the proposed model outperforming both a lexicon-based baseline and existing work for four of the five moral foundations (for 'care' the lexicon-based approach performed better than the other two).\nThe user study described was carefully planned and executed. Although the results were not 100% conclusive, they suggest a clear impact of moral framing on both liberal and conservative audiences, with both being more affected by moral arguments, and liberals in particular finding arguments with relevant moral framing more effective. ",
    "strengths": "Overall the paper is well written and clear, providing interesting insights on the effects of moral framing on persuasiveness. The implementation work is well described and highly reproducible. The BERT classifier used for identifying morally framed statements shows good results, which although not perfect, show a solid improvement over existing work.\nAlthough previous work in the field has addressed belief-based claim generation, there has been little to no work previously assessing the persuasiveness of such claims on an audience. As such, the user study included in this paper is extremely welcome and offers good indication that moral framing can impact how arguments are received. ",
    "weaknesses": "The only real concern here may be how well other factors are controlled for in the user study. With the method described for extracting morally framed argument components on a given topic, it is possible that other differences may also be present. Some of these may be connected to the moral position -- certain moral positions may bring with them certain phrasing or aspects of the topic that are more appealing to a given audience. And some of these may arise by chance given the relatively small sample size. I do not view this as a major weakness, but it may benefit from some further consideration. ",
    "comments": "The assignment of a single moral label to a sentence seems quite definitive. It may be more often the case that a sentence corresponds somewhat to two or more moral foundations, and this might be better modelled as a score for each foundation. Is this something that has been considered?\nThe sentence starting on line 279 (\"Having considerable higher effectiveness on average signals the advantage of using the proposed dataset.\") seems incomplete? ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "663a3c9d5c721bdf",
    "paper_id": "ARR_2022_144",
    "reviewer": null,
    "paper_summary": "This paper offers an argument generation system designed to highlight the particular moral status of considerations adduced in favor or against controversial proposals in summarizing query searches.  The work is motivated by the theoretical framework of psychologist Jonathan Haidt.  The key technical ingredient is a moral-argument filter, learned in a creative way from remote supervision and fine-tuning of a transformer language model, that can assist in content selection in IBM's Project Debater API. ",
    "strengths": "The paper represents a compelling attempt to test and build on an influential theory of the cognitive science and policy implications of moral reasoning.  The machine learning experiments are creative and well-chosen.  The results are promising and include a wide range of preliminary validation. ",
    "weaknesses": "The quantitative results of the main machine learning model are not great -- improving F1 score from a baseline of .25 to .4 or so.  On the one hand, F1 is not really the result you want for the downstream use case: precision is really important if you're selecting things from the web to present to a user.  What does precision look like here?  But on the other hand, the results so far are clearly making a lot of mistakes.  At the very least, if this is a compelling problem, the paper should spend some time on error analysis and insight.  If the inferences of the ML method are bad enough, it undermines all the contributions later on.\nThe human subjects experiments are small -- six subjects.  This isn't a deal breaker but it's uncomfortable, and the presentation doesn't really acknowledge that this is a preliminary evaluation (percentages with three significant figures, top numbers bolded, no statistics, no acknowledgment of the key status of particular individual items or raters as potential outliers).  It's also an evaluation on a proxy task.  The paper is forthright about the ethical issues that come with trying to persuade people but I found it a bit hard to understand what the ultimate vision was: a system that can provide people with the reasons for and against a proposal they'd find most compelling?  The arguments here seemed all to have the same direction -- all in favor, all against, but with different moral bases.  Why would you want such a one-sided summary? ",
    "comments": "(I provided full details in strengths and weaknesses above.) ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "a59147f405b42080",
    "paper_id": "ARR_2022_144",
    "reviewer": "Ivan Habernal",
    "paper_summary": "This is a very-well written paper that manipulates (more persuasive) argument generation by filtering moral arguments for the given audience before letting the IBM API generate the final output. Although it heavily relies on IBM API, it poses its own research question (moral arguments for audiences), builds a distantly-supervised classifier for moral arguments, and conducts a nicely designed user study. ",
    "strengths": "- very clear writing, easy to understand key points - interesting application of distant supervision for detecting moral arguments (better than few-shot fine-tuning) - interesting user study with conservative and liberal audiences - code submitted along with the paper ",
    "weaknesses": "- I'm afraid the final number of participants in the user study (3 + 3) makes any drawn conclusions rather unsubstantiated; One would expect some confidence intervals (classical stats approach) or high-density intervals (with Bayesian approach) to see, how much we can trust the study in Table 5 and 6 - Not sure, how much IBM API is stable/reproducible. See a similar argument by Kilgarriff, A. (2006). Googleology is bad science. Computational Linguistics, 33(1):147–151 ",
    "comments": "Missing reference to Lukin et al. 2017 (EACL) How exactly do you formulate queries (378)?\nRegarding the Ethical statement: Well... this is super weak argumentation: you rely on IBM's black box but at the same time claim only correct solution is to aim for being transparent to the user. So you weren't but still did the study. What should we take from it? \nOn the other hand, I think the ethical statement section is mostly non-sense anyway, and I wouldn't know what to write here myself, if I were the author. There is no guarantee that transparency leads to non-abusive technology, to begin with. So I'm just going to ignore this section completely. ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "48f9a3caf3ba7742",
    "paper_id": "ARR_2022_144",
    "reviewer": null,
    "paper_summary": "The paper investigates methods to automatically generate morally framed arguments (relying on a specific stance, on the given topic focusing on the given morals), and analyses the effect of these arguments on different audiences (namely, as liberals and conservatives). ",
    "strengths": "- The topic of the paper is potentially interesting to the ACL audience in general, and extremely interesting in particular to the Argument Mining (and debating technology) research community. Investigating methods to inject morals into argument generation systems to make arguments more effective and convincing is a very valuable step in the field (opening at the same time ethical issues).\n- The paper is clear, well written and nicely structured - The experimental setting is well described and the applied methods are technically sound. It relies on the solid framework of the IBM Debater technology. ",
    "weaknesses": "- very limited size of the user study  (6 people in total, 3 liberals and 3 conservatives). Moreover, a \"stereotypical\" hypothesis of their political vision is somehow assumed) - the Cohen’s κ agreement was 0.32 on the moral assignment -> while the authors claim that this value is in line with other subjective argument-related annotations, I still think it is pretty low and I wonder about the reliability of such annotation. ",
    "comments": "[line 734] Ioana Hulpu? - > check reference ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]