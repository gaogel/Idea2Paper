[
  {
    "review_id": "6d5b5cb633d8448f",
    "paper_id": "ARR_2022_127",
    "reviewer": "Seung-Hoon Na",
    "paper_summary": "This paper proposes the regularity-aware and regularity-agnostic models for Chinese NER and present the combined loss to train them. The regularity-aware model first performs a local self-attention applied over span representation and combine the resulting attentive representation with original span representation. The regularity-agnostic model is based on the biaffine attention decoder to mainly focus on detecting the boundary of an entity mention. Both regularity-aware and -agnostic models are trained such their backbone hidden representations are orthogonal. In the experiment results, the regularity-aware models lead to improvements over the previous models on three Chinese NER datasets. ",
    "strengths": "- The presentation is relatively clear and the paper is easy to follow.\n- The proposed regularity-aware and -agnostic models and their joint training method are interesting and novel.  - The experiment results show strong performance, outperforming recent models on three Chinese NER datasets. ",
    "weaknesses": "- It seems that the regularity-aware models don’t exploit any lexicon, but only relying on local self-attention over span representations formulated in Eq. (6). It is not clear how this local self-attention of Eq. (6) is related to “regularity” or “regular types” of entities. The motivation of paper is not very tightly related to the proposed regularity-aware models, i.e. how it can be seen as an alternative model of the previous lexicon-based models?  - During inference, regularity-aware models are only used as in Section 3.5. Then, the regularity-agnostic models are not used during inference? Is this model used in only the orthogonality space restriction of Eq. (16)? Detailed descriptions are provided.  - In Table 3, it is not clear what is the difference b/w reg-aware&agnostic and RICON models. The models compared in the experiments need to be clearly described.  - The proposed models could be applicable to other English NER datasets, i.e. the regularity concept can be extended to other languages. Otherwise, the authors need to present why the addressed issue of the proposed model appears in Chinese NER task, not in other English NER tasks, . ",
    "comments": "- (please see weaknesses) ",
    "overall_score": "2.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]