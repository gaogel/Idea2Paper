[
  {
    "review_id": "fa4054a7eeb47c2a",
    "paper_id": "ARR_2022_212",
    "reviewer": null,
    "paper_summary": "The authors propose mix-GLT, a technique for training non-autoregressive transformer (NAT). NAT models have suffered from multi-modality problem, where each input may have multiple valid outputs. The proposed mix-GLT combines the 1) latent transformer (LT) that uses discrete latent variables to reduce multi-modality problems with 2) glancing transformer (GLAT), a fast and strong NAT baseline. \nmix-GLT is trained with three objectives: Length prediction + Latent prediction (from LT) w/ Glancing (from GLAT)+ Output modeling w/ glancing (from GLAT).\nThe authors compare the proposed methods on machine translation, paraphrase generation and dialog generation benchmarks. In their experiments, mix-GLT outperforms other strong NAT baselines with higher accuracy, while adding some extra computation. The authors also show that mix-GLT performance does not improve with knowledge distillation (KD) from autoregressive teacher (AT), implying mix-GLT suffer less from multi-modality problem. The authors also show other studies including ablation of use discrete latent variables and glancing training and different hyperparameters for latent vocabulary. ",
    "strengths": "A strong empirical improvement over GLAT and Latent Transformer baselines while not reducing speed much.\nmix-GLT outperformed the baselines even without KD from AT.\nComprehensive analysis of the impact of the different combinations of latent variables and glancing training. ",
    "weaknesses": "Limited novelty. Both building blocks of the mix-GLT: GLAT and LT are from previous papers.\nEven after reading Sec. 2, the proposed method in Sec. 3 would be hard to follow for those who have not read GLAT and LT papers. Since they are two core backgrounds of the paper, they should be explained in more detail.\nIs it possible to improve the performance of NAT/GLAT baselines with a simpler technique (ex. increasing model parameter) to have similar latency with mix-GLT? That would give a more direct comparison between models.\nDoes the delta between w/ and w/o AT necessarily indicate the sentence level multi-modality understanding? ( Table 3) Would there be a better way/metric/visualization to show?\nWhy is the decoder called 'mix-' decoder? I originally expected it to be a mixture model.\nDoes using z_obs  / z during training make difference? ( line 251) What does GSZ mean in Eq. 11 (line 254)?\nmaybe typo (line 171) '...predicting latent variables unavoidable sacrifice the overall decoding efficiency.'\nThe use of length prediction needs to be cited - Marjan Ghazvininejad, Omer Levy, Yinhan Liu, and Luke Zettlemoyer. 2019. Mask-predict: Parallel decoding of conditional masked language models. In EMNLP ",
    "comments": "See weakness ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]