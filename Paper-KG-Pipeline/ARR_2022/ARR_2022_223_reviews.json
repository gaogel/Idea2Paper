[
  {
    "review_id": "cbcbf4f35b926e4e",
    "paper_id": "ARR_2022_223",
    "reviewer": null,
    "paper_summary": "The paper investigates the transferability of \"soft prompts\" across tasks and models. It shows that initializing with prompts from different tasks is helpful in terms of both quality and convergence speed. For transferring prompts across models, the results are mainly negative, showing that cross-model transfer is still a hard problem. Finally, the paper shows that similarity of induced neural activation is a better measure of prompt similarity than simpler embedding-based metrics. ",
    "strengths": "This is a well-structured and useful study of prompt transferability. It adds to recent findings from Vu et al. 2021 that soft prompts can effectively be transferred between related tasks. There are several novel contributions, including (1) that prompt transfer can speed up prompt tuning, and (2) that induced neural activations are a strong measure of prompt similarity. ",
    "weaknesses": "The methodology for cross-model transfer (section 5) was less clear, and seemed less well-motivated than the rest of the paper. If I'm understanding correctly, for Distance Minimizing, the projector is trained on a single datapoint (??), which seems unlikely to work well, and the Task Tuning approach doesn't seem preferable to just training a prompt on the target model directly. Regardless, covering more details around training and evaluation would be useful. This could include for training: # of training datapoints, batch size, steps, and for evaluation: specifying whether reported numbers are on dev or test sets.\nMore direct comparison to Vu et al. 2021 would be useful, as the goals and methods are very similar. One point of comparison could be pointing out that overlap of neuron activation works better than Vu et al.'s embedding-based similarity. ",
    "comments": "=== General comments === Is cross-model prompt transfer something that would be useful in practice? If so, it would be helpful to add some motivation for why this is an interesting problem to solve.\nLester et al. 2021 found the original T5 checkpoints didn't work well for prompt tuning, and used LM-adapted checkpoints instead. Is this work using the original checkpoints? If so, it might be interesting to comment on why they might be working well here but not there. A related question: Is the evaluation of T5 for classification tasks doing unconstrained generation, or is it a \"ranking\" approach, just scoring the likelihood of the legal labels?\nVu et al. 2021 is mentioned as \"concurrent\", but was online 3 months before the ARR Jan deadline, so should be considered \"recent\", not concurrent. See the Citation section here: https://www.aclweb.org/adminwiki/index.php?title=ACL_Policies_for_Submission,_Review_and_Citation Vu et al. 2021 seems very similar: (1) investigates prompt transfer through initialization, (2) tests different transferability indicators, (3) discusses building a \"prompt warehouse\" for retrieving related tasks and improving prompt tuning. It would be useful to include a bit more description of their approach and the differences/similarities with this work.\n=== Specific comments === p.2 -- It's not clear what TPT stands for when the term is introduced.\np.2,3 -- In introducing \"soft prompts\" (p.2) and prompt tuning (section 3.1), the Li and Liang 2021 paper is presented as just tuning input embedding vectors. However Li and Liang's core results are using \"prefix tuning\" which overwrites activations in intermediate layers as well. This should be clarified. Lester et al. 2021 may be a better reference for tuning prompts that only affect the input layer.\np.3 \"In this work, we show that prompt transfer can remedy, improve the effectiveness to some extent with knowledge transfer, and empirically analyze the transferability of prompts across tasks and PLMs.\" -- Grammar seems off here.\np.4 I'm confused why Fig 3b laptop => laptop is 99. Shouldn't the diagonal be 100 by definition? If not, it would be useful to explain why not.\np.4 \"Considering RoBERTa can only predict a single token\" -- I'm not understanding why RoBERTa can't be used for extractive QA tasks. Isn't this typically done by adding a QA head on top of BERT type models? Would that not work here? If not, it would be useful to explain why not.\np.5 \"a larger and heterogeneous target PLM\" -- I'm not sure what heterogeneous means here.\np.5 \"In the experiments, we select laptop and MNLI for the projector learning.\" -- Could you say why these two tasks were selected?\np.5 The description of Distance Minimizing and Task Tuning could be clearer. From Table 2, it looks like these methods are only training the projector on one task at a time? For Distance Minimizing, does this mean you're training a perceptron on a single datapoint? Does the loss go to 0 in this case? I'm surprised the laptop=>laptop and MNLI=>MNLI transfer scores aren't 100%. Could you say more about how many datapoints you're training on and how many steps of optimization you do?\np.6 For Task Tuning, training the projector directly on the training task examples seems very similar to prompt tuning itself. Is there any added value to training the projector weights as opposed to just optimizing the projector's output (the prompt) directly?   p.6 Are C_concat and C_average the same prompt similarity metrics used in Vu et al. 2021? If so it would be useful to mention this.\np.8 Is there a reason why T5 XL size is left out of Figure 4? ",
    "overall_score": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "a6f82cf1b57c8583",
    "paper_id": "ARR_2022_223",
    "reviewer": null,
    "paper_summary": "Motivated by empirical findings that training models with Prompt Tuning can achieve the same performance as fully fine-tuning a model, but the training takes much longer to reach the same performance, they explore ways to exploit knowledge from already trained prompts.\nThey explore using already trained prompts to transfer knowledge between tasks (using the same frozen model) and also the transfer of prompt between _different_ frozen models. For between task transfer, they either directly re-use a prompt from a source task on the target task or they use the prompt learned from the source task as the initialization point for the target task.\nFor between model transfer, they uses these same methods but include a learnable `projector` (a small, 2 layer neural-network) that maps the prompts from one frozen model to another be using the projected prompt in one of the methods mentioned above. They have two methods for learning this `projector`. In the first method, which they call _Distance Minimization_, they minimize the $L_2$ distance between a projected source prompt (trained on the source frozen model) and a target prompt (a prompt trained on the same task using the target model). In the second method (_Task Tuning_) they learn the `projector` via backpropagation. In this case they take a prompt trained on a source task $P_s$, project it ($Proj(P_s)$) and then use that when prompt tuning the target model. Gradient updates are only applied to the projector.\nThey also look at several methods of prompt similarity and use them to predict prompt transferability. They main methods are Cosine and Euclidean distances between prompt tokens and their novel model activation similarity where prompts are fed into frozen models and the activations of the feed-forward layers are recorded. The call this method _ON_. ### Results Their first results look at the performance of directly re-using a prompt trained on a source task for a downstream task. They find that this can produce strong performance (measured in relative performance, the direct source to target prompt transfer performance divided by the performance researched from directly training on the target task) within clusters of similar tasks.\nTheir second results look at the performance of using a prompt learned on a source task to initialize the prompt for a target task and then doing Prompt Tuning. They find that this method can give consistent gains in terms of task performance as well as speed of convergence.\nTheir third set results examine transfer across models. They find that direct re-use of a prompt projected by the `projector` learned via the _Distance Minimization_ method results in poor performance, especially within the Sentiment tasks. They find that direct reuse of a prompt projected by a `projector` learned with their _Task Tuning_ method does better especially when the tasks are within the same cluster. They also look at how using a _Task Tuning_ prompt to initialize training of a new prompt performs and finds that it can lead to some improvements in task performance and small improvements in convergence speed.\nTheir final set of results examine use prompt similarity methods to predict prompt transferablity (in the context of direct prompt reuse). They find that all methods are able to distinguish between multiple prompts (created by training with different random seeds) trained for the same task from prompts trained for other tasks. They also find that _ON_ produces a ranking of similar prompts that best correlate with direct reuse performance (using Spearman's rank correlation scores). They also find that the correlation decreases as the size of the frozen model grows. ",
    "strengths": "The strengths of the paper include:  * Experiments on many different and diverse datasets, 17 with a good mixture of sentiment, NLI, EJ, Paraphrase detection, and Question answers. \n  * Experiments across many model sizes and architectures, including encoder-only models like RoBERTa instead of just the encoder-decoder and decoder-only models we see else where. \n  * The inclusion of small motivating experiments like the convergence speed are a great way to establish the importance of the work and the impact it would have. \n   * The use of the same methods (direct reuse of prompts and using prompts as initialization) in different settings (cross task transfer with the same model and cross model transfer with the same task) and similar results in each demonstrate the robustness of the method. \n   * Not only does their novel prompt similarity method (_ON_ based on model activations when processing the prompt) work great at predicting direct use similarity, it also captures the non-linear way the model interacts with the prompt in a way that simple methods like token similarity can. ",
    "weaknesses": "The majority of the weaknesses in the paper seem to stem from confusion and inconsistencies between some of the prose and the results.\n1. Figure 2, as it is, isn't totally convincing there is a gap in convergence times. The x-axis of the graph is time, when it would have been more convincing using steps. Without an efficient, factored attention for prompting implementation a la [He et al. (2022)](https://arxiv.org/abs/2110.04366) prompt tuning can cause slow downs from the increased sequence length. With time on the x-axis it is unclear if prompt tuning requires more steps or if each step just takes more time. Similarly, this work uses $0.001$ for the learning rate. This is a lot  smaller than the suggested learning rate of $0.3$ in [Lester et al (2021)](https://aclanthology.org/2021.emnlp-main.243/), it would have been better to see if a larger learning rate would have closed this gap. Finally, this gap with finetuning is used as a motivating examples but the faster convergence times of things like their initialization strategy is never compared to finetuning. \n2. Confusion around output space and label extraction. In the prose (and Appendix A.3) it is stated that labels are based on the predictions at `[MASK]` for RoBERTa Models and the T5 Decoder for generation. Scores in the paper, for example the random vector baseline for T5 in Table 2 suggest that the output space is restricted to only valid labels as a random vector of T5 generally produces nothing. Using this rank classification approach should be stated plainly as direct prompt reuse is unlikely to work for actual T5 generation. \n3. The `laptop` and `restaurant` datasets don't seem to match their descriptions in the appendix. It is stated that they have 3 labels but their random vector performance is about 20% suggesting they actually have 5 labels? \n4. Some relative performance numbers in Figure 3 are really surprising, things like $1$ for `MRPC` to `resturant` transfer seem far too low, `laptop` source to `laptop` target on T5 doesn't get 100, Are there errors in the figure or is where something going wrong with the datasets or implementation? \n5. Prompt similarities are evaluated based on correlation with zero-shot performance for direct prompt transfer. Given that very few direct prompt transfers yield gain in performance, what is actually important  when it comes to prompt transferability is how well the prompt works as an initialization and does that boost performance. Prompt similarity tracking zero-shot performance will be a good metric if that is in turn correlated with transfer performance.  The numbers from Table 1 generally support that this as a good proxy method as 76% of datasets show small improvements when using the best zero-shot performing prompt as initialization when using T5 (although only 54% of datasets show improvement for RoBERTa). However Table 2 suggests that this zero-shot performance isn't well correlated with transfer performance. In only 38% of datasets does the best zero-shot prompt match the best prompt to use for transfer (And of these 5 successes 3 of them are based on using MNLI, a dataset well known for giving strong transfer results [(Phang et al., 2017)](https://arxiv.org/abs/1811.01088)). Given that zero-shot performance doesn't seem to be correlated with transfer performance (and that zero-shot transfer is relatively easy to compute) it seems like _ON_'s strong correlation would not be very useful in practice. \n6. While recent enough that it is totally fair to call [Vu et al., (2021)](https://arxiv.org/abs/2110.07904) concurrent work, given the similarity of several approaches there should be a deeper discussion comparing the two works. Both the prompt transfer via initialization and the prompt similarity as a proxy for transferability are present in that work. Given the numerous differences (Vu et al transfer mostly focuses on large mixtures transferring to tasks and performance while this work focuses on task to task transfer with an eye towards speed. _ ON_ as an improvement over the Cosine similarities which are also present in Vu et al) it seems this section should be expanded considering how much overlap there is. \n7. The majority of Model  transfer results seem difficult to leverage. Compared to cross-task transfer, the gains are minimal and the convergence speed ups are small. Coupled with the extra time it takes to train the projector for _Task Tuning_ (which back propagation with the target model) it seems hard to imagine situations where this method is worth doing (that knowledge is useful). Similarly, the claim on line 109 that model transfer can significantly accelerate prompt tuning seems lie an over-claim. \n8. Line 118 claims `embedding distances of prompts do not well indicate prompt transferability` but Table 4 shows that C$_{\\text{average}}$ is not far behind _ON_. This claim seems over-reaching and should instead be something like \"our novel method of measuring prompt similarity via model activations is better correlated with transfer performance than embedding distance based measures\" ",
    "comments": "1. Line 038: They state that GPT-3 showed extremely large LM can give remarkable improvements. I think it would be correct to have one of their later citations on continually developed LM as the one that showed that. GPT-3 mostly showed promise for Few-Shot evaluation, not that it get really good performance on downstream tasks. \n2. Line 148: I think it would make sense to make a distinction between hard prompt work updates the frozen model (Schick and Sch√ºtez, etc) from ones that don't. \n3. Line 153: I think it makes sense to include [_Learning How to Ask: Querying LMs with Mixtures of Soft Prompts_ (Qin and Eisner, 2021)](https://aclanthology.org/2021.naacl-main.410.pdf) in the citation list for work on soft prompts. \n4. Figure 3: The coloring of the PI group makes the text very hard to read in Black and White. \n5. Table 1: Including the fact that the prompt used for initialization is the one that performed best in direct transfer in the caption as well as the prose would make the table more self contained. \n6. Table 2: Mentioning that the prompt used as cross model initialization is from _Task Tuning_ in the caption would make the table more self contained. \n7. Line 512: It is mentioned that _ON_ has a drop when applied to T$5_{\\text{XXL}}$ and it is suggested this has to do with redundancy as the models grow. I think this section could be improved by highlighting that the Cosine based metrics have a similar drop (suggesting this is a fact of the model rather than the fault of the _ON_ method). Similarly, Figure 4 shows the dropping correlation as the model grows. Pointing out the that the _ON_ correlation for RoBERTA$_{\\text{large}}$ would fit the tend of correlation vs model size (being between T5 Base and T5 Large) also strengths the argument but showing it isn't an artifact of _ON_ working poorly on encoder-decoder models. I think this section should also be reordered to show that this drop is correlated with model size. Then the section can be ended with hypothesizing and limited exploration of model redundancy. \n8. Figure 6. It would have been interesting to see how the unified label space worked for T5 rather than RoBERTAa as the generative nature of T5's decoding is probably more vulnerable to issue stemming from different labels. \n9. _ ON_ could be pushed farther. An advantage of prompt tuning is that the prompt is transformed by the models attention based on the value of the prompt. Without having an input to the model, the prompts activations are most likely dissimilar to the kind of activations one would expect when actually using the prompt. \n10. Line 074: This sentence is confusing. Perhaps something like \"Thus\" over \"Hence only\"? \n11. Line 165: Remove \"remedy,\" ",
    "overall_score": "3.5 ",
    "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work."
  }
]