[
  {
    "review_id": "3c700b87b28663e2",
    "paper_id": "ARR_2022_125",
    "reviewer": "Zi Lin",
    "paper_summary": "In biaffine graph-based architecture for semantic dependency parsing, arcs are predicted independently from each other. This paper proposed auxiliary tasks that introduce some form of interdependence of arc decisions. The experimental results on English and French show that training recurrent word-token representations both for the SDP tasks and auxiliary tasks are systematically beneficial. ",
    "strengths": "- The motivation of adding the auxiliary tasks is clear to me.\n- Most part of the paper is easy to follow. ",
    "weaknesses": "- It seems that how to choose the set of auxiliary tasks for French and English are based on empirical results. An analysis section is highly recommended to validate why some set of auxiliary can work better than others.\n- It take some while for me to understand the auxiliary tasks, so I suggest adding a SDP graph examples and have the description of auxiliary tasks based on the example.\n- If the results of FG and HC are not fully comparable to the proposed methods, comparing the numbers in table 3 is not that informative. It is hard for me to conclude that the proposed methods is strong enough to compete with others. I suggest doing the experiment in the same settings as FG and HC, or using the proposed auxiliary tasks with their system and show the difference in section 4.4. ",
    "comments": "Please see summary of weaknesses. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "70873263db44d699",
    "paper_id": "ARR_2022_125",
    "reviewer": null,
    "paper_summary": "This paper proposed the use of a set of auxiliary tasks to capture some form of interdependency between arcs in semantic dependency parsing in order to provide a simpler implementation that can obtain robust performance, which achieved near-SOTA and systematic performance on one English SemEval-2015 dataset and one French deep syntactic cyclic graphs. ",
    "strengths": "For a short paper, this paper is well organized and written. The main research question is motivated by efficiently and clearly discussing pros and cons of the previous work and how the current work is related and differs from them. The overall flow of the presentation of the motivations and arguments is smooth, making it easy to follow. The description of the experiments and the discussion of the results are clear and succinct. The use of auxiliary tasks to capture some form of interdependence between arcs for semantic dependency parsing while remaining a reasonable complexity and competitive results seems a contribution to me. ",
    "weaknesses": "1. A more explicit definition / description / explanation of what is meant by “systematic” performance gains or improvements seems to be necessary since this is one of the proposed advantages of the proposed method, and it would also be better if you could explicitly describe the connection between \"systematic performance gain\" and the robustness of model performance. The aforementioned two aspects could make the contribution of the proposed method more obvious and salient.   2. A brief discussion of the motivation and selection of the auxiliary tasks seem needed in Section 3. Some of the issues were described in the first paragraph of Section 3, which could be viewed as implicitly motivating the use of the auxiliary tasks, but it still seems not very sufficient: the jump from problem description to “Hence the idea of using auxiliary tasks taking into account all the heads'' seems abrupt. I think a brief elaboration on why these 4 tasks are designed and chosen would make the picture clearer and would also echo the argument mentioned at the beginning, which is to capture some form of interdependence between arcs. ",
    "comments": "Content: 1. Line 027: what did you mean by “plain dependency parsing”? Syntactic dependency? It would be clearer if a reference is added here to provide an example.  2. Line 047: it would be better and more explicit if the task and/or dataset is mentioned as part of the “achieve state-of-the-art results” statement.  3. Lines 078-079 / Line 08: For clarity, it would be better if the evaluation metric is mentioned here to better understand the scale of the improvement; this would also be helpful to understand the results reported in this paper for comparability: the expression “labelled F-measure scores (LF1) (including ROOT arcs)” was used in Fernández-González and Gómez-Rodríguez (2020).  4. Line 079: Since ID and OOD are not widely used acronyms and this is the first time of them being used, it would be better to define them first and use ID or OOD thereafter: e.g. in-domain (ID) and out-of-domain (OOD)  5. Lines 130-135: The example wrt the mwe label can probably benefit from a visualization of the description, if the space is allowed. Also, the clause “which never happens in the training set” may benefit from a little rephrasing - if I’m not mistaken, this example was mentioned here to support the claim that “impossible sets of labels for the set of heads of a given dependent”. Does this mean that such a combination is incorrect and thus not possible? If so, saying that such scenarios never happen in the training set could mean that it is likely to conceptually have such combinations exist, but it’s just that it’s not seen in the training data.   6. Is there a specific reason for choosing 9 as the number of runs? ( Conventionally, 3, 5, or 10 are common).  Typos: - Line 018: near-soa > did you mean “near-state-of-the-art (sota)”? Define before using an acronym - Line 036: decision > decisions  - Line 062: propose > proposes   - Line 097: biafine > biaffine  - footnote2: a dot (.) is missing after “g” in “e.g” - only pointed this out as a typo as you used “e.g.” throughout the paper as in Lines 157 / 225 / 230 / 231  - Line 202: experimented > experiment - only pointed this out as a typo as you seem to use present tense to describe other people’s work as in Lines 047 / 078 / 081 - Line 250: provide > provides/provided; significative > significant (also caption in Table 1) Miscellaneous: 1. Inconsistent use of uppercase / lowercase in the title: tasks > Task; boost > Boost 2. Inconsistent intext citation styles:     - Lines 077-078 & 277-278: (Fernández-González and Gómez-Rodríguez, 2020) > Fernández-González and Gómez-Rodríguez (2020)     - Lines 276-277: (He and Choi, 2020) > He and Choi (2020) 3. Placement of footnotes: the footnotes follow the ending punctuation, as shown in the *ACL Template Section 4.1: Lines 104 / 166 / 170 / 231 / 239 / 249 / 250 / 266.  4. FYI - the link to the code in footnote 5 didn’t seem to work; it said “Transfer expired: Sorry, this transfer has expired and is not available any more”. Not sure if I need to register an account to get to the code.   5. Add the abbreviation “LF” after “labeled Fscore” on Line 245 so that the use of “LF” later can be attributed to. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "fa94bed3bbc96280",
    "paper_id": "ARR_2022_125",
    "reviewer": null,
    "paper_summary": "This paper proposes to use multi-task learning to improve semantic dependency parsing. Based on the baseline biaffine dependency parser (Dozat and Manning, 2017), four auxiliary tasks are proposed: H (predicting the number of governors for each token), D (predicting the number of dependents for each token), S (predicting all the arc labels for each token in a classification manner), and B (predicting the number of incoming arcs with each label for each token). They evaluate the best configurations of multi-task learning (combinations of auxiliary tasks and final prediction flows) on one French and three English datasets and find that their methods outperform the vanilla biaffine parser.\nOverall, the paper is clear to understand and seems to perform solid empirical experiments. However, the proposed method seems too trivial and underperforms the state-of-the-art parser on the English datasets by a large margin. More in-depth analyses are also needed. ",
    "strengths": "- Solid empirical experiments: Four datasets are used. Each result is averaged from 9 runs, and significance tests are performed.\n- Clear writing: While some typos exist and the overall writing can be improved, the paper is very clear to understand. ",
    "weaknesses": "- Trivial method: These all four auxiliary tasks seem too trivial to achieve the objective of improving arc consistency prediction since all these tasks are still performed on each token independently.\n- Lack of in-depth analysis: Using the B+H auxiliary tasks designed to predict the number of governors and dependents for each token only improves the prediction of the number of heads by 0.6 points. This improvement seems pretty trivial. It would be better if the authors could provide more thorough analyses of why these auxiliary tasks are helpful.\n- Underperform SOTA by a large margin: The methods underperform the SOTA parsers by a large margin because no POS or lemma information is used, and the authors claim their methods are directly usable with their system. The paper would be much more solid if the authors could verify this hypothesis. ",
    "comments": "- Line 079: Explain ID and OOD.\n- Line 130: Explain DM dataset.\n- Line 138-144: Why the accuracy of predicting the number of heads is related to the observation that zero-head tokens are predicted too frequently.  - Footnote 1: Why use the representation of the first subword of a word? Did you try using the average of all the subwords?\n- Code link seems to be expired. ",
    "overall_score": "2.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]