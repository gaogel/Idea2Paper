[
  {
    "review_id": "090711ce39f91942",
    "paper_id": "ARR_2022_236",
    "reviewer": null,
    "paper_summary": "This paper works on multimodal misinformation detection, particularly in the context of \"miscaptioned\" images on Twitter. The authors collect a large multimodal data of 884k tweets on the topics of Climate Change, COVID-19, and Military Vehicles. Next, a misinformation detection method, based on CLIP embeddings is proposed. The effectiveness of this approach is demonstrated on (a) a simulated dataset mismatched image captions and (b) real-world data of misinformation. ",
    "strengths": "- The paper is very well written and structured.\n- The collected dataset could be useful for multimodal research.\n- The authors demonstrate strong gains over the baseline approach of detection using zero-shot CLIP.\n- Experiments are mostly rigorous (for a short paper). ",
    "weaknesses": "- My main criticism is that the \"mismatched\" image caption dataset is artificial and may not capture the kind of misinformation that is posted on platforms like Twitter. For instance, someone posting a fake image of a lockdown at a particular place may not just be about a mismatch between the image and the caption, but may rather require fact-checking, etc. Moreover, the in-the-wild datasets on which a complementary evaluation is conducted are also more about mismatched image-caption and not the real misinformation (lines 142-143). Therefore, the extent to which this dataset can be used for misinformation detection is limited. I would have liked to see this distinction between misinformation and mismatched image captions being clear in the paper.\n- Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected \"pristine\" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.   - Since this is a new dataset, I would have liked to see evaluation of more models (other than just CLIP). But given that it is only a short paper, it is probably not critical (the paper makes enough contributions otherwise) ",
    "comments": "- Table 4 and Table 5: Are the differences statistically significant? ( especially important because the hNews and Twitter datasets are really small) - Lines 229-240: Are the differences between the topics statistically significant? ",
    "overall_score": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "e32028fea0e2a5b6",
    "paper_id": "ARR_2022_236",
    "reviewer": "Florian Kunneman",
    "paper_summary": "The paper addresses a sub-topic in the realm of mis/disinformation, namely images posted on social media with a misleading caption. With a focus on the topics of climate change, covid-19 and military vehicles, a dataset is collected from twitter using an iterative process to come to the best set of query terms, and selecting posts that contain at least one image. Half of the data is then articifially made out-of-context, by both randomly connecting captions to images, and doing so in a more informed way by selecting on semantic similarity. Two existing datasets consisting of images and deceptive captions are then used for evaluation, and the zero-shot CLIP model is used as baseline on the task. The new dataset is used to finetune this baseline model, comparing different multi-modal fusion techniques. This approach was found to outperform the baseline model, and several insights were offered by inspecting the influence of text clustering and image OCR'ing. ",
    "strengths": "- Clear and proper data collection procedure - The use of multiple evaluation sets - The insight that multiplying the image- and textual CLIP embeddings is beneficial - Analysis of the percentage of hard and random negatives and their impact - Insights into the value of Optical Character Recognition on performance - Insights into the value of tweet text clustering ",
    "weaknesses": "- Little insight into the actual problem domain. How does the new dataset and its labels compare to actual cases of out-of-context captions?\n- Lack of a manual inspection of genuine deceptive captions in the dataset, neither of the random or hard samples that were generated nor of model output - Application of a single model with and without fine-tuning ",
    "comments": "- The introduction could benefit from a clear example of the problem domain - 'falsified' is an ambiguous term to use, as it also refers to the research practice where one tries to falsify a hypothesis.\n- Line 111 - 113: 'by retrieving the image of the sample with the greatest textual similarity for a given caption' - it is not clear to me based on this description how these hard negatives were generated ",
    "overall_score": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "026afc14b184ea20",
    "paper_id": "ARR_2022_236",
    "reviewer": null,
    "paper_summary": "The paper deals with misinformation detection in Twitter, using multimodal data (images and text). \nAuthors focus on three topics: COVID, climate change, and military vehicles. ",
    "strengths": "Authors perform experiements on multiple settings, using several quality metrics. \nThe results show a superiority of the proposed method. \nPartial ablation study is also reported. ",
    "weaknesses": "The paper is not very clearly writtem. Too much information is moved to Appendix. \nSome important details of data collection and data generation are omitted. \nThe main issue is a lack of novelty - authors use exisiting tools and approaches for tweets classification. ",
    "comments": "I did not notice many typos. However, the paper organization can be improved. \nFor example, data generation and data collection can be described in the same section. \nAlso, motivation behind some experiments is not very clear. Interpretation of the results are offen missing. \nBelow are some detailed comments/questions:  1. What \"in-the-wild\" term means? \n2. Evaluation metrics can be explained in a few words (or a relevant reference can be provided). \n3. I did not understand why Multiply option was selected for the experiments, when Concat+Dot clearly outperforms it in 4 cases, as shown in Table 3. \n4. Conclusions from Table 4 are missing. \n5. The motivation behind experiment with clustering and interpretation of its results are missing/unclear. ",
    "overall_score": "2 = Revisions Needed: This paper has some merit, but also significant flaws, and needs work before it would be of interest to the community.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]