[
  {
    "review_id": "a40d8ae07e3e76a7",
    "paper_id": "ARR_2022_231",
    "reviewer": null,
    "paper_summary": "This paper presents a knowledge distillation method based on MAML. The paper's main aim is to address two drawbacks of existing knowledge distillation methods: (1) teacher being unaware of the capacity of the student, and (2) teacher not optimised for distillation.\nThe paper assumes the teacher network is trainable and proposes a simple technique that, in the MAML framework, applies \"pilot updates\" in the inner loop to make the teacher and student networks more compatible. Pilot updates are essentially MAML's original inner loop, but applied twice, the first time to update the teacher and the second time to update the student.\nExperiments are performed on distilling BERT and evaluated on a number of tasks/datasets in the GLUE benchmark. The proposed method consistently outperforms the compared baselines on almost all tasks.\nOn the CV side, the proposed method is evaluated on distilling ResNet and VGG. The method achieves comparable performance with SOTA model CRD. ",
    "strengths": "- Addresses knowledge distillation, an important technique with wide application in machine learning, natural language processing and computer vision.\n- The proposed method achieves SOTA performance on a large number of tasks in NLP, in distilling BERT.\n- The presentation is clear, logical and easy to follow.\n- A comparative analysis on the tradeoff between model performance and computational costs. ",
    "weaknesses": "- The proposed method is quite simple. Its essence is the \"pilot update\" mechanism, which basically applies the inner loop of MAML twice, once to update the teacher, and once to update the student.\n- The achieved performance boost is moderate.\n- The comparative analysis in Table 2 shows that, with significantly more computational costs than the two compared baselines (PKD and ProKT), the model achieves a modest performance gain of about 0.5 absolute F1 points. This analysis, though very welcome, demonstrates that the proposed method suffers a large computational penalty for a small accuracy/F1 gain. ",
    "comments": "This paper is a resubmission from ARR August, and it has not been significantly improved. The only major addition is Table 2 that describes the tradeoff between computational cost and model performance. Therefore, the majority of reviewers' comments from ARR August still holds (I was one of the original reviewers). ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]