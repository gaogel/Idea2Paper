[
  {
    "review_id": "c1af730b370c44b1",
    "paper_id": "ARR_2022_56",
    "reviewer": "Sheng Shen",
    "paper_summary": "This paper proposes a contrastive learning framework to learn phrase representations in an unsupervised way. Cluster-assisted contrastive learning is proposed to reduce noisy in-batch negatives by selecting negatives from clusters. Extensive experiments on entity clustering and topical phrase mining show the effectiveness of the proposed methods. Case studies are also provided that demonstrate coherent and diverse topical phrases can be founded by UCTopic without supervision. ",
    "strengths": "- The paper is well-written and clearly presented.  - The proposed cluster-assisted contrastive learning objective is well-motivated and effective when finetuning the encoder on the target task further. Extensive experimental results are provided to show the significance of the proposed UCTopic versus baseline methods.  - Detailed discussion is also provided for different datasets to further analyze the effectiveness of the proposed method with respect to informativeness, diversity of the constructed phrases, and the source of phrase semantics. ",
    "weaknesses": "- The details of how to apply K-Means methods to obtain the pseudo labels when using the CCL and how the number of clusters will affect the final performance is missing. ",
    "comments": "- Given that sentence length might affect in Table 1, additional statistics of the pre-trained sentence length versus the might be good to provide.  - Could the author provide a more detailed description of how to construct the pre-training phrases and how many pre-training phrases are present and how these phrases overlapped with the downstream tasks? ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "264cb031349df77a",
    "paper_id": "ARR_2022_56",
    "reviewer": null,
    "paper_summary": "The paper proposes a contrastive learning-based method for phrase representations and topic mining. Results show that the method achieves the best performance on topic mining and phrase representations. The proposed model can extract more diverse phrases. ",
    "strengths": "1. The paper applies unsupervised contrastive learning to topic modeling, which is suitable for the unsupervised task. Results show that the proposed model can extract more diverse phrases. \n2. The authors also find that in the finetuning process, in-batch negative samples have a bad influence on the performance. So they propose a topic-assist contrastive learning method to reduce noises and turn the original finetuning process into a topic-specific finetuning process. \n3. Experiemnts results show that the proposed model achieves good performance on several datasets. ",
    "weaknesses": "1. It seems that the biggest contribution of the paper is to apply contrastive learning to topic modeling, which is of limited novelty. \n2. Batch is a sampling method, so what is the major difference between batch and the proposed one? This limits the performance significantly. ",
    "comments": "Suggestions: 1. It is a little bit confusing in the assumptions in section 1. First of all, “The phrase semantics are determined by their context.”. As for the examples in Figure 1, the semantic of phrase “ United States” is fixed and not influenced by the context. I think the writers want to express: if we mask “United States”, we can still infer the mask phrase by its context. \n2. Writing should be strengthened. ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]