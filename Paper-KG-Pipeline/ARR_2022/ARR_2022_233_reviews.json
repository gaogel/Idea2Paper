[
  {
    "review_id": "27ade04e48adbdaf",
    "paper_id": "ARR_2022_233",
    "reviewer": null,
    "paper_summary": "The paper proposes a new benchmark for the task of analogical reasoning, framing it as two tasks over a new corpus. The tasks consist of a multiple-choice question answering and the generation of explanations related to the query and each candidate answer. In the experimental part, the benchmark is used to test a set of neural language models, whose performances are not satisfactory and highlight the difficulty of this benchmark.\nThe paper is well written and easy to understand. The background section provides all the necessary material to understand and contextualize the task. \nThe task itself is very interesting, well-motivated, and offers a new difficult challenge to the NLP community. \nThe experimental part and the analysis of the result are satisfactory.\nThe dataset creation process seems to have some weak points but it is still acceptable if some additional details are provided. It is not clear whether the authors plan to release this dataset publicly or whether it will remain private. ",
    "strengths": "The task is well designed and motivated. It provides a difficult challenge that sets a high bar for current models and that will be very useful to evaluate future NLP-based reasoning systems. ",
    "weaknesses": "Additional details regarding the creation of the dataset would be helpful to solve some doubts regarding its robustness. It is not stated whether the dataset will be publicly released. ",
    "comments": "1) Additional reference regarding explainable NLP Datasets: \"Detecting and explaining unfairness in consumer contracts through memory networks\" (Ruggeri et al 2021) 2) Some aspects of the creation of the dataset are unclear and the authors must address them.  First of all, will the author release the dataset or will it remain private? \nAre the guidelines used to train the annotators publicly available? \nHaving a single person responsible for the check at the end of the first round may introduce biases. A better practice would be to have more than one checker for each problem, at least on a subset of the corpus, to measure the agreement between them and, in case of need, adjust the guidelines. \nIt is not clear how many problems are examined during the second round and the agreement between the authors is not reported. \nIt is not clear what is meant by \"accuracy\" during the annotation stages.\n3) Additional metrics that may be used to evaluate text generation: METEOR (http://dx.doi.org/10.3115/v1/W14-3348), SIM(ile) (http://dx.doi.org/10.18653/v1/P19-1427).\n4) Why have the authors decided to use the colon symbol rather than a more original and less common symbol? Since the colon has usually a different meaning in natural language, do they think it may have an impact?\n5) How much are these problems language-dependent? Meaning, if these problems were perfectly translated into another language, would they remain valid? What about the R4 category? Additional comments about these aspects would be beneficial for future works, cross-lingual transfers, and multi-lingual settings.\n6) In Table 3, it is not clear whether the line with +epsilon refers to the human performance when the gold explanation is available or to the roberta performance when the golden explanation is available? \nIn any case, both of these two settings would be interesting to know, so I suggest, if it is possible, to include them in the comparison if it is possible.\n7) The explanation that must be generated for the query, the correct answer, and the incorrect answers could be slightly different. Indeed, if I am not making a mistake, the explanation for the incorrect answer must highlight the differences w.r.t. the query, while the explanation for the answer must highlight the similarity. It would be interesting to analyze these three categories separately and see whether if there are differences in the models' performances. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]