[
  {
    "review_id": "a0821b6cbd2b3bfb",
    "paper_id": "ARR_2022_63",
    "reviewer": "Zhengyuan Liu",
    "paper_summary": "In this work, the authors proposed a unified model of task-oriented dialogue understanding and response generation. The two major enhancements are adopting task-oriented dialogue pre-training on a data collection, and introducing the prompt-based learning for the multi-task capability via one model. From the experimental results, the pre-training strategy proved useful to improve the performance on the benchmark MultiWOZ. ",
    "strengths": "While the idea of task-specific pre-training is not new, it is still interesting, and the proposed method proved effective in leveraging the language backbone T5, and can be potentially applied to other models and tasks. ",
    "weaknesses": "1. There are some other contemporary state-of-the-art models, the authors can consider citing and including them for an extensive comparison. \n2. It will be good to see some analysis and insights on different combinations of pre-training datasets introduced in Table 1. ",
    "comments": "Here are some questions: 1. Since some of the sub-tasks, like dialogue state tracking, require a fixed format of the output, if the model generation is incomplete or in an incorrect format, how can we tackle this issue? \n2. The dialogue multi-task pre-training introduced in this work is quite different from the original language modeling (LM) pre-training scheme of backbones like T5. Thus I was curious about why not pre-train the language backbone on the dialogue samples first with the LM scheme, then conduct the multi-task pre-training? Will this bring some further improvement? \n3. It will be good to see some results and analysis on the lengthy dialogue samples. For instance, will the performance drop on the lengthy dialogues? ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]