[
  {
    "review_id": "de80fb5dcb7742d5",
    "paper_id": "ARR_2022_50",
    "reviewer": null,
    "paper_summary": "The paper described a set of approaches from unsupervised to weakly supervised to segment sentences into words and morphemes under an extremely low-resource setting. The approaches are tested on two languages currently (still) being documented.\nIt would be very interesting to follow the developments, should a system such as the presented one be implemented and used in actual fieldwork. Two of the points I can think of are the feedback from fieldworkers as well as the (hopefully) incremental precision of the system. It would also be extremely interesting to test how much input is needed to reach acceptable levels of automated segmentation. ",
    "strengths": "The authors work in a low-resource setting, and their work is very valuable and could greatly help linguistic fieldworkers with (semi-)automatic segmentation of either recorded speech or transcripts.  The authors convincingly show that the chosen approach outperforms two not-so-simple baselines (SentencePiece and Morfessor).\nOverall, the language is mostly clear and the structure logical.\nIn response to one of the earlier reviews, the authors have added experiments with Morfessor, which is also highly laudable. ",
    "weaknesses": "Unfortunately, the paper is very difficult to follow in parts particularly because of the extensive use of abbreviations.\nRegarding word vs morpheme, what exactly is the conclusion reached? It is not entirely clear, yet it seems to be one of the main questions in the paper.\nThe final sentence in the paper claims that the proposed algorithm may help speed up annotation processes, and as such must run fast. That holds only true if the algorithm is run on-the-fly, and if human annotators are used to incrementally teach the algorithm. Otherwise the algorithm can be run independently of the annotators; one could pre-segment all data automatically and then present it to annotators for correction. ",
    "comments": "In line 165, you don't explain \"DP\", although it is recoverable through the context. ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "7da87fbc09ca4328",
    "paper_id": "ARR_2022_50",
    "reviewer": null,
    "paper_summary": "This paper takes advantage of data (utterances, annotations) that is produced as part of language documentation to use for automatic linguistic segmentation, mainly at the word level. The authors worked with two languages that are still being documented Mboshi and Japhug. Both languages have data available but at different levels of granularity. They used Bayesian non-parametric approaches with different variations of the n-gram models and data used in addition to an incremental data training approach. The evaluation is reported in terms of F1 on three different segmentation levels (morphological boundary level, token level, and type-level). The discussion points out that the weakly supervised performs better than fully supervised and of course better than unsupervised. On the other hand, morpheme-based segmentation benefits more from underserviced training. ",
    "strengths": "- The paper is well written and easy to follow.\n- The languages that were targeted are still being documented and they resemble a real low-resource setting.\n- The core approach is easy to follow and replicate. It lends itself to being easily explainable in terms of behavior and performance.\n- This effort can be helpful as an enabling technology for the language documentation process. ",
    "weaknesses": "There are no major weaknesses in this version of the paper. \nThe weaknesses mentioned in the previous review have been addressed in the current version for the most part. ",
    "comments": "I appreciate the authors taking the time to improve upon the paper. I believe this work would definitely facilitate the documentation process whether in progress or as a post-processing step. It would be really useful for a future version of this work to include a use case study on using this process and reporting how useful it was. This would definitely increase the value of this work among both NLP and language documentation communities. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "4b71ef5ed6c8af64",
    "paper_id": "ARR_2022_50",
    "reviewer": null,
    "paper_summary": "NOTE: This is my re-review of this submission. For that reason, this review will be much shorter and will focus on the resubmission with respect to my previous review. For my summary of the work, please see the initial review.\nThis version of the paper shows some small improvements over the previous submission, and my positive view of this paper remains positive. ",
    "strengths": "The strengths of this paper remain the same as in my previous review, with the addition of a comparison to Morfessor as an important baseline for unsupervised word segmentation. Thanks for adding this - I was happy to see the results. ",
    "weaknesses": "The authors have addressed 1.5 of the 2 weaknesses listed in my previous review. The remaining issue is perhaps one of personal preference, but I still feel some earlier discussion of the need for low-compute approaches to linguistic analysis would be appropriate.  Otherwise, thanks for addressing the concerns noted in the previous review. ",
    "comments": "Nothing to note here. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]