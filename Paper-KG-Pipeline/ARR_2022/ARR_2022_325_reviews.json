[
  {
    "review_id": "cff563c41391b46a",
    "paper_id": "ARR_2022_325",
    "reviewer": null,
    "paper_summary": "This paper presents a method for inducing unsupervised tags for word sequences, later to be compared against POS tags. The method combines a CRF autoencoder (CRF-AE) for the first step of inducing pseudo-labels, with a rule-based approach for the later step of regenerating the word sequence from these pseudotags. The system hinges on contextualized character-based ELMo representations and some algebraic modifications that are empirically proven to be useful. The system outperforms previous approaches to this problem on both the PTB dataset and a small subset of UD treebanks. ",
    "strengths": "The system outperforms existing ones on the task of unsupervised POS tag induction. ",
    "weaknesses": "This might be a bit unfair, but a serious concern of mine is the task itself. Barring any further information on the evaluation protocol, most notably on the *number of induced tags allowed for the system*, one can think of a trivial system that outperforms any of the ones presented by almost 10 points on PTB - just learn a tag for each vocabulary item. The authors offer no analysis of their system's actual outputs, of which \"many\" correlate with which \"one\" in the evaluation scheme, and although they use a second metric that contains a component that mitigates this problem, they leave this detail out and send the reader to the appendix to understand what the metric does (this is completely unacceptable. If you're using a metric, especially if you're arguing that it's better than the other one, at least explain the virtues of it along general lines in the main text).\nThe multilingual evaluation claims to be of \"the UD data\", a dataset of over 100 languages, yet the authors only select ten without explaining why. The availability of ELMo embeddings, in a resource tucked away in the appendix as well, is for about 45 languages and the intersection is significantly more than 10. ",
    "comments": "Section 3 (the Methods) is very difficult to follow. Almost every subsection selectively gets rid of part of the notation without warning, and inconsistencies abound. For example, in Figure 3, are h_i vectors or lists of vectors (with different layers) and if the latter, at what part are they aggregated? What's the input and output of the minus operand (it has two outgoing arrows?); Is W^s the same as W^{hourglass}? Is it a new parameter? Why is the (unnumbered) equation before line 427 referring to equation (8) as replacing it but taking different inputs (forward and backward h's as opposed to h's from different indices, which by the way may or may not be indicating layers or words, since the variable \"i\" is reserved for neither). Using vague terminology like \"purified representations\" (l.253) doesn't help this overall messiness, making the system very hard to understand.\nIn equation (12), there's a dependence on the vocabulary. How do OOV words get generated under this formulation?\nDo you fine-tune the ELMo parameters?\nOther stuff: - The last sentence of the first paragraph is unfinished.\n- l.64 an-->a - l.294 spareness-->sparseness - l.328 I'm pretty sure you meant \"supervised\" rather than \"unsupervised\", these are the standard splits for *all* WSJ tasks, not limited to dependency parsing either. ",
    "overall_score": "2.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]