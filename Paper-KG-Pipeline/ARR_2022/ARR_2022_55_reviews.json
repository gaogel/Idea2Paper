[
  {
    "review_id": "f5c1fe52015eca7b",
    "paper_id": "ARR_2022_55",
    "reviewer": null,
    "paper_summary": "(This is a resubmission that is re-assigned to me, so I'm pasting my previous comments here and append updates at the end for each section.)\nThis paper builds upon the existing round-trip-based definition of NMT adversarial examples (RTT, Zhang et al. 2021) and points out a problem with the approach -- that is, the semantic destruction that happened during round-trip translation might be solely introduced by the back-translation process. To alleviate this problem, the paper proposes an alternative definition -- Doubly Round-trip Translation (DRTT), that adds another round-trip translation process starting from the target-side, to ensure that the semantic destruction is not solely introduced by back-translation.\nSince this definition does not enforce source-side semantic agreement, the authors choose to build bilingual adversarial pairs (such that the target-side sentence will still have the same semantic as the perturbed source). To build such adversarial examples, the paper proposes a generation procedure that involves finding phrase pairs between parallel sentence pairs and conduct paired phrasal replacement with MLM and TLM.\nExperiments show that training NMT models on such adversarial examples as augmentation data (?) can improve the model's robustness on both artificially and naturally noisy test sets compared to several other existing adversarial example generation methods.\n=== UPDATE === The change introduced by the authors are as follows as far as I can tell: 1. moved around the notation explanation closer to where they are used so it's easier for the readers to follow. \n2. added a paragraph right before Section 4 to explain how the generated adversarial data is used. \n3. added an extra dataset created with CharSwap method (Michel et al. 2019) to test under robustness under meaning-preserving perturbations 4. case study moved to the appendix. ",
    "strengths": "+ The re-formulation of existing adversarial example generation is very insightful and will make a good reference for the community. It also made the proposal very well-motivated. \n+ Both the DRTT definition and the phrasal replacement algorithm are very clearly presented. ( However, the experiment section is much less so, see my first and second point for weakness.) \n+ The empirical improvements seem significant for both artificially and naturally noisy test sets. The baseline used for comparison seems strong and valid. ",
    "weaknesses": "+ My biggest concern with this paper is that there is a very significant transition gap between Section 3 and 4. Specifically, Section 3 is talking about a data generation method, while Section 4 starts with \"We evaluate our model under... \". It took a second pass through the earlier sections and some educated guess to conclude that the \"model\" is trained using adversarial data. \n+ Related to the above point, I don't think an adequate amount of experimental details are given for future work to reproduce the results in this paper at all. For example, if I understood correctly and the models are indeed trained on the adversarial data, what are the resulting training data size for the baseline and different augmentation methods? Are the original (non-adversarial) training data still used? Also, for results in Table 2, what is the system that is used to conduct back-translation? Is it the same system across different methods or not? \n+ The phrasal replacement procedure implicitly assume that the TLM to generate correct translation for the phrase so and still makes a valid translation pair. I hope to see some analysis on how strong is that assumption.\n=== UPDATE === The first and the second point has been properly addressed in the author's reply and the revision. As for the third point, the authors offered the case study and the improvement in robustness as a reply, which does prove part of it, but what I have in mind is something more specific like comparing the human evaluation score (as the translation of $x_{\\delta}$) of (1) $y_{\\delta}$; (2) $y_{\\delta}'$; (3) human translation of $x_\\delta$. Ideally, (1) and (3) should be very close while (2) should be much worse. This is obviously too much to ask for resubmission on such a short cycle, though.\nI do concur with the reviewer ag2n's concern that the paper \"is very parsimonious when it comes to citing previous work\". I don't think this is really improved in this version though. ",
    "comments": "All my suggestions in the previous version have been addressed. The only minor comment I'll make here is that the baseline in Table 3 has been updated from 35.11 to 35.02 in this version. I'm not sure what has led to the change. ",
    "overall_score": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]