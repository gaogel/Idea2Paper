[
  {
    "review_id": "3a14a02b9c496d9b",
    "paper_id": "ARR_2022_200",
    "reviewer": "Thanh-Tung Nguyen",
    "paper_summary": "This paper proposes an explicit future information utilization framework to improve monotonic attention mechanisms in simultaneous machine translation. Specifically, the authors try several approaches to integrate the future information from language modeling into the multi-head monotonic attention. Instead of using future information from language modeling directly in the output layer of the attention model, they transform the monotonic attention mechanism to explicitly use the future information. Several experiments show some improvement over the SoTA multi-head monotonic attention models. ",
    "strengths": "This paper proposes an approach to incorporate information from language modeling models into simultaneous neural machine translation. The ideas are novel and the experiments show some improvement over the baseline. ",
    "weaknesses": "We can have more baselines to compare with the proposed models. It should be better if authors compare with some simple combinations of multi-head monotonic attention model and language ( joint training in target sides) or LMs as the re-rank metrics to the predicted target output. ",
    "comments": "If you finetune XLM in the target language, do we get better results for MMA-XLM? The high accuracy of SLM is because of its training on in-domain data. Therefore we should expect the same thing when fine-tuning XLM in the target language. ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]