{
  "paper_id": "ARR_2022_136",
  "title": "CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark",
  "conference": "ARR",
  "domain": {
    "research_object": "该论文主要研究的是中文生物医学领域的文本数据，聚焦于自然语言处理任务中的语言理解问题。",
    "core_technique": "论文采用和评估了基于Transformer架构的预训练语言模型（如BERT、ERNIE等），并针对中文生物医学文本进行了适配和优化。",
    "application": "论文成果可应用于中文生物医学领域的问答系统、信息抽取、文本分类、命名实体识别等实际场景，提升相关智能医疗和生物信息处理能力。",
    "domains": [
      "自然语言处理",
      "生物医学信息学",
      "人工智能"
    ]
  },
  "ideal": {
    "core_idea": "首次提出了面向中文生物医学语言理解的综合性评测基准CBLUE，并系统评估了多种中文预训练模型。",
    "tech_stack": [
      "生物医学自然语言处理（BioNLP）",
      "评测基准（Benchmark）",
      "预训练语言模型",
      "命名实体识别",
      "信息抽取",
      "短文本分类",
      "问答系统",
      "语义相似度计算"
    ],
    "input_type": "多种中文生物医学文本任务的数据，包括实体识别、信息抽取、诊断归一化、分类、问答等文本输入",
    "output_type": "各项任务的模型性能评估结果与基准分数"
  },
  "skeleton": {
    "problem_framing": "论文从实际痛点和学术gap双重角度引出问题。首先强调人工智能在医疗和生物医学领域的广泛应用，指出英文主导的评测基准推动了模型发展，但同时指出中文领域缺乏类似的评测体系，尤其是在生物医学自然语言处理（BioNLP）领域。通过强调中文使用者占全球人口的四分之一，却没有中文生物医学语言理解评测基准，突出实际需求和学术空白，形成鲜明对比，强化问题的紧迫性和重要性。",
    "gap_pattern": "论文批评现有方法时，采用了‘现有方法忽视了X’和‘现有方法在Y场景下失效’的逻辑。具体表现为：现有主流基准和数据集几乎全部为英文，导致相关智能系统和模型发展具有英美中心倾向，忽视了中文等其他语言的独特语言特性和实际需求。此外，指出生物医学语料的标注需要专家参与，现有基准在跨语言和专业领域的适用性存在不足。通过对比英文和中文的资源分布，突出现有方法的局限性和不适用性。",
    "method_story": "方法部分采用了‘先整体后局部’的叙述策略。首先介绍了CBLUE基准的整体设计，包括覆盖的八大任务类型，突出其全面性和开放性。随后，聚焦于模型评测流程，说明对多种主流中文预训练模型进行系统性评估，并简要描述了任务适配（如针对每个任务增加输出层并微调模型）。方法介绍以任务和模型为主线，逐步展开细节，便于读者理解整体框架和具体实现。",
    "experiments_story": "实验部分采用了‘主实验+多模型对比’的策略。首先系统性地对CBLUE基准上的各类中文预训练模型进行主实验评测，涵盖BERT、RoBERTa、ALBERT、ZEN、MacBERT、PCL-MedBERT等多种模型。实验内容包括不同模型在各任务上的性能对比、模型规模对效果的影响、不同预训练策略（如whole word masking）的表现差异，以及小模型与大模型的效率比较。实验分析还结合具体任务表现，讨论模型与数据分布的适配性，突出基准的挑战性和未来改进空间。"
  },
  "tricks": [
    {
      "name": "现实需求强调",
      "type": "writing-level",
      "purpose": "突出研究的实际意义和紧迫性，增强说服力",
      "location": "introduction",
      "description": "通过强调中文用户占全球人口的四分之一，但缺乏中文生物医学语言理解评测基准，突出研究的必要性和现实需求。"
    },
    {
      "name": "领域空白填补",
      "type": "writing-level",
      "purpose": "展示工作的创新性和独特贡献",
      "location": "introduction",
      "description": "明确指出此前没有中文生物医学语言理解评测基准，强调本工作是首次提出该基准。"
    },
    {
      "name": "多任务覆盖",
      "type": "method-level",
      "purpose": "提升方法的完备性和适用性，增强说服力",
      "location": "introduction / method",
      "description": "提出包含八项任务的综合性评测基准，涵盖实体识别、信息抽取、分类、问答等多种任务，展示方法的广泛适用性。"
    },
    {
      "name": "系统性基线评测",
      "type": "experiment-level",
      "purpose": "证明实验的充分性和结果的可靠性",
      "location": "experiments",
      "description": "对11种主流中文预训练语言模型进行系统性评测，全面展示模型在各任务上的表现。"
    },
    {
      "name": "模型性能对比",
      "type": "experiment-level",
      "purpose": "突出基准的挑战性和现有方法的不足，鼓励后续研究",
      "location": "experiments",
      "description": "通过对比不同规模、结构和预训练策略的模型表现，指出当前模型距离单人水平仍有较大差距，强调任务难度。"
    },
    {
      "name": "案例分析",
      "type": "experiment-level",
      "purpose": "提升可解释性，帮助读者理解方法的挑战和细节",
      "location": "introduction / experiments",
      "description": "通过具体案例分析，揭示中文生物医学语言理解中的挑战和语言差异。"
    },
    {
      "name": "开放平台愿景",
      "type": "writing-level",
      "purpose": "增强社区参与感和未来影响力，提升说服力",
      "location": "introduction",
      "description": "提出建立类似GLUE的开放平台，鼓励社区贡献数据集，展示长期影响和开放性。"
    },
    {
      "name": "详细实验设置披露",
      "type": "experiment-level",
      "purpose": "提升实验的可复现性和科学性",
      "location": "experiments",
      "description": "详细说明实验所用工具、环境、超参数设置，便于他人复现和验证。"
    },
    {
      "name": "分阶段任务设计",
      "type": "method-level",
      "purpose": "增强方法的结构性和可解释性",
      "location": "experiments",
      "description": "将部分任务（如CMeIE）分为实体识别和关系分类两个阶段，细化任务流程。"
    },
    {
      "name": "与现有基准对比铺垫",
      "type": "writing-level",
      "purpose": "突出本工作与主流英文基准的差异和创新性",
      "location": "introduction",
      "description": "介绍BLURB、PubMedQA等英文基准，铺垫中文基准的必要性和创新点。"
    },
    {
      "name": "工具包和代码开源",
      "type": "method-level",
      "purpose": "提升方法的可复用性和社区价值",
      "location": "introduction / experiments",
      "description": "公开基线模型源码和工具包，便于后续研究和社区使用。"
    }
  ]
}