[
  {
    "review_id": "03ec0e75773d7691",
    "paper_id": "ARR_2022_189",
    "reviewer": null,
    "paper_summary": "The paper describes the realisation of text-to-speech models for low-resource endangered languages spoken in Canada. The work focuses on the quality of speech synthesis models when less than 5 hours or speech are available to train the model. It compares different architectures on two endangered languages (for one of them, the training set is built by the authors), showing that FastSpeech is less data-hungry than Tacotron, hence supporting its adoption in these cases. Finally, it validates that the resulting systems produce utterances of acceptable quality via human judgment. ",
    "strengths": "The paper has the merit of demonstrating that acceptable TTS can be built even with limited amount of data. The work is well motivated, but its main strength is the extensive manual evaluation carried out to assess the quality of the produced utterances, providing a human-centered validation of the system outputs. ",
    "weaknesses": "A few statements are debatable:  - \"the positive assessment of a few widely respected and community-engaged language speakers is practically sufficient to assess the pedagogical value [...]\": this sentence is not motivated and seems an assumption made by the authors at their convenience;  - \"for additional discussion of the social and environmental ramifications of this change [...]\": although it is an appreciable improvement, it is hard to believe that a ~30% speedup can have societal impact, as it is not changing abruptly the amount of resources needed to train/employ a system; All the experiments in the paper are limited to a single-speaker training set. It would be interesting to understand the effect of having multiple-speaker training sets. With this respect, I am not sure why \"it was deemed inappropriate to use the voices of speakers who passed away\". Although the right to be forgotten supports this approach, these speakers donated their voices to save endangered languages: as such, not exploiting these data seems to go against their own goal when they donated the utterances. However, it is a delicate topic, so the approach adopted by the paper is reasonable. Less motivated, instead, in my opinion, is the choice of removing \"utterances containing non Kanien'kéha characters\": proper names in different languages may also occur in real use cases.\nFinally, the work has no mention that the produced data and code will be released. In addition, the outputs are evaluated only with manual analysis. As a result, this work will be hard to reproduce. ",
    "comments": "Since the systems have been build for pedagogical purposes, why not trying to use it as a teacher for machines as well in self-supervised trainings? This would complete the work in my opinion.\nLines 212-215 are hard to read, I am still not 100% sure about the meaning of the sentence. At line 264 the citation style should be \"(Kumar, 2017)\". \nI have not been able to fully understand section 3.3, especially the second paragraph. Probably I am missing contextual knowledge, such as the meaning of PENÁĆ and who are the people named there. An introduction to these concepts may help readers with different background knowledge as I am.\nSection 4.2.1 is titled \"Architecture choice\" but it seems more a literature review of the existing approaches than a description of the choices made in this work. Maybe the title can be improved.\nLines 491-492 cannot be understood before reading the appendix. Maybe they can be improved with a few more details.\nMUSHRA may need a reference (line 504).\nIn the caption of figure 2 'Phone' is mentioned but it is not present in the picture.\nThe expression \"donor model\" sounds weird to me (552), I would advice replacing with something different (pre-trained model, ..).\nWhy the human evaluations for the two languages were different?\nIn line 956 there is a comma between the subject and the verb. \nIn line 972 there is a repetition \"not need be required\". ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]