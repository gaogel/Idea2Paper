[
  {
    "review_id": "4b7c5fafdc37dc7c",
    "paper_id": "ARR_2022_0",
    "reviewer": "Kevin Stowe",
    "paper_summary": "This papers presents a new dataset for evaluating model performance on figurative language. The dataset is Wino-grad style: it contains figurative phrases with divergent meanings, allowing assessment of whether models can correctly identify the appropriate interpretation of figurative phrases. This dataset is composed via crowdsourcing, with extensive expert validation. An analysis of the dataset is presented, including some linguistic analysis of what types of figuration and syntactic structures are present, as well as analysis of what kinds of metaphors. They then evaluate two types of models on the proposed task of identifying the correct meaning. Additional prompting-based improvements are proposed, as well as some analysis of how the models can work for generating interpretations. Comprehensive error analysis is performed, with a particular eye for where model performance and human performance differ, offering some interesting insights into the difficulty of processing metaphoric language. ",
    "strengths": "The paper is clear and well-written, and presents a strong background knowledge of the linguistic aspects of metaphor. It is framed particularly well wrt. the background literature, and I personally found the authors attention to the linguistic aspects of the proposed dataset to be encouraging. The proposed dataset seems very well thought out, and the extensive expert checks indicate that the resulting dataset is of high quality. This kind of resource is currently valuable not just for the inspection of LMs as proposed, but also for paraphrasing and generation tasks, and I see this dataset being a very valuable resource. The analysis performed is good, and the difference between the auto-regressive and masked language models is an interesting finding. ",
    "weaknesses": "There is of course a fair amount of creativity in the pairs, which may yield some strange inferences. There's something about these paired sentences with the sarcastic reading (\"bright as coal\", \"easy as kindergarten for a newborn\") that seem particuarly forced: I wouldn't expect these sentences to be very common in everyday language, and then perhaps LM performance is expected, as they wouldn't have seen these things before. I feel they're a strange kind of sentence with a somewhat difficult semantics even for humans. For the pilot pair, I'm not sure I would get the intended meanings for the difference between \"ballet dancer\" and \"modern dancer\" - I feel the \"dancer\" alone could yield either reading. So there may be quirks in the dataset that make it tricky to interpret exactly what's going on with model performance.\nI'm curious about the strong performance of the RoBERTa model - could it be that this kind of classification task isn't particularly difficult, if we give the model enough training and data? I think the paper shows clear difficulties in the generation and inference aspects, but perhaps classification is getting better. ",
    "comments": "It should be noted that there are also lots of recent work on novel, challenging NLI sets, which the authors seem aware of. There are some recent and contemporary works that also deal with figurative language (Chakrabarty 2021, \"Figurative Language in RTE\", Poliak 2018, https://aclanthology.org/D18-1007/) the authors should be aware of: in my mind, this work provides a novel, high-quality dataset, they could benefit from clarifying how their work is distinct from these.\nA possible question: this work seems focused on starting with metaphors and getting literal interpretations. I think there are also significant applications the other way, particularly in metaphor generation, where the literal interpretations can be used as input and the metaphors as output. ",
    "overall_score": "4.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "e825a3c26bb00f83",
    "paper_id": "ARR_2022_0",
    "reviewer": null,
    "paper_summary": "This paper introduced Fig-QA, a Winograd-style nonliteral language understanding task consisting of correctly interpreting paired figurative phrases with divergent meanings.  The authors evaluated the performance of several language models with Fig-QA and found that the models achieve performance significantly over chance, but  largely fall beyond human performance. ",
    "strengths": "The authors opened a new perspective in the milieu of ACL with this figurative language.  They first organized its typology and presented quantitative results.  Then the authors crowdsourced 10256 metaphors and call it Fig-QA, a new benchmark task.  It can be used to evaluate the nonliteral reasoning abilities of language models, from multiple viewpoint of zero-shot, fine-tuned and prompting methods.  The overall proposal introduced a novel way to evaluate the language models, and showed how they indeed are far from human capability.\nThe discussion of the limitation of the present language models is rich and this work provides an important understanding about SOTA language models in the way different from other language model evaluation papers. ",
    "weaknesses": "I guess that there are theories about figurative languages. How can section 3.2 be grounded among such theory? If he authors could ground their results in relation to such theories, I think this paper will have even larger impact. ",
    "comments": "Just a conjecture, but I am a non-native and I am certain that I am worse than GPT in these tasks. \nI guess that a non-native human cannot do well in these tasks, in general. \n If you crowdsource the performance on Fig-QA among non-natives, it might reveal what could be  necessary to train LMs.\n-It would be good if Fig-QA is  built for  other languages as well.\n-Although I did not find any datasets nor software attached, I highly recommend Fig-QA to be publicised. ",
    "overall_score": "5 = Top-Notch: This is one of the best papers I read recently, of great interest for the (broad or narrow) sub-communities that might build on it.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "a0978ce3bfde73b6",
    "paper_id": "ARR_2022_0",
    "reviewer": null,
    "paper_summary": "This paper tests the ability of large pretrained language models to assign correct meanings to creative metaphors in English, and also to generate plausible interpretations of such metaphors. Pretrained models are capable of doing this, but performance falls far short of human-level in both zero-shot and pre-trained settings. Models employ a variety of interpretive heuristics such as absolute probability of the answer. ",
    "strengths": "Tasks for which pre-trained LMs do a significantly worse job than humans even with fine-tuning are relatively hard to find. This task is hard even for fine-tuned GPT-3, which makes it a good potential benchmark for future research into figurative language. There is an extensive set of experiments and a relatively large novel dataset. Overall, I think this paper is quite strong. ",
    "weaknesses": "The paper \"A howling success or a working sea? Testing what BERT knows about metaphors\" by Pedinotti et al, Blackbox NLP 21, should be cited here if possible. ",
    "comments": "I suspect the forced-choice setting makes a significant difference to human interpreters, as suggested in ft. 2 \"The opposing meanings help to avoid ambiguity in the correct answer, make the task intuitive for human annotators, and help prevent annotation artifacts that have plagued other NLI datasets.\" Seeing both possible answers makes the pragmatic scale on which the metaphor is being interpreted very explicit (for metaphor interpretation as structural mapping of conceptual domains, see eg Gentner and Bowdle 2008). For example, \"He hustles like he's a billionaire's son\" (Table 1) might be interpreted as a description of how he gets things done (by using his contacts and social privilege) or of how hard he works (not hard because he doesn't need to). The scale \"not at all - hardcore\" selects the meaning. Similarly, \"She was as embarrassed as a kid who got an A\" might mean she is ashamed of being called a nerd, or proud of her academic success. Natural examples of creative metaphors with unexpected scalar mappings are not hard to find (for instance, the American F4 Phantom jet was nicknamed the \"Flying Brick\"; while as a conventional metaphor, \"flies like a brick\" means \"cannot fly at all\", the intended meaning is that the Phantom was unaerodynamic but had a very powerful engine).\nWhile the paper notes that humans do not have a problem processing metaphors (sec 7.3), I believe these results are for metaphors in context, and often for conventional ones, not creative ones. I am not sure of whether similar results apply to creative metaphors, especially out of context. ",
    "overall_score": "4.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]