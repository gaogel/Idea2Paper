[
  {
    "review_id": "03cb8483ba66064a",
    "paper_id": "ARR_2022_353",
    "reviewer": null,
    "paper_summary": "The paper considers the problem of structural bias in NLI, such as predicting the entailment from hypothesis only. \nThe problem is addressed by reformulating the NLI task as a generative task where a model is conditioned on the biased subset of the input and the label and generates the remaining subset of the input.\nWith this formulation, they achieve a bias-free model, with the downside of decreased performance on the actual task. Then they further analyze the cause for this poor performance and propose ways to mitigate it and partially close the gap with the performance of the discriminative model by discriminative fine-tuning.\nThe contributions of the paper are addressing the structural bias in NLI, building a generative model that leads to unbiased models, and improving its performance with discriminative fine-tuning (allowing some bias into the model). ",
    "strengths": "The examined problem is interesting and well-motivated.\nThe paper is well-written and overall easy to follow.\nThe paper provides a way to remove structural bias for the NLI task, examines the cause for its poor performance and proposes ways to improve it by balancing the bias-performance trade-off.\nThe proposed formulation and analysis can lead to interesting extensions in future work. ",
    "weaknesses": "The proposed bias-free model, although eliminating the bias, suffers from poor performance. One thing that the paper would benefit from would be stressing the practical implication of the approach, having in mind that it is not useful in itself for solving the task. ",
    "comments": "255-257: Can you please explain why this is the case? ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "03548accf24c9811",
    "paper_id": "ARR_2022_353",
    "reviewer": null,
    "paper_summary": "This paper considers structural bias to be cases where the label can be predicted based on a specific feature (the authors motivate this with the example of hypothesis-only bias in NLI). They quantify this as a delta measure: the difference between accuracy on the standard test set and a \"hard\" test set. The latter is composed of all the wrongly predicted examples by a biased model. It then proposes to frame classification problems as generation problems and claims that a generative model with a uniform prior can overcome this bias. However, empirical results show that this is achieved at the expense of classification accuracy. The authors demonstrate  by finetuning the generative model as a classifier, however this reintroduces bias according to their delta metric. ",
    "strengths": "1. Interesting idea with theoretical grounding. \n2. Clear and well-written. \n3. Strong empirical results: 5-10 point reduction in the metric used to measure bias. Is there a difference between in-domain and out-of-domain performance for MNLI? ",
    "weaknesses": "1. The authors claim that this is a novel formulation of the NLI task but cite a prior paper that formulates this task the same way. The novelty is in the generative implementation and the scope of the claim should be reduced. \n2. I might have missed this, but the actual implementation of how p(y|B) is set to a uniform distribution during inference is not described. I would be happy to retract this point if the other reviewers saw this described somewhere. Additionally, how is training with a uniform prior actually implemented (Table 3)? \n3. One experiment I would like to see (and is missing at the moment) is a performance comparison of all models on challenge sets such as ANLI since they were constructed specifically to weed out models that perform well on the original test set using spurious features. \n4. Is there a reason for using only BERT over RoBERTa since the latter is generally a stronger baseline? Could a model pretrained on a larger and more diverse dataset be less prone to structural bias? \n5. The authors do not state if the code will be released. ",
    "comments": "L50: \"Furthermore, models that contain such biases may make surprising predictions when the bias is present, causing problems in critical systems.\": Such as?\nL291: Is BERT trained with forward masks when using the autoregressive loss? ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]