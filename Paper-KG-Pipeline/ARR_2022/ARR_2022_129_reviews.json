[
  {
    "review_id": "2d2c451d0cf3d6c8",
    "paper_id": "ARR_2022_129",
    "reviewer": null,
    "paper_summary": "This work collects a large-scale RC dataset for educational purposes. The dataset is annotated by annotators with knowledge in education and similar areas. The curated questions cover different elements of a story, such as the identification of characters and the causal relationship among events, testing children’s RC skills.  Based on the newly collected dataset, the author proposes a pipeline-based system of question-answer pair generation (QAG) conditioned on given stories. The pipeline contains three components, including a heuristic-based answer generation model, a BART-based question generation model, and a BERT-based QA-pairs ranking model. The answer generation model extracts noun phrases and description of events with an off-the-shelf POS tagging model and a SRL model, respectively, as answer candidates. The QG model takes as input the concatenation of QA-pairs. The ranking model scores a list of QA-pairs.\nThe author conducts automatic and human evaluations. The proposed QAG system achieves higher ROUGE-L than the adopted baseline systems. Moreover, human evaluators verify that the QA pairs from the proposed system have higher readability and relevance with respect to the stories. ",
    "strengths": "The collected dataset is a significant contribution for both the QA community and the fields of NLP applications for education. Almost all of the existing work on automatic QA focuses on training models to answer questions. This work investigates how to construct QA tasks to test children’s RC skills. As far as I know, this field is very underexplored though it is vital. ",
    "weaknesses": "1. Despite the good motivation, there is a lack of empirical justification for the collected dataset. As mentioned in the paper, there are some underlying principles adopted for data collection. It would be more convincing to present some evidences showing that the new dataset really achieves that goal, comprehensively evaluating children’s RC skills, in comparison to existing datasets (e.g. NarrativeQA).  2. The evaluation of QAG methods is problematic. The author adopted the ROUGE-L metric for evaluation, but there is no justification that this metric is reliable in the addressed task: How likely a QA-pair with high ROUGE score can faithfully evaluate children’s RC skills? Although they conduct human evaluation, the evaluation metrics are not closely related to the educational purposes. \nThe author also mentions a user study: “A preliminary user study with 12 pairs of parents and children between the ages of 3-8 suggests that this application powered by our QAG system can successfully maintain engaging conversations with children about the story content. In addition, both parents and children found the system useful, enjoyable, and easy to use”. But no details are included, and hence it is hard to determine whether the user study successfully proves the claims in the paper.  3. There are lots of details of data collection that are missing in the paper. What are the sources of stories? What are the coding templates used? ",
    "comments": "“we developed a new RC dataset targeting students from kindergarten to eighth grade” Isn’t this range too broad? Are they expected to process the same or similar levels of RC skills?  You have observed that the questions asked by education experts are open-ended. Why does the designed QAG system extracts answer candidates from stories directly?\nTypos: Line 271: “question generation module (QG) module” Line 303: Missing citation of the Spacy library ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "4ae737b1ea00a29f",
    "paper_id": "ARR_2022_129",
    "reviewer": null,
    "paper_summary": "This paper introduces a new dataset FAIRYTALEQA with text extracted from fairy tales, claimed to be appropriate for K-8th grade, with human annotations of QA pairs. A hybrid approach is proposed for generating QA pairs. Heuristic rules are applied to the design of the model for answer extraction (e.g., extracting named entities and noun chunks as potential answers and semantic roles of verbs as annotated in Propbank). These heuristics help in identifying answers of a specific type appropriate for narratives (identifying characters, setting, plot etc). Then a model is fine tuned on this dataset to generate questions. The model is compared with models fine-tuned with other datasets. The generated QA pairs are ranked via a classification task that decides if a QA pair is more similar to a ground truth pair or not. Automated evaluation metrics include calculating a ROUGE-L score. Human evaluation metrics are reported on three evaluation questions for 70QA pairs. ",
    "strengths": "Overall this paper is tackling the issue of domain specific QA generation, specifically comprehension of narrative fictional passages for K-8.  This is an important area with a high potential for impact and problem has been adequately been described. Below I list the major strengths of the paper. However, there are sever issues with the paper that need to be addressed.  + Substantial effort for creating new dataset of fairytales annotated with QA pairs. \n+ Interdisciplinary work with education experts which leads to introducing a specific set of comprehension questions appropriate for the fiction narrative genre. \n+ Human annotation and evaluation ",
    "weaknesses": "-p. 3 the authors point out that general purpose QA are not appropriate for the education domain and that they solve this problem by developing a new dataset targeting students K-8th grade. This is a big range of grades where fundamental reading comprehension strategies and abilities apply. Reading comprehension questions for Kindergarteners are vastly different from 8th grade middle schoolers. \n-The authors overgeneralize when making claims about the problem they solve in many places. I advise the authors to be precise with their claims.  The current work does not solve the problem of QA in education. It makes a contribution for asking specific type of basic reading comprehension questions for the genre  fiction fairy tale genre. \n-It is not clear if the improved results reported in Table 3 are statistically significant. It would be useful to add a discussion as to whether the reported improvement was within expectation given the effort for annotating a specialized dataset. \n-p.5 It is not clear how the proposed classification task can be extended for ranking QA pairs beyond the specific dataset. The method relies on having available ground truth pairs. ",
    "comments": "p.3 Casual Relationship --> Causal Relationship p.7/8 The human evaluation questions that are reported are 3  (readability, question relevancy, answer relevancy) but the intercoder reliability is evaluated on 4 dimensions. Which is the fourth dimension. Also, can you provide more details about how you computed Krippendoff's alpha? The reported agreement for 5 annotators on questions of relevancy is surprisingly high. Maybe it would be helpful to explain why you didn't choose answer accuracy. ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]