[
  {
    "review_id": "01a966ca7689800b",
    "paper_id": "ARR_2022_289",
    "reviewer": "Eleftheria Briakou",
    "paper_summary": "The paper describes a novel prompt-based transfer learning approach for text generation i.e., PTG. Their method learns a fixed number of source prompts (represented as vectors) based on source generation tasks and then derives a target prompt for the target generation task. The latter is achieved using an adaptive attention mechanism that utilizes both task-level and instance-level information. The effectiveness of the proposed method is evaluated on a diverse set of source-target tasks under different experimental conditions that test how the approach performs under various data-scarce settings (defined w/ respect to the target task). Additionally, the source prompts will be released as an open-source library to enable future experimentation on prompt-based transfer learning in other tasks/domains. ",
    "strengths": "The main strengths of the paper are as follows: a) The paper outlines a novel transfer learning approach that employs prompt-based generation with pre trained language models; b) Experiments are comprehensive (covering a wide range of approaches: LM-based fine-tuning, prompt-based generation & transfer learning). The results are satisfying and robust across different conditions (datasets, metrics, data-sizes); c) The analysis is informative. It gives insights into the role of different components of the proposed approach through ablation experiments and further confirms the author’s intuition on the properties of the learned source prompts via semantic similarity analysis;   d) The authors will release their source prompts as an open-source library which can be used to test how the approach can be adapted/extended to model other generation tasks; e) Overall, the paper is clearly presented and is well written (I have several recommendations on how writing can be further fleshed out but I believe that all my comments can be reasonably addressed in the camera ready version). ",
    "weaknesses": "The (minor) weaknesses of the paper are: a) Several clarifications need to be made to ensure reproducibility (see detailed comments below); b) The authors should address/clarify the choice of the hyperparameter λ used in the approach (see [General Question] below); c) The paper does a good job of explaining how/where the proposed approach works. Yet, we are left unclear on its limitations. What are some target tasks that could potentially be hard to benefit from transfer learning under the PTG framework? If space permits, the authors could add a paragraph explaining their thoughts on “Limitations”. ",
    "comments": "[General Question] The final matching score between the instance x and prompt p_t requires setting a hyperparameter λ that defines the relevant contribution of the task- and instance-level information. Can you expand on how you choose this parameter for each task? More crucially, how sensitive are your results on the choice of this parameter per task and dataset choice? Do you have results on how the performance varies across different choices of λ, or alternatively, do you have an intuition on what types of target tasks might benefit more from larger instance- vs. task-information contributions?\n[Section 1] High-level comment: The problems of adaptation to (a) a new task vs. (b) a new domain are discussed in parallel. The introduction can be fleshed out a bit more to present the two problems separately and clarify/motivate how the proposed approach targets each of them. Additionally, the terms “domain” and “datasets” are used interchangeably throughout the paper (i.e., “domain” is line 46 vs. “datasets” in lines 62). Consider sticking to one term throughout the paper?\n[Section 1] line 39: “corpus” -> “corpora” [Section 1] lines 55-58: Is this the first work that framed generation tasks as language modeling by predicting the next token given previous tokens?   [Section 1] line 78: Can you expand on what the “soft prompts” are? It is hard to understand what is meant out-of-context.\n[Section 1] line 63: In the previous paragraph you refer to the problems of adapting to a new task and datasets. Starting in line 63 you are listing prompt-based methods that address only the first part of the issue. Consider clarifying what you mean by “this problem” in beginning of the  paragraph?\n[Section 1] lines 84-86: Can you provide some more context on whether/how the difficulty of transferring relates to the similarity/nature of the two tasks (i.e.,  source and target)?  [Section 1] lines 89-94: Can you give examples of two different instances drawn from the same task that should be handled using different prompts? This is crucial, to understand why incorporating instance-level information is needed and can help motivate better your approach!\n[Section 1] line 110: At this point of the paper it is unclear what “prompt clusters” referred to.  [Section 1] lines 128-134: Since this is a transfer setting that involved different levels of supervision from multiple tasks and domains (source vs. target datasets) it is not super clear what the “fully-supervised” and “few-shot settings” refer to. It becomes clearer later on in the experimentation section that those terms refer to target sets, but it would be better to clarify early on.  [Section 3] line 197: Another benefit of working with continuous prompts (that is also specific to your approach) is that it allows you to form prompt clusters, something that is not straightforward to do with discrete prompts. Maybe use this to explain/motivate why you opt for continuous prompts early on the problem formulation?\n[Section 3] lines 224-227: Need to clarify: Are the source prompts learned jointly for all source tasks?\n[Section 4] lines 242-248: Does that imply that both source prompts and clustered prompts are included in the pool? Can you clarify on what the size of the source prompt pool is (in a similar way you are introducing source prompt dimensions in the preliminary section)? To make understanding easier, I would start by outlining what the role of source prompts and cluster prompts is in the beginning of 4.1 and then expand on the more low-level details of how each of them is computed.  [Section 4] lines 379-401: The ‘Model Discussion’ subsection seems more of an outline of the approach rather than an actual discussion. Consider rephrasing it and move it in the beginning of section 4?\n[Section 5] lines 425-446: When describing the baselines it would be useful to comment on whether each of them (a) employs transfer or not, (b) is prompt-based or not . Ideally, such a clusterings should be shown in the Tables and discussion in the results section as it seems there are multiple layers of comparisons made here.   [Section 5] lines 448-449: Which PLM is used by your approach? ( I saw that the answer to the question is BART based on details provided in the Appendix, but it is preferable that this context is provided in the Experimental Setup to help make comparisons with other baselines more meaningful) [Section 5] Subsection Header: “Fully-Supervised Setting” is not self-explanatory. Can you describe the two settings “supervised” and “few-shot” in the experimental setup before moving to the results discussion?\n[Section 5] lines 470-482: It is unclear what the exact experimental setting looks like for each of the “cross-task” and ‘cross-dataset“ sets of experiments. I would move details on that in the Experimental Setup section and discuss only the results in this one. Additionally, I have the feeling that whatever is discussed in text in this paragraph could be more concisely presented in a Table. Can you explicitly show what the source and target tasks are for each of the  Tables 1 and 2?\n[Section 5] line 468: I would replace the term “cross-dataset” with “within-task” to make it clear that the setting is constrained on datasets of the same nature (cross-tasks might as well refer to two datasets from different tasks).\n[Section 5] 493-509: For each of the paragraphs describing the results, can you add specific references to the Tables the reader should refer to?  [Section 5] Tables 1 & 2:  The first 4 rows of Tables 1 and 2 collapse to the same settings and are thus repeated. Consider removing them from Table 2?\n[Section 5] Tables 1 & 2: Apart from discussing the results of Table 1 and 2 in isolation, could you also add a small discussion on how your approach yields similar performances across the “cross-task” and “cross-dataset” settings? What are the implications of that? How does that inform future decisions when choosing the source tasks given a known target task?\n[Section 5] Tables 3 & 4: It is very hard to read those Tables. I believe what is really important to get out of those numbers is how metrics change in a relative manner when moving to fewer and fewer samples. Consider, instead replacing those tables with graphs (with #of samples in the x axis) and present just one or two metrics in the main text (moving the rest to Appendix for completeness)?\n[Section 5] line 558: You mentions that the drop is *significant*, did you employ significance testing for numbers presented in Table 5? If not, consider smoothing out the text and maybe just quantify the drops. ",
    "overall_score": "4 = This paper represents solid work, and is of significant interest for the (broad or narrow) sub-communities that might build on it.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "b1a87aee239b1301",
    "paper_id": "ARR_2022_289",
    "reviewer": null,
    "paper_summary": "This paper proposes PTG to transfer learned soft prompts to boost the performance on target generation tasks. Compared with previous work on prompt transfer learning, PTG further demonstrates that (1) combining information from multiple source tasks and (2) introducing task-level and instance-level attention mechanisms are helpful. Despite the simple technical design, the experiments show that PTG achieves superior performance than the baseline methods on several NLG tasks. ",
    "strengths": "1. The paper is well written and well-motivated. \n2. The experiments demonstrate the superiority of the proposed method. Sufficient ablation studies are conducted to show the effectiveness of several core designs. ",
    "weaknesses": "1. A critical limitation is how practical the method is given that the proposed method relies on a series of source tasks for building the prompt pool. For real-world applications, for example, it may occur that (1) the quality (annotation, data amount) of source tasks is poor, (2) the source and target tasks are dissimilar so that the knowledge transfer among these tasks may be helpless, as shown by [1], (3) when the number and diversity of source tasks increase to a certain degree, could the proposed PTG still work? How would the proposed method perform under such situations? Or at least analyses should be conducted to show the impacts of the above factors.\n2. The proposed method is only evaluated on BART-large model, it would make the paper much stronger if other representative PLMs are also tested, e.g., T5.\n3. Baselines need to be introduced more clearly. It is unknown whether both the PREFIX-TUNING baseline and the SPOT baseline also choose BART-large as the backbone PLM same as the proposed method PTG.\n4. ( Minor) The proposed method is tied to prompt tuning & generation tasks. It would be interesting if PTG could be extended to other parameter-efficient algorithms (e.g., Adapter and LoRA) and discriminative tasks.\n[1] SPoT: Better Frozen Model Adaptation through Soft Prompt Transfer ",
    "comments": "Typos: (Line 514) For each size, we sample and 5 different datasets and average over 2 training random seeds. ( remove the first ''and'') ",
    "overall_score": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",
    "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work."
  },
  {
    "review_id": "2657a42b67131e9d",
    "paper_id": "ARR_2022_289",
    "reviewer": "Yusheng Su",
    "paper_summary": "The authors propose a novel lightweight prompt-based transfer learning approach in the data-scarce situation for NLG tasks called PTG, which (1) introduce task-level and instance-level attention mechanisms and (2) can combine multiple trained prompts into a new prompt and adapt to the new generation task. Besides, PTG can perform competitively or better than conventional fine-tuning. ",
    "strengths": "- Overall, this paper is written well.\n- The authors provide sufficient experiments and ablation studies. ",
    "weaknesses": "- A minor suggestion (Line:270): In the subsection of Clustering Source Prompts, you use position-agnostic Euclidean distances (Su et al., 2021) to calculate similarity degree/weight. I'm just curious why you did not utilize the model stimulation similarity, which is a better prompt similarity metric in (Su et al., 2021) and might help PTG select appropriate source prompts to improve the target task performance in your work?  - A minor concern: Have you ever tried to apply the PTG on PREFIX-TUNING instead of Prompt tuning? Since PREFIX-TUNING was initially proposed for NLG tasks and is relatively easier to train (faster convergence and better performance), it might be the reason that we seldom (or never) use prompt tuning for NLG tasks.  Su et al. https://arxiv.org/abs/2111.06719 ",
    "comments": "Refer to Summary Of Weaknesses ",
    "overall_score": "3.5 ",
    "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work."
  }
]