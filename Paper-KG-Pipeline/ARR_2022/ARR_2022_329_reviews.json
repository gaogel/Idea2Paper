[
  {
    "review_id": "d85a9e84b9e1945c",
    "paper_id": "ARR_2022_329",
    "reviewer": null,
    "paper_summary": "This paper introduces a new benchmark for the under-researched Turkish language, involving seven different NLP tasks. For each task, authors provide one or two datasets some of which are created by them while some others are taken from existing literature. The authors also establish baseline results on each dataset using various models. ",
    "strengths": "The paper has a clear motivation and tackles an important problem since there is a clear need for more benchmarks that will promote the development of better models for many under-studied languages. The proposed benchmark can help facilitate the development of better-performing models for the Turkish language. ",
    "weaknesses": "Although the paper is mostly easy to follow due to its simple and clear organization, it is not very clearly written. Some sentences are not clear and the text contains many typos. Although the included tasks can definitely be helpful, the proposed benchmark does not include many important tasks that require higher-level language understanding such as natural language inference, question answering, coreference resolution, etc. Also, the authors do not mention making the benchmark publicly available by providing a link which is very important and I hope will be the case. ",
    "comments": "General comments and suggestions: 1) The name of the \"Evaluation\" element can be changed to \"Metrics\" since 'evaluation' can have a more general meaning. Even better, the corresponding sections can be removed and the metrics can be briefly mentioned along with the datasets or in the captions of the tables since most, if not all, of the metrics are well-known and used as standard practice.  2) I feel like sentence segmentation and spellchecking & correction tasks do not fit well in the same benchmark alongside the other tasks that focus more on semantics. Ideally, one should be able to use the same model/architecture for all of the tasks in the benchmark by training it on their corresponding training sets. However, these two tasks feel lower level than the other tasks probably better handled somewhat separately than others.  3) Are there any reasons behind the selected baselines? For instance, why the Haded attention- RNN and Adaptive transformer instead of more traditional LSTMs and normal encoder-decoder transformers (or decoder only transformers like GPT models)?\n Specific questions, comments, and typos: - Lines 89-90: -> In Mukayese we focus on - Lines 99-100: rewrite more clearly using full sentences or more clear format (i.e., We define two requirements ...: (i) accessible with ... (ii) sharable format)  - Line 100: what is 'sharable format'?\n- Lines 104-105: I believe the argument about inefficiency and cost of large supervised datasets is not correct. Finetuning a model on labeled training data for the target task is usually computationally not so costly and having more diverse supervised data almost always greatly helps.  - Line 119: -> one or more metrics - Line 119-120: please rewrite (a) - Lines 127-129: please rewrite in a more clear way.\n- Line 135: -> for each of the following ... - Lines 138-145: The paragraph starts with talking about general language modeling and formulates it, and then claims this is only a special type of language modeling (i.e., autoregressive language modeling).\n- Line 151: -> AR language modeling Line 156: What exactly is a language modeling dataset? Is it simply a text corpus?  - Table 2 caption:\"mean average\" ?\n- Lines 173-174: What is a character language modeling dataset? What is the difference from the normal language modeling dataset? If there is no difference, is there a need for a separate dataset?\n- Line 188 -> generalize to - Lines 201-204: please rewrite in a more clear way.\n- Lines 214-215: Are the same models trained for both normal language modeling and character-level language modeling without any architectural changes? Is this possible? More details are needed.\n- Table 4 caption: these are not the results - Line 247: \"Where\" cannot be in a new sentence - Table 7 caption: metric (F1) should be mentioned in the caption as well - Line 338: \"converting them to\" converting what?\n- Line 344: with -> on - Line 363: no space before \"In\" - Lines 396-400: Do you remove the remaining 9% from the dataset? If annotators could not come up with the correct words after 10 predictions probably it means the target word is not recoverable, right?\n- Line 465: -> perform - Line 471: -> such as - Table 13 is not referenced in the text - Lines 497-498: one with (pre-training) and two without pre-training - Line 509: -> in ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]