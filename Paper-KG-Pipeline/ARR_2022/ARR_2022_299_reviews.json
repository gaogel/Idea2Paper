[
  {
    "review_id": "1da2bd4a793d8b2b",
    "paper_id": "ARR_2022_299",
    "reviewer": null,
    "paper_summary": "This paper focuses on revising errors made by a generation model while it generates additional tokens continuously. When generating the i-th token, the proposed approach will predict the token at position i-1 again for a potential revising. Compared to the first prediction for position i-1, the second prediction adds the predicted i-1 token as extra context in the self-attention module. So, it may make a different decision. A new hyperparameter is introduced to control the number of tokens can be refined. ",
    "strengths": "1. The proposed method can be integrated into a language model which using attention mechanism easily, which could benefits its adoption if this method shows the effectiveness.\n2. Extensive experiment results show there are some gains for this method compared to some baselines on four generation tasks. ",
    "weaknesses": "1. While the proposed method could make change to the predicted token, whether it make an improvement is questionable. During training, let the i-th position see the (i+1)th position is an information leak, because the token at (i+1)th position is exactly the target. The training process do not teach the model to do any correction.  2. Lack of a comparison to show refinement during inference improves the generation quality. It's good to see the statistic of the number of refinements, but this only approves the change instead of an improvement. ",
    "comments": "1. In Table 2, the ablation experiment has a setting only add \"local Constraint\" but without \"Refinement Mask\". How could it make a change on the prediction result? As the input (includes the tokens can be attended) is the same for every prediction at a position. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "32528ea92c39b938",
    "paper_id": "ARR_2022_299",
    "reviewer": "Zhaopeng Tu",
    "paper_summary": "This paper proposes a novel refinement method to synchronously refine the previously generated words and generate the next word for language generation models. The authors accomplish this goal with an interesting implementation without introducing additional parameters. Specifically, the authors reuses the context vectors at previous decoding steps (i.e., c_1, c_2, ..., c_{i-2}) to calculate the refined probabilities in a similar way to the standard generation probabilities (the only difference is that using c_{0<n<i-1} instead of c_{i-1}). A refinement operation will be conducted at a previous position, where the refinement probability is greater than the generation probability.\nTo reduce the computational cost and potential risk of \"over-refinement\", the authors design a local constraint that narrow the refinement span to the N nearest tokens. In model training, the authors randomly select future target words not greater than N to cover a variety of different future contexts as bleu parts. ",
    "strengths": "1. A novel approach to accomplish the modeling of future context. \n2. Comprehensive experiments to validate the effectiveness of the proposed approach across different tasks (e.g., standard and simultaneous machine translation, storytelling, and text summarization). \n3. Detailed analyses to show how each component (e.g., the hyper parameter N, local constraints and refinement mask) works. ",
    "weaknesses": "The main concern is the measure of the inference speed. The authors claimed that \"the search complexity of decoding with refinement as consistent as that of the original decoding with beam search\" (line 202), and empirically validated that in Table 1 (i.e., #Speed2.).\nEven with local constraint, the model would conduct 5 (N=5) more softmax operations over the whole vocabulary (which is most time-consuming part in inference) to calculate the distribution of refinement probabilities for each target position. Why does such operations only marginally decrease the inference speed (e.g., form 3.7k to 3.5k tokens/sec for Transformer-base model)?\nHow do we measure the inference speed? Do you follow Kasai et al., (2021) to measure inference speed when translating in mini-batches as large as the hardware allows. I guess you report the batch decoding speed since the number is relatively high. Please clarify the details and try to explain why the refinement model hardly affect the inference speed.\nThe score will be increased if the authors can address the concern.\n[1] Jungo Kasai, Nikolaos Pappas, Hao Peng, James Cross, and Noah Smith. Deep Encoder, Shallow Decoder: Reevaluating Non-autoregressive Machine Translation. ICLR 2021. ",
    "comments": "1. Line118: SelfAtt_c => Cross Attention, the attention network over the encoder representations is generally called as cross attention.\n2. Ablation study in Section 4.1.3 should be conducted on validation sets instead of test sets (similar to Section 4.1.2). In addition, does the refinement mask in Table 2 denote that randomly selecting future target words no greater than N in model training (i.e., Line 254)?\n3. Is PPL a commonly-used metric for storytelling? ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work."
  },
  {
    "review_id": "01ad9f64bb9ed791",
    "paper_id": "ARR_2022_299",
    "reviewer": "Xiang Li0",
    "paper_summary": "This paper propose a novel encoder-decoder model for language generation. It revises the potential errors by considering the representation of  the target future context and generates the next target token synchronously. Despite a slight reduction in decoding speed compared to the Transformer baselines in three machine translation tasks, the proposed model is superior to competitive comparison systems in translation quality and efficiency using the same number of parameters as the Transformer. And the improvement is also demonstrated in other three language generation task, including summarization, storytelling and simultaneous machine translation. ",
    "strengths": "I really enjoyed reading this paper. The research question is clearly stated, and the experimental results are fascinating to go through. ",
    "weaknesses": "It is not clear when deletion operation is used in the refinement period. ",
    "comments": "1. How to ensure that the consecutive refined BEP tokens can be valid whole words after de-BPE operation? \n2. I suggest show the the effect of the proposed model with beam search during the refinement stage for the translation quality, although with inferior inference speed. \n3. Although achieving an improvement, what are the main advantages of this proposed over some simple and effective methods without additional parameters and speed reduction, such as back-translation and R-drop? \n4. The ratio between the generation and the refinement probabilities in the whole training objective is 1 in the equation 12. Is it the optimal setting? ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work."
  },
  {
    "review_id": "fd35e7df6ba77a07",
    "paper_id": "ARR_2022_299",
    "reviewer": null,
    "paper_summary": "This paper presents a method to improve text generation by refining the target output while decoding. The idea is that at each decoding step t, the model predicts not only the current word but re-processes and predicts previous ones (in a window of k-words), so some generation errors can be corrected. ",
    "strengths": "-   The idea of simultaneously refining and decoding is sound. It has the potential to improve text generation with low overhead of resources.\n-   The results on three tasks show that their model works well in comparison to a standard model, however, comparisons with similar methods are missing. ",
    "weaknesses": "- Comparison of results with equivalent methods described on related work are missing. For example, post-editing or multiple pass decoders.\n- The general idea is well described but several details are not clear in the paper (refer to comments for details). This information is important for understanding and reproducibility, and it is necessary for properly reviewing the work. ",
    "comments": "1 . Line 050: A common way to address the issue is using beam search.\n2. Lines 076-079 Question: Do you replace a word independent of the context? For example, if you replace w_i, what happens with w_{i-1} and w_{i+1}. This could create incoherent text. I will assume that it is better to replace the whole phrase instead of independent words. \n3. Lines 197-198 Question: How do you calculate the joint probability P(y_1, y_2, ..| y_{<i}, X) if this part is not autoregressive?  I suggest adding the equation to the paper for clarity.\n4. Line 197 Question: What it means the “0”? \n5. Lines 185-191 Question: Same question as in (2).  It is not clear if you replace the whole phrase or each word? \n6. Lines 199-207. The decoding algorithm mixing greedy and beam search should be added as pseudocode for clarity. \n7. Equation 12. I assume that P_g and P_r shared the same parameters, so their calculation varies only for the use of different masks. Does this imply making two passes of the decoder? Please clarify these points in the paper. \n8. As far as I understand, the refining mask is used only for training. Is this correct? ",
    "overall_score": "2.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]