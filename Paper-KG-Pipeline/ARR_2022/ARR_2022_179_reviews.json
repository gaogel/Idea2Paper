[
  {
    "review_id": "b6471bd64d652cce",
    "paper_id": "ARR_2022_179",
    "reviewer": null,
    "paper_summary": "This short paper presents an alternative to label smoothing for neural MT.  In one variant (\"weighted\"), the amount of training-time probability assigned to every vocab item except the correct one at each time step in the decoder is varied based on whether the item appears in the source-side vocabulary, target-side vocabulary, or both.  The more fully explored variant (\"masked\") just sets the source-side vocabulary weight to 0 and distributes the smoothed weight evenly over the vocabulary items in the other two classes.  Both variants of label smoothing are motivated by a noticed drop in BLEU scores whenever label smoothing is done with source/target vocabulary sharing also turned on.  Label smoothing and vocabulary sharing each improve scores over the Transformer-base, but using the two together hurts performance.\nMasked label smoothing (with vocab sharing) is tested against Transformer-base, vocab sharing alone, label smoothing alone, and vocab sharing + label smoothing together.  Evaluation is mainly by BLEU score on a variety of data sets (IWSLT, WMT, CASIA) and language pairs.  The score differences are not large -- I think never more than 0.5 BLEU) -- but the authors report that using masked label smoothing statistically outperforms regular label smoothing when vocab selection is also turned on.\nThe short paper fits in four pages as required.  There is a one-page appendix.  There's also a small software package with a README as part of the uploaded submission -- but it's not mentioned in the text and so I almost missed it. ",
    "strengths": "- Focused, straightforward idea with a clear intuitive and practical motivation.  I think people will be interested in this paper's observations and proposed solution.  I expect that the details necessary for re-implementation are relatively clear.\n- Evaluation on several different language pairs, which includes some diversity of character set / writing system.  Both into and out of English is covered. ",
    "weaknesses": "- The use of the appendix is a little more integral than I think would be ideal.  You have to read the appendix in order to know the conclusions of some analysis; the main text only says that such-and-such \"matters,\" but now how.\n- It's not always clear which results are meaningful.  Some rows of the main table have statistical significance tests, but other metrics, analysis, and data points do not.  Since the score differences are overall quite small, this detracts from the reliability of the results.  Readers might be inclined to decide that the techniques aren't worth trying. ",
    "comments": "SUGGESTIONS - It's great to have at least some statistical significance tests.  Since they were added only after the first revision of the paper, I felt like they weren't fully integrated into the main text or all of the results tables (like Table 5).  For example, the beginning of Section 5.2 (line 201) says that LS \"outperforms\" VS+LS... but we do not have any significance tests between these two configurations.  These differences of 0.1 BLEU may actually be ties, in which case \"outperforms\" is incorrect.\n- In Table 3(b), is it really the case that 23.19 BLEU for RO-EN is statistically better than 23.15, or is this a typo?  I've never seen significance at p < 0.01 for such a tiny score change.\n- Even without a formal significance test, can we have some intuition about this ECE score in Section 6.1 / Table 4?  How is this score computed and what properties of MT training does it reflect?  I've never heard of this metric before, so I don't really understand what it is or whether an improvement of 0.7 is meaningful.  (Whereas I know quite well how much intuitive importance to give 0.7 BLEU.)\n- In Table 5, I would think that one intuitive configuration is to devalue the source vocabulary without shutting it off completely, via parameter settings like 0.5-0.5-0.25.  If your original motivation is correct, I would expect a configuration like that to do better than your current best result, 0.5-0-0.5.  Doesn't the current result indicate that the source-side vocabulary is somehow quite helpful -- the opposite of your initial claim?\nOTHER TYPOS OR CONFUSION - Why don't the numbers at the top of Table 5 (for baseline label smoothing) match the scores given in Table 3 for LS?  Aren't these identical configurations?\n- How do you decide when model training is over?  Is it a fixed number of epochs or updates, or after perplexity on the dev set converges (for some definition), etc.  Appendix B doesn't say.  Since your experiments modify the kind of signal that the model receives during training, I was wondering if that has a confounding effect on the results.  (For example, if MLS makes the training last 25 percent more updates than the baseline, then is the increased performance partially due to the mere fact of training longer?)\n- The WLS parameters \"1-1-0\" in Section 4.2 (line 175) directly contradict the claim in Section 4.1 (line 150) that these three values must sum to 1. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "395f3cafa7469125",
    "paper_id": "ARR_2022_179",
    "reviewer": null,
    "paper_summary": "The paper argues that jointly adopting both label smoothing and vocabulary sharing techniques can be conflicting. This comes from a simple intuition that words have zero overlap with the possible target words should have no chance to appear in the target sentence. They propose to use a very simple approach (Masked label smoothing) that addresses the conflict. Results show that the method gains very incremental improvement in translation accuracy and some improvement in model calibration.  Overall the conflict is interesting but the result is very mixed. ",
    "strengths": "The paper presents an interesting conflict between these two approaches. I personally never thought about this before. ",
    "weaknesses": "Cons: From translation accuracy perspective, the improvement from fixing the conflict is very incremental. This is clearly shown as the translation improvement is only +0.47 at its best. In other cases (e.g. in the specific case (EN-RO - Table 5)), the improvement is +0.04 (from 23.15 to 23.19), which I would say it does not imply anything that is particularly meaningful.  able 5 also shows that assigning B_s to 0 seems does not matter that much (i.e. it does not hurt the performance that much). This shows that despite the intuition, the conflict actually does not matter in reality.\nThere might be improvement in model calibration, but with the current paper it is not clear from Table 4 the improvement means big thing or not. ",
    "comments": "Typo + writing - as shown in Table 6,7 for both BLEU 207 and chrF -> 6, 7 - As shown in Table 3 , MLS 212 achieves consistent improvement over the origi213 nal label smoothing in both the original and the 214 balanced multilingual translation dataset under all 215 translation directions -> Table 3, MLS - About ECE score: There should be an explanation from the paper.\n- Writing style: the authors use the word outperform a lot in the paper. Clearly the improvement does not reflect an \"outperforming\" by anymeans. I suggest the authors lower the use of word... ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]