[
  {
    "review_id": "a2dbb4d1f528e4c6",
    "paper_id": "ARR_2022_154",
    "reviewer": "Sheng Shen",
    "paper_summary": "This work proposes a multi-modal contrastive learning objective to learn text representation based on both visual and text information. Extensive experiments have been provided to show the improvement of the proposed method on seven semantic textual similarity tasks. Besides, additional analyses show that the proposed method leads to better semantical alignment of the learned sentence representation. ",
    "strengths": "- The paper is well-written and clearly presented;  - The idea of using visually grounded corpus to help learn sentence representation is well-motivated and novel. Extensive experiments (with random seeds) have been provided to show the significance of the proposed methods. Detailed ablation studies and analyses have also been provided to show the impact of dataset, pre-trained model and sub-tasks;  - The idea can be potentially scaled up to many noisy image-text pairs on the web. ",
    "weaknesses": "- in Table 1, it seems using Flickr textual corpus only can bring improvement for SimCSE on downstream tasks but not for using COCO, is there a detailed explanation for this? ( also, will adding additional textual data, e.g. more Wikipedia texts bring similar improvement)?  - Tan and Bansal (2020) shows the significant difference between the image-text corpus and wiki corpus (small sentence length, usually less than 77 and so on.) But the dataset statistics have not been discussed here, wondering will become a problem or not for sentence representation learning. ",
    "comments": "See above ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]