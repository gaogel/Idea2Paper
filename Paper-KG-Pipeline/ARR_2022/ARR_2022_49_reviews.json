[
  {
    "review_id": "46241641ba69ff15",
    "paper_id": "ARR_2022_49",
    "reviewer": null,
    "paper_summary": "The authors state that prior work encode passages for distantly supervised relation extraction exclusively one-by-one. They propose a simple BERT based architecture that processes several passages simultaneously and thus allows for inter-sentence information flow during encoding. They provide results that show that their method outperforms prior work. ",
    "strengths": "1) The experimental execution of the paper is of high quality. Results are presented across four datasets and compared against several relevant/recent baselines. Further, meaningful ablation studies are presented.  2) The proposed architecture seems to work and yields performance improvements.  3) Overall the paper is well-written and easy to follow. The scope and contributions fit my expectation of a short paper. ",
    "weaknesses": "1) I miss an analysis for which specific examples encoding multiple passages help. Adding an analysis where models that encode passages independently of each other fail compared to the proposed approach would be helpful. Similarly, this is a weak point in the motivation of the paper: it is not immediately clear to me why encoding passages simultaneously should be beneficial. You remain very vague here (e.g., line 046) and do not provide convincing arguments.  2) A central argument of the paper is that encoding passages simultaneously is helpful. Why do you restrict yourself to a maximum sequence length of 512 by using BERT? There is pretrained language models e.g., based on Longformer where you could encode more passages. Providing more insights how the number of passages influence performance (beyond what is mentioned in paragraph around line 243) would make the paper stronger. ",
    "comments": "1) Why do you use uncased PLMs?  2) Potentially relevant but not cited work: Chu et al. 2021 (https://openreview.net/pdf?id=8smkJ2ekBRC), Wang et al. 2019 (https://aclanthology.org/P19-1132.pdf) 3) Please fix the bibliography and cite published versions rather than arxiv versions (e.g., Loshchilov et al. 2017). ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]