[
  {
    "review_id": "7f02b5e5a8d97efb",
    "paper_id": "ARR_2022_218",
    "reviewer": "Ziming Huang",
    "paper_summary": "This paper conducts an extensive empirical study to assess the faithfulness of post-hoc explanations and the performance of select-then-predict models in out-of-domain settings. There are some interesting findings: post-hoc explanation fails faithfulness in out-of-domain; select-then-predict explanation performs well in out-of-domain settings. ",
    "strengths": "1. This paper has a complete logic, from problem definition to description to experiment to analysis, explaining the main idea of ​​this article in detail. This paper and the provided data and code are very clear and well-written.\n2. The motivation and research questions in this paper are very interesting in the explanation domain.\n3. The paper conducts extensive experiments in out-of-domain setting explanation methods comparison based on different models to prove the post-hoc explanation sufficiency and comprehensiveness show misleading increases in out-of-domain settings and select-then-predict classifiers can be used in out-of-domain settings. ",
    "weaknesses": "Generally, the explanation paper will provide some intuitive examples to support their own arguments to make it more clear. It is recommended that add some examples in the camera-ready version to increase readability. ",
    "comments": "Checking the weakness will help. ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "8eecd9d9385a6456",
    "paper_id": "ARR_2022_218",
    "reviewer": null,
    "paper_summary": "The paper reports an extensive study of explanations in out-of-domain settings. The authors examine the faithfulness of post-hoc explanations (feature attribution approaches) and the out-of-domain performances of inherently faithful models (select-then-predict). The results suggest that random baselines should be used when evaluating post-hoc explanations and that select-then-predict models perform well in out-of-domain settings. ",
    "strengths": "The authors carry out an extensive analysis of explanations in out-of-domain settings considering various datasets and models. The authors describe clearly the motivation, research questions, and contributions of their work. They also introduced in an accessible way the concepts and terminology that are relevant to this research line. The analyses are thorough and their methods well-described.  Besides reporting the results of the analyses, the authors recommend practices for robust evaluation, such as the usage of a random baseline when evaluating post-hoc explanations out-of-domain.  The authors show that select-then-predict models perform rather well in out-of-domain settings, which is a relevant contribution for building real-world applications. ",
    "weaknesses": "It would have been nice to see some examples of predictions and explanations of the models in the paper, and in general some insights from qualitative inspections of the data. ",
    "comments": "- ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "9f320459a1665d8d",
    "paper_id": "ARR_2022_218",
    "reviewer": null,
    "paper_summary": "The paper measures the out-of-domain faithfulness of post-hoc explanations and the explanations of select-then-predict models. For post-hoc explanations, results show that in many cases, sufficiency and comprehensiveness are better for out-of-domain than for in-domain explanations. This leads to the suggestions,that these models generalize better than other models. Select-then-predict models also perform surprisingly well in out-of-domain settings. However, the authors find that using sufficiency and comprehensiveness for evaluation of faithfulness can be misleading. They hence suggest comparing the faithfulness of post-hoc explanations to a random attribution baseline to ensure robustness. ",
    "strengths": "The paper is well written and easy to understand. It follows a logical structure which makes it easy to follow the experiments and line of argumentation. There are only few grammatical mistakes. A very detailed evaluation is given for both normalized sufficiency and normalized comprehensiveness, as well as full-text F1. The models were trained and evaluated on multiple datasets and domains to ensure unbiased results. A zip file has been attached to reproduce results. The file contains Python code, along with required packages and a detailed README.md. References are given where necessary. All experiments are well explained, and details for datasets, models, and analyses are given in the appendix. ",
    "weaknesses": "There are quite a few punctuation errors and grammatical errors (l. 41, 137, 457), as well as orthographic inconsistencies (e.g., versions of “full-text” l. 95, 343, Table 3, 459, etc.). Some of the findings are printed in italic letters in an inconsistent way (e.g., 391-396, 427-438, 555-561, but not 567-581), also, this makes reading a little inconvenient. Spaces between tables, formulas, and text are also inconsistent (e.g., l. 285, 372). ",
    "comments": "Please correct the punctuation errors and review the consistency of words (orthography with or without dashes, upper and lowercase letters). Remove italic conclusions to ensure better readability. ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]