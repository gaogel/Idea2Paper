[
  {
    "review_id": "9322fb74f7ab53a6",
    "paper_id": "ARR_2022_172",
    "reviewer": "Mahmoud Mohammadi",
    "paper_summary": "The main contribution area of the authors is studying the impact of Trojan tracks on language models internals depicting the following specific contributions: Analyzing the effect of Trojan attacks on attention behaviors of BERT models and highlighting the attention focus drifting effect on Trojaned models. \nProposing a technique to detect the Trojaned BERT models using attention behavior analysis called Attention-based Trojan Detector Preparing and sharing a repository of the Trojaned and clean models with corresponding data and triggers. ",
    "strengths": "Authors have formulated the attention focus drifting ( in page 4) which helps a lot in shaping a mathematical base to understand the attention focus drifting problem they are working on. \nAuthors also have done an organized investigation and framed a methodology for quantifying the effect of attention focus drifting for Trojaned models and classifying different token types. This provides a strong base to build both the Trojaned model detectors and possible defense mechanisms. \nThe analysis (table 2) showing the significance of different attention drifting problems for both Trojaned and clean models provides an meaningful insight on quantification and understating  of the attention drifting behavior. ",
    "weaknesses": "Attention attribution and entropy terms in section 3.2.1 needs a little more context or definition or example. \nDifference between adversarial perturbation and Triggers is not clear in section 4 and why perturbations cannot cause attention focus drifting while can change the classification changes. \nWhy Phrase Candidate Generator and Non-Phrase Candidate Generator are required is not clear. \nSeems the section 3.3 be not very relevant or required to both attention monitor and Trojan perturbation generation on section 4.1. The space can be used to add more clarity and more examples for section 4.1. \nAlso needs more clarity on how the thresholds/ stats detailed in section 3 on quantifying the attention drifting have been utilized in section 4.1 for attention monitor. More examples and references to those attention drifting will provide much stronger utilizing them in shaping the architecture of the attention monitor. \nSection 4.1 as the main section to generate Triggers and Trojan detection needs some examples for clarification or at least in Appendix. \nSection 4.1: It is not clear whether all possible perturbation candidates are used for all clean samples or a subset of perturbations? Applying all neutral words to a full dataset can make the whole pipeline of Figure 6 not feasible, or maybe I missed anything? Or maybe by reaching the beta=15 you will stop searching for each sample? Does any under/overfitting problem can happen for Trojan detection when evaluating the ASR on unseen Trojan perturbation?\nRegarding table 5: Is not clear how the ASR is calculated. For example 96% for IMDB shows that the we had a posinoied/purturbed dataset ( with injected triggers) and trained our model ( to have a Trojaned model and then evaluated that model with another unseen poisoned test dataset and the result is called ASR ( same for clean) ? The percentages are for all data samples, 96% of how many data samples? More clarity is very useful here. \nRegarding Table 6: It is also not clear to what performance the numbers ( acc or auc) refer to? Data samples, Trojaned models? ",
    "comments": "More examples on section 4 in comparison to attention focus formulating efforts done in section 3 will add extra visibility and application to the entire research ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]