[
  {
    "review_id": "ca04bf1a0e948cf1",
    "paper_id": "ARR_2022_23",
    "reviewer": null,
    "paper_summary": "This paper consists of two main parts. First, the authors constructed MedLAMA, a biomedical knowledge probing benchmark dataset based on the UMLS knowledge graph. Second, the authors proposed a contrastive-probe method based on retrieval-based probing and self-supervised contrastive learning. When the authors applied several conventional proving methods with existing pre-trained language models (PLMs) to MedLAMA, other methods showed poor performances, because they have weaknesses in handling multi-token answers. The authors insisted that the proposed contrastive-probe method achieved significant improvement on probing with MedLAMA data. The authors also performed extensive experiments including human expert evaluation to demonstrate characteristics of their dataset and method. ",
    "strengths": "- The authors constructed a new biomedical knowledge probing benchmark dataset MedLAMA.\n- The authors proposed a contrastive-probe method, which can be used without labelled data. ",
    "weaknesses": "- The performance of the contrastive-probe method in the main result using MedLAMA was significantly better than other methods (mask average), while the performances using BioLAMA were similar between mask average and the contrastive-probe method. Thus, the authors need to clearly show why the performances in two datasets are different.    - The authors mentioned “2Prompt-based probing approaches such as Auto-Prompt (Shin et al., 2020a), SoftPrompt (Qin and Eisner, 2021), and OptiPrompt (Zhong et al., 2021) need additional labelled data for fine-tuning prompts, but we restrict the scope of our investigation to methods that do not require task data.“ However, it might be better to show the performances of these methods on the MedLAMA dataset, in order to show the validity of the MedLAMA dataset.\n- Additionally, it seems to be an unfair comparison because only the contrastive-probe method was additionally pre-trained for the cloze-style task. ",
    "comments": "- It would be better that comparison with BioLAMA is placed in the main result, not in the Appendix. ",
    "overall_score": "2 = Borderline: This paper has some merit, but also significant flaws. It does not warrant publication at top-tier venues, but might still be a good pick for workshops.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "2851a25a648fa21a",
    "paper_id": "ARR_2022_23",
    "reviewer": null,
    "paper_summary": "This paper presents a carefully curated biomedical probing benchmark - MedLAMA. The authors transform the relations in the UMLS knowledge graph to form triples for probing PLMs’ biomedical knowledge. The curated benchmark consists of 19 relations with 1000 examples each. The authors find that existing probing methods fail to achieve meaningful performance on MedLAMA. Therefore, they propose to “rewire the PLM parameters for retrieval”. They propose Contrastive-Probe that fine-tunes the PLM with little data from the original pretraining corpora. The proposed method improves ACC@1 and ACC@10 on MedLAMA to 7%/27%. The authors further conduct experiments for different PLMs, layers, and different relations. Finally, they perform an examination by an expert on the predicted answers and find that the automatic evaluation might underestimate the performance of the models. ",
    "strengths": "- The paper is well-written and easy to follow. The presentation of the ideas is clear.\n- This paper presents a biomedical probing benchmark that might reinforce the research in this direction.\n- The paper proposes a simple method to make PLMs more suitable for retrieval probing, hence improving the probing results. The improved results make the comparison on the benchmark more meaningful. ",
    "weaknesses": "The technical novelty is rather lacking. Although I believe this doesn't affect the contribution of this paper. ",
    "comments": "- You mention that you only select 10 answers from all correct answers, why do you do this? Does this affect the underestimation of the performances?\n- Do you think generative PLMs that are pretrained on biomedical texts could be more suitable for solving the multi-token problem? ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "d63319b2d2727d66",
    "paper_id": "ARR_2022_23",
    "reviewer": null,
    "paper_summary": "The paper presents a new benchmark for Pre-trained Language Model (PLM) knowledge probing in the biomedical domain called MedLAMA and a new probing approach called Contrastive-Probe.\nThe dataset is based on the UMLS metathesaurus. It is composed of a set of 19 handpicked relations with 1k instances per relation. Based on these relations, the authors devised 19 prompts.\nThe paper also presents a new method for probing PLMs, called Contrastive-Probe. The method is a 2-step approach which begins by a light pre-training phase based on a cloze-stype self-retrieving task. Then the actual probing is performed using MedLAMA. Contrastive-Probe do not rely on the MLM head of the PLM being probed. The authors start by encoding the prompt and the available entities (extracted from UMLS). Then the 10 closest entity representations are selected with a Nearest Neighbor Search. One of the advantage of this approach is the possibility to easily include answers with multiple tokens.\nThe paper apply MedLAMA using Contrastive-Prove and other approaches on different PLMs (general and biomedical domain) and show that their approach allow to better measure the biomedical knowledge included in the models. ",
    "strengths": "The paper is building on previous work in the relatively new domain of PLM knowledge probing (most citations are after 2020). The authors manage to take some distance with the domain and present a interesting overview of the current literature. They build upon this literature and present a new benchmark + a new method for knowledge probing.\nPrincipal strengths of the paper: 1. The paper present an in-depth state-of-the-art related to PLM knowledge probing. \n2. There is an extensive comparison to the literature. \n3. There is an interesting and extensive analysis of the proposed approach. \n4. The authors investigates how much knowledge is stored in each Transformer layer. \n5. The paper include robustness tests (sensitivity to the random seed). ",
    "weaknesses": "1. According to table 8, the authors performed 500 training steps during the rewire phase. Why is this hyper-parameter not optimized? It seems by looking at Figure 4 and 9, that the best performance append before reaching this point. ",
    "comments": "None ",
    "overall_score": "4.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  },
  {
    "review_id": "2065b40112433f7a",
    "paper_id": "ARR_2022_23",
    "reviewer": null,
    "paper_summary": "The paper 'Rewire-then-Probe: A Contrastive Recipe for Probing Biomedical Knowledge of Pre-trained Language Models' studies the biomedical knowledge contained in PLMs with factual probing techniques. To this end, the authors propose a new biomedical probing suite derived from UMLS and evaluate different PLMs with multiple probing techniques on it. After discovering that these probing techniques lead to very low accuracy values, the authors design a novel 'rewiring' pretraining objective that boosts accuracy scores for retrieval-based probing. ",
    "strengths": "- Factual probing is an important technique to study the factual knowledge internalized by PLMs and it is underresearched in the biomedical domain.\n- The authors provide a novel large factual probing dataset for the biomedical domain that contains manually verified triples.\n- The proposed 'rewiring' pretraining objective improves probing accuracy by a large margin without resorting to supervised training.\n- The paper is well written and code and data are available ",
    "weaknesses": "I only have minor concerns with this paper: - The authors investigate only one prompt per relation which may be suboptimal - Supervised prompting methods were excluded from the evaluation, but they might provide interesting points of comparison ",
    "comments": "None ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]