[
  {
    "review_id": "ece4ceaf28899de3",
    "paper_id": "ARR_2022_180",
    "reviewer": null,
    "paper_summary": "This paper proposes a domain knowledge transfer framework, that distills an existing domain-specific model into another general domain student PLM, which significantly improves the students' performance in domain tasks, and in some cases even outperforms the teacher model.\nIt introduces two effective distillation techniques, calibrated teacher training and activation boundary distillation, both proven beneficial for the results. ",
    "strengths": "1.Enabling new PLMs with domain-specific knowledge and capability is of great importance, and the DoKTra framework introduces a novel perspective(to my knowledge), which instead of continuing pretraining on the new PLM, distill an existing domain PLM into the new PLM, saving a lot of computational costs to achieve the expected performance.\n2.DoKTra seems to have the potential of combining the strength of both new PLM and old domain PLM, it not only significantly surpass the general domain student, but also outperforms the domain student for several tasks and on average. The results are very promising.\n3.The implementation of two distillation strategies is proving effective by ablation experiments. ",
    "weaknesses": "1.DoKTra is upper bounded by a full further-pretraining on domain corpus using new PLM, i.e., pretrain a Bio-ALBERT-xlarge. which I assume is an inevitable trade-off for saving the pretraining cost.\n2.The framework should be tested on more diverse domains.\n3.DoKTra should be compared to TAPT[1], which also do not require a full further pretraining, and also provide domain adaptation capability.\n[1]Donâ€™t Stop Pretraining: Adapt Language Models to Domains and Tasks ",
    "comments": "See weaknesses 2 and 3. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work."
  },
  {
    "review_id": "fd753407dbcfb49e",
    "paper_id": "ARR_2022_180",
    "reviewer": null,
    "paper_summary": "The paper \"Domain Knowledge Transferring for Pre-trained Language Model via Calibrated Activation Boundary Distillation\" proposes to transfer the benefits of advances in general domain LMs, such as fewer parameters or more accurate predictions, to domain-specific LMs via knowledge distillation. For this, a general-domain student model is trained to mimic the neuron activation patterns of a calibrated domain-specific teacher model. The authors show that that this domain adapatation scheme transfers the benefits of the general-domain student models: fewer parameters with same performance for ALBERT and improved performance over BERT for RoBERTa. ",
    "strengths": "Strengths: - The problem of adapting advanced general-domain PLMs to specific domains without further LM-style pretraining is an important topic, because if successful it would allow researchers in specialized domains without access to large GPU clusters to reap the benefits of advances in LM research.\n- The proposed solution of using knowledge distillation is sound and the results are promising, as they suggest that advanced PLMs can be domain adapted while retaining their improvements over BERT.\n- Code is available ",
    "weaknesses": "Weaknesses: I have two major conerns with the work: - The first is whether the proposed method generalizes to other contexts. The experiments are limited to one domain (biomedical), one task (text classification) and a single teacher model (BioBERT) and it is not clear what happens if any of these parameters are changed. It would be especially important to report results on NER datasets as this is a standard benchmark for biomedical PLMs.    - The second concern is whether the reported experiments actually allow to conclude that the proposed method actually works as intended. Couldn't there be other reasons for the reported improvements of the domain-adapted models over the finetuned ones (-ft vs DoKTra in Table 2)? For instance, it would seem important to compare to using a general-domain teacher to make sure the improvements are actually due to transferred domain knowledge and not because of other factors related to distillation. Relatedly, what would happen when using an ALBERT-sized BERT model as student? This would be important to confirm that the improvements are actually because architectural improvements of the student were utilized. Also, it is mentioned that the student model is also finetuned with a calibration regularizer. It would be important to include ALEBRT-ft + calibration regularizer in the ablation study to make sure that the observed improvements do not stem from the calibration regularizer.\nAdditionally, I have also one minor comment. A comparison to domain-specific PLMs adapted with continued pretraining would be informative, as it would allow researchers to determine how much performance could be gained if they invested the resources to do that. For this, e.g. the RoBERTa model of [1] could be used. More generally, a comparison with state of the art on the used datasets is missing.\n[1]: Gururangan, Suchin, et al. \"Don't stop pretraining: adapt language models to domains and tasks.\" ",
    "comments": "Is final classification performed after linear layer that equalizes dimensions or before? ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]