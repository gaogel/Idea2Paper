[
  {
    "review_id": "21382763e074c203",
    "paper_id": "ARR_2022_73",
    "reviewer": null,
    "paper_summary": "The paper presents NAKDIMON, a character-LSTM Hebrew diacritizer that does not use a morphological analyzer/dictionary.  It compares its performance to SOTA on the task; and presents a new test set for the task that is more diverse that previous sets. ",
    "strengths": "The paper is well written and clear. The work is well motivated and enough related work is presented.  The error analysis is clear and helpful. This is a good short paper with a negative result. ",
    "weaknesses": "The paper presents inconclusive negative results: it is unclear what the results would look like had millions of additional words that are *automatically diacritized* using Dicta are added in Nakdimon's training. The authors added 1.3M such words. I think an additional experiment that uses more words (automatically diacritized) or just generated word forms from a morphological dictionary can make the result more convincing (negative or positive it may be). ",
    "comments": "- it would help to report OOV rates; and performance on in-vocabulary vs OOV for your system.\n- the discussion of Arabic use of diacritization is not accurate. Arabic \"dots\" are not optional in common use of Arabic; diacritical marks (vowels, nunation, gemination) are.  Check out  https://aclanthology.org/N07-2014.pdf https://aclanthology.org/2007.mtsummit-papers.20.pdf - I am puzzled by a difference between the Dicta test set and the new test set: the difference between CHA and WOR for the Dicta test set is much bigger that the respective difference in the new test set (between 1.4 and 2.4 times bigger).  Any thoughts on that? ",
    "overall_score": "2 = Revisions Needed: This paper has some merit, but also significant flaws, and needs work before it would be of interest to the community.",
    "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work."
  }
]