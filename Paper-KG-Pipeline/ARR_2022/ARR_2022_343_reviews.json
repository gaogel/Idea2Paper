[
  {
    "review_id": "9b00457f4a1c03fc",
    "paper_id": "ARR_2022_343",
    "reviewer": null,
    "paper_summary": "The paper discusses the use of character-level machine translation: although recent work on this direction shows advantages over subword-level models, character-level models are rarely used in practice in the WMT shared tasks, and the authors attribute it to higher higher computational costs. The paper presents and extensive literature survey and an empirical evaluation of three character-level architectures, two of which have not been used in MT before. Experimental findings include: character-level models are less susceptible to the beam search curse; character-level models are more robust to source noise but generally underperform the subword models; sampling-based decoding methods produce poor quality outputs with character-level models. The paper also presents a two-step decoding method which is comparable to subword-level decoders in terms of efficiency, but reports poor translation quality. ",
    "strengths": "The paper is very well-written and accessible to the NLP audience beyond the MT community. It raises an important question of the practical advantages and disadvantages of character-level approaches in MT, and presents a thorough literature survey, analysis of recent shared task submissions, and extensive empirical evaluation. My expertise in MT is limited, but the methodology looks well-thought-out to me and the experimental setup seems robust. ",
    "weaknesses": "1. I would have liked to see more discussion and empirical analysis of how typological differences between languages affect the relative performance of subword- and character-level models. The claims of the paper are quite broad, but there is not enough evidence to convince me that the conclusions hold generally across languages. More specifically:    * I assume that character-level MT would be more beneficial for languages with logographic writing systems (e.g. Chinese) than the ones that use alphabets. My understanding is that the WMT shared task language pairs are skewed towards European languages: if that is the case, I think the shared task analysis could have considered more venues, e.g. CWMT, The Workshop on Asian Translation, or IWSLT 2020 Open Domain Translation task (ja-zh, 20M training sentences). In any case, I think the paper should have listed the WMT languages and discussed if the use/mention of character-level methods varies by language, script, or morphological patterns.\n    * The empirical evaluation on the WMT dataset is also only performed on European languages (English, German, and Czech). In the smaller experiments, which were used for model selection, character-level models outperform subword-level ones on English-Arabic translation; this aligns with the findings of Shaham and Levy (2020), who attribute it to non-concatenative morphology of Semitic languages. So I am left wondering if the conclusions would have been different if the larger-data evaluation included languages with non-concatenative morphologies.\n2. This might be beyond the scope of the paper, but I am also wondering if the same conclusions extend to unsupervised and semi-supervised cases (for details, see the following section). ",
    "comments": "- I am wondering if the conclusions about the performance gap between character-level and subword-level models apply to unsupervised MT. For example, character-level FSTs for unsupervised translation between closely related languages (e.g. Serbian and Bosnian) perform very well [1]. An unsupervised subword-level neural MT model for the same task tested demonstrates very poor results [2], but training the same model with character-level tokenization yields a great improvement [3].\n- I think that the process of selecting the best-performing model in a smaller experiment and then evaluating it extensively is reasonable, but I am wondering if evaluating the two rejected models, CANINE and Charformer, in terms of robustness could have given other interesting insights. I understand that evaluating all types of models on the larger dataset might not be feasible, but maybe the robustness testing could somehow be included in the smaller-data experiments too?\n- I might be missing something, but I could not figure out if the results for the proposed two-step decoding method are reported anywhere in the paper. Are the results in Table 1 with decoder downsampling obtained with or without the two-step method? If without, then how much worse does the two-step method perform?\n- Table 2 is not colorblind-friendly (red-green is the most common type of colorblindness). I would suggest changing the colors and/or specifying in the caption that the top number in each cell corresponds to chrF and the bottom one to COMET.\n- In Table 3, en-cs BPE to character results are missing the recall on lemmas/forms seen in training.\n[1] Pourdamghani, Nima, and Kevin Knight. \" Deciphering related languages.\" EMNLP 2017.\n[2] He, Junxian, et al. \"A Probabilistic Formulation of Unsupervised Text Style Transfer.\" ICLR 2019.\n[3] Ryskina, Maria, et al. \"Comparative Error Analysis in Neural and Finite-state Models for Unsupervised Character-level Transduction.\" SIGMORPHON 2021. ",
    "overall_score": "3.5 ",
    "confidence": "2 =  Willing to defend my evaluation, but it is fairly likely that I missed some details, didn't understand some central points, or can't be sure about the novelty of the work."
  },
  {
    "review_id": "f3961de5b71adfa5",
    "paper_id": "ARR_2022_343",
    "reviewer": null,
    "paper_summary": "This paper is about character-level machine translation. The first part is a survey of 1) existing methods to enhance character-level translaiton and 2) methods used for WMT submissions in the past. The second part are experiments on character-level MT that compare different ways of encoding and decoding sequences of characters. ",
    "strengths": "1) Comprehensive literature review: - I believe that the survey of existing literature is very comprehensive and covers most important works in this area. This survey in itself is valuable for the MT research community.\n2) Experiments have reasonable methodology: - In my opinion, the techniques that are compared (e.g. Lee-style, Canine, etc.) are adequately chosen.\n- The evaluation procedures (choice of metrics, several training runs to report variance, confidence intervals) are also adequate.\n- I think that the large systems trained on WMT data for EN-CS and EN-DE are representative of the capabilities of current systems.\n3) The findings are novel and actionable: - The paper does indeed provide novel evidence for why WMT submissions rarely have character-level systems.\n- While previous works have claimed that character-level MT has higher domain robustness, this paper shows that this is not the case, and the findings are backed up by more reliable data.\n- A further general conclusion is that most works on char-level MT focused on enhancing encoding, while decoding is under-explored. I think that is a reasonable and straightforward suggestion for future work. ",
    "weaknesses": "1) Questionable usefulness of experiments on small datasets - As the paper itself states in the beginning, a possible weakness of earlier works is that their experiments were conducted on small datasets. In such cases it is unclear whether conclusions also apply to current MT systems trained on large datasets.\n- This criticism also applies to this paper under review, since many experiments are conducted using IWSLT data. I would like the paper to at least acknowledge this weakness.\n- It is questionable whether the outcome of the IWSLT experiments can be used to sub-select experimental conditions to try on the larger WMT data sets.\n2) The paper is too dismissive of MBR decoding, without much evidence - The paper implies that MBR is always worse than other decoding methods and that \"beam search with length normalization is the best decoding algorithm\".\n- I would change the wording to be more careful about describing the MBR results. Sampling-based MBR decoding is very new in MT and has a lot of potential to be more optimized in the future. It is simply not well optimized yet. For instance, very recent published works such as https://arxiv.org/abs/2111.09388 show that MBR improves considerably if a learned metric like COMET is used as the utility function for MBR.\n- I also take issue with this sentence: \"Sampling from character-level models leads to very poor translation quality that in turn also influences the MBR decoding that leads to much worse results than beam search.\" I believe that the quality of sampling is not necessarily indicative of the ability of MBR to pick a good translation from a pool of samples. ",
    "comments": "1) Suggestion for the general literature review: - The only kind of work I would add are proposals to learn an ideal segmentation (instead of fixing the segmentation before starting the training). One paper I think would make sense is: https://www.cl.uni-heidelberg.de/~sokolov/pubs/kreutzer18learning.pdf (also has character-level MT in the title).\n2) Questions: - How many samples are the \"sample\" results in Table 2 based on? I believe this could be a point estimate of just 1 random sample. It is expected that the quality of one sample will be low, but the variance will also be very high. It would be better to show the mean and standard deviation of many samples, or at least clarify how exactly this result is produced.\n3) Figures and presentation: - I believe that the color scheme in tables is confusing at times. In Table 1, I think it is confusing that a deeper shade of red means better results. In Table 2 it is non-intuitive that the first row is CHRF and the second row is COMET - and the table is quite hard to read.\n4) Typos: - \"and thus relatively frequent occurrence of out-of-vocabulary tokens.\" - a word is missing here - \"The model shrinks character sequences into less hidden states\" -> \"fewer hidden states\" - \"does not apply non-linearity\" -> \"does not apply a non-linearity\" ",
    "overall_score": "4.5 ",
    "confidence": "5 = Positive that my evaluation is correct. I read the paper very carefully and am familiar with related work."
  }
]