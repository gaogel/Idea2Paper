[
  {
    "review_id": "30c560d953cfea79",
    "paper_id": "ARR_2022_128",
    "reviewer": "Kai Zhao",
    "paper_summary": "This paper extends the worst-case-aware curriculum learning work (Zhang et al., 2020) to zero-shot dependency parsing. The key idea is to control the sampling to sample more from languages with higher losses. Empirical analyses show that the proposed method improves zero-shot accuracy on unseen languages, outperforming uniform sampling or sampling proportionally to the size of the treebanks. ",
    "strengths": "- Zero-shot dependency parsing is a difficult but valuable task in both academia and industry.\n- The proposed method is intuitive.\n- The empirical analyses were conducted on a large set of languages, which makes the results more convincible. ",
    "weaknesses": "- Based on the empirical analyses (Table 2), it is not clear if pure worst-case-aware sampling ($\\phi=0$) is consistently better than pure loss-proportional sampling ($\\phi=1$). It seems the results on mBERT and XLM-R give different conclusions. The relatively small differences between $\\phi=0$ and $\\phi=1$ in the mBERT and XLM-R settings also make me wonder if the differences are statistically significant.\n- The proposed work is an extension of previous Zhang et al., 2020. So the contribution seems a bit incremental. ",
    "comments": "- Line 183: it would be good to show what languages are used in the PLM pretraining and if there is a difference of the proposed method between the seen and unseen languages.\n- Table 2: there are only three values of $\\phi$ listed, and there are no difference between $\\phi=0.5$ and $\\phi=1$. It would be good to show more details on how $\\phi$ affects the final performance.\n- Table 3: which PLM is used in this experiment? What is the $\\phi$? ",
    "overall_score": "2.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "c6c39076e1b552f9",
    "paper_id": "ARR_2022_128",
    "reviewer": null,
    "paper_summary": "The paper applies the worst-case aware curriculum learning algorithm to zero-shot cross-lingual transfer for dependency parsing. The main contribution is the empirical study on cross-lingual dependency parsing, which shows that this method improves accuracy in the zero-shot setting, compared to sampling data uniformly across treebanks from different languages, or proportionally to the size of the treebanks. ",
    "strengths": "1. The method description is very clear 2. I appreciate the analysis with varying homogeneity of training samples. Although the results are conflicting, it's nice to check how the selection of training languages impacts the effectiveness of this method. ",
    "weaknesses": "1. No significance test: The paper claims that applying this curriculum learning method improves the zero-shot performance over the baselines by 0.4-1.2 LAS score. However, are the improvements significant? There's no significance test mentioned in the paper. \n2. The results in the analysis with varying homogeneity of training samples (Section 4) are conflicting: they don't support the hypothesis that this method improves more when some language types are underrepresented in the training samples. Therefore, it's unclear why this method improves over baseline methods, i.e. does it help the model learn from skewed training samples or is it something else? \n3. I'd expect a discussion of how this method relates to and differs from existing methods on zero-shot cross-lingua transfer. For example, meta-learning methods optimize the initial parameters [1] or optimization algorithms [2] during fine-tuning toward better performance on outlier/unseen languages.\n[1] Zero-Shot Cross-Lingual Transfer with Meta Learning. Farhad Nooralahzadeh, Giannis Bekoulis, Johannes Bjerva, Isabelle Augenstein. 2020.\n[2] Soft Layer Selection with Meta-Learning for Zero-Shot Cross-Lingual Transfer. Weijia Xu, Batool Haider, Jason Krone, Saab Mansour. 2021. ",
    "comments": "For the analysis with varying homogeneity of training samples, would it help to look at performance on the romance and the outlier languages separately? Also, it might also be helpful to consider the typological distance between different language genera. ",
    "overall_score": "2.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "fbced420613c2621",
    "paper_id": "ARR_2022_128",
    "reviewer": null,
    "paper_summary": "This paper adopts a multi-task learning method from (Zhang et al 2020) in the zero-shot multilingual dependency parsing scenario. The method aims to use curriculum learning to optimize the worst-case-aware loss rather than averaged loss in common multi-task learning settings.  The experimental results show that this method outperforms general sampling method baselines in zero-shot learning and achieves state-of-the-art when using mBERT and XLM-R. They also explore the effect of varying the homogeneity of training languages by comparing models trained on homogeneous languages (e.g. GERMANIC, ROMANCE, SLAVIC) and skewed samples (e.g. ROMANCE+EU/AR/TR/ZH). Their results performs consistently better than baseline on all training samples but there is no clear pattern to show in which case this method works best and why. ",
    "strengths": "This paper shows that the worst-case-aware loss optimization is helpful for zero-shot multilingual dependency parsing. Although the method is model-agnostic and simple, it achieves the state-of-the-art without any external resource, which is not always available for low-resource languages. It would be interesting to also explore the effect of this method in other multilingual tasks.  The paper empirically investigates how the method performs with different training languages settings and gives an interesting analysis. ",
    "weaknesses": "Although the method performs consistently well with different training languages, there is less discussion on why and how this method helps for the test languages. For example, it is not clear why both the highest and lowest gain come from the skewed samples group (e.g. ROM+EU and ROM+AR). ",
    "comments": "1. There is less evidence on how the method helps target languages, so it would be interesting to do some qualitative analysis to see how this method helps a specific language compared with the baseline sampling method. For example, it may be possible to compare the baseline with the proposed model trained on ROM+EU to see where the benefit comes from.\n2. The paper mainly experiments with the biaffine dependency parser. It will also be interesting to experiments with other recently proposed parsers to see if the method is consistently better or not.  3. The paper does not refer to the table 3 when discussing the results in section 4. ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]