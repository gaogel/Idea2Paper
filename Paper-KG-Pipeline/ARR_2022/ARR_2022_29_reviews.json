[
  {
    "review_id": "0483e42a23383462",
    "paper_id": "ARR_2022_29",
    "reviewer": "Shuyang Cao",
    "paper_summary": "This paper studies efficiently processes long input sequences for conditional language generation with Transformer. In addition to local attentions where each token attends to its neighbors within a limited distance, their proposed efficient efficient attention mechanism dynamically constructs global tokens by aggregating representations for input tokens that are divided into even blocks (Transient Global Attention). The global tokens can be attended by all input tokens and thus enable long-range interaction. They adopt the design of T5 for other components of the Transformer and pre-train the model using the gap sentence prediction objective by Pegasus. The resulting model, LongT5, outperforms existing models designed for long sequences on long document summarization with arXiv, PubMed, BigPatent, and MediaSum. On NaturalQuestion and TriviaQA, their largest model also outperforms existing models. Their additional experiments show that increasing the input length and model size usually lead to improved performance. ",
    "strengths": "- Their pre-trained models are helpful for the research community.\n- Their proposed efficient Transformer model (LongT5) performs better than the model with the original attention mechanism (T5) across different input lengths. It also achieves better performance on different datasets than results reported by existing work. ",
    "weaknesses": "While I appreciate their effort in building pre-trained models for long sequences, I do have the following concerns: - The results by other models (Table 2, Table 4 bottom) are taken from the original papers whose experiments might not be conducted under the same computational limit (i.e., GPU/TPU memory). It is likely that the improved performance is due to the authors' better computational resource that allows longer inputs. Also, it is unclear if techniques that can reduce memory usage (e.g., gradient checkpointing) are used by the authors in this paper to reach longer inputs.\n- In Table 4 and Figure 2, due to the lack of T5 models pre-trained with the Gap Sentence Prediction objective, It is unclear the better performance by LongT5 is due to the Transient Global Attention or the Gap Sentence Prediction pre-training objective.\n- There lacks some ablation studies for the Transient Global Attention (e.g., is it necessary to use all global tokens?). ",
    "comments": "- Would be better if the input lengths used by different comparisons are listed. ",
    "overall_score": "3 = Good: This paper makes a reasonable contribution, and might be of interest for some (broad or narrow) sub-communities, possibly with minor revisions.",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]