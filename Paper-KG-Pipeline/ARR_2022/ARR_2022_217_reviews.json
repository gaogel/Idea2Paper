[
  {
    "review_id": "e6004c2ee71daf90",
    "paper_id": "ARR_2022_217",
    "reviewer": null,
    "paper_summary": "XDBERT is a cross-model model for NLU that is distilled from CLIP-T, the vision-grounded text encoder of CLIP. The resulted model outperforms its teachers on GLUE and other tasks. ",
    "strengths": "- The idea behind the paper is intuitive and straightforward.\n- The results are a little surprising to me and they look good.\n- Experimental details are provided in the appendix, which is good for reproducibility.\n- The authors only use Wiki103 as the distillation corpus, which is easy to reproduce and desirable for computational budgets. ",
    "weaknesses": "### Original comments from previous review - The main concern is the technical novelty of this paper. There is little novelty since the distillation technique is not new while the idea of cross-model training is not new, either.\n- I'm surprised that in Table 1 CLIP-T performs so badly even when finetuned. Can you explain why?\n- Is it possible that the improvement of the distilled cross-modal model comes from ensembling instead of visual grounding?\n- I suggest the authors make the weights publically available (can be anonymous during double-blind reviewing) so the results can be easily verified by the community.\nAlthough some details are vague and need further investigation, I think the contribution is enough to be accepted as a short paper.\n### After rebuttal Thanks for resubmitting the paper with a response. I've carefully read the response.\n- **Re. To Reviewer2.1**: First, distilling B task into A task improves A is not so surprising. This idea has been explored in Intermediate-Task Transfer Learning. I agree that distilling cross-modal model to a single-modal model is somewhat novel. However, even self-distillation can improve accuracy so I have no idea to what extent is cross-modal distillation helpful.\n- **Re. To Reviewer2.2**: Good answer.\n- **Re. To Reviewer2.3**: Sorry for the confusion. Yes, when fine-tuning there is only the BERT encoder used. However, I suspect that substituting CLIP with any pretrained LM would also work. But again, as the authors point out, \"the linguistic competence is CLIP-T very low\" so this paper has some insights and presents previously unknown results. That's why I gave this paper a score of 3.5.\n- **Re. To Reviewer2.4**: That should be easy. Just manually reassign the weights in the original `state_dict` to a `state_dict` of a Hugging Face BERT model. That would work.\nThus, I keep my recommendation of weak acceptance for this paper and I suggest other reviewers consider increasing their scores. ",
    "comments": "N/A ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]