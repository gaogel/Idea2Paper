[
  {
    "review_id": "ef0069c46d65c881",
    "paper_id": "ARR_2022_312",
    "reviewer": null,
    "paper_summary": "This paper introduces coherence boosting, in which a weight is used at inference time to mix the log-probabilities generated by a language model on long- and short-term contexts.  The approach is evaluated on a large number of NLP tasks/datasets, with convincing results. ",
    "strengths": "- The idea is elegant in it’s simplicity, yet impactful.  This is reminiscent of “Improving Neural Language Models with a Continuous Cache” (https://arxiv.org/abs/1612.04426) which is also applied at inference time. \n\t* The paper is very well written and easy to follow. \n\t* Illustrations clearly demonstrate the problem and solution. \n\t* The mathematical development of the model is concise and easy to follow -- enough detail is provide for re-implementation. \n\t* State-of-the-art results are achieved on the LAMBADA dataset, and the approach is additive on a number of other tasks. \n\t* A number of novel metrics are introduced. ",
    "weaknesses": "- There approach may have limitations w/r/t to model scale (e.g., \\alpha approaching zero in Figure 2) -- although the approach is still additive on GPT-3 (one of the largest language models available) in Table 1. \n\t* It is not clear that results are durable w/r/t to non-vanilla models (e.g., results in Table 4 are far short of SOTA on these tasks) -- is coherence boosting still additive for more sophisticated approaches. ",
    "comments": "I liked this paper and believe that the NLP community would benefit from knowing about this simple, yet impactful approach.  It would be great to see if context boosting is still impactful versus more sophisticated approaches.  Two questions: 1.  Should Section 1.1 be Section 2.0 instead? \n2.  How does Table 3 in this paper relate to Tables 2 of Zhang 2020b (re: DigloGPT,Beam 345 seems to perform better)? ",
    "overall_score": "4 = Strong: This paper is of significant interest (for broad or narrow sub-communities), and warrants acceptance in a top-tier *ACL venue if space allows.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]