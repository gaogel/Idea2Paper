[
  {
    "review_id": "55ef0fca438c05eb",
    "paper_id": "ARR_2022_313",
    "reviewer": null,
    "paper_summary": "This paper proposes a prompt-based data augmentation pipeline, PromDA, targeting to improve model performance for low-resource NLU with high quality synthetic data. PromDA  adds soft prompt to public pretrained language models for data generation and further apply consistency filtering to formulate a iterative model improvement pipeline. From the experiment results, PromDA shows great performance boosting to the baseline model and competitive performance compared to other few-shot NLU methods.  Overall, this paper is well-written and contains comprehensive experiment results and analysis. However, it lacks some details/discussions on the pipeline settings: 1. PromDA uses T5-Large for data augmentation and BERT-base for NLU model. It would be interesting to include the comparison of using different pretrained langauge model for data augmentation to understand the benefits of model size in this pipeline. Especially, except LAMBADA and BACK T, all other previous methods seem not using model larger than BERT-base in their pipelines. If PromDA could consistently outperform the baseline and other models with smaller data augmentation model (e.g. BERT-base), it will further justify the effectiveness of proposed pipeline.\n2. The iterative consistency filtering might propagate errors in the synthetic data especially with high number of iterations. Is there any mechanism used to reduce such error propagation? And more analysis and insights to the optimal number of iterations would be helpful. ",
    "strengths": "1. Interesting intuition of applying prompt-based language model for data augmentation. \n2. Comprehensive experiment results and analysis. ",
    "weaknesses": "1. Lack of some key comparisons for the pipeline settings. ",
    "comments": "My major suggestion is to have some analysis on the effect of model size for data augmentation. ",
    "overall_score": "3.5 ",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  }
]