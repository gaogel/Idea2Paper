[
  {
    "review_id": "be568f104f0e1b63",
    "paper_id": "ARR_2022_132",
    "reviewer": null,
    "paper_summary": "This paper focuses on generating word definitions written in plain and familiar words for language learners. \nThe task of simple definition generation proposed in this paper is challenging due to the lack of training data unlike ordinary definition generation. \nThe proposed method tackles this problem by combining two decoders that share their parameters---the one is for generating a definition for a given word and the other is for generating a simple sentence. \nThe experimental results show that the proposed method outperforms pipeline approaches and the effectiveness of the parameter sharing scheme. ",
    "strengths": "1) This paper proposes a novel challeging task of simple definition generation.\n2) The proposed style transfer approach using parameter sharing outperforms strong pipeline approaches. ",
    "weaknesses": "3) The explanation for baselines lacks the details of training settings, resulting in difficulty in reproducing. \nHow to train the LOG-CaD and MASS models for complex definitions and the ACCESS and MUSS models for simple definitions? \nWhich datasets did the authors use for those models?\n4) It is hard to say that the performance improvements by the proposed method are significant. \nFrom the ablation study, SimpDefiner trained without LM and TR must be identical to the baseline MASS model. \nHowever, the baseline MASS model achieves 24.00 BLEU score on Complex whereas SimpDefiner trained only with definition generation achieves 25.02. \nThe authors should explain this difference. \nFurthermore, the output of the baseline MASS model should be evaluated on Simple without applying ACCESS or MUSS. \nIf the baseline MASS model achieves 25.02 BLEU score on Complex, it would also achieve 13.66 BLEU score on Simple as shown in Table 6, which is higher than the performances of pipelines on Table 3. ",
    "comments": "5) The explanation for the inference should be described in Section 3, although Section 1 implies that SimpDefiner produces not only a simple definition but also a complex one. \nAlso, Section 5.1 should indicate that complex definitions from Gen. Decoder are evaluated on Complex (from the OD test set) and simple definitions from Rec. Decoder are evaluated on Simple (from the OALD test set).\n6) The expression of \"fully unsupervised\" in Section 1 is a bit misleading, because an alternative standard dictionary containing complex definitions can be used for training.\n7) Conducting multiple runs with different seeds is preferable. ",
    "overall_score": "3 = Good: This paper is of interest to the *ACL audience and could be published, but might not be appropriate for a top-tier publication venue. It would likely be a strong paper in a suitable workshop.",
    "confidence": "3 =  Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math or experimental design."
  },
  {
    "review_id": "e5cf4b80a9f86f60",
    "paper_id": "ARR_2022_132",
    "reviewer": null,
    "paper_summary": "This paper extends the definition generation task to generate simple definitions instead of complex ones. The authors propose a multitasking framework that consists of complex definition generation, text reconstruction and language modeling tasks to indirectly learn the probability of a simple definition, given the word and its context. The authors experiment on English and Chinese datasets and use a set of measures such as BLEU, SemanticSimilarity, SARI and HSK level (for Chinese); and also compare to the baseline pipelines that consists of definition generation and a simplifier. They report improved scores for all cases. ",
    "strengths": "The paper is generally well-written and the task is well motivated. The parameter-sharing scheme is interesting and could be further investigated to disentangle other factors from text. The experimental design is well-thought, i.e., contains suitable baselines and measures, as well as manual evaluation (in some cases). ",
    "weaknesses": "- The proposed method is not explained with adequate details (more below).\n- The results (Table 3, 4) seem to be from single runs, hence are not trustworthy. They should be given as the average of multiple runs with different random seeds. ",
    "comments": "- 006-007: The sentence sounds weird. Maybe delete the \"better\" at the end - 017: You can hint at which components you refer to - 017: jointly - 060: how do you define a difficult word?\n- 091-93: it just repeats the same sentence with no clarification. then you jump into the tiny details.   - 131: capable of learning - 135: space before ( - 152-155: more suitable then what? e.g., isn't T5 also suitable?\n- 188-191: why would you need to generate both of them? isn't simplification the point of this task?\n- 225: in -> of - 250-252: what is the hypothesis? why do you think sharing decoders among two tasks would help? the motivation is missing - 267: focusd -> focused - 285-287: repeating the same sentence - 304: Don't you only have the corrupted text as the input for the reconstruction (also as shown in Fig 2)? But this equation tells you do language modeling on the original text?   - In general for Sec 3.2: you mention that only some parameters are shared, but then you use the same \\theta as the model parameters for each sub task. This is confusing. Please separate the parameters for clarity.  - 317: dived -> divided - 340: I'm very confused how the complexity \"c\". what's the range of c? how is it defined? is it learned during training without supervision and how? how do you control for it?\n- 350: what exactly is a query? the output vector from the cross-attention?\n- 433: The model is heavily based on MASS, but it is not described properly.   - 440-441: Why don't you tune your lambda parameters on the dev test for fair comparison? Please also add how you initialize the parameters that are not part of MASS, e.g., the ones in Eq. 8.\n- 547: sharing 571: extra space before comma ",
    "overall_score": "3.5 ",
    "confidence": "4 = Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings."
  }
]